[{"authors":["admin"],"categories":null,"content":"After doing a research master's, I became a PhD student and graduate teaching assistant in Psychology at Lancaster University, where I investigated how conceptual processing\u0026mdash;that is, the comprehension of the meaning of words\u0026mdash;is supported by linguistic and sensorimotor brain systems, and how research on this topic is influenced by methodological aspects such as the operationalisation of variables and the sample size of experiments. Currently, I am a postdoctoral fellow at UiT The Arctic University of Norway, where I am investigating the behavioural and neural underpinnings of multilingualism. Throughout my research, I have used methods such as behavioural and electroencephalographic experiments, corpus analysis, statistics and programming. The materials from my research are available at https://osf.io/25u3x. My CV is available here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://pablobernabeu.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"After doing a research master's, I became a PhD student and graduate teaching assistant in Psychology at Lancaster University, where I investigated how conceptual processing\u0026mdash;that is, the comprehension of the meaning of words\u0026mdash;is supported by linguistic and sensorimotor brain systems, and how research on this topic is influenced by methodological aspects such as the operationalisation of variables and the sample size of experiments. Currently, I am a postdoctoral fellow at UiT The Arctic University of Norway, where I am investigating the behavioural and neural underpinnings of multilingualism.","tags":null,"title":"Pablo Bernabeu","type":"authors"},{"authors":[],"categories":["research methods"],"content":"\rOpenSesame offers options to counterbalance properties of the stimulus across participants. However, in cases of more involved assignments of session parameters across participants, it becomes necessary to write a bit of Python code in an inline script, which should be placed at the top of the timeline. In such a script, the participant-specific parameters are loaded in from a csv file. Below is a minimal example of the csv file.\n\r\r\r\rparticipant\rlanguage\rtraining_list\rtest_list\rexperiment_list\r\r\r\r1\rMini-English\r1\r1\r1\r\r2\rMini-Norwegian\r1\r1\r1\r\r3\rMini-English\r2\r2\r1\r\r4\rMini-Norwegian\r2\r2\r1\r\r5\rMini-English\r1\r3\r2\r\r6\rMini-Norwegian\r1\r3\r2\r\r7\rMini-English\r2\r4\r2\r\r8\rMini-Norwegian\r2\r4\r2\r\r\r\rBelow is the corresponding inline script. The code .iloc[0] at the end of the lines is used to select a cell.\n\r# Assigning participant-specific parameters automatically in OpenSesame\rimport csv # handle csv file\rimport pandas as pd # handle data frames\rparticipant_parameters = pd.read_csv(exp.get_file(\u0026#39;stimuli/parameters per participant.csv\u0026#39;))\rvar.participant = var.subject_nr\rvar.language = participant_parameters.loc[participant_parameters[\u0026#39;participant\u0026#39;] == var.subject_nr][\u0026#39;language\u0026#39;].iloc[0]\rvar.training_list = participant_parameters.loc[participant_parameters[\u0026#39;participant\u0026#39;] == var.subject_nr][\u0026#39;training_list\u0026#39;].iloc[0]\rvar.test_list = participant_parameters.loc[participant_parameters[\u0026#39;participant\u0026#39;] == var.subject_nr][\u0026#39;test_list\u0026#39;].iloc[0]\rvar.experiment_list = participant_parameters.loc[participant_parameters[\u0026#39;participant\u0026#39;] == var.subject_nr][\u0026#39;experiment_list\u0026#39;].iloc[0]\r\r","date":1684281600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684281600,"objectID":"0a75f39305dfd092000bbe97b550af25","permalink":"https://pablobernabeu.github.io/2023/assigning-participant-specific-parameters-automatically-in-opensesame/","publishdate":"2023-05-17T00:00:00Z","relpermalink":"/2023/assigning-participant-specific-parameters-automatically-in-opensesame/","section":"post","summary":"OpenSesame offers options to counterbalance properties of the stimulus across participants. However, in cases of more involved assignments of session parameters across participants, it becomes necessary to write a bit of Python code in an inline script, which should be placed at the top of the timeline. In such a script, the participant-specific parameters are loaded in from a csv file. Below is a minimal example of the csv file.","tags":["research methods","s"],"title":"Assigning participant-specific parameters automatically in OpenSesame","type":"post"},{"authors":[],"categories":["linguistics"],"content":"\rMany Romance languages allow the movement of pronominal object clitics to the preverbal position (Hanson \u0026amp; Carlson, 2014; Labotka et al., 2023). That is, instead of saying La maestra lo ha detto ‘The teacher has said it’, it is possible to say Lo ha detto la maestra ‘It has said the teacher’. The latter is a marked phrasing that increases the attention to the subject of the sentence. While English syntax does not allow this movement of the clitic, the marked sentence could be translated as ‘The teacher has said it herself’ or “It’s the teacher that has said it”. How does Google Translate (GT) deal with this in May 2023? For this specific case, GT opts for ‘The teacher said’, which is a good, idiomatic option. When it comes to translating to Spanish, GT returns ‘El profesor dijo’, which is the direct equivalent of the English translation. This option is valid in some varieties of Spanish in America. Nonetheless, it must be noted that a more direct translation from the Italian form would have been very good.\nGT has a greater trouble when the content of the sentence is slightly more complex. For instance, Lo cerca la maestra ‘Him is seeking the teacher’ is stripped of its markedness in the translation to English—i.e., ‘The teacher is looking for him’. The alternative option, to preserve the focus on the subject—e.g., “It’s the teacher that’s looking for him”—would require some syntactic liberties, and hence entail some risks. So, playing it safe is understandable. Our next step is checking the translation to some Romance languages that allow the same movement to preverbal position that is present in the original Italian sentence, Lo cerca la maestra. The natural translations would be Îl caută profesorul (Romanian) or Lo busca la profesora (Spanish). In contrast, GT returns the equivalents of the English translation—i.e., Profesorul îl caută and El profesor lo busca.1 These options are problematic because the focus on the subject is lost—that is, unnecessarily lost.\nIn fairness, machine translation is an absolute feat overall, and it’s only going to improve with the growth of language models. So, how much of a piece of cake will it be for GT to crack some of these syntactic details, and to preserve syntactic forms when the systems of both languages are alike?\nReferences\rLabotka, D., Sabo, E., Bonais, R., Gelman, S. A., \u0026amp; Baptista, M. (2023). Testing the effects of congruence in adult multilingual acquisition with implications for creole genesis. Cognition, 235, 105387. https://doi-org.mime.uit.no/10.1016/j.cognition.2023.105387\nHanson, A. E. S., \u0026amp; Carlson, M. T. (2014). The roles of first language and proficiency in L2 processing of Spanish clitics: Global effects. Language Learning, 64(2), 310-342. https://doi-org.mime.uit.no/10.1111/lang.12050\n\r\rGT had an error of gender in the translation to El profesor.↩︎\n\r\r\r","date":1684281600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684281600,"objectID":"04ef742b99131ee067508761d7d2d365","permalink":"https://pablobernabeu.github.io/2023/pronominal-object-clitics-in-preverbal-position-are-a-hard-nut-to-crack-for-google-translate/","publishdate":"2023-05-17T00:00:00Z","relpermalink":"/2023/pronominal-object-clitics-in-preverbal-position-are-a-hard-nut-to-crack-for-google-translate/","section":"post","summary":"Many Romance languages allow the movement of pronominal object clitics to the preverbal position (Hanson \u0026amp; Carlson, 2014; Labotka et al., 2023). That is, instead of saying La maestra lo ha detto ‘The teacher has said it’, it is possible to say Lo ha detto la maestra ‘It has said the teacher’. The latter is a marked phrasing that increases the attention to the subject of the sentence. While English syntax does not allow this movement of the clitic, the marked sentence could be translated as ‘The teacher has said it herself’ or “It’s the teacher that has said it”.","tags":["linguistics","syntax","translation","s"],"title":"Pronominal object clitics in preverbal position are a hard nut to crack for Google Translate","type":"post"},{"authors":[],"categories":["research methods"],"content":"\rIn the preparation of projects, files are often downloaded from OSF. It is good to provide the URL addresses used for the downloads, and it’s even better to add the version of the file to the URL. This specification helps reduce the possibility of inaccuracies later, should the file be modified. The version can be specified by adding ?version=X to the download link. For instance, the seventh version is specified in the link https://osf.io/hx6tz/download?version=7.\n","date":1684195200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684195200,"objectID":"dc077344d34aa8f1b16260087144109d","permalink":"https://pablobernabeu.github.io/2023/specifying-version-number-in-osf-download-links/","publishdate":"2023-05-16T00:00:00Z","relpermalink":"/2023/specifying-version-number-in-osf-download-links/","section":"post","summary":"In the preparation of projects, files are often downloaded from OSF. It is good to provide the URL addresses used for the downloads, and it’s even better to add the version of the file to the URL. This specification helps reduce the possibility of inaccuracies later, should the file be modified. The version can be specified by adding ?version=X to the download link. For instance, the seventh version is specified in the link https://osf.","tags":["research methods","s"],"title":"Specifying version number in OSF download links","type":"post"},{"authors":[],"categories":["research methods","statistics"],"content":"\rThe need for covariates—or nuisance variables—in statistical analyses is twofold. The first reason is purely statistical and the second reason is academic.\nFirst, the use of covariates is often necessary when the variable(s) of interest in a study may be connected to, and affected by, some satellite variables (Bottini et al., 2022; Elze et al., 2017; Sassenhagen \u0026amp; Alday, 2016). This complex scenario is the most common one due to the multivariate, dynamic, interactive nature of the real world.\nSecond, the use of covariates is often necessary to prevent the development of bogus, redundant theories. Academics are strongly rewarded for developing theories and models. As we know, wherever there are strong rewards, there are serious risks. An academic could—consciously or not—produce a theory/model that is too closely related to an existing theory. So closely related are these theories that the second version hardly warrants a name of its own. In such a scenario, covariates are useful and indeed necessary to vet the unique nature of the second version. That is, the first and the second version must be tested in the same model, and the variables corresponding to the first version can be construed as covariates. Both the developers of the theories and the readers can compare the effects corresponding to each version to assess the degree of separation between them.\nThe perverted use of covariates (Stefan \u0026amp; Schönbrodt, 2023)—however frequent and harmful—stands completely orthogonal to the correct usage, in the same way that a stethoscope can be used for good or for bad purposes. It would be poorly informed and misleading to conflate the correct and the incorrect uses, or to reject the use of covariates altogether due to the incorrect uses.\nThe effects of interest in correlational/observational studies can be subject to mediation and moderation by satellite variables. These variables cannot be manipulated in correlational/observational studies, but they can—and often should—be included as covariates in analyses, to ward off spurious results and to vet similar theories.\nReferences\rBottini, R., Morucci, P., D’Urso, A., Collignon, O., \u0026amp; Crepaldi, D. (2022). The concreteness advantage in lexical decision does not depend on perceptual simulations. Journal of Experimental Psychology: General, 151(3), 731–738. https://doi.org/10.1037/xge0001090\nElze, M. C., Gregson, J., Baber, U., Williamson, E., Sartori, S., Mehran, R., Nichols, M., Stone, G. W., \u0026amp; Pocock, S. J. (2017). Comparison of propensity score methods and covariate adjustment: Evaluation in 4 cardiovascular studies. Journal of the American College of Cardiology, 69(3), 345-357.\nSassenhagen, J., \u0026amp; Alday, P. M. (2016). A common misapplication of statistical inference: Nuisance control with null-hypothesis significance tests. Brain and Language, 162, 42-45. https://doi.org/10.1016/j.bandl.2016.08.001\nStefan, A. M., \u0026amp; Schönbrodt, F. D. (2023). Big little lies: A compendium and simulation of p-hacking strategies. Royal Society Open Science, 10(2), 220346. https://doi.org/10.1098/rsos.220346\n\r\n\r","date":1684108800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684108800,"objectID":"3006b3b13da03672741cd712494ede7f","permalink":"https://pablobernabeu.github.io/2023/covariates-are-necessary-to-validate-the-variables-of-interest-and-to-prevent-bogus-theories/","publishdate":"2023-05-15T00:00:00Z","relpermalink":"/2023/covariates-are-necessary-to-validate-the-variables-of-interest-and-to-prevent-bogus-theories/","section":"post","summary":"The need for covariates—or nuisance variables—in statistical analyses is twofold. The first reason is purely statistical and the second reason is academic.\nFirst, the use of covariates is often necessary when the variable(s) of interest in a study may be connected to, and affected by, some satellite variables (Bottini et al., 2022; Elze et al., 2017; Sassenhagen \u0026amp; Alday, 2016). This complex scenario is the most common one due to the multivariate, dynamic, interactive nature of the real world.","tags":["research methods","statistics","conflation","s"],"title":"Covariates are necessary to validate the variables of interest and to prevent bogus theories","type":"post"},{"authors":[],"categories":["R","statistics"],"content":"\r\r\rwindow.xaringanExtraClipboard(null, {\"button\":\"Copy Code\",\"success\":\"Copied!\",\"error\":\"Press Ctrl+C to Copy\"})\r\rHere I share the format applied to tables presenting the results of Bayesian models in Bernabeu (2022; the table for frequentist models is covered in this other post). The sample table presents a Bayesian mixed-effects model that was fitted using the R package brms (Bürkner et al., 2022). The mixed effects were driven by the maximal principle (Brauer \u0026amp; Curtin, 2018). The format of the table resembles one of the examples published by the American Psychological Association. However, there are also deviations from those examples. For instance, in the present table, the effects are grouped under informative labels to facilitate the readers’ comprehension, using the kableExtra package (Zhu, 2022). Furthermore, the random slopes are specified using superscript letters and a footnote. The table can be reproduced using the materials at https://osf.io/gt5uf.\nLoading packages and the results of the models\rlibrary(dplyr)\rlibrary(knitr)\rlibrary(tibble)\rlibrary(stringr)\rlibrary(lmerTest)\rlibrary(kableExtra)\r# Load Bayesian results summary\rsemanticpriming_summary_weaklyinformativepriors_exgaussian = readRDS(gzcon(url(\u0026#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_summary_weaklyinformativepriors_exgaussian.rds?raw=true\u0026#39;)))\r\rAdjusting the names of the effects\rFirst, to facilitate the understanding of the results, the original names of the effects will be adjusted in the brms summary.\n# Rename effects in plain language and specify the random slopes\r# (if any) for each effect, in the footnote. For this purpose, # superscripts are added to the names of the appropriate effects.\r# # In the interactions below, word-level variables are presented # first for the sake of consistency (the order does not affect # the results in any way). Also in the interactions, double # colons are used to inform the \u0026#39;bayesian_model_table\u0026#39; # function that the two terms in the interaction must be split # into two lines.\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_attentional_control\u0026#39;] = \u0026#39;Attentional control\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_attentional_control\u0026#39;] = \u0026#39;Attentional control\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_vocabulary_size\u0026#39;] = \u0026#39;Vocabulary size \u0026lt;sup\u0026gt;a\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_recoded_participant_gender\u0026#39;] = \u0026#39;Gender \u0026lt;sup\u0026gt;a\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_target_word_frequency\u0026#39;] = \u0026#39;Word frequency\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_target_number_syllables\u0026#39;] = \u0026#39;Number of syllables\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_word_concreteness_diff\u0026#39;] = \u0026#39;Word-concreteness difference\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_cosine_similarity\u0026#39;] = \u0026#39;Language-based similarity \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_visual_rating_diff\u0026#39;] = \u0026#39;Visual-strength difference \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_recoded_interstimulus_interval\u0026#39;] = \u0026#39;Stimulus onset asynchrony (SOA) \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_word_concreteness_diff:z_vocabulary_size\u0026#39;] = \u0026#39;Word-concreteness difference :: Vocabulary size\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_word_concreteness_diff:z_recoded_interstimulus_interval\u0026#39;] = \u0026#39;Word-concreteness difference : SOA\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_word_concreteness_diff:z_recoded_participant_gender\u0026#39;] = \u0026#39;Word-concreteness difference : Gender\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_attentional_control:z_cosine_similarity\u0026#39;] = \u0026#39;Language-based similarity :: Attentional control\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_attentional_control:z_visual_rating_diff\u0026#39;] = \u0026#39;Visual-strength difference :: Attentional control\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_vocabulary_size:z_cosine_similarity\u0026#39;] = \u0026#39;Language-based similarity :: Vocabulary size\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_vocabulary_size:z_visual_rating_diff\u0026#39;] = \u0026#39;Visual-strength difference :: Vocabulary size\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_recoded_participant_gender:z_cosine_similarity\u0026#39;] = \u0026#39;Language-based similarity : Gender\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_recoded_participant_gender:z_visual_rating_diff\u0026#39;] = \u0026#39;Visual-strength difference : Gender\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_cosine_similarity:z_recoded_interstimulus_interval\u0026#39;] = \u0026#39;Language-based similarity : SOA \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[\rrownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == \u0026#39;z_visual_rating_diff:z_recoded_interstimulus_interval\u0026#39;] = \u0026#39;Visual-strength difference : SOA \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;\r\rbayesian_model_table()\rThis custom function was used in Bernabeu (2022), with a PDF output. In the current scenario, however, we have an HTML output. In the above function, the code used for the \\(\\widehat{R}\\) tailored to the HTML output (\u0026amp;Rcirc;) does not render properly.\n\rInstead, the LaTeX code $\\\\widehat{R}$ must be used. Therefore, we’ll correct this error and load the function below.\n# Function used in the manuscript to present summaries from \u0026#39;brms\u0026#39; models # in APA-formatted tables. The only obligatory argument to be supplied is # a summary of a \u0026#39;brms\u0026#39; model.\rbayesian_model_table = function(model_summary, show_intercept = TRUE, select_effects = NULL, order_effects = NULL, format = NULL, # If interaction_symbol_x = TRUE, replace double colons with # times symbols followed by line breaks and indentation. # Then, replace single colons with times symbols.\rinteraction_symbol_x = FALSE,\rcaption = \u0026#39;Summary of the lmerTest model.\u0026#39;) {\rrequire(dplyr)\rrequire(knitr)\rrequire(tibble)\rrequire(stringr)\rrequire(lmerTest)\rrequire(kableExtra)\r# Create data frame\rmodel_summary = data.frame(Effect = rownames(model_summary$fixed), Estimate = model_summary$fixed$Estimate, SE = model_summary$fixed$Est.Error, CrI_2.5 = model_summary$fixed$`l-95% CI`, CrI_97.5 = model_summary$fixed$`u-95% CI`, Rhat = model_summary$fixed$Rhat,\rrow.names = NULL)\r# Process credible intervals and present both inside square brackets\rmodel_summary$CrI_2.5 = model_summary$CrI_2.5 %\u0026gt;% # Round off and keep trailing zeros\rsprintf(\u0026#39;%.2f\u0026#39;, .) %\u0026gt;% # Remove minus sign from pure zeros\rsub(\u0026#39;-0.00\u0026#39;, \u0026#39;0.00\u0026#39;, .)\rmodel_summary$CrI_97.5 = model_summary$CrI_97.5 %\u0026gt;% # Round off and keep trailing zeros\rsprintf(\u0026#39;%.2f\u0026#39;, .) %\u0026gt;% # Remove minus sign from pure zeros\rsub(\u0026#39;-0.00\u0026#39;, \u0026#39;0.00\u0026#39;, .)\rmodel_summary$CrI_95 = paste0(\u0026#39;[\u0026#39;, model_summary$CrI_2.5, \u0026#39;, \u0026#39;, model_summary$CrI_97.5, \u0026#39;]\u0026#39;)\r# If show_intercept = FALSE, remove it\rif(isFALSE(show_intercept)) {\rmodel_summary = model_summary %\u0026gt;% filter(!grepl(\u0026#39;Intercept\u0026#39;, Effect))\r# Put \u0026#39;Intercept\u0026#39; in parentheses\r} else if(!is.null(model_summary[model_summary$Effect == \u0026#39;Intercept\u0026#39;, \u0026#39;Effect\u0026#39;])) {\rmodel_summary[model_summary$Effect == \u0026#39;Intercept\u0026#39;, \u0026#39;Effect\u0026#39;] = \u0026#39;(Intercept)\u0026#39;\r}\r# If select_effects was supplied, apply it and order effects accordingly\rif(!is.null(select_effects)) {\rmodel_summary = model_summary %\u0026gt;% filter(Effect %in% select_effects) %\u0026gt;%\rarrange(factor(Effect, levels = select_effects))\r}\r# If order_effects was supplied, apply order\rif(!is.null(order_effects)) {\rmodel_summary = model_summary %\u0026gt;%\rarrange(factor(Effect, levels = order_effects))\r}\r# Round other values\rmodel_summary$Estimate = model_summary$Estimate %\u0026gt;% as.numeric %\u0026gt;% # Round off and keep trailing zeros\rsprintf(\u0026#39;%.2f\u0026#39;, .) %\u0026gt;% # Remove minus sign from pure zeros\rsub(\u0026#39;-0.00\u0026#39;, \u0026#39;0.00\u0026#39;, .)\rmodel_summary$SE = model_summary$SE %\u0026gt;% as.numeric %\u0026gt;% # Round off and keep trailing zeros\rsprintf(\u0026#39;%.2f\u0026#39;, .)\rmodel_summary$Rhat = model_summary$Rhat %\u0026gt;% as.numeric %\u0026gt;% # Round off and keep trailing zeros\rsprintf(\u0026#39;%.2f\u0026#39;, .)\r# Order columns\rmodel_summary = model_summary %\u0026gt;% select(Effect, Estimate, SE, CrI_95, Rhat)\r# Right-align all columns after first one\ralign = c(\u0026#39;l\u0026#39;, \u0026#39;r\u0026#39;, \u0026#39;r\u0026#39;, \u0026#39;r\u0026#39;, \u0026#39;r\u0026#39;)\r# Establish latex or HTML format: if no format supplied, # try to obtain it from knitr, or apply HTML\rif(missing(format) || is.null(format)) {\rif(knitr::is_latex_output()) {\rformat = \u0026#39;latex\u0026#39;\r} else format = \u0026#39;html\u0026#39;\r}\r# HTML format\rif(format == \u0026#39;html\u0026#39;) {\r# If interaction_symbol_x = TRUE, replace double colons with times # symbols followed by line breaks and indentation. Then, replace # single colons with times symbols.\rif(interaction_symbol_x) {\rmodel_summary$Effect = model_summary$Effect %\u0026gt;% gsub(\u0026#39;::\u0026#39;, \u0026#39; \u0026amp;times; \u0026lt;br\u0026gt; \u0026amp;nbsp;\u0026amp;nbsp;\u0026#39;, .) %\u0026gt;%\rgsub(\u0026#39;:\u0026#39;, \u0026#39; \u0026amp;times; \u0026#39;, .)\r}\r# Output table\rmodel_summary %\u0026gt;% # Remove header of first column and rename other headers\rrename(\u0026#39; \u0026#39; = \u0026#39;Effect\u0026#39;, \u0026#39;\u0026amp;beta;\u0026#39; = \u0026#39;Estimate\u0026#39;, \u0026#39;\u0026lt;i\u0026gt;SE\u0026lt;/i\u0026gt;\u0026#39; = \u0026#39;SE\u0026#39;, \u0026#39;95% CrI\u0026#39; = \u0026#39;CrI_95\u0026#39;, \u0026#39;$\\\\widehat{R}$\u0026#39; = \u0026#39;Rhat\u0026#39;) %\u0026gt;%\r# Present table\rkbl(digits = 2, booktabs = TRUE, escape = FALSE, align = align,\r# Caption of the table (default unless specified)\rcaption = caption, # Disable occasional line gap (https://stackoverflow.com/a/49018919/7050882)\rlinesep = \u0026#39;\u0026#39;) %\u0026gt;%\r# Apply nice kableExtra format\rkable_styling() %\u0026gt;%\r# Center-align header row\rrow_spec(0, align = \u0026#39;c\u0026#39;)\r# LaTeX format\r} else {\r# If interaction_symbol_x = TRUE, replace double colons with times # symbols followed by line breaks and indentation. Then, replace # single colons with times symbols.\rif(interaction_symbol_x) {\rmodel_summary$Effect = model_summary$Effect %\u0026gt;% gsub(\u0026#39;::\u0026#39;, \u0026#39; $\\\\\\\\times$ \\n \\\\\\\\hspace{0.3cm}\u0026#39;, .) %\u0026gt;%\rgsub(\u0026#39;:\u0026#39;, \u0026#39; $\\\\\\\\times$ \u0026#39;, .)\r}\rmodel_summary$Effect = model_summary$Effect %\u0026gt;%\r# Escape underscores to avoid error in table\rstr_replace_all(\u0026#39;_\u0026#39;, \u0026#39;\\\\\\\\_\u0026#39;) %\u0026gt;%\r# Allow line breaks in the names of the effects\r# (used in the interactions)\rkableExtra::linebreak(align = \u0026#39;l\u0026#39;)\r# Output table\rmodel_summary %\u0026gt;% # Remove header of first column and rename other headers\rrename(\u0026#39; \u0026#39; = \u0026#39;Effect\u0026#39;, \u0026#39;$\\\\upbeta$\u0026#39; = \u0026#39;Estimate\u0026#39;, \u0026#39;$SE$\u0026#39; = \u0026#39;SE\u0026#39;, \u0026#39;95\\\\% CrI\u0026#39; = \u0026#39;CrI_95\u0026#39;, \u0026#39;$\\\\widehat R$\u0026#39; = \u0026#39;Rhat\u0026#39;) %\u0026gt;%\r# Present table\rkbl(digits = 2, booktabs = TRUE, escape = FALSE, align = align,\r# Caption of the table (default unless specified)\rcaption = caption, # Disable occasional line gap (https://stackoverflow.com/a/49018919/7050882)\rlinesep = \u0026#39;\u0026#39;) %\u0026gt;%\r# Apply nice kableExtra format\rkable_styling() %\u0026gt;%\r# Center-align header row\rrow_spec(0, align = \u0026#39;c\u0026#39;)\r}\r}\r\rThe function in use\r# Create table (using custom function from the \u0026#39;R_functions\u0026#39; folder)\rbayesian_model_table(\rsemanticpriming_summary_weaklyinformativepriors_exgaussian,\rorder_effects = c(\u0026#39;(Intercept)\u0026#39;,\r\u0026#39;Attentional control\u0026#39;,\r\u0026#39;Vocabulary size \u0026lt;sup\u0026gt;a\u0026lt;/sup\u0026gt;\u0026#39;,\r\u0026#39;Gender \u0026lt;sup\u0026gt;a\u0026lt;/sup\u0026gt;\u0026#39;,\r\u0026#39;Word frequency\u0026#39;,\r\u0026#39;Number of syllables\u0026#39;,\r\u0026#39;Word-concreteness difference\u0026#39;,\r\u0026#39;Language-based similarity \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;,\r\u0026#39;Visual-strength difference \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;,\r\u0026#39;Stimulus onset asynchrony (SOA) \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;,\r\u0026#39;Word-concreteness difference :: Vocabulary size\u0026#39;,\r\u0026#39;Word-concreteness difference : SOA\u0026#39;,\r\u0026#39;Word-concreteness difference : Gender\u0026#39;,\r\u0026#39;Language-based similarity :: Attentional control\u0026#39;,\r\u0026#39;Visual-strength difference :: Attentional control\u0026#39;,\r\u0026#39;Language-based similarity :: Vocabulary size\u0026#39;,\r\u0026#39;Visual-strength difference :: Vocabulary size\u0026#39;,\r\u0026#39;Language-based similarity : Gender\u0026#39;,\r\u0026#39;Visual-strength difference : Gender\u0026#39;,\r\u0026#39;Language-based similarity : SOA \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;,\r\u0026#39;Visual-strength difference : SOA \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;),\r# Replace colons in the names of interactions with times symbols\rinteraction_symbol_x = TRUE, # No title\rcaption = NULL) %\u0026gt;%\r# Group predictors under headings\rpack_rows(\u0026#39;Individual differences\u0026#39;, 2, 4) %\u0026gt;% pack_rows(\u0026#39;Target-word lexical covariates\u0026#39;, 5, 6) %\u0026gt;% pack_rows(\u0026#39;Prime--target relationship\u0026#39;, 7, 9) %\u0026gt;% pack_rows(\u0026#39;Task condition\u0026#39;, 10, 10) %\u0026gt;% pack_rows(\u0026#39;Interactions\u0026#39;, 11, 21) %\u0026gt;% # Apply white background to override default shading in HTML output\rrow_spec(1:21, background = \u0026#39;white\u0026#39;) %\u0026gt;%\r# Highlight covariates\rrow_spec(c(2, 5:7, 11:15), background = \u0026#39;#FFFFF1\u0026#39;) %\u0026gt;%\r# Format\rkable_classic(full_width = FALSE, html_font = \u0026#39;Cambria\u0026#39;) %\u0026gt;%\r# Footnote describing abbreviations, random slopes, etc. # LaTeX code used to format the text.\rfootnote(escape = FALSE, threeparttable = TRUE, general_title = \u0026#39;\u0026lt;br\u0026gt;\u0026#39;, general = paste(\u0026#39;*Note*. \u0026amp;beta; = Estimate based on $z$-scored predictors; *SE* = standard error;\u0026#39;,\r\u0026#39;CrI = credible interval. Yellow rows contain covariates. Some interactions are \u0026#39;,\r\u0026#39;split over two lines, with the second line indented. \u0026lt;br\u0026gt;\u0026#39;, \u0026#39;\u0026lt;sup\u0026gt;a\u0026lt;/sup\u0026gt; By-word random slopes were included for this effect.\u0026#39;,\r\u0026#39;\u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt; By-participant random slopes were included for this effect.\u0026#39;))\r\r\r\rβ\r\rSE\r\r95% CrI\r\r\\(\\widehat{R}\\)\r\r\r\r\r\r(Intercept)\r\r0.00\r\r0.00\r\r[0.00, 0.01]\r\r1.00\r\r\rIndividual differences\r\r\r\rAttentional control\r\r0.00\r\r0.00\r\r[0.00, 0.01]\r\r1.00\r\r\r\rVocabulary size a\r\r-0.01\r\r0.00\r\r[-0.01, 0.00]\r\r1.00\r\r\r\rGender a\r\r0.00\r\r0.00\r\r[0.00, 0.01]\r\r1.00\r\r\rTarget-word lexical covariates\r\r\r\rWord frequency\r\r-0.11\r\r0.00\r\r[-0.12, -0.11]\r\r1.00\r\r\r\rNumber of syllables\r\r0.07\r\r0.00\r\r[0.06, 0.07]\r\r1.00\r\r\rPrime–target relationship\r\r\r\rWord-concreteness difference\r\r0.01\r\r0.00\r\r[0.00, 0.01]\r\r1.00\r\r\r\rLanguage-based similarity b\r\r-0.06\r\r0.00\r\r[-0.07, -0.06]\r\r1.00\r\r\r\rVisual-strength difference b\r\r0.01\r\r0.00\r\r[0.01, 0.01]\r\r1.00\r\r\rTask condition\r\r\r\rStimulus onset asynchrony (SOA) b\r\r0.03\r\r0.01\r\r[0.02, 0.04]\r\r1.00\r\r\rInteractions\r\r\r\rWord-concreteness difference × Vocabulary size\r\r0.00\r\r0.00\r\r[0.00, 0.00]\r\r1.00\r\r\r\rWord-concreteness difference × SOA\r\r0.00\r\r0.00\r\r[0.00, 0.00]\r\r1.00\r\r\r\rWord-concreteness difference × Gender\r\r0.00\r\r0.00\r\r[0.00, 0.00]\r\r1.00\r\r\r\rLanguage-based similarity × Attentional control\r\r0.00\r\r0.00\r\r[-0.01, 0.00]\r\r1.00\r\r\r\rVisual-strength difference × Attentional control\r\r0.00\r\r0.00\r\r[0.00, 0.00]\r\r1.00\r\r\r\rLanguage-based similarity × Vocabulary size\r\r0.00\r\r0.00\r\r[-0.01, 0.00]\r\r1.00\r\r\r\rVisual-strength difference × Vocabulary size\r\r0.00\r\r0.00\r\r[0.00, 0.00]\r\r1.00\r\r\r\rLanguage-based similarity × Gender\r\r0.00\r\r0.00\r\r[-0.01, 0.00]\r\r1.00\r\r\r\rVisual-strength difference × Gender\r\r0.00\r\r0.00\r\r[0.00, 0.00]\r\r1.00\r\r\r\rLanguage-based similarity × SOA b\r\r0.00\r\r0.00\r\r[0.00, 0.00]\r\r1.00\r\r\r\rVisual-strength difference × SOA b\r\r0.00\r\r0.00\r\r[0.00, 0.00]\r\r1.00\r\r\r\r\r\r\r\r\r\r Note. β = Estimate based on \\(z\\)-scored predictors; SE = standard error; CrI = credible interval. Yellow rows contain covariates. Some interactions are split over two lines, with the second line indented. a By-word random slopes were included for this effect. b By-participant random slopes were included for this effect.\r\r\r\r\rShading specific rows\rShading specific rows is done differently when the output is PDF, as shown below (see p. 170 in Bernabeu, 2022).\n\r\r\rSession info\rIf you encounter any blockers while reproducing the table using the materials at https://osf.io/gt5uf, my current session info may be useful.\nsessionInfo()\r## R version 4.2.2 (2022-10-31 ucrt)\r## Platform: x86_64-w64-mingw32/x64 (64-bit)\r## Running under: Windows 10 x64 (build 19045)\r## ## Matrix products: default\r## ## locale:\r## [1] LC_COLLATE=English_United States.utf8 ## [2] LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8\r## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## attached base packages:\r## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages:\r## [1] kableExtra_1.3.4 lmerTest_3.1-3 lme4_1.1-31 ## [4] Matrix_1.5-1 stringr_1.5.0 tibble_3.1.8 ## [7] dplyr_1.0.10 knitr_1.41 xaringanExtra_0.7.0\r## ## loaded via a namespace (and not attached):\r## [1] tidyselect_1.2.0 xfun_0.36 bslib_0.4.2 ## [4] splines_4.2.2 lattice_0.20-45 colorspace_2.0-3 ## [7] vctrs_0.5.1 generics_0.1.3 viridisLite_0.4.1 ## [10] htmltools_0.5.4 yaml_2.3.6 utf8_1.2.2 ## [13] rlang_1.0.6 jquerylib_0.1.4 pillar_1.8.1 ## [16] nloptr_2.0.3 withr_2.5.0 glue_1.6.2 ## [19] DBI_1.1.3 uuid_1.1-0 lifecycle_1.0.3 ## [22] munsell_0.5.0 blogdown_1.16 gtable_0.3.1 ## [25] rvest_1.0.3 evaluate_0.19 fastmap_1.1.0 ## [28] fansi_1.0.3 Rcpp_1.0.9 scales_1.2.1 ## [31] cachem_1.0.6 webshot_0.5.4 jsonlite_1.8.4 ## [34] systemfonts_1.0.4 ggplot2_3.3.5 digest_0.6.31 ## [37] stringi_1.7.8 bookdown_0.31 numDeriv_2016.8-1.1\r## [40] grid_4.2.2 cli_3.4.1 tools_4.2.2 ## [43] magrittr_2.0.3 sass_0.4.4 pkgconfig_2.0.3 ## [46] MASS_7.3-58.1 xml2_1.3.3 svglite_2.1.0 ## [49] httr_1.4.4 assertthat_0.2.1 minqa_1.2.5 ## [52] rmarkdown_2.19 rstudioapi_0.14 R6_2.5.1 ## [55] boot_1.3-28 nlme_3.1-160 compiler_4.2.2\r\rReferences\rBernabeu, P. (2022). Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power. Lancaster University. https://doi.org/10.17635/lancaster/thesis/1795\nBrauer, M., \u0026amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. Psychological Methods, 23(3), 389–411. https://doi.org/10.1037/met0000159\nBürkner, P.-C., Gabry, J., Weber, S., Johnson, A., Modrak, M., Badr, H. S., Weber, F., Ben-Shachar, M. S., \u0026amp; Rabel, H. (2022). Package ’brms’. CRAN. https://cran.r-project.org/web/packages/brms/brms.pdf\nZhu, H. (2022). Package ’kableExtra’. CRAN. https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf\n\r\r","date":1672272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672272000,"objectID":"b1392cb5949a0c807058805dab18b8f9","permalink":"https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/","publishdate":"2022-12-29T00:00:00Z","relpermalink":"/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/","section":"post","summary":"Here I share the format applied to tables presenting the results of Bayesian models in Bernabeu (2022). The sample table presents a mixed-effects model that was fitted using the R package 'brms' (Bürkner et al., 2022).","tags":["R","statistics","rstats","brms","credible intervals","s"],"title":"A table of results for Bayesian mixed-effects models: Grouping variables and specifying random slopes","type":"post"},{"authors":[],"categories":["R","statistics"],"content":"\r\r\rwindow.xaringanExtraClipboard(null, {\"button\":\"Copy Code\",\"success\":\"Copied!\",\"error\":\"Press Ctrl+C to Copy\"})\r\rHere I share the format applied to tables presenting the results of frequentist models in Bernabeu (2022; the table for Bayesian models is covered in this other post). The sample table presents a mixed-effects model that was fitted using the R package lmerTest (Kuznetsova et al., 2022). The mixed effects were driven by the maximal principle (Brauer \u0026amp; Curtin, 2018). The format of the table resembles one of the examples published by the American Psychological Association. However, there are also deviations from those examples. For instance, in the present table, the effects are grouped under informative labels to facilitate the readers’ comprehension, using the kableExtra package (Zhu, 2022). Furthermore, the random slopes are specified using superscript letters and a footnote. The table can be reproduced using the materials at https://osf.io/gt5uf.\nLoading packages and the results of the models\rlibrary(dplyr)\rlibrary(knitr)\rlibrary(tibble)\rlibrary(stringr)\rlibrary(lmerTest)\rlibrary(kableExtra)\r# Load frequentist coefficients (estimates and confidence intervals)\rKR_summary_semanticpriming_lmerTest =\rreadRDS(gzcon(url(\u0026#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/KR_summary_semanticpriming_lmerTest.rds?raw=true\u0026#39;)))\rconfint_semanticpriming_lmerTest =\rreadRDS(gzcon(url(\u0026#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/confint_semanticpriming_lmerTest.rds?raw=true\u0026#39;)))\r\rAdjusting the names of the effects\rFirst, to facilitate the understanding of the results, the original names of the effects will be adjusted in the lmerTest summary and in the confidence intervals object.\nIncidentally, the confidence intervals were obtained using the confint.merMod function from the lme4 package, as neither lmerTest nor lme4 currently provide confidence intervals in their default results output. However, computing the confidence intervals is uncomplicated (see code).\n# Rename effects in plain language and specify the random slopes\r# (if any) for each effect, in the footnote. For this purpose, # superscripts are added to the names of the appropriate effects.\r# # In the interactions below, word-level variables are presented # first for the sake of consistency (the order does not affect # the results in any way). Also in the interactions, double # colons are used to inform the \u0026#39;frequentist_model_table\u0026#39; # function that the two terms in the interaction must be split # into two lines.\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_attentional_control\u0026#39;] = \u0026#39;Attentional control\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_vocabulary_size\u0026#39;] = \u0026#39;Vocabulary size \u0026lt;sup\u0026gt;a\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_recoded_participant_gender\u0026#39;] = \u0026#39;Gender \u0026lt;sup\u0026gt;a\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_target_word_frequency\u0026#39;] = \u0026#39;Word frequency\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_target_number_syllables\u0026#39;] = \u0026#39;Number of syllables\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_word_concreteness_diff\u0026#39;] = \u0026#39;Word-concreteness difference\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_cosine_similarity\u0026#39;] = \u0026#39;Language-based similarity \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_visual_rating_diff\u0026#39;] = \u0026#39;Visual-strength difference \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_recoded_interstimulus_interval\u0026#39;] = \u0026#39;Stimulus onset asynchrony (SOA) \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_word_concreteness_diff:z_vocabulary_size\u0026#39;] = \u0026#39;Word-concreteness difference :: Vocabulary size\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_word_concreteness_diff:z_recoded_interstimulus_interval\u0026#39;] = \u0026#39;Word-concreteness difference : SOA\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_word_concreteness_diff:z_recoded_participant_gender\u0026#39;] = \u0026#39;Word-concreteness difference : Gender\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_attentional_control:z_cosine_similarity\u0026#39;] = \u0026#39;Language-based similarity :: Attentional control\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_attentional_control:z_visual_rating_diff\u0026#39;] = \u0026#39;Visual-strength difference :: Attentional control\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_vocabulary_size:z_cosine_similarity\u0026#39;] = \u0026#39;Language-based similarity :: Vocabulary size\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_vocabulary_size:z_visual_rating_diff\u0026#39;] = \u0026#39;Visual-strength difference :: Vocabulary size\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_recoded_participant_gender:z_cosine_similarity\u0026#39;] = \u0026#39;Language-based similarity : Gender\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_recoded_participant_gender:z_visual_rating_diff\u0026#39;] = \u0026#39;Visual-strength difference : Gender\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_recoded_interstimulus_interval:z_cosine_similarity\u0026#39;] = \u0026#39;Language-based similarity : SOA \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(KR_summary_semanticpriming_lmerTest$coefficients)[\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) == \u0026#39;z_recoded_interstimulus_interval:z_visual_rating_diff\u0026#39;] = \u0026#39;Visual-strength difference : SOA \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;\r# Next, change the names in the confidence intervals object\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_attentional_control\u0026#39;] = \u0026#39;Attentional control\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_vocabulary_size\u0026#39;] = \u0026#39;Vocabulary size \u0026lt;sup\u0026gt;a\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_recoded_participant_gender\u0026#39;] = \u0026#39;Gender \u0026lt;sup\u0026gt;a\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_target_word_frequency\u0026#39;] = \u0026#39;Word frequency\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_target_number_syllables\u0026#39;] = \u0026#39;Number of syllables\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_word_concreteness_diff\u0026#39;] = \u0026#39;Word-concreteness difference\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_cosine_similarity\u0026#39;] = \u0026#39;Language-based similarity \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_visual_rating_diff\u0026#39;] = \u0026#39;Visual-strength difference \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_recoded_interstimulus_interval\u0026#39;] = \u0026#39;Stimulus onset asynchrony (SOA) \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_word_concreteness_diff:z_vocabulary_size\u0026#39;] = \u0026#39;Word-concreteness difference :: Vocabulary size\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_word_concreteness_diff:z_recoded_interstimulus_interval\u0026#39;] = \u0026#39;Word-concreteness difference : SOA\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_word_concreteness_diff:z_recoded_participant_gender\u0026#39;] = \u0026#39;Word-concreteness difference : Gender\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_attentional_control:z_cosine_similarity\u0026#39;] = \u0026#39;Language-based similarity :: Attentional control\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_attentional_control:z_visual_rating_diff\u0026#39;] = \u0026#39;Visual-strength difference :: Attentional control\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_vocabulary_size:z_cosine_similarity\u0026#39;] = \u0026#39;Language-based similarity :: Vocabulary size\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_vocabulary_size:z_visual_rating_diff\u0026#39;] = \u0026#39;Visual-strength difference :: Vocabulary size\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_recoded_participant_gender:z_cosine_similarity\u0026#39;] = \u0026#39;Language-based similarity : Gender\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_recoded_participant_gender:z_visual_rating_diff\u0026#39;] = \u0026#39;Visual-strength difference : Gender\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_recoded_interstimulus_interval:z_cosine_similarity\u0026#39;] = \u0026#39;Language-based similarity : SOA \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;\rrownames(confint_semanticpriming_lmerTest)[\rrownames(confint_semanticpriming_lmerTest) == \u0026#39;z_recoded_interstimulus_interval:z_visual_rating_diff\u0026#39;] = \u0026#39;Visual-strength difference : SOA \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;\r\rfrequentist_model_table()\rThe following custom function was used.\n\rLoading the function from GitHub\rsource(\u0026#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/raw/main/R_functions/frequentist_model_table.R\u0026#39;)\r\r\rThe function in use\r# Create table (using custom function from the \u0026#39;R_functions\u0026#39; folder)\rfrequentist_model_table(\rKR_summary_semanticpriming_lmerTest, confint_semanticpriming_lmerTest,\rorder_effects = c(\u0026#39;(Intercept)\u0026#39;,\r\u0026#39;Attentional control\u0026#39;,\r\u0026#39;Vocabulary size \u0026lt;sup\u0026gt;a\u0026lt;/sup\u0026gt;\u0026#39;,\r\u0026#39;Gender \u0026lt;sup\u0026gt;a\u0026lt;/sup\u0026gt;\u0026#39;,\r\u0026#39;Word frequency\u0026#39;,\r\u0026#39;Number of syllables\u0026#39;,\r\u0026#39;Word-concreteness difference\u0026#39;,\r\u0026#39;Language-based similarity \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;,\r\u0026#39;Visual-strength difference \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;,\r\u0026#39;Stimulus onset asynchrony (SOA) \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;,\r\u0026#39;Word-concreteness difference :: Vocabulary size\u0026#39;,\r\u0026#39;Word-concreteness difference : SOA\u0026#39;,\r\u0026#39;Word-concreteness difference : Gender\u0026#39;,\r\u0026#39;Language-based similarity :: Attentional control\u0026#39;,\r\u0026#39;Visual-strength difference :: Attentional control\u0026#39;,\r\u0026#39;Language-based similarity :: Vocabulary size\u0026#39;,\r\u0026#39;Visual-strength difference :: Vocabulary size\u0026#39;,\r\u0026#39;Language-based similarity : Gender\u0026#39;,\r\u0026#39;Visual-strength difference : Gender\u0026#39;,\r\u0026#39;Language-based similarity : SOA \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;,\r\u0026#39;Visual-strength difference : SOA \u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt;\u0026#39;),\r# Replace colons in the names of interactions with times symbols\rinteraction_symbol_x = TRUE, # No title\rcaption = NULL) %\u0026gt;%\r# Group predictors under headings\rpack_rows(\u0026#39;Individual differences\u0026#39;, 2, 4) %\u0026gt;% pack_rows(\u0026#39;Target-word lexical covariates\u0026#39;, 5, 6) %\u0026gt;% pack_rows(\u0026#39;Prime--target relationship\u0026#39;, 7, 9) %\u0026gt;% pack_rows(\u0026#39;Task condition\u0026#39;, 10, 10) %\u0026gt;% pack_rows(\u0026#39;Interactions\u0026#39;, 11, 21) %\u0026gt;% # Apply white background to override default shading in HTML output\rrow_spec(1:21, background = \u0026#39;white\u0026#39;) %\u0026gt;%\r# Highlight covariates\rrow_spec(c(2, 5:7, 11:15), background = \u0026#39;#FFFFF1\u0026#39;) %\u0026gt;%\r# Format\rkable_classic(full_width = FALSE, html_font = \u0026#39;Cambria\u0026#39;) %\u0026gt;%\r# Footnote describing abbreviations, random slopes, etc. # LaTeX code used to format the text.\rfootnote(escape = FALSE, threeparttable = TRUE, general_title = \u0026#39;\u0026lt;br\u0026gt;\u0026#39;, general = paste(\u0026#39;*Note*. \u0026amp;beta; = Estimate based on $z$-scored predictors; *SE* = standard error;\u0026#39;,\r\u0026#39;CI = confidence interval. Yellow rows contain covariates. Some interactions are \u0026#39;,\r\u0026#39;split over two lines, with the second line indented. \u0026lt;br\u0026gt;\u0026#39;, \u0026#39;\u0026lt;sup\u0026gt;a\u0026lt;/sup\u0026gt; By-word random slopes were included for this effect.\u0026#39;,\r\u0026#39;\u0026lt;sup\u0026gt;b\u0026lt;/sup\u0026gt; By-participant random slopes were included for this effect.\u0026#39;))\r\r\r\rβ\r\rSE\r\r95% CI\r\rt\r\rp\r\r\r\r\r\r(Intercept)\r\r0.00\r\r0.00\r\r[0.00, 0.01]\r\r1.59\r\r.112\r\r\rIndividual differences\r\r\r\rAttentional control\r\r0.00\r\r0.00\r\r[0.00, 0.00]\r\r-0.56\r\r.577\r\r\r\rVocabulary size a\r\r0.00\r\r0.00\r\r[0.00, 0.00]\r\r0.02\r\r.987\r\r\r\rGender a\r\r0.00\r\r0.00\r\r[0.00, 0.00]\r\r-0.03\r\r.979\r\r\rTarget-word lexical covariates\r\r\r\rWord frequency\r\r-0.16\r\r0.00\r\r[-0.16, -0.15]\r\r-49.40\r\r\u0026lt;.001\r\r\r\rNumber of syllables\r\r0.07\r\r0.00\r\r[0.07, 0.08]\r\r22.81\r\r\u0026lt;.001\r\r\rPrime–target relationship\r\r\r\rWord-concreteness difference\r\r0.01\r\r0.00\r\r[0.01, 0.02]\r\r3.48\r\r.001\r\r\r\rLanguage-based similarity b\r\r-0.08\r\r0.00\r\r[-0.08, -0.07]\r\r-22.44\r\r\u0026lt;.001\r\r\r\rVisual-strength difference b\r\r0.01\r\r0.00\r\r[0.01, 0.02]\r\r4.18\r\r\u0026lt;.001\r\r\rTask condition\r\r\r\rStimulus onset asynchrony (SOA) b\r\r0.06\r\r0.01\r\r[0.04, 0.07]\r\r7.47\r\r\u0026lt;.001\r\r\rInteractions\r\r\r\rWord-concreteness difference × Vocabulary size\r\r0.00\r\r0.00\r\r[0.00, 0.01]\r\r1.31\r\r.189\r\r\r\rWord-concreteness difference × SOA\r\r0.00\r\r0.00\r\r[0.00, 0.01]\r\r2.57\r\r.010\r\r\r\rWord-concreteness difference × Gender\r\r0.00\r\r0.00\r\r[-0.01, 0.00]\r\r-0.97\r\r.332\r\r\r\rLanguage-based similarity × Attentional control\r\r-0.01\r\r0.00\r\r[-0.01, 0.00]\r\r-2.46\r\r.014\r\r\r\rVisual-strength difference × Attentional control\r\r0.00\r\r0.00\r\r[0.00, 0.00]\r\r0.24\r\r.810\r\r\r\rLanguage-based similarity × Vocabulary size\r\r-0.01\r\r0.00\r\r[-0.01, 0.00]\r\r-2.34\r\r.020\r\r\r\rVisual-strength difference × Vocabulary size\r\r0.00\r\r0.00\r\r[-0.01, 0.00]\r\r-1.37\r\r.172\r\r\r\rLanguage-based similarity × Gender\r\r0.00\r\r0.00\r\r[-0.01, 0.00]\r\r-0.79\r\r.433\r\r\r\rVisual-strength difference × Gender\r\r0.00\r\r0.00\r\r[0.00, 0.01]\r\r1.46\r\r.144\r\r\r\rLanguage-based similarity × SOA b\r\r0.01\r\r0.00\r\r[0.00, 0.01]\r\r3.22\r\r.001\r\r\r\rVisual-strength difference × SOA b\r\r0.00\r\r0.00\r\r[-0.01, 0.00]\r\r-2.25\r\r.025\r\r\r\r\r\r\r\r\r\r Note. β = Estimate based on \\(z\\)-scored predictors; SE = standard error; CI = confidence interval. Yellow rows contain covariates. Some interactions are split over two lines, with the second line indented. a By-word random slopes were included for this effect. b By-participant random slopes were included for this effect.\r\r\r\r\rShading specific rows\rShading specific rows is done differently when the output is PDF, as shown below (see p. 62 in Bernabeu, 2022).\n\r\r\rSession info\rIf you encounter any blockers while reproducing the table using the materials at https://osf.io/gt5uf, my current session info may be useful.\nsessionInfo()\r## R version 4.2.2 (2022-10-31 ucrt)\r## Platform: x86_64-w64-mingw32/x64 (64-bit)\r## Running under: Windows 10 x64 (build 19045)\r## ## Matrix products: default\r## ## locale:\r## [1] LC_COLLATE=English_United States.utf8 ## [2] LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8\r## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## attached base packages:\r## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages:\r## [1] kableExtra_1.3.4 lmerTest_3.1-3 lme4_1.1-31 ## [4] Matrix_1.5-1 stringr_1.5.0 tibble_3.1.8 ## [7] dplyr_1.0.10 knitr_1.41 xaringanExtra_0.7.0\r## ## loaded via a namespace (and not attached):\r## [1] tidyselect_1.2.0 xfun_0.36 bslib_0.4.2 ## [4] splines_4.2.2 lattice_0.20-45 colorspace_2.0-3 ## [7] vctrs_0.5.1 generics_0.1.3 viridisLite_0.4.1 ## [10] htmltools_0.5.4 yaml_2.3.6 utf8_1.2.2 ## [13] rlang_1.0.6 jquerylib_0.1.4 pillar_1.8.1 ## [16] nloptr_2.0.3 withr_2.5.0 glue_1.6.2 ## [19] DBI_1.1.3 uuid_1.1-0 lifecycle_1.0.3 ## [22] munsell_0.5.0 blogdown_1.16 gtable_0.3.1 ## [25] rvest_1.0.3 evaluate_0.19 fastmap_1.1.0 ## [28] fansi_1.0.3 Rcpp_1.0.9 scales_1.2.1 ## [31] cachem_1.0.6 webshot_0.5.4 jsonlite_1.8.4 ## [34] systemfonts_1.0.4 ggplot2_3.3.5 digest_0.6.31 ## [37] stringi_1.7.8 bookdown_0.31 numDeriv_2016.8-1.1\r## [40] grid_4.2.2 cli_3.4.1 tools_4.2.2 ## [43] magrittr_2.0.3 sass_0.4.4 pkgconfig_2.0.3 ## [46] MASS_7.3-58.1 xml2_1.3.3 svglite_2.1.0 ## [49] httr_1.4.4 assertthat_0.2.1 minqa_1.2.5 ## [52] rmarkdown_2.19 rstudioapi_0.14 R6_2.5.1 ## [55] boot_1.3-28 nlme_3.1-160 compiler_4.2.2\r\rReferences\rBernabeu, P. (2022). Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power. Lancaster University. https://doi.org/10.17635/lancaster/thesis/1795\nBrauer, M., \u0026amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. Psychological Methods, 23(3), 389–411. https://doi.org/10.1037/met0000159\nKuznetsova, A., Brockhoff, P. B., \u0026amp; Christensen, R. H. B. (2022). Package ’lmerTest’. CRAN. https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf\nZhu, H. (2022). Package ’kableExtra’. CRAN. https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf\n\r\r","date":1672272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672272000,"objectID":"76438e72432949f7f3e7ec3744994479","permalink":"https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/","publishdate":"2022-12-29T00:00:00Z","relpermalink":"/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/","section":"post","summary":"Here I share the format applied to tables presenting the results of frequentist models in Bernabeu (2022). The sample table presents a mixed-effects model that was fitted using the R package 'lmerTest' (Kuznetsova et al., 2022).","tags":["R","statistics","rstats","lmerTest","lme4","confidence intervals","s"],"title":"A table of results for frequentist mixed-effects models: Grouping variables and specifying random slopes","type":"post"},{"authors":[],"categories":["R","data visualisation"],"content":"\rWhereas the direction of main effects can be interpreted from the sign of the estimate, the interpretation of interaction effects often requires plots. This task is facilitated by the R package sjPlot (Lüdecke, 2022). In Bernabeu (2022), the sjPlot function called plot_model served as the basis for the creation of some custom functions. One of these functions is alias_interaction_plot, which allows the plotting of interactions between a continuous variable and a categorical variable. Importantly, the categorical variable is replaced with an alias variable. This feature allows the back-transformation of the categorical variable to facilitate the communication of the results, for instance, when the categorical variable was sum-coded, which has been recommended for mixed-effects models (Brauer \u0026amp; Curtin, 2018).\nBelow, we’ll use the function with a model fitted using lmerTest (Kuznetsova et al., 2022), although the function also works with several other models (see sjPlot manual). The plot can be reproduced using the materials at https://osf.io/gt5uf.\nAlias interaction plot\rThe function\r\r\rThe function in use\r\r\n\rReferences\rBernabeu, P. (2022). Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power. Lancaster University. https://doi.org/10.17635/lancaster/thesis/1795\nBrauer, M., \u0026amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. Psychological Methods, 23(3), 389–411. https://doi.org/10.1037/met0000159\nKuznetsova, A., Brockhoff, P. B., \u0026amp; Christensen, R. H. B. (2022). Package ’lmerTest’. CRAN. https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf\nLüdecke, D. (2022). Package ’sjPlot’. CRAN. https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf\n\r\r","date":1672012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672012800,"objectID":"e6979835a88a63c8e8e698615abbb6b4","permalink":"https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-alias-variables/","publishdate":"2022-12-26T00:00:00Z","relpermalink":"/2022/plotting-two-way-interactions-from-mixed-effects-models-using-alias-variables/","section":"post","summary":"Whereas the direction of main effects can be interpreted from the sign of the estimate, the interpretation of interaction effects often requires plots. This task is facilitated by the R package sjPlot (Lüdecke, 2022). In Bernabeu (2022), the sjPlot function called plot_model served as the basis for the creation of some custom functions. One of these functions is alias_interaction_plot, which allows the plotting of interactions between a continuous variable and a categorical variable.","tags":["R","visualisation","plotting","alias","sjPlot","linear mixed-effects models","statistics","s"],"title":"Plotting two-way interactions from mixed-effects models using alias variables","type":"post"},{"authors":[],"categories":["R","data visualisation"],"content":"\rWhereas the direction of main effects can be interpreted from the sign of the estimate, the interpretation of interaction effects often requires plots. This task is facilitated by the R package sjPlot (Lüdecke, 2022). In Bernabeu (2022), the sjPlot function called plot_model served as the basis for the creation of some custom functions. Two of these functions are deciles_interaction_plot and sextiles_interaction_plot. These functions allow the plotting of interactions between two continuous variables. In the case of deciles_interaction_plot, one of the variables is divided into ten bins, known as deciles, and the other variable is unchanged. In the case of sextiles_interaction_plot, one of the variables is divided into six bins, or sextiles, and the other variable is unchanged.\nBelow, we’ll use these functions with models fitted using lmerTest (Kuznetsova et al., 2022), although the functions also work with several other models (see sjPlot manual). The plots can be reproduced using the materials at https://osf.io/gt5uf.\nDeciles interaction plot\rThe function\r\r\rThe function in use\r\r\n\r\rSextiles interaction plot\rThe function\r\r\rThe function in use\r\r\n\rReferences\rBernabeu, P. (2022). Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power. Lancaster University. https://doi.org/10.17635/lancaster/thesis/1795\nKuznetsova, A., Brockhoff, P. B., \u0026amp; Christensen, R. H. B. (2022). Package ’lmerTest’. CRAN. https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf\nLüdecke, D. (2022). Package ’sjPlot’. CRAN. https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf\n\r\r","date":1672012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672012800,"objectID":"795ff41b768a6f22abfeca4c3ccbf2f7","permalink":"https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-ten-or-six-bins/","publishdate":"2022-12-26T00:00:00Z","relpermalink":"/2022/plotting-two-way-interactions-from-mixed-effects-models-using-ten-or-six-bins/","section":"post","summary":"Whereas the direction of main effects can be interpreted from the sign of the estimate, the interpretation of interaction effects often requires plots. This task is facilitated by the R package sjPlot (Lüdecke, 2022). In Bernabeu (2022), the sjPlot function called plot_model served as the basis for the creation of some custom functions. Two of these functions are deciles_interaction_plot and sextiles_interaction_plot. These functions allow the plotting of interactions between two continuous variables.","tags":["R","visualisation","plotting","deciles","sextiles","sjPlot","linear mixed-effects models","statistics","s"],"title":"Plotting two-way interactions from mixed-effects models using ten or six bins","type":"post"},{"authors":[],"categories":["R","statistics"],"content":"\r\r\rwindow.xaringanExtraClipboard(null, {\"button\":\"Copy Code\",\"success\":\"Copied!\",\"error\":\"Press Ctrl+C to Copy\"})\rFrequentist and Bayesian statistics are sometimes regarded as fundamentally different philosophies. Indeed, can both methods qualify as philosophies, or is one of them just a pointless ritual? Is frequentist statistics about \\(p\\) values only? Are frequentist estimates diametrically opposed to Bayesian posterior distributions? Are confidence intervals and credible intervals irreconcilable? Will R crash if lmerTest and brms are simultaneously loaded? If only we could fit frequentist and Bayesian models to the same data and plot the results together, we might get a glimpse into these puzzles.\nAll the analyses shown below can be reproduced using the materials at https://osf.io/gt5uf. The combination of the frequentist and the Bayesian estimates in the same plot is achieved using the following custom function from Bernabeu (2022).\nVisualising frequentist and Bayesian estimates in one plot\rBoth frequentist and Bayesian statistics offer the options of hypothesis testing and parameter estimation (Cumming, 2014; Kruschke \u0026amp; Liddell, 2018; Rouder et al., 2018; Schmalz et al., 2022; Tendeiro \u0026amp; Kiers, 2019, 2022; van Ravenzwaaij \u0026amp; Wagenmakers, 2022). In the statistical analyses conducted by Bernabeu (2022), hypothesis testing was performed within the frequentist framework, whereas parameter estimation was performed within both the frequentist and the Bayesian frameworks (for other examples of the estimation approach, see Milek et al., 2018; Pregla et al., 2021; Rodríguez-Ferreiro et al., 2020).\n\r\nWe’ll load the function from GitHub.\nsource(\u0026#39;https://raw.githubusercontent.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/main/R_functions/frequentist_bayesian_plot.R\u0026#39;)\r# Presenting the frequentist and the Bayesian estimates in the same plot. # For this purpose, the frequentist results are merged into a plot from # brms::mcmc_plot()\r# install.packages(\u0026#39;devtools\u0026#39;)\r# library(devtools)\r# install_version(\u0026#39;tidyverse\u0026#39;, \u0026#39;1.3.1\u0026#39;) # Due to breaking changes, Version 1.3.1 is required.\r# install_version(\u0026#39;ggplot2\u0026#39;, \u0026#39;5.3.5\u0026#39;) # Due to breaking changes, Version 5.3.5 is required.\rlibrary(tidyverse)\rlibrary(ggplot2)\rlibrary(Cairo)\r# Load frequentist coefficients (estimates and confidence intervals)\rKR_summary_semanticpriming_lmerTest =\rreadRDS(gzcon(url(\u0026#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/KR_summary_semanticpriming_lmerTest.rds?raw=true\u0026#39;)))\rconfint_semanticpriming_lmerTest =\rreadRDS(gzcon(url(\u0026#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/confint_semanticpriming_lmerTest.rds?raw=true\u0026#39;)))\r# Below are the default names of the effects\r# rownames(KR_summary_semanticpriming_lmerTest$coefficients)\r# rownames(confint_semanticpriming_lmerTest)\r# Load Bayesian posterior distributions\rsemanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian = readRDS(gzcon(url(\u0026#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian.rds?raw=true\u0026#39;)))\r# Below are the default names of the effects\r# levels(semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter)\r# Reorder the components of interactions in the frequentist results to match # with the order present in the Bayesian results.\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) =\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_recoded_interstimulus_interval:z_cosine_similarity\u0026#39;, replacement = \u0026#39;z_cosine_similarity:z_recoded_interstimulus_interval\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_recoded_interstimulus_interval:z_visual_rating_diff\u0026#39;, replacement = \u0026#39;z_visual_rating_diff:z_recoded_interstimulus_interval\u0026#39;)\rrownames(confint_semanticpriming_lmerTest) = rownames(confint_semanticpriming_lmerTest) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_recoded_interstimulus_interval:z_cosine_similarity\u0026#39;, replacement = \u0026#39;z_cosine_similarity:z_recoded_interstimulus_interval\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_recoded_interstimulus_interval:z_visual_rating_diff\u0026#39;, replacement = \u0026#39;z_visual_rating_diff:z_recoded_interstimulus_interval\u0026#39;)\r# Create a vector containing the names of the effects. This vector will be passed # to the plotting function.\rnew_labels = semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter %\u0026gt;% unique %\u0026gt;%\r# Remove the default \u0026#39;b_\u0026#39; from the beginning of each effect\rstr_remove(\u0026#39;^b_\u0026#39;) %\u0026gt;%\r# Put Intercept in parentheses\rstr_replace(pattern = \u0026#39;Intercept\u0026#39;, replacement = \u0026#39;(Intercept)\u0026#39;) %\u0026gt;%\r# First, adjust names of variables (both in main effects and in interactions)\rstr_replace(pattern = \u0026#39;z_target_word_frequency\u0026#39;,\rreplacement = \u0026#39;Target-word frequency\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_target_number_syllables\u0026#39;,\rreplacement = \u0026#39;Number of target-word syllables\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_word_concreteness_diff\u0026#39;,\rreplacement = \u0026#39;Word-concreteness difference\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_cosine_similarity\u0026#39;,\rreplacement = \u0026#39;Language-based similarity\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_visual_rating_diff\u0026#39;,\rreplacement = \u0026#39;Visual-strength difference\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_attentional_control\u0026#39;,\rreplacement = \u0026#39;Attentional control\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_vocabulary_size\u0026#39;,\rreplacement = \u0026#39;Vocabulary size\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_recoded_participant_gender\u0026#39;,\rreplacement = \u0026#39;Gender\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_recoded_interstimulus_interval\u0026#39;,\rreplacement = \u0026#39;SOA\u0026#39;) %\u0026gt;%\r# Show acronym in main effect of SOA\rstr_replace(pattern = \u0026#39;^SOA$\u0026#39;,\rreplacement = \u0026#39;Stimulus onset asynchrony (SOA)\u0026#39;) %\u0026gt;%\r# Second, adjust order of effects in interactions. In the output from the model, # the word-level variables of interest (i.e., \u0026#39;z_cosine_similarity\u0026#39; and # \u0026#39;z_visual_rating_diff\u0026#39;) sometimes appeared second in their interactions. For # better consistency, the code below moves those word-level variables (with # their new names) to the first position in their interactions. Note that the # order does not affect the results in any way.\rsub(\u0026#39;(\\\\w+.*):(Language-based similarity|Visual-strength difference)\u0026#39;, \u0026#39;\\\\2:\\\\1\u0026#39;, .) %\u0026gt;%\r# Replace colons denoting interactions with times symbols\rstr_replace(pattern = \u0026#39;:\u0026#39;, replacement = \u0026#39; \u0026amp;times; \u0026#39;)\r# Create plot\rplot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian =\rfrequentist_bayesian_plot(KR_summary_semanticpriming_lmerTest,\rconfint_semanticpriming_lmerTest,\rsemanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian,\rlabels = new_labels, interaction_symbol_x = TRUE,\rvertical_line_at_x = 0, x_title = \u0026#39;Effect size (\u0026amp;beta;)\u0026#39;,\rlegend_ncol = 1) + theme(legend.position = \u0026#39;bottom\u0026#39;)\r\r\nplot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian\rFrequentist and Bayesian estimates are not so polar opposites, are they? What is more, the larger differences between some estimates are the result of the priors that were set on the corresponding effects. With uninformative priors, the frequentist and the Bayesian estimates are virtually identical.\n\rNow it’s time to consider in earnest:\n\rIs frequentist statistics about \\(p\\) values only? Are frequentist estimates diametrically opposed to Bayesian posterior distributions? Are confidence intervals and credible intervals irreconcilable? Will R crash if lmerTest and brms are simultaneously loaded?\n\r\rSession info\rIf you encounter any blockers while reproducing the above analyses using the materials at https://osf.io/gt5uf, my current session info may be useful. For instance, the legend of the plot may not show if the latest versions of the ggplot2 and the tidyverse packages are used. Instead, ggplot2 3.3.5 and tidyverse 1.3.1 should be installed using install_version('ggplot2', '3.3.5') and install_version('tidyverse', '1.3.1').\nsessionInfo()\r## R version 4.2.3 (2023-03-15 ucrt)\r## Platform: x86_64-w64-mingw32/x64 (64-bit)\r## Running under: Windows 10 x64 (build 22621)\r## ## Matrix products: default\r## ## locale:\r## [1] LC_COLLATE=English_United States.utf8 ## [2] LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8\r## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## attached base packages:\r## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages:\r## [1] ggtext_0.1.2 Cairo_1.6-0 forcats_1.0.0 ## [4] stringr_1.5.0 dplyr_1.1.1 purrr_1.0.1 ## [7] readr_2.1.4 tidyr_1.3.0 tibble_3.2.1 ## [10] ggplot2_3.3.5 tidyverse_1.3.1 knitr_1.42 ## [13] xaringanExtra_0.7.0\r## ## loaded via a namespace (and not attached):\r## [1] Rcpp_1.0.10 lubridate_1.9.2 digest_0.6.31 utf8_1.2.3 ## [5] plyr_1.8.8 R6_2.5.1 cellranger_1.1.0 ggridges_0.5.4 ## [9] backports_1.4.1 reprex_2.0.2 evaluate_0.21 highr_0.10 ## [13] httr_1.4.6 blogdown_1.16 pillar_1.9.0 rlang_1.1.0 ## [17] uuid_1.1-0 readxl_1.4.2 rstudioapi_0.14 jquerylib_0.1.4 ## [21] rmarkdown_2.21 labeling_0.4.2 munsell_0.5.0 gridtext_0.1.5 ## [25] broom_1.0.4 compiler_4.2.3 modelr_0.1.11 xfun_0.38 ## [29] pkgconfig_2.0.3 htmltools_0.5.5 tidyselect_1.2.0 bookdown_0.33.3 ## [33] fansi_1.0.4 crayon_1.5.2 tzdb_0.4.0 dbplyr_2.3.2 ## [37] withr_2.5.0 commonmark_1.9.0 grid_4.2.3 jsonlite_1.8.4 ## [41] gtable_0.3.3 lifecycle_1.0.3 DBI_1.1.3 magrittr_2.0.3 ## [45] scales_1.2.1 cli_3.4.1 stringi_1.7.12 cachem_1.0.7 ## [49] farver_2.1.1 fs_1.6.1 xml2_1.3.3 bslib_0.4.2 ## [53] generics_0.1.3 vctrs_0.6.1 tools_4.2.3 glue_1.6.2 ## [57] markdown_1.5 hms_1.1.3 fastmap_1.1.1 yaml_2.3.7 ## [61] timechange_0.2.0 colorspace_2.1-0 rvest_1.0.3 haven_2.5.2 ## [65] sass_0.4.6\r\rReferences\rBernabeu, P. (2022). Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power. Lancaster University. https://doi.org/10.17635/lancaster/thesis/1795\nCumming, G. (2014). The new statistics: Why and how. Psychological Science, 25(1), 7–29. https://doi.org/10.1177/0956797613504966\nKruschke, J. K., \u0026amp; Liddell, T. M. (2018). The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. Psychonomic Bulletin \u0026amp; Review, 25(1), 178–206.\nMilek, A., Butler, E. A., Tackman, A. M., Kaplan, D. M., Raison, C. L., Sbarra, D. A., Vazire, S., \u0026amp; Mehl, M. R. (2018). “Eavesdropping on happiness” revisited: A pooled, multisample replication of the association between life satisfaction and observed daily conversation quantity and quality. Psychological Science, 29(9), 1451–1462. https://doi.org/10.1177/0956797618774252\nPregla, D., Lissón, P., Vasishth, S., Burchert, F., \u0026amp; Stadie, N. (2021). Variability in sentence comprehension in aphasia in German. Brain and Language, 222, 105008. https://doi.org/10.1016/j.bandl.2021.105008\nRodríguez-Ferreiro, J., Aguilera, M., \u0026amp; Davies, R. (2020). Semantic priming and schizotypal personality: Reassessing the link between thought disorder and enhanced spreading of semantic activation. PeerJ, 8, e9511. https://doi.org/10.7717/peerj.9511\nRouder, J. N., Haaf, J. M., \u0026amp; Vandekerckhove, J. (2018). Bayesian inference for psychology, part IV: Parameter estimation and Bayes factors. Psychonomic Bulletin \u0026amp; Review, 25(1), 102–113. https://doi.org/10.3758/s13423-017-1420-7\nSchmalz, X., Biurrun Manresa, J., \u0026amp; Zhang, L. (2021). What is a Bayes factor? Psychological Methods. https://doi.org/10.1037/met0000421\nTendeiro, J. N., \u0026amp; Kiers, H. A. L. (2019). A review of issues about null hypothesis Bayesian testing. Psychological Methods, 24(6), 774–795. https://doi.org/10.1037/met0000221\nTendeiro, J. N., \u0026amp; Kiers, H. A. L. (2022). On the white, the black, and the many shades of gray in between: Our reply to van Ravenzwaaij and Wagenmakers (2021). Psychological Methods, 27(3), 466–475. https://doi.org/10.1037/met0000505\nvan Ravenzwaaij, D., \u0026amp; Wagenmakers, E.-J. (2022). Advantages masquerading as “issues” in Bayesian hypothesis testing: A commentary on Tendeiro and Kiers (2019). Psychological Methods, 27(3), 451–465. https://doi.org/10.1037/met0000415\n\r\r","date":1671753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671753600,"objectID":"892657f2f258f7626a3a632b6c423db1","permalink":"https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/","publishdate":"2022-12-23T00:00:00Z","relpermalink":"/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/","section":"post","summary":"Frequentist and Bayesian statistics are sometimes regarded as fundamentally different philosophies. Indeed, can both qualify as philosophies or is one of them just a pointless ritual? Is frequentist statistics only about $p$ values? Are frequentist estimates diametrically opposed to Bayesian posterior distributions? Are confidence intervals and credible intervals irreconcilable? Will R crash if lmerTest and brms are simultaneously loaded?","tags":["statistics","Bayesian statistics","frequentist statistics","linear-mixed effects models","lmerTest","brms","plotting","data visualisation","s"],"title":"Why can't we be friends? Plotting frequentist (lmerTest) and Bayesian (brms) mixed-effects models","type":"post"},{"authors":[],"categories":["R","statistics"],"content":"\r\r\rwindow.xaringanExtraClipboard(null, {\"button\":\"Copy Code\",\"success\":\"Copied!\",\"error\":\"Press Ctrl+C to Copy\"})\rlibrary(dplyr)\rlibrary(ggplot2)\rlibrary(ggridges)\rlibrary(ggtext)\rlibrary(patchwork)\rlibrary(papaja)\rThis post presents a code-through of a Bayesian workflow in R, which can be reproduced using the materials at https://osf.io/gt5uf. The content is closely based on Bernabeu (2022), which was in turn based on lots of other references. In addition to those, you may wish to consider Nicenboim et al. (n.d.), a book in preparation that is already available online (https://vasishth.github.io/bayescogsci/book).\nIn Bernabeu (2022), a Bayesian analysis was performed to complement the estimates that had been obtained in the frequentist analysis. Whereas the goal of the frequentist analysis had been hypothesis testing, for which \\(p\\) values were used, the goal of the Bayesian analysis was parameter estimation. Accordingly, we estimated the posterior distribution of every effect, without calculating Bayes factors (for other examples of the same estimation approach, see Milek et al., 2018; Pregla et al., 2021; Rodríguez-Ferreiro et al., 2020; for comparisons between estimation and hypothesis testing, see Cumming, 2014; Kruschke \u0026amp; Liddell, 2018; Rouder et al., 2018; Schmalz et al., 2021; Tendeiro \u0026amp; Kiers, 2019, in press; van Ravenzwaaij \u0026amp; Wagenmakers, 2021). In the estimation approach, the estimates are interpreted by considering the position of their credible intervals in relation to the expected effect size. That is, the closer an interval is to an effect size of 0, the smaller the effect of that predictor. For instance, an interval that is symmetrically centred on 0 indicates a very small effect, whereas—in comparison—an interval that does not include 0 at all indicates a far larger effect.\nThis analysis served two purposes: first, to ascertain the interpretation of the smaller effects—which were identified as unreliable in the power analyses—, and second, to complement the estimates obtained in the frequentist analysis. The latter purpose was pertinent because the frequentist models presented convergence warnings—even though it must be noted that a previous study found that frequentist and Bayesian estimates were similar despite convergence warnings appearing in the frequentist analysis (Rodríguez-Ferreiro et al., 2020). Furthermore, the complementary analysis was pertinent because the frequentist models presented residual errors that deviated from normality—even though mixed-effects models are fairly robust to such a deviation (Knief \u0026amp; Forstmeier, 2021; Schielzeth et al., 2020). Owing to these precedents, we expected to find broadly similar estimates in the frequentist analyses and in the Bayesian ones. Across studies, each frequentist model has a Bayesian counterpart, with the exception of the secondary analysis performed in Study 2.1 (semantic priming) that included vision-based similarity as a predictor. The R package ‘brms’, Version 2.17.0, was used for the Bayesian analysis (Bürkner, 2018; Bürkner et al., 2022).\nPriors\rPriors are one of the hardest nuts to crack in Bayesian statistics. First, it can be useful to inspect what priors can be set in the model. Second, it is important to visualise a reasonable set of priors based on the available literature or any other available sources. Third, just before fitting the model, the adequacy of a range of priors should be assessed using prior predictive checks. Fourth, posterior predictive checks were performed to assess the consistency between the observed data and new data predicted by the posterior distributions. Fifth, the influence of the priors on the results should be assessed through a prior sensitivity analysis (Lee \u0026amp; Wagenmakers, 2014; Van de Schoot et al., 2021; also see Bernabeu, 2022; Pregla et al., 2021; Rodríguez-Ferreiro et al., 2020; Stone et al., 2021; Stone et al., 2020).\n\r1. Checking what priors can be set\rThe brms::get_prior function can be used to check what effects in the model can be assigned a prior. The output (see example) will include the current (perhaps default) prior on each effect.\n\r\n\r2. Determining the priors\rThe priors were established by inspecting the effect sizes obtained in previous studies as well as the effect sizes obtained in our frequentist analyses of the present data (reported in Studies 2.1, 2.2 and 2.3 below). In the first regard, the previous studies that were considered were selected because the experimental paradigms, variables and analytical procedures they had used were similar to those used in our current studies. Specifically, regarding paradigms, we sought studies that implemented: (I) semantic priming with a lexical decision task—as in Study 2.1—, (II) semantic decision—as in Study 2.2—, or (III) lexical decision—as in Study 2.3. Regarding analytical procedures, we sought studies in which both the dependent and the independent variables were \\(z\\)-scored. We found two studies that broadly matched these criteria: Lim et al. (2020) (see Table 5 therein) and Pexman \u0026amp; Yap (2018) (see Tables 6 and 7 therein). Out of these studies, Pexman \u0026amp; Yap (2018) contained the variables that were most similar to ours, which included vocabulary size (labelled ‘NAART’) and word frequency.\nBased on both these studies and on the frequentist analyses reported below, a range of effect sizes was identified that spanned between β = -0.30 and β = 0.30. This range was centred around 0 as the variables were \\(z\\)-scored. The bounds of this range were determined by the largest effects, which appeared in Pexman \u0026amp; Yap (2018). Pexman et al. conducted a semantic decision study, and split the data set into abstract and concrete words. The two largest effects they found were—first—a word concreteness effect in the concrete-words analysis of β = -0.41, and—second—a word concreteness effect in the abstract-words analysis of β = 0.20. Unlike Pexman et al., we did not split the data set into abstract and concrete words, but analysed these sets together. Therefore, we averaged between the aforementioned values, obtaining a range between β = -0.30 and β = 0.30.\nIn the results of Lim et al. (2020) and Pexman \u0026amp; Yap (2018), and in our frequentist results, some effects consistently presented a negative polarity (i.e., leading to shorter response times), whereas some other effects were consistently positive. We incorporated the direction of effects into the priors only in cases of large effects that had presented a consistent direction (either positive or negative) in previous studies and in our frequentist analyses in the present studies. These criteria were matched by the following variables: word frequency—with a negative direction, as higher word frequency leads to shorter RTs (Brysbaert et al., 2018; Brysbaert et al., 2016; Lim et al., 2020; Mendes \u0026amp; Undorf, 2021; Pexman \u0026amp; Yap, 2018)—, number of letters and number of syllables—both with positive directions (Barton et al., 2014; Beyersmann et al., 2020; Pexman \u0026amp; Yap, 2018)—, and orthographic Levenshtein distance—with a positive direction (Cerni et al., 2016; Dijkstra et al., 2019; Kim et al., 2018; Yarkoni et al., 2008). We did not incorporate information about the direction of the word concreteness effect, as this effect can follow different directions in abstract and concrete words (Brysbaert et al., 2014; Pexman \u0026amp; Yap, 2018), and we analysed both sets of words together. In conclusion, the four predictors that had directional priors were covariates. All the other predictors had priors centred on 0. Last, as a methodological matter, it is noteworthy that most of the psycholinguistic studies applying Bayesian analysis have not incorporated any directional information in priors (e.g., Pregla et al., 2021; Rodríguez-Ferreiro et al., 2020; Stone et al., 2020; cf. Stone et al., 2021).\nPrior distributions\rThe choice of priors can influence the results in consequential ways. To assess the extent of this influence, prior sensitivity analyses have been recommended. These analyses are performed by comparing the effect of more and less strict priors—or, in other words, priors varying in their degree of informativeness. The degree of variation is adjusted through the standard deviation, and the means are not varied (Lee \u0026amp; Wagenmakers, 2014; Stone et al., 2020; Van de Schoot et al., 2021).\nIn this way, we compared the results obtained using ‘informative’ priors (\\(SD\\) = 0.1), ‘weakly-informative’ priors (\\(SD\\) = 0.2) and ‘diffuse’ priors (\\(SD\\) = 0.3). These standard deviations were chosen so that around 95% of values in the informative priors would fall within our initial range of effect sizes that spanned from -0.30 to 0.30. All priors are illustrated in the figure below. These priors resembled others from previous psycholinguistic studies (Pregla et al., 2021; Stone et al., 2021; Stone et al., 2020). For instance, Stone et al. (2020) used the following priors: \\(Normal\\)(0, 0.1), \\(Normal\\)(0, 0.3) and \\(Normal\\)(0, 1). The range of standard deviations we used—i.e., 0.1, 0.2 and 0.3—was narrower than those of previous studies because our dependent variable and our predictors were \\(z\\)-scored, resulting in small estimates and small \\(SD\\)s (see Lim et al., 2020; Pexman \u0026amp; Yap, 2018). These priors were used on the fixed effects and on the standard deviation parameters of the fixed effects. For the correlations among the random effects, an \\(LKJ\\)(2) prior was used (Lewandowski et al., 2009). This is a ‘regularising’ prior, as it assumes that high correlations among random effects are rare (also used in Rodríguez-Ferreiro et al., 2020; Stone et al., 2021; Stone et al., 2020; Vasishth et al., 2018).\n# Set seed number to ensure exact reproducibility # of the random distributions\rset.seed(123)\r# The code below plots all our types of priors. Each distribution # contains 10,000 simulations, resulting in 90,000 rows.\r# The green vertical rectangle shows the range of plausible effect # sizes based on previous studies that applied a similar analysis # (Lim et al., 2020, https://doi.org/10.1177/1747021820906566; # Pexman \u0026amp; Yap, 2018, https://doi.org/10.1037/xlm0000499) as # well as on the frequentist analyses of the current data.\rpriors = data.frame(\rinformativeness = as.factor(c(rep(\u0026#39;Informative priors (*SD* = 0.1)\u0026#39;, 30000),\rrep(\u0026#39;Weakly-informative priors (*SD* = 0.2)\u0026#39;, 30000),\rrep(\u0026#39;Diffuse priors (*SD* = 0.3)\u0026#39;, 30000))), direction = as.factor(c(rep(\u0026#39;negative\u0026#39;, 10000), rep(\u0026#39;neutral\u0026#39;, 10000),\rrep(\u0026#39;positive\u0026#39;, 10000),\rrep(\u0026#39;negative\u0026#39;, 10000), rep(\u0026#39;neutral\u0026#39;, 10000),\rrep(\u0026#39;positive\u0026#39;, 10000),\rrep(\u0026#39;negative\u0026#39;, 10000), rep(\u0026#39;neutral\u0026#39;, 10000),\rrep(\u0026#39;positive\u0026#39;, 10000))),\rdirection_and_distribution = as.factor(c(rep(\u0026#39;Negative (*M* = -0.1)\u0026lt;br\u0026gt;*Normal*(-0.1, 0.1)\u0026#39;, 10000), rep(\u0026#39;Neutral (*M* = 0)\u0026lt;br\u0026gt;*Normal*(0, 0.1)\u0026#39;, 10000),\rrep(\u0026#39;Positive (*M* = 0.1)\u0026lt;br\u0026gt;*Normal*(0.1, 0.1)\u0026#39;, 10000),\rrep(\u0026#39;Negative (*M* = -0.1)\u0026lt;br\u0026gt;*Normal*(-0.1, 0.2)\u0026#39;, 10000),\rrep(\u0026#39;Neutral (*M* = 0)\u0026lt;br\u0026gt;*Normal*(0, 0.2)\u0026#39;, 10000),\rrep(\u0026#39;Positive (*M* = 0.1)\u0026lt;br\u0026gt;*Normal*(0.1, 0.2)\u0026#39;, 10000),\rrep(\u0026#39;Negative (*M* = -0.1)\u0026lt;br\u0026gt;*Normal*(-0.1, 0.3)\u0026#39;, 10000), rep(\u0026#39;Neutral (*M* = 0)\u0026lt;br\u0026gt;*Normal*(0, 0.3)\u0026#39;, 10000),\rrep(\u0026#39;Positive (*M* = 0.1)\u0026lt;br\u0026gt;*Normal*(0.1, 0.3)\u0026#39;, 10000))),\restimate = c(rnorm(10000, m = -0.1, sd = 0.1),\rrnorm(10000, m = 0, sd = 0.1),\rrnorm(10000, m = 0.1, sd = 0.1),\rrnorm(10000, m = -0.1, sd = 0.2),\rrnorm(10000, m = 0, sd = 0.2),\rrnorm(10000, m = 0.1, sd = 0.2),\rrnorm(10000, m = -0.1, sd = 0.3),\rrnorm(10000, m = 0, sd = 0.3),\rrnorm(10000, m = 0.1, sd = 0.3))\r)\r# Order factor levels\rpriors$informativeness = ordered(priors$informativeness, levels = c(\u0026#39;Informative priors (*SD* = 0.1)\u0026#39;, \u0026#39;Weakly-informative priors (*SD* = 0.2)\u0026#39;, \u0026#39;Diffuse priors (*SD* = 0.3)\u0026#39;))\rpriors$direction = ordered(priors$direction, levels = c(\u0026#39;negative\u0026#39;, \u0026#39;neutral\u0026#39;, \u0026#39;positive\u0026#39;))\rpriors$direction_and_distribution =\rordered(priors$direction_and_distribution,\rlevels = c(\u0026#39;Negative (*M* = -0.1)\u0026lt;br\u0026gt;*Normal*(-0.1, 0.1)\u0026#39;, \u0026#39;Neutral (*M* = 0)\u0026lt;br\u0026gt;*Normal*(0, 0.1)\u0026#39;,\r\u0026#39;Positive (*M* = 0.1)\u0026lt;br\u0026gt;*Normal*(0.1, 0.1)\u0026#39;,\r\u0026#39;Negative (*M* = -0.1)\u0026lt;br\u0026gt;*Normal*(-0.1, 0.2)\u0026#39;, \u0026#39;Neutral (*M* = 0)\u0026lt;br\u0026gt;*Normal*(0, 0.2)\u0026#39;,\r\u0026#39;Positive (*M* = 0.1)\u0026lt;br\u0026gt;*Normal*(0.1, 0.2)\u0026#39;,\r\u0026#39;Negative (*M* = -0.1)\u0026lt;br\u0026gt;*Normal*(-0.1, 0.3)\u0026#39;, \u0026#39;Neutral (*M* = 0)\u0026lt;br\u0026gt;*Normal*(0, 0.3)\u0026#39;,\r\u0026#39;Positive (*M* = 0.1)\u0026lt;br\u0026gt;*Normal*(0.1, 0.3)\u0026#39;))\r# PLOT zone\rcolours = c(\u0026#39;#7276A2\u0026#39;, \u0026#39;black\u0026#39;, \u0026#39;#A27272\u0026#39;)\rfill_colours = c(\u0026#39;#CCCBE7\u0026#39;, \u0026#39;#D7D7D7\u0026#39;, \u0026#39;#E7CBCB\u0026#39;)\r# Initialise plot (`aes` specified separately to allow # use of `geom_rect` at the end)\rggplot() +\r# Turn to the distributions\rstat_density_ridges(data = priors, aes(x = estimate, y = direction_and_distribution, color = direction, fill = direction),\rgeom = \u0026#39;density_ridges_gradient\u0026#39;, alpha = 0.7, jittered_points = TRUE, quantile_lines = TRUE, quantiles = c(0.025, 0.975), show.legend = F) +\rscale_color_manual(values = colours) + scale_fill_manual(values = fill_colours) + # Adjust X axis to the random distributions obtained\rscale_x_continuous(limits = c(min(priors$estimate), max(priors$estimate)), n.breaks = 6, expand = c(0.04, 0.04)) +\rscale_y_discrete(expand = expansion(add = c(0.18, 1.9))) +\r# Facets containing the three models varying in informativeness\rfacet_wrap(vars(informativeness), scales = \u0026#39;free\u0026#39;, dir = \u0026#39;v\u0026#39;) +\r# Vertical line at x = 0\rgeom_vline(xintercept = 0, linetype = \u0026#39;dashed\u0026#39;, color = \u0026#39;grey50\u0026#39;) +\rxlab(\u0026#39;Effect size (\u0026amp;beta;)\u0026#39;) + ylab(\u0026#39;Direction of the prior and corresponding distribution\u0026#39;) +\rtheme_minimal() +\rtheme(axis.title.x = ggtext::element_markdown(size = 12, margin = margin(t = 9)),\raxis.text.x = ggtext::element_markdown(size = 11, margin = margin(t = 4)),\raxis.title.y = ggtext::element_markdown(size = 12, margin = margin(r = 9)),\raxis.text.y = ggtext::element_markdown(lineheight = 1.6, colour = colours),\rstrip.background = element_rect(fill = \u0026#39;grey98\u0026#39;, colour = \u0026#39;grey90\u0026#39;,\rlinetype = \u0026#39;solid\u0026#39;),\rstrip.text = element_markdown(size = 11, margin = margin(t = 7, b = 7)),\rpanel.spacing.y = unit(9, \u0026#39;pt\u0026#39;), panel.grid.minor = element_blank(), plot.margin = margin(8, 8, 9, 8)\r) +\r# Shaded rectangle containing range of previous effects\rgeom_rect(data = data.frame(x = 1), xmin = -0.3, xmax = 0.3, ymin = -Inf, ymax = Inf, fill = \u0026#39;darkgreen\u0026#39;, alpha = .3)\r\rPriors used in the three studies. The green vertical rectangle shows the range of plausible effect sizes based on previous studies and on our frequentist analyses. In the informative priors, around 95% of the values fall within the range.\n\r\n\r\r3. Prior predictive checks\rThe adequacy of each of these priors was assessed by performing prior predictive checks, in which we compared the observed data to the predictions of the model (Van de Schoot et al., 2021). Furthermore, in these checks we also tested the adequacy of two model-wide distributions: the traditional Gaussian distribution (default in most analyses) and an exponentially modified Gaussian—dubbed ‘ex-Gaussian’—distribution (Matzke \u0026amp; Wagenmakers, 2009). The ex-Gaussian distribution was considered because the residual errors of the frequentist models were not normally distributed (Lo \u0026amp; Andrews, 2015), and because this distribution was found to be more appropriate than the Gaussian one in a previous, related study (see supplementary materials of Rodríguez-Ferreiro et al., 2020). The ex-Gaussian distribution had an identity link function, which preserves the interpretability of the coefficients, as opposed to a transformation applied directly to the dependent variable (Lo \u0026amp; Andrews, 2015). The results of these prior predictive checks revealed that the priors were adequate, and that the ex-Gaussian distribution was more appropriate than the Gaussian one, converging with Rodríguez-Ferreiro et al. (2020). Therefore, the ex-Gaussian distribution was used in the final models.\n\rModels with a Gaussian distribution\rThe figures below show the prior predictive checks for the Gaussian models. These plots show the maximum, mean and minimum values of the observed data (\\(y\\)) and those of the predicted distribution (\\(y_{rep}\\), which stands for replications of the outcome). The way of interpreting these plots is by comparing the observed data to the predicted distribution. The specifics of this comparison vary across the three plots. First, in the upper plot, which shows the maximum values, the ideal scenario would show the observed maximum value (\\(y\\)) overlapping with the maximum value of the predicted distribution (\\(y_{rep}\\)). Second, in the middle plot, showing the mean values, the ideal scenario would show the observed mean value (\\(y\\)) overlapping with the mean value of the predicted distribution (\\(y_{rep}\\)). Last, in the lower plot, which shows the minimum values, the ideal scenario would have the observed minimum value (\\(y\\)) overlapping with the minimum value of the predicted distribution (\\(y_{rep}\\)). While the overlap need not be absolute, the closer the observed and the predicted values are on the X axis, the better. As such, the three predictive checks below—corresponding to models that used the default Gaussian distribution—show that the priors fitted the data acceptably but not very well.\nSee plot on GitHub\n\rPrior predictive checks for the Gaussian, informative prior model from the semantic priming study. \\(y\\) = observed data; \\(y_{rep}\\) = predicted data.\n\rSee plot on GitHub\n\rPrior predictive checks for the Gaussian, weakly-informative prior model from the semantic priming study. \\(y\\) = observed data; \\(y_{rep}\\) = predicted data.\n\rSee plot on GitHub\n\rPrior predictive checks for the Gaussian, diffuse prior model from the semantic priming study. \\(y\\) = observed data; \\(y_{rep}\\) = predicted data.\n\r\rModels with an exponentially-modified Gaussian (i.e., ex-Gaussian) distribution\rIn contrast to the above results, the figures below demonstrate that, when an ex-Gaussian distribution was used, the priors fitted the data far better, which converged with the results of a similar comparison performed by Rodríguez-Ferreiro et al. (2020; see supplementary materials of the latter study).\nSee plot on GitHub\n\rPrior predictive checks for the ex-Gaussian, informative prior model from the semantic priming study. \\(y\\) = observed data; \\(y_{rep}\\) = predicted data.\n\rSee plot on GitHub\n\rPrior predictive checks for the ex-Gaussian, weakly-informative prior model from the semantic priming study. \\(y\\) = observed data; \\(y_{rep}\\) = predicted data.\n\rSee plot on GitHub\n\rPrior predictive checks for the ex-Gaussian, diffuse prior model from the semantic priming study. \\(y\\) = observed data; \\(y_{rep}\\) = predicted data.\n\r\n\r\r4. Posterior predictive checks\rBased on the results from the prior predictive checks, the ex-Gaussian distribution was used in the final models. Next, posterior predictive checks were performed to assess the consistency between the observed data and new data predicted by the posterior distributions (Van de Schoot et al., 2021). The figure below presents the posterior predictive checks for the latter models. The interpretation of these plots is simple: the distributions of the observed (\\(y\\)) and the predicted data (\\(y_{rep}\\)) should be as similar as possible. As such, the plots below suggest that the results are trustworthy.\n\rSee plot on GitHub\n\rPosterior predictive checks for the (ex-Gaussian) models from the semantic priming study. The observed data (\\(y\\)) and the predicted data (\\(y_{rep}\\)) almost entirely overlap with each other, demonstrating a very good fit.\n\r\n\r5. Prior sensitivity analysis\rIn the main analysis, the informative, weakly-informative and diffuse priors were used in separate models. In other words, in each model, all priors had the same degree of informativeness (as done in Pregla et al., 2021; Rodríguez-Ferreiro et al., 2020; Stone et al., 2021; Stone et al., 2020). In this way, a prior sensitivity analysis was performed to acknowledge the likely influence of the priors on the posterior distributions—that is, on the results (Lee \u0026amp; Wagenmakers, 2014; Stone et al., 2020; Van de Schoot et al., 2021).\nWe’ll first load a custom function (frequentist_bayesian_plot) from GitHub.\nsource(\u0026#39;https://raw.githubusercontent.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/main/R_functions/frequentist_bayesian_plot.R\u0026#39;)\r# Presenting the frequentist and the Bayesian estimates in the same plot. # For this purpose, the frequentist results are merged into a plot from # brms::mcmc_plot()\r# install.packages(\u0026#39;devtools\u0026#39;)\r# library(devtools)\r# install_version(\u0026#39;tidyverse\u0026#39;, \u0026#39;1.3.1\u0026#39;) # Due to breaking changes, Version 1.3.1 is required.\r# install_version(\u0026#39;ggplot2\u0026#39;, \u0026#39;5.3.5\u0026#39;) # Due to breaking changes, Version 5.3.5 is required.\rlibrary(tidyverse)\rlibrary(ggplot2)\rlibrary(Cairo)\r# Load frequentist coefficients (estimates and confidence intervals)\rKR_summary_semanticpriming_lmerTest =\rreadRDS(gzcon(url(\u0026#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/KR_summary_semanticpriming_lmerTest.rds?raw=true\u0026#39;)))\rconfint_semanticpriming_lmerTest =\rreadRDS(gzcon(url(\u0026#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/confint_semanticpriming_lmerTest.rds?raw=true\u0026#39;)))\r# Below are the default names of the effects\r# rownames(KR_summary_semanticpriming_lmerTest$coefficients)\r# rownames(confint_semanticpriming_lmerTest)\r# Load Bayesian posterior distributions\rsemanticpriming_posteriordistributions_informativepriors_exgaussian = readRDS(gzcon(url(\u0026#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_posteriordistributions_informativepriors_exgaussian.rds?raw=true\u0026#39;)))\rsemanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian =\rreadRDS(gzcon(url(\u0026#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian.rds?raw=true\u0026#39;)))\rsemanticpriming_posteriordistributions_diffusepriors_exgaussian = readRDS(gzcon(url(\u0026#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_posteriordistributions_diffusepriors_exgaussian.rds?raw=true\u0026#39;)))\r# Below are the default names of the effects\r# levels(semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter)\r# Reorder the components of interactions in the frequentist results to match # with the order present in the Bayesian results.\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) =\rrownames(KR_summary_semanticpriming_lmerTest$coefficients) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_recoded_interstimulus_interval:z_cosine_similarity\u0026#39;, replacement = \u0026#39;z_cosine_similarity:z_recoded_interstimulus_interval\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_recoded_interstimulus_interval:z_visual_rating_diff\u0026#39;, replacement = \u0026#39;z_visual_rating_diff:z_recoded_interstimulus_interval\u0026#39;)\rrownames(confint_semanticpriming_lmerTest) = rownames(confint_semanticpriming_lmerTest) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_recoded_interstimulus_interval:z_cosine_similarity\u0026#39;, replacement = \u0026#39;z_cosine_similarity:z_recoded_interstimulus_interval\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_recoded_interstimulus_interval:z_visual_rating_diff\u0026#39;, replacement = \u0026#39;z_visual_rating_diff:z_recoded_interstimulus_interval\u0026#39;)\r# Create a vector containing the names of the effects. This vector will be passed # to the plotting function.\rnew_labels = semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter %\u0026gt;% unique %\u0026gt;%\r# Remove the default \u0026#39;b_\u0026#39; from the beginning of each effect\rstr_remove(\u0026#39;^b_\u0026#39;) %\u0026gt;%\r# Put Intercept in parentheses\rstr_replace(pattern = \u0026#39;Intercept\u0026#39;, replacement = \u0026#39;(Intercept)\u0026#39;) %\u0026gt;%\r# First, adjust names of variables (both in main effects and in interactions)\rstr_replace(pattern = \u0026#39;z_target_word_frequency\u0026#39;,\rreplacement = \u0026#39;Target-word frequency\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_target_number_syllables\u0026#39;,\rreplacement = \u0026#39;Number of target-word syllables\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_word_concreteness_diff\u0026#39;,\rreplacement = \u0026#39;Word-concreteness difference\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_cosine_similarity\u0026#39;,\rreplacement = \u0026#39;Language-based similarity\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_visual_rating_diff\u0026#39;, replacement = \u0026#39;Visual-strength difference\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_attentional_control\u0026#39;,\rreplacement = \u0026#39;Attentional control\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_vocabulary_size\u0026#39;,\rreplacement = \u0026#39;Vocabulary size\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_recoded_participant_gender\u0026#39;,\rreplacement = \u0026#39;Gender\u0026#39;) %\u0026gt;%\rstr_replace(pattern = \u0026#39;z_recoded_interstimulus_interval\u0026#39;,\rreplacement = \u0026#39;SOA\u0026#39;) %\u0026gt;%\r# Show acronym in main effect of SOA\rstr_replace(pattern = \u0026#39;^SOA$\u0026#39;,\rreplacement = \u0026#39;Stimulus onset asynchrony (SOA)\u0026#39;) %\u0026gt;%\r# Second, adjust order of effects in interactions. In the output from the model, # the word-level variables of interest (i.e., \u0026#39;z_cosine_similarity\u0026#39; and # \u0026#39;z_visual_rating_diff\u0026#39;) sometimes appeared second in their interactions. For # better consistency, the code below moves those word-level variables (with # their new names) to the first position in their interactions. Note that the # order does not affect the results in any way.\rsub(\u0026#39;(\\\\w+.*):(Language-based similarity|Visual-strength difference)\u0026#39;, \u0026#39;\\\\2:\\\\1\u0026#39;, .) %\u0026gt;%\r# Replace colons denoting interactions with times symbols\rstr_replace(pattern = \u0026#39;:\u0026#39;, replacement = \u0026#39; \u0026amp;times; \u0026#39;) # Create plots, beginning with the informative-prior model\rplot_semanticpriming_frequentist_bayesian_plot_informativepriors_exgaussian =\rfrequentist_bayesian_plot(KR_summary_semanticpriming_lmerTest,\rconfint_semanticpriming_lmerTest,\rsemanticpriming_posteriordistributions_informativepriors_exgaussian,\rlabels = new_labels, interaction_symbol_x = TRUE,\rvertical_line_at_x = 0, x_title = \u0026#39;Effect size (\u0026amp;beta;)\u0026#39;, x_axis_labels = 3, note_frequentist_no_prior = TRUE) +\rggtitle(\u0026#39;Prior *SD* = 0.1\u0026#39;)\r#####\rplot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian =\rfrequentist_bayesian_plot(KR_summary_semanticpriming_lmerTest,\rconfint_semanticpriming_lmerTest,\rsemanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian,\rlabels = new_labels, interaction_symbol_x = TRUE,\rvertical_line_at_x = 0, x_title = \u0026#39;Effect size (\u0026amp;beta;)\u0026#39;,\rx_axis_labels = 3, note_frequentist_no_prior = TRUE) +\rggtitle(\u0026#39;Prior *SD* = 0.2\u0026#39;) +\rtheme(axis.text.y = element_blank())\r#####\rplot_semanticpriming_frequentist_bayesian_plot_diffusepriors_exgaussian =\rfrequentist_bayesian_plot(KR_summary_semanticpriming_lmerTest,\rconfint_semanticpriming_lmerTest,\rsemanticpriming_posteriordistributions_diffusepriors_exgaussian,\rlabels = new_labels, interaction_symbol_x = TRUE,\rvertical_line_at_x = 0, x_title = \u0026#39;Effect size (\u0026amp;beta;)\u0026#39;, x_axis_labels = 3, note_frequentist_no_prior = TRUE) +\rggtitle(\u0026#39;Prior *SD* = 0.3\u0026#39;) + theme(axis.text.y = element_blank())\r\r\nThe figure below presents the posterior distribution of each effect in each model. The frequentist estimates are also shown to facilitate the comparison.\nplot_semanticpriming_frequentist_bayesian_plot_informativepriors_exgaussian +\rplot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian +\rplot_semanticpriming_frequentist_bayesian_plot_diffusepriors_exgaussian +\rplot_layout(ncol = 3, guides = \u0026#39;collect\u0026#39;) \u0026amp; theme(legend.position = \u0026#39;bottom\u0026#39;)\r\rEstimates from the frequentist analysis (in red) and from the Bayesian analysis (in blue) for the semantic priming study, in each model. The frequentist means (represented by points) are flanked by 95% confidence intervals. The Bayesian means (represented by vertical lines) are flanked by 95% credible intervals in light blue (in some cases, the interval is occluded by the bar of the mean)\n\r\nA blog post on the frequentist-Bayesian plots is also available.\nSession info\rIf you encounter any blockers while reproduce the above analyses using the materials at https://osf.io/gt5uf, my current session info may be useful. For instance, the legend of the last plot may not show if the latest versions of the ggplot2 and the tidyverse packages are used. Instead, ggplot2 3.3.5 and tidyverse 1.3.1 should be installed using install_version('ggplot2', '3.3.5') and install_version('tidyverse', '1.3.1').\nsessionInfo()\r## R version 4.2.3 (2023-03-15 ucrt)\r## Platform: x86_64-w64-mingw32/x64 (64-bit)\r## Running under: Windows 10 x64 (build 22621)\r## ## Matrix products: default\r## ## locale:\r## [1] LC_COLLATE=English_United States.utf8 ## [2] LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8\r## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## attached base packages:\r## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages:\r## [1] Cairo_1.6-0 forcats_1.0.0 stringr_1.5.0 ## [4] purrr_1.0.1 readr_2.1.4 tidyr_1.3.0 ## [7] tibble_3.2.1 tidyverse_1.3.1 papaja_0.1.1 ## [10] tinylabels_0.2.3 patchwork_1.1.2 ggtext_0.1.2 ## [13] ggridges_0.5.4 ggplot2_3.3.5 dplyr_1.1.1 ## [16] knitr_1.42 xaringanExtra_0.7.0\r## ## loaded via a namespace (and not attached):\r## [1] Rcpp_1.0.10 lubridate_1.9.2 digest_0.6.31 utf8_1.2.3 ## [5] plyr_1.8.8 cellranger_1.1.0 R6_2.5.1 backports_1.4.1 ## [9] reprex_2.0.2 evaluate_0.21 httr_1.4.6 highr_0.10 ## [13] blogdown_1.16 pillar_1.9.0 rlang_1.1.0 readxl_1.4.2 ## [17] uuid_1.1-0 rstudioapi_0.14 jquerylib_0.1.4 rmarkdown_2.21 ## [21] labeling_0.4.2 munsell_0.5.0 gridtext_0.1.5 broom_1.0.4 ## [25] compiler_4.2.3 modelr_0.1.11 xfun_0.38 pkgconfig_2.0.3 ## [29] htmltools_0.5.5 tidyselect_1.2.0 bookdown_0.33.3 fansi_1.0.4 ## [33] crayon_1.5.2 tzdb_0.4.0 dbplyr_2.3.2 withr_2.5.0 ## [37] commonmark_1.9.0 grid_4.2.3 jsonlite_1.8.4 gtable_0.3.3 ## [41] lifecycle_1.0.3 DBI_1.1.3 magrittr_2.0.3 scales_1.2.1 ## [45] cli_3.4.1 stringi_1.7.12 cachem_1.0.7 farver_2.1.1 ## [49] fs_1.6.1 xml2_1.3.3 bslib_0.4.2 generics_0.1.3 ## [53] vctrs_0.6.1 tools_4.2.3 glue_1.6.2 markdown_1.5 ## [57] hms_1.1.3 fastmap_1.1.1 yaml_2.3.7 timechange_0.2.0\r## [61] colorspace_2.1-0 rvest_1.0.3 haven_2.5.2 sass_0.4.6\r\rReferences\rBarton, J. J. S., Hanif, H. M., Eklinder Björnström, L., \u0026amp; Hills, C. (2014). The word-length effect in reading: A review. Cognitive Neuropsychology, 31(5-6), 378–412. https://doi.org/10.1080/02643294.2014.895314\r\rBernabeu, P. (2022). Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power. Lancaster University. https://doi.org/10.17635/lancaster/thesis/1795\r\rBeyersmann, E., Grainger, J., \u0026amp; Taft, M. (2020). Evidence for embedded word length effects in complex nonwords. Language, Cognition and Neuroscience, 35(2), 235–245. https://doi.org/10.1080/23273798.2019.1659989\r\rBrysbaert, M., Mandera, P., \u0026amp; Keuleers, E. (2018). The word frequency effect in word processing: An updated review. Current Directions in Psychological Science, 27(1), 45–50. https://doi.org/10.1177/0963721417727521\r\rBrysbaert, M., Stevens, M., Mandera, P., \u0026amp; Keuleers, E. (2016). The impact of word prevalence on lexical decision times: Evidence from the Dutch Lexicon Project 2. Journal of Experimental Psychology: Human Perception and Performance, 42(3), 441–458. https://doi.org/10.1037/xhp0000159\r\rBrysbaert, M., Warriner, A. B., \u0026amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904–911. https://doi.org/10.3758/s13428-013-0403-5\r\rBürkner, P.-C. (2018). Advanced Bayesian multilevel modeling with the R package brms. The R Journal, 10(1), 395–411. https://journal.r-project.org/archive/2018/RJ-2018-017/index.html\r\rBürkner, P.-C., Gabry, J., Weber, S., Johnson, A., Modrak, M., Badr, H. S., Weber, F., Ben-Shachar, M. S., \u0026amp; Rabel, H. (2022). Package ’brms’. CRAN. https://cran.r-project.org/web/packages/brms/brms.pdf\r\rCerni, T., Velay, J.-L., Alario, F.-X., Vaugoyeau, M., \u0026amp; Longcamp, M. (2016). Motor expertise for typing impacts lexical decision performance. Trends in Neuroscience and Education, 5(3), 130–138. https://doi.org/10.1016/j.tine.2016.07.007\r\rCumming, G. (2014). The new statistics: Why and how. Psychological Science, 25(1), 7–29. https://doi.org/10.1177/0956797613504966\r\rDijkstra, T., Wahl, A., Buytenhuijs, F., Halem, N. V., Al-Jibouri, Z., Korte, M. D., \u0026amp; Rekké, S. (2019). Multilink: A computational model for bilingual word recognition and word translation. Bilingualism: Language and Cognition, 22(4), 657–679. https://doi.org/10.1017/S1366728918000287\r\rKim, M., Crossley, S. A., \u0026amp; Skalicky, S. (2018). Effects of lexical features, textual properties, and individual differences on word processing times during second language reading comprehension. Reading and Writing, 31(5), 1155–1180. https://doi.org/10.1007/s11145-018-9833-x\r\rKnief, U., \u0026amp; Forstmeier, W. (2021). Violating the normality assumption may be the lesser of two evils. Behavior Research Methods. https://doi.org/10.3758/s13428-021-01587-5\r\rKruschke, J. K., \u0026amp; Liddell, T. M. (2018). The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. Psychonomic Bulletin \u0026amp; Review, 25(1), 178–206. https://doi.org/10.3758/s13423-016-1221-4\r\rLee, M. D., \u0026amp; Wagenmakers, E.-J. (2014). Bayesian cognitive modeling: A practical course. Cambridge University Press. https://doi.org/10.1017/CBO9781139087759\r\rLewandowski, D., Kurowicka, D., \u0026amp; Joe, H. (2009). Generating random correlation matrices based on vines and extended onion method. Journal of Multivariate Analysis, 100(9), 1989–2001. https://doi.org/10.1016/j.jmva.2009.04.008\r\rLim, R. Y., Yap, M. J., \u0026amp; Tse, C.-S. (2020). Individual differences in Cantonese Chinese word recognition: Insights from the Chinese Lexicon Project. Quarterly Journal of Experimental Psychology, 73(4), 504–518. https://doi.org/10.1177/1747021820906566\r\rLo, S., \u0026amp; Andrews, S. (2015). To transform or not to transform: Using generalized linear mixed models to analyse reaction time data. Frontiers in Psychology, 6, 1171. https://doi.org/10.3389/fpsyg.2015.01171\r\rMatzke, D., \u0026amp; Wagenmakers, E.-J. (2009). Psychological interpretation of the ex-Gaussian and shifted Wald parameters: A diffusion model analysis. Psychonomic Bulletin \u0026amp; Review, 16(5), 798–817. https://doi.org/10.3758/PBR.16.5.798\r\rMendes, P. S., \u0026amp; Undorf, M. (2021). On the pervasive effect of word frequency in metamemory. Quarterly Journal of Experimental Psychology, 17470218211053329. https://doi.org/10.1177/17470218211053329\r\rMilek, A., Butler, E. A., Tackman, A. M., Kaplan, D. M., Raison, C. L., Sbarra, D. A., Vazire, S., \u0026amp; Mehl, M. R. (2018). “Eavesdropping on happiness” revisited: A pooled, multisample replication of the association between life satisfaction and observed daily conversation quantity and quality. Psychological Science, 29(9), 1451–1462. https://doi.org/10.1177/0956797618774252\r\rNicenboim, B., Schad, D., \u0026amp; Vasishth, S. (n.d.). An introduction to Bayesian data analysis for cognitive science. Chapman and Hall/CRC Statistics in the Social and Behavioral Sciences Series.\r\rPexman, P. M., \u0026amp; Yap, M. J. (2018). Individual differences in semantic processing: Insights from the Calgary semantic decision project. Journal of Experimental Psychology: Learning, Memory, and Cognition, 44(7), 1091–1112. https://doi.org/10.1037/xlm0000499\r\rPregla, D., Lissón, P., Vasishth, S., Burchert, F., \u0026amp; Stadie, N. (2021). Variability in sentence comprehension in aphasia in German. Brain and Language, 222, 105008. https://doi.org/10.1016/j.bandl.2021.105008\r\rRodríguez-Ferreiro, J., Aguilera, M., \u0026amp; Davies, R. (2020). Semantic priming and schizotypal personality: Reassessing the link between thought disorder and enhanced spreading of semantic activation. PeerJ, 8, e9511. https://doi.org/10.7717/peerj.9511\r\rRouder, J. N., Haaf, J. M., \u0026amp; Vandekerckhove, J. (2018). Bayesian inference for psychology, part IV: Parameter estimation and Bayes factors. Psychonomic Bulletin \u0026amp; Review, 25(1), 102–113. https://doi.org/10.3758/s13423-017-1420-7\r\rSchielzeth, H., Dingemanse, N. J., Nakagawa, S., Westneat, D. F., Allegue, H., Teplitsky, C., Réale, D., Dochtermann, N. A., Garamszegi, L. Z., \u0026amp; Araya‐Ajoy, Y. G. (2020). Robustness of linear mixed‐effects models to violations of distributional assumptions. Methods in Ecology and Evolution, 11(9), 1141–1152. https://doi.org/10.1111/2041-210X.13434\r\rSchmalz, X., Biurrun Manresa, J., \u0026amp; Zhang, L. (2021). What is a Bayes factor? Psychological Methods. https://doi.org/10.1037/met0000421\r\rStone, K., Malsburg, T. von der, \u0026amp; Vasishth, S. (2020). The effect of decay and lexical uncertainty on processing long-distance dependencies in reading. PeerJ, 8, e10438. https://doi.org/10.7717/peerj.10438\r\rStone, K., Veríssimo, J., Schad, D. J., Oltrogge, E., Vasishth, S., \u0026amp; Lago, S. (2021). The interaction of grammatically distinct agreement dependencies in predictive processing. Language, Cognition and Neuroscience, 36(9), 1159–1179. https://doi.org/10.1080/23273798.2021.1921816\r\rTendeiro, J. N., \u0026amp; Kiers, H. A. L. (2019). A review of issues about null hypothesis Bayesian testing. Psychological Methods, 24(6), 774–795. https://doi.org/10.1037/met0000221\r\rTendeiro, J. N., \u0026amp; Kiers, H. A. L. (in press). On the white, the black, and the many shades of gray in between: Our reply to van Ravenzwaaij and Wagenmakers (2021). Psychological Methods.\r\rVan de Schoot, R., Depaoli, S., Gelman, A., King, R., Kramer, B., Märtens, K., Tadesse, M. G., Vannucci, M., Willemsen, J., \u0026amp; Yau, C. (2021). Bayesian statistics and modelling. Nature Reviews Methods Primers, 1, 3. https://doi.org/10.1038/s43586-020-00003-0\r\rvan Ravenzwaaij, D., \u0026amp; Wagenmakers, E.-J. (2021). Advantages masquerading as “issues” in Bayesian hypothesis testing: A commentary on Tendeiro and Kiers (2019). Psychological Methods. https://doi.org/10.1037/met0000415\r\rVasishth, S., Nicenboim, B., Beckman, M. E., Li, F., \u0026amp; Kong, E. J. (2018). Bayesian data analysis in the phonetic sciences: A tutorial introduction. Journal of Phonetics, 71, 147–161. https://doi.org/10.1016/j.wocn.2018.07.008\r\rYarkoni, T., Balota, D., \u0026amp; Yap, M. J. (2008). Moving beyond Coltheart’s N: A new measure of orthographic similarity. Psychonomic Bulletin \u0026amp; Review, 15(5), 971–979. https://doi.org/10.3758/PBR.15.5.971\r\r\r\r\r\r","date":1671667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671667200,"objectID":"2895a175c5fb1f734016c2e19a19dbbe","permalink":"https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/","publishdate":"2022-12-22T00:00:00Z","relpermalink":"/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/","section":"post","summary":"This post presents a run-through of a Bayesian workflow in R. The content is *closely* based on Bernabeu (2022), which was in turn based on lots of other references, also cited here.","tags":["Bayesian statistics","linear mixed-effects models","priors","predictive checks","sensitivity analysis","R","visualisation","brms","s"],"title":"Bayesian workflow: Prior determination, predictive checks and sensitivity analyses","type":"post"},{"authors":["Bernabeu, P., Lynott, D., \u0026 Connell, L."],"categories":["conceptual processing","embodied cognition","statistical power","research methods","statistics"],"content":"\rReference\rBernabeu, P., Lynott, D., \u0026amp; Connell, L. (2022). Language and vision in conceptual processing: Multilevel analysis and statistical power. OSF. https://osf.io/dnskh\n\r\r","date":1665792000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665792000,"objectID":"2df7d61d7171559226e1a31f94ee9097","permalink":"https://pablobernabeu.github.io/publication/language-vision-conceptual-processing/","publishdate":"2022-10-22T09:10:27Z","relpermalink":"/publication/language-vision-conceptual-processing/","section":"publication","summary":"Research has suggested that conceptual processing depends on both language-based and vision-based information. We tested this interplay at three levels of the experimental structure: individuals, words and tasks. To this end, we drew on three existing, large data sets that implemented the paradigms of semantic priming, semantic decision and lexical decision. We extended these data sets with measures of language-based and vision-based information, and analysed how the latter variables interacted with participants' vocabulary size and gender, and also with presentation speed in the semantic priming study. We performed the analysis using mixed-effects models that included a comprehensive array of fixed effects---including covariates---and random effects. First, we found that language-based information was more important than vision-based information. Second, in the semantic priming study---whose task required distinguishing between words and nonwords---, both language-based and vision-based information were more influential when words were presented faster. Third, a 'task-relevance advantage' was identified in higher-vocabulary participants. Specifically, in lexical decision, higher-vocabulary participants were more sensitive to language-based information than lower-vocabulary participants. In contrast, in semantic decision, higher-vocabulary participants were more sensitive to word concreteness. Fourth, we demonstrated the influence of the analytical method on the results. These findings support the interplay between language and vision in conceptual processing, and demonstrate the influence of measurement instruments on the results. Last, we estimated the sample size required to reliably investigate various effects. We found that 300 participants were sufficient to examine the effect of language-based information contained in words, whereas more than 1,000 participants were necessary to examine the effect of vision-based information and the interactions of both former variables with vocabulary size, gender and presentation speed. In conclusion, this power analysis reveals the need to increase sample sizes when conducting research on perceptual simulation and individual differences.","tags":["semantic processing","semantic priming","semantic decision","lexical decision","language","vision","visual strength","cognition","psycholinguistics","reading","linear mixed-effects models","frequentist statistics","lme4","lmerTest","Bayesian statistics","brms","power analysis","sample size","simr","rstats","R"],"title":"Language and vision in conceptual processing: Multilevel analysis and statistical power","type":"publication"},{"authors":["Bernabeu, P."],"categories":["conceptual processing","embodied cognition","statistical power","research methods","statistics"],"content":"\n\u0026nbsp; Online book format \r\rReference Bernabeu, P. (2022). Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power. Lancaster University. https://doi.org/10.17635/lancaster/thesis/1795\n\r","date":1665705600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665705600,"objectID":"785b1b124f6f042a721b1cec60f9de86","permalink":"https://pablobernabeu.github.io/publication/pablo-bernabeu-2022-phd-thesis/","publishdate":"2022-10-22T00:00:00Z","relpermalink":"/publication/pablo-bernabeu-2022-phd-thesis/","section":"publication","summary":"Research has suggested that conceptual processing depends on both language-based and sensorimotor information. In this thesis, I investigate the nature of these systems and their interplay at three levels of the experimental structure---namely, individuals, words and tasks. In Study 1, I contributed to a multi-lab replication of the object orientation effect, which has been used to test sensorimotor simulation. The effect did not appear across any of the 18 languages examined, and it was not influenced by individual differences in mental rotation. Next, in Study 2, we drew on three existing data sets that implemented semantic priming, semantic decision and lexical decision. We extended these data sets with measures of language-based and vision-based information, and analysed their interactions with participants' vocabulary size and gender, and with presentation speed. The analysis had a conservative structure of fixed and random effects. First, we found that language-based information was more important than vision-based information. Second, in the semantic priming study---whose task required distinguishing between words and nonwords---, both language-based and vision-based information were more influential when words were presented faster. Third, a 'task-relevance advantage' was identified in higher-vocabulary participants. Specifically, in lexical decision, higher-vocabulary participants were more sensitive to language-based information than lower-vocabulary participants. In contrast, in semantic decision, higher-vocabulary participants were more sensitive to word concreteness. Fourth, we demonstrated the influence of the analytical method on the results. Last, we estimated the sample size required to investigate various effects. We found that 300 participants were sufficient to examine the effect of language-based information in words, whereas more than 1,000 participants were necessary to examine the effect of vision-based information and the interactions of both former variables with vocabulary size, gender and presentation speed. This power analysis reveals the need to increase sample sizes when conduct research on perceptual simulation and individual differences.","tags":["object orientation effects","replication","semantic processing","semantic priming","semantic decision","lexical decision","language","vision","visual strength","cognition","psycholinguistics","reading","linear mixed-effects models","frequentist statistics","lme4","lmerTest","Bayesian statistics","brms","power analysis","sample size","simr","rstats","R"],"title":"Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power","type":"publication"},{"authors":[],"categories":[],"content":"\rI recommend caution when receiving an invitation from Polar Insight (also dubbed ‘Polar Intelligence’) to complete a survey. When I received one, it ended up in a deceptive change of the terms. The deception emerged after two emails exchanged with the company’s managing director, James Tattersfield, and an employee.\nThe case began when the managing director invited me to complete two tasks in exchange for a $125 USD voucher. The two tasks were a survey (‘short form’, in the director’s words) and an hour-long interview. Below is the email:\n\rFrom: James Tattersfield james@polarintelligence.com\nSent: Thursday, January 27, 2022 12:33:32 PM\nTo: De Juan Bernabeu, Pablo p.bernabeu@lancaster.ac.uk\nSubject: [External] Research Interview Request\nThis email originated outside the University. Check before clicking links or attachments.\nHi Pablo,\nI’m emailing you to discuss your participation in a global research study that my organisation (Polar Insight, a research firm based in London) is conducting to better understand how academic researchers are discovering and accessing scholarly articles outside university campus.\nAt this point in the project, we are particularly interested in hearing from those who have completed - or are currently completing - a PhD or post-doctorate degree.\nMy research team has identified you as a potential respondent and asked me to reach out to see whether you may be interested in taking part in one of our online, paid research interviews in February. Does this sound like something you would be interested in?\nDetails:\nFormat: Online Video Interview\nTime required: 60 minutes\nWhen: February 2022\nPrep. required: None\nReward for taking part: $125 USD (via our gifting partner, Tremendous)\nNext steps:\nIf you’d like to take part, please reconfirm your suitability using this short form: https://polarinsight.typeform.com/res-dis\n\rOnce we receive your answers, myself or a member of the team will be in contact to get you scheduled in.\n\rAfter the interview is completed, we’ll be in touch to process your reward.\n\r\rIf you have any questions, please do feel free to reply to this email and I’ll do my best to answer them.\nThanks,\nJames\n--\nJames Tattersfield\nManaging Director\nUK - 68 Hanbury St, London, E1 5JL, United Kingdom\nRegistered in England and Wales as a Private Limited Company.\n‘Polar Insight’ is a registered trademark owned by Polar Insight Limited.\nCompany number: 10587770 VAT: GB 279 4939 34\nIf you no longer want to receive emails regarding our research projects, you are welcome to unsubscribe from any future communications here\n\r\nFollowing this invitation, I—and presumably many other researchers—responded to a survey (see some screenshots below) that provided the company with data that might be used for business purposes.\n\r\r\r\r\r\r\r\r\nOnce the company had received my survey completed, an employee wrote back to change the original terms, on the grounds that ‘we have had a high volume of responses to our survey, so unfortunately, not everyone will be selected on this occasion’. Such a change of the terms was unethical, as it happened after I had accepted the invitation from the company under the original terms. Crucially, if the company had truthfully specified the terms in the first email, they would not have received back anywhere as many surveys completed. Ethics is important in the distribution of surveys, even in business. It is noteworthy that Polar Insight boasts a portfolio of clients on its website that covers a range of private sectors as well as public bodies. It is, therefore, striking that this company decided to scam a PhD student.\nI emailed the company and the CEO James Tattersfield to request my reward. After they had ignored my emails, and then replied with evasive responses, I contacted the CEO on LinkedIn. Again, he yielded evasive responses only.\nThe present case incidentally demonstrates that the role of ethics committees—tasked with assessing surveys, among other things—goes beyond gross themes. Economic deception is one of the negligent acts that must be prevented.\nSurvey scams resembling the one I suffered have been documented:\n\rhttps://moneydoneright.com/fast-money/online-work/surveys/surveys-for-money\n\rhttps://www.accc.gov.au/media-release/watch-out-for-scam-surveys-and-fake-gift-card-offers\n\rhttps://www.facebook.com/dollargeneral/posts/pfbid032BmUQrAUh7p9yuw8eQKavZZrUC7zsGeBiT2E4efDEASCFSsem7vLKNiipgWNqYjpl\n\r\rFinally, the present case begs the following question: is James Tattersfield—a University of Ediburgh alumnus and the CEO of a profitable consulting company—also a scammer and a defaulter?\n\n","date":1647302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647302400,"objectID":"0c4ef4e34a1e8c260c2561477153b261","permalink":"https://pablobernabeu.github.io/2022/deception-in-a-survey-from-polar-insight-/-polar-intelligence/","publishdate":"2022-03-15T00:00:00Z","relpermalink":"/2022/deception-in-a-survey-from-polar-insight-/-polar-intelligence/","section":"post","summary":"I recommend caution when receiving an invitation from Polar Insight (also dubbed ‘Polar Intelligence’) to complete a survey. When I received one, it ended up in a deceptive change of the terms. The deception emerged after two emails exchanged with the company’s managing director, James Tattersfield, and an employee.\nThe case began when the managing director invited me to complete two tasks in exchange for a $125 USD voucher. The two tasks were a survey (‘short form’, in the director’s words) and an hour-long interview.","tags":["survey","scam","fraud","unpaid","s"],"title":"Deception in a survey from Polar Insight / Polar Intelligence","type":"post"},{"authors":[],"categories":["R"],"content":"\r\rThe function knit_deleting_service_files() helps avoid (R) Markdown knitting errors caused by files and folders remaining from previous knittings (e.g., manuscript.tex, ZHJhZnQtYXBhLlJtZA==.Rmd, manuscript.synctex.gz). The only obligatory argument for this function is the name of a .Rmd or .md file. The optional argument is a path to a directory containing this file.\nThe function first offers deleting potential service files and folders, for which the user’s approval is requested in the console (see screenshot below). Next, the document is knitted. Last, the function offers deleting potential service files and folders again.\nNOTE: The deletions, if accepted, are irreversible as they are made through unlink(). Therefore, our familiar adage truly applies: this function comes with ABSOLUTELY NO WARRANTY. Please ensure you understand the source code before using the function.\nScreenshot of the function in use\n\r","date":1639777566,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639777566,"objectID":"065f76c7178db1ca66c5ba5b0d0a2908","permalink":"https://pablobernabeu.github.io/2021/avoiding-knitting-errors-in-r-markdown-using-knit-deleting-service-files/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2021/avoiding-knitting-errors-in-r-markdown-using-knit-deleting-service-files/","section":"post","summary":"The function knit_deleting_service_files() helps avoid (R) Markdown knitting errors caused by files and folders remaining from previous knittings (e.g., manuscript.tex, ZHJhZnQtYXBhLlJtZA==.Rmd, manuscript.synctex.gz). The only obligatory argument for this function is the name of a .Rmd or .md file. The optional argument is a path to a directory containing this file. The function first offers deleting potential service files and folders in the directory. A confirmation is required in the console (see screenshot below). Next, the document is knitted. Last, the function offers deleting potential service files and folders again.","tags":["s","R Markdown","papaja","render"],"title":"Avoiding (R) Markdown knitting errors using knit_deleting_service_files()","type":"post"},{"authors":[],"categories":["R","linear mixed-effects models"],"content":"\r\r\r\rwindow.xaringanExtraClipboard(null, {\"button\":\"Copy Code\",\"success\":\"Copied!\",\"error\":\"Press Ctrl+C to Copy\"})\rLinear mixed-effects models (LMM) offer a consistent way of performing regression and analysis of variance tests which allows accounting for non-independence in the data. Over the past decades, LMMs have subsumed most of the General Linear Model, with a steady increase in popularity (Meteyard \u0026amp; Davies, 2020). Since their conception, LMMs have presented the challenge of model convergence. In essence, the issue of convergence boils down to the widespread tension between parsimony and completeness in data analysis. That is, on the one hand, a good model must allow an accurate, parsimonious analysis of each predictor, and thus, it must not be overfitted with too many parameters. Yet, on the other hand, the model must be complete enough to account for a sufficient amount of variation in the data. In LMMs, any predictors that entail non-independent observations (also known as repeated measures) will normally bring both fixed and random effects into the model. Where a few of these predictors coexist, models often struggle to find enough information in the data to account for every predictor—and especially, for every random effect. This difficulty translates into convergence warnings (Brauer \u0026amp; Curtin, 2018; Singmann \u0026amp; Kellen, 2019). In this article, I review the issue of convergence before presenting a new plotting function in R that facilitates the diagnosis of convergence by visualising the fixed effects fitted by different optimization algorithms (also dubbed optimizers).\nCompleteness versus parsimony\rBoth fixed and random effects comprise intercepts and slopes. The pressure exerted by each of those types of effects on the model is determined by the number of data points involved by each. First, slopes are more demanding than intercepts, as they involve a (far) larger number of data points. Second, random effects are more demanding than fixed effects, as random effects entail the number of estimates required for fixed effects times the number of levels in the grouping factor. As a result, on the most lenient end of the scale lies the fixed intercept, and on the heaviest end lie the random slopes. Convergence warnings in LMMs are often due to the random slopes alone.\nSounds easy, then! Not inviting the random slopes to the party should solve the problem. Indeed, since random slopes involve the highest number of estimates by far, removing them does often remove convergence warnings. This, however, leads to a different problem. Surrendering the information provided by random slopes can result in the violation of the assumption of independence of observations. For years, the removal of random slopes due to convergence warnings was standard practice. Currently, in contrast, proposals increasingly consider other options, such as removing random effects if they do not significantly improve the fit of the model (Matuschek et al., 2017), and keeping the random slopes in the model in spite of the convergence warnings to safeguard the assumption of independence (see Table 17 in Brauer \u0026amp; Curtin, 2018; Singmann \u0026amp; Kellen, 2019).\n\rThe multiple-optimizers sanity check from lme4::allFit()\rFramed within the drive to maintain random slopes wherever possible, the developers of the ‘lme4’ package propose a sanity check that uses a part of the ‘lme4’ engine called ‘optimizer’. Every model has a default optimizer, unless a specific one is chosen through control = lmerControl(optimizer = '...') (in lmer models) or control = glmerControl(optimizer = '...') (in glmer models). The seven widely-available optimizers are:\n\rbobyqa\rNelder_Mead\rnlminbwrap\rnmkbw\roptimx.L-BFGS-B\rnloptwrap.NLOPT_LN_NELDERMEAD\rnloptwrap.NLOPT_LN_BOBYQA\r\rTo assess whether convergence warnings render the results invalid, or on the contrary, the results can be deemed valid in spite of the warnings, Bates et al. (2021) suggest refitting models affected by convergence warnings with a variety of optimizers. The authors argue that if the different optimizers produce practically-equivalent results, the results are valid. The allFit function from the ‘lme4’ package allows the refitting of models using a number of optimizers. To use the seven optimizers listed above, two extra packages must be installed: ‘dfoptim’ and ‘optimx’ (see lme4 manual). The output from allFit() contains several statistics on the fixed and the random effects fitted by each optimizer (see example).\n\rPlotting the fixed effects from allFit()\rSeveral R users have ventured into plotting the allFit() output but there is not a function in ‘lme4’ yet at the time of writing (Oct 2021). I have just developed a function that takes the output from allFit(), tidies it, selects the fixed effects and plots them using ‘ggplot2’. The function is shown below, and can be copied through the Copy Code button at the top right corner. It can be renamed by changing plot.fixef.allFit to another valid name.\n# Plot the results from the fixed effects produced by different optimizers. This function # takes the output from lme4::allFit(), tidies it, selects fixed effects and plots them.\rplot.fixef.allFit = function(allFit_output, # Set the same Y axis limits in every plot\rshared_y_axis_limits = TRUE,\r# Multiply Y axis limits by a factor (only # available if shared_y_axis_limits = TRUE)\rmultiply_y_axis_limits = 1, # Number of decimal points\rdecimal_points = NULL,\r# Select predictors\rselect_predictors = NULL, # Number of rows\rnrow = NULL, # Y axis title\ry_title = \u0026#39;Fixed effect\u0026#39;,\r# Alignment of the Y axis title\ry_title_hjust = NULL,\r# Add number to the names of optimizers\rnumber_optimizers = TRUE,\r# Replace colon in interactions with x\rinteraction_symbol_x = TRUE) {\rrequire(lme4)\rrequire(dplyr)\rrequire(reshape2)\rrequire(stringr)\rrequire(scales)\rrequire(ggplot2)\rrequire(ggtext)\rrequire(patchwork)\rlibrary(Cairo)\r# Tidy allFit output\r# Extract fixed effects from the allFit() output\rallFit_fixef = summary(allFit_output)$fixef %\u0026gt;% # Select fixed effects in the allFit results\rreshape2::melt() %\u0026gt;% # Structure the output as a data frame\rrename(\u0026#39;Optimizer\u0026#39; = \u0026#39;Var1\u0026#39;, \u0026#39;fixed_effect\u0026#39; = \u0026#39;Var2\u0026#39;) # set informative names\r# If number_optimizers = TRUE, assign number to each optimizer and place it before its name\rif(number_optimizers == TRUE) {\rallFit_fixef$Optimizer = paste0(as.numeric(allFit_fixef$Optimizer), \u0026#39;. \u0026#39;, allFit_fixef$Optimizer)\r}\r# If select_predictors were supplied, select them along with the intercept (the latter required)\rif(!is.null(select_predictors)) {\rallFit_fixef = allFit_fixef %\u0026gt;% dplyr::filter(fixed_effect %in% c(\u0026#39;(Intercept)\u0026#39;, select_predictors))\r}\r# Order variables\rallFit_fixef = allFit_fixef[, c(\u0026#39;Optimizer\u0026#39;, \u0026#39;fixed_effect\u0026#39;, \u0026#39;value\u0026#39;)]\r# PLOT. The overall plot is formed of a first row containing the intercept and the legend # (intercept_plot), and a second row containing the predictors (predictors_plot), # which may in turn occupy several rows.\r# If multiply_y_axis_limits was supplied but shared_y_axis_limits = FALSE,\r# warn that shared_y_axis_limits is required.\rif(!multiply_y_axis_limits == 1 \u0026amp; shared_y_axis_limits == FALSE) {\rmessage(\u0026#39;The argument `multiply_y_axis_limits` has not been used because \\n it requires `shared_y_axis_limits` set to TRUE.\u0026#39;)\r}\r# If extreme values were entered in y_title_hjust, show warning\rif(!is.null(y_title_hjust)) {\rif(y_title_hjust \u0026lt; 0.5 | y_title_hjust \u0026gt; 6) {\rmessage(\u0026#39;NOTE: For y_title_hjust, a working range of values is between 0.6 and 6.\u0026#39;)\r}\r}\r# If decimal_points were supplied, convert number to the format used in \u0026#39;scales\u0026#39; package\rif(!is.null(decimal_points)) {\rdecimal_points = ifelse(decimal_points == 1, 0.1, ifelse(decimal_points == 2, 0.01, ifelse(decimal_points == 3, 0.001, ifelse(decimal_points == 4, 0.0001, ifelse(decimal_points == 5, 0.00001, ifelse(decimal_points == 6, 0.000001, ifelse(decimal_points == 7, 0.0000001, ifelse(decimal_points == 8, 0.00000001, ifelse(decimal_points == 9, 0.000000001, ifelse(decimal_points == 10, 0.0000000001,\rifelse(decimal_points == 11, 0.00000000001,\rifelse(decimal_points == 12, 0.000000000001,\rifelse(decimal_points == 13, 0.0000000000001,\rifelse(decimal_points == 14, 0.00000000000001,\rifelse(decimal_points \u0026gt;= 15, 0.000000000000001, 0.001\r)))))))))))))))\r}\r# First row: intercept_plot\r# Select intercept data only\rintercept = allFit_fixef %\u0026gt;% dplyr::filter(fixed_effect == \u0026#39;(Intercept)\u0026#39;)\rintercept_plot = intercept %\u0026gt;%\rggplot(., aes(fixed_effect, value, colour = Optimizer)) +\rgeom_point(position = position_dodge(1)) +\rfacet_wrap(~fixed_effect, scale = \u0026#39;free\u0026#39;) +\rguides(colour = guide_legend(title.position = \u0026#39;left\u0026#39;)) +\rtheme_bw() + theme(axis.title = element_blank(), axis.ticks.x = element_blank(),\raxis.text.x = element_blank(), strip.text = element_text(size = 10, margin = margin(t = 4, b = 6)),\rstrip.background = element_rect(fill = \u0026#39;grey96\u0026#39;),\rlegend.margin = margin(0.3, 0, 0.8, 1, \u0026#39;cm\u0026#39;), legend.title = element_text(size = unit(15, \u0026#39;pt\u0026#39;), angle = 90, hjust = 0.5))\r# Second row: predictors_plot\r# Select all predictors except intercept\rpredictors = allFit_fixef %\u0026gt;% dplyr::filter(!fixed_effect == \u0026#39;(Intercept)\u0026#39;)\r# If interaction_symbol_x = TRUE (default), replace colon with times symbol x between spaces\rif(interaction_symbol_x == TRUE) {\r# Replace colon in interactions with \\u00D7, i.e., x; then set factor class\rpredictors$fixed_effect = predictors$fixed_effect %\u0026gt;% str_replace_all(\u0026#39;:\u0026#39;, \u0026#39; \\u00D7 \u0026#39;) %\u0026gt;% factor()\r}\r# Order predictors as in the original output from lme4::allFit()\rpredictors$fixed_effect = factor(predictors$fixed_effect, levels = unique(predictors$fixed_effect))\r# Set number of rows for the predictors excluding the intercept.\r# First, if nrow argument supplied, use it\rif(!is.null(nrow)) {\rpredictors_plot_nrow = nrow - 1 # Subtract 1 as intercept row not considered\r# Else, if nrow argument not supplied, calculate sensible number of rows: i.e., divide number of\r# predictors (exc. intercept) by 2 and round up the result. For instance, 7 predictors --\u0026gt; 3 rows\r} else predictors_plot_nrow = (length(unique(predictors$fixed_effect)) / 2) %\u0026gt;% ceiling()\rpredictors_plot = ggplot(predictors, aes(fixed_effect, value, colour = Optimizer)) +\rgeom_point(position = position_dodge(1)) +\rfacet_wrap(~fixed_effect, scale = \u0026#39;free\u0026#39;,\r# Note that predictors_plot_nrow was defined a few lines above\rnrow = predictors_plot_nrow, # Wrap names of predictors with more than 54 characters into new lines\rlabeller = labeller(fixed_effect = label_wrap_gen(width = 55))) +\rlabs(y = y_title) +\rtheme_bw() + theme(axis.title.x = element_blank(), axis.text.x = element_blank(),\raxis.ticks.x = element_blank(),\raxis.title.y = ggtext::element_markdown(size = 14, margin = margin(0, 15, 0, 0)),\rstrip.text = element_text(size = 10, margin = margin(t = 4, b = 6)),\rstrip.background = element_rect(fill = \u0026#39;grey96\u0026#39;), legend.position = \u0026#39;none\u0026#39;)\r# Below, the function scale_y_continuous is applied conditionally to avoid overriding settings. First, # if shared_y_axis_limits = TRUE and decimal_points were supplied, set the same Y axis limits in # every plot and set decimal_points. By default, also expand limits by a seventh of its original # limit, and allow further multiplication of limits through multiply_y_axis_limits.\rif(shared_y_axis_limits == TRUE \u0026amp; !is.null(decimal_points)) {\rintercept_plot = intercept_plot +\rscale_y_continuous(limits = c(min(allFit_fixef$value) - allFit_fixef$value %\u0026gt;% abs %\u0026gt;% max / 7 * multiply_y_axis_limits,\rmax(allFit_fixef$value) + allFit_fixef$value %\u0026gt;% abs %\u0026gt;% max / 7 * multiply_y_axis_limits), # Set number of decimal points\rlabels = scales::label_number(accuracy = decimal_points))\rpredictors_plot = predictors_plot + scale_y_continuous(limits = c(min(allFit_fixef$value) - allFit_fixef$value %\u0026gt;% abs %\u0026gt;% max / 7 * multiply_y_axis_limits,\rmax(allFit_fixef$value) + allFit_fixef$value %\u0026gt;% abs %\u0026gt;% max / 7 * multiply_y_axis_limits), # Set number of decimal points\rlabels = scales::label_number(accuracy = decimal_points))\r# Else, if shared_y_axis_limits = TRUE but decimal_points were not supplied, do as above but without\r# setting decimal_points.\r} else if(shared_y_axis_limits == TRUE \u0026amp; is.null(decimal_points)) {\rintercept_plot = intercept_plot +\rscale_y_continuous(limits = c(min(allFit_fixef$value) - allFit_fixef$value %\u0026gt;% abs %\u0026gt;% max / 7 * multiply_y_axis_limits,\rmax(allFit_fixef$value) + allFit_fixef$value %\u0026gt;% abs %\u0026gt;% max / 7 * multiply_y_axis_limits),\r# Set number of decimal points\rlabels = scales::label_number(accuracy = decimal_points))\rpredictors_plot = predictors_plot + scale_y_continuous(limits = c(min(allFit_fixef$value) - allFit_fixef$value %\u0026gt;% abs %\u0026gt;% max / 7 * multiply_y_axis_limits,\rmax(allFit_fixef$value) + allFit_fixef$value %\u0026gt;% abs %\u0026gt;% max / 7 * multiply_y_axis_limits),\r# Set number of decimal points\rlabels = scales::label_number(accuracy = decimal_points))\r# Else, if shared_y_axis_limits = FALSE and decimal_points were supplied, set decimal_points. } else if(shared_y_axis_limits == FALSE \u0026amp; !is.null(decimal_points)) {\r# Set number of decimal points in both plots\rintercept_plot = intercept_plot +\rscale_y_continuous(labels = scales::label_number(accuracy = decimal_points))\rpredictors_plot = predictors_plot +\rscale_y_continuous(labels = scales::label_number(accuracy = decimal_points))\r}\r# Plot matrix: based on number of predictors_plot_nrow, adjust height of Y axis title\r# (unless supplied), and assign space to intercept_plot and predictors_plot\rif(predictors_plot_nrow == 1) {\r# If y_title_hjust supplied, use it\rif(!is.null(y_title_hjust)) {\rpredictors_plot = predictors_plot + theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))\r# Otherwise, set a sensible height\r} else predictors_plot = predictors_plot + theme(axis.title.y = ggtext::element_markdown(hjust = 3.6))\rlayout = c(\rpatchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0), # intercept row\rpatchwork::area(t = 7.3, r = 9, b = 11, l = 0) # predictors row(s)\r)\r} else if(predictors_plot_nrow == 2) {\r# If y_title_hjust supplied, use it\rif(!is.null(y_title_hjust)) {\rpredictors_plot = predictors_plot + theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))\r# Otherwise, set a sensible height\r} else predictors_plot = predictors_plot + theme(axis.title.y = ggtext::element_markdown(hjust = 1.4))\rlayout = c(\rpatchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0), # intercept row\rpatchwork::area(t = 7.3, r = 9, b = 16, l = 0) # predictors row(s)\r)\r} else if(predictors_plot_nrow == 3) {\r# If y_title_hjust supplied, use it\rif(!is.null(y_title_hjust)) {\rpredictors_plot = predictors_plot + theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))\r# Otherwise, set a sensible height\r} else predictors_plot = predictors_plot + theme(axis.title.y = ggtext::element_markdown(hjust = 0.92))\rlayout = c(\rpatchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0), # intercept row\rpatchwork::area(t = 7.3, r = 9, b = 21, l = 0) # predictors row(s)\r)\r} else if(predictors_plot_nrow == 4) {\r# If y_title_hjust supplied, use it\rif(!is.null(y_title_hjust)) {\rpredictors_plot = predictors_plot + theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))\r# Otherwise, set a sensible height\r} else predictors_plot = predictors_plot + theme(axis.title.y = ggtext::element_markdown(hjust = 0.8))\rlayout = c(\rpatchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0), # intercept row\rpatchwork::area(t = 7.3, r = 9, b = 26, l = 0) # predictors row(s)\r)\r} else if(predictors_plot_nrow == 5) {\r# If y_title_hjust supplied, use it\rif(!is.null(y_title_hjust)) {\rpredictors_plot = predictors_plot + theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))\r# Otherwise, set a sensible height\r} else predictors_plot = predictors_plot + theme(axis.title.y = ggtext::element_markdown(hjust = 0.73))\rlayout = c(\rpatchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0), # intercept row\rpatchwork::area(t = 7.3, r = 9, b = 31, l = 0) # predictors row(s)\r)\r} else if(predictors_plot_nrow \u0026gt; 5) {\r# If y_title_hjust supplied, use it\rif(!is.null(y_title_hjust)) {\rpredictors_plot = predictors_plot + theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))\r# Otherwise, set a sensible height\r} else predictors_plot = predictors_plot + theme(axis.title.y = ggtext::element_markdown(hjust = 0.65))\rlayout = c(\rpatchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0), # intercept row\rpatchwork::area(t = 7.3, r = 9, b = 36, l = 0) # predictors row(s)\r)\r# Also, advise user to consider distributing predictors into several plots\rmessage(\u0026#39; Many rows! Consider distributing predictors into several plots \\n using argument `select_predictors`\u0026#39;)\r} # Add margin\rpredictors_plot = predictors_plot + theme(plot.margin = margin(15, 15, 15, 15))\r# Return matrix of plots\rwrap_plots(intercept_plot, predictors_plot, design = layout,\r# The 2 below corresponds to intercept_plot and predictors_plot\rnrow = 2)\r}\r\r\nOptional arguments\rBelow are the optional arguments allowed by the function, with their default values.\n\r# Set the same Y axis limits in every plot\rshared_y_axis_limits = TRUE,\r# Multiply Y axis limits by a factor (only # available if shared_y_axis_limits = TRUE)\rmultiply_y_axis_limits = 1, # Number of decimal points\rdecimal_points = NULL,\r# Select predictors\rselect_predictors = NULL, # Number of rows\rnrow = NULL, # Y axis title\ry_title = \u0026#39;Fixed effect\u0026#39;,\r# Alignment of the Y axis title\ry_title_hjust = NULL,\r# Add number to the names of optimizers\rnumber_optimizers = TRUE,\r# Replace colon in interactions with x\rinteraction_symbol_x = TRUE\r\rThe argument shared_y_axis_limits deserves a comment. It allows using the same Y axis limits (i.e., range) in all plots or, alternatively, using plot-specific limits. The parameter is TRUE by default to prevent overinterpretations of small differences across optimizers (see the first figure below). In contrast, when shared_y_axis_limits = FALSE, plot-specific limits are used, which results in a narrower range of values in the Y axis (see the second figure below). Since data points will span the entire Y axis in that case, any difference across optimizers—regardless of its relative importance—might be perceived as large, unless the specific range of values in each plot is noticed.\n\r\rUse case\rLet’s test the function on a new analysis of the English Lexicon Project (Balota et al., 2007; Yap et al., 2012) that I’ve conducted for a forthcoming study.\n# Read in allFit() output\rm1_allFit_convergence = readRDS(\u0026#39;m1_allFit_convergence.rds\u0026#39;)\r# To select specific predictors, first return their names\rcolnames(summary(m1_allFit_convergence)$fixef)\r## [1] \u0026quot;(Intercept)\u0026quot; ## [2] \u0026quot;z_orthographic_Levenshtein_distance\u0026quot; ## [3] \u0026quot;z_word_concreteness\u0026quot; ## [4] \u0026quot;z_vocabulary_age\u0026quot; ## [5] \u0026quot;z_recoded_participant_gender\u0026quot; ## [6] \u0026quot;z_word_frequency\u0026quot; ## [7] \u0026quot;z_visual_rating\u0026quot; ## [8] \u0026quot;z_word_concreteness:z_vocabulary_age\u0026quot; ## [9] \u0026quot;z_word_concreteness:z_recoded_participant_gender\u0026quot;\r## [10] \u0026quot;z_vocabulary_age:z_word_frequency\u0026quot; ## [11] \u0026quot;z_vocabulary_age:z_visual_rating\u0026quot; ## [12] \u0026quot;z_recoded_participant_gender:z_word_frequency\u0026quot; ## [13] \u0026quot;z_recoded_participant_gender:z_visual_rating\u0026quot;\rA subset of these effects is selected below using the argument select_predictors. Notice that the intercept is plotted by default on the first row, along with the legend that lists all the optimizers used.\nplot.fixef.allFit(m1_allFit_convergence, select_predictors = c(\u0026#39;z_vocabulary_age\u0026#39;,\r\u0026#39;z_recoded_participant_gender\u0026#39;,\r\u0026#39;z_word_frequency\u0026#39;,\r\u0026#39;z_vocabulary_age:z_word_frequency\u0026#39;,\r\u0026#39;z_recoded_participant_gender:z_word_frequency\u0026#39;), # Increase padding at top and bottom of Y axis\rmultiply_y_axis_limits = 1.3)\rThe plot produced by plot.fixef.allFit() by default replaces the colons in interaction effects (e.g., z_vocabulary_age:z_word_frequency) with ’ × ’ to facilitate the visibility (this can be overriden by setting interaction_symbol_x = FALSE). Yet, it is important to note that any interactions passed to select_predictors must have the colon, as that is the symbol present in the lme4::allFit() output.\nThe output of plot.fixef.allFit() is a ggplot2 object that can be stored for further use, as in the example below, in which new parameters are used.\nplot_m1_allFit_convergence = plot.fixef.allFit(m1_allFit_convergence, select_predictors = c(\u0026#39;z_vocabulary_age\u0026#39;,\r\u0026#39;z_vocabulary_age:z_word_frequency\u0026#39;), # Use plot-specific Y axis limits\rshared_y_axis_limits = FALSE,\rdecimal_points = 7, # Move up Y axis title\ry_title_hjust = -11.8,\ry_title = \u0026#39;Fixed effect (\\u03B2)\u0026#39;) # \\u03B2 = beta letter\r# Modify aspect further using `ggplot2::theme()`\rplot_m1_allFit_convergence =\rplot_m1_allFit_convergence + theme(axis.title.y = element_text(size = 12))\r## Error in `merge_element()`:\r## ! Only elements of the same class can be merged\r# Print plot\rplot_m1_allFit_convergence\r# Plot can be saved to disk as pdf, png, etc. through `ggplot2::ggsave()`\r# ggsave(\u0026#39;plot_m1_allFit_convergence.pdf\u0026#39;, plot_m1_allFit_convergence, # device = cairo_pdf, width = 9, height = 9, dpi = 900)\r\n\rReferences\rBalota, D. A., Yap, M. J., Hutchison, K. A., Cortese, M. J., Kessler, B., Loftis, B., Neely, J. H., Nelson, D. L., Simpson, G. B., \u0026amp; Treiman, R. (2007). The English Lexicon Project. Behavior Research Methods, 39, 445–459. https://doi.org/10.3758/BF03193014\nBates, D., Maechler, M., Bolker, B., Walker, S., Christensen, R. H. B., Singmann, H., Dai, B., Scheipl, F., Grothendieck, G., Green, P., Fox, J., Bauer, A., \u0026amp; Krivitsky, P. N. (2021). Package ‘lme4’. CRAN. https://cran.r-project.org/web/packages/lme4/lme4.pdf\nBrauer, M., \u0026amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. Psychological Methods, 23(3), 389–411. https://doi.org/10.1037/met0000159\nMatuschek, H., Kliegl, R., Vasishth, S., Baayen, H., \u0026amp; Bates, D. (2017). Balancing type 1 error and power in linear mixed models. Journal of Memory and Language, 94, 305–315. https://doi.org/10.1016/j.jml.2017.01.001\nMeteyard, L., \u0026amp; Davies, R. A. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092\nSingmann, H., \u0026amp; Kellen, D. (2019). An introduction to mixed models for experimental psychology. In D. H. Spieler \u0026amp; E. Schumacher (Eds.), New methods in cognitive psychology (pp. 4–31). Psychology Press.\nYap, M. J., Balota, D. A., Sibley, D. E., \u0026amp; Ratcliff, R. (2012). Individual differences in visual word recognition: Insights from the English Lexicon Project. Journal of Experimental Psychology: Human Perception and Performance, 38, 1, 53–79. https://doi.org/10.1037/a0024177\n\r\r","date":1636915568,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636915568,"objectID":"ef5142572a939044659c17baecffa1ac","permalink":"https://pablobernabeu.github.io/2021/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2021/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/","section":"post","summary":"When a model has struggled to find enough information in the data to account for every predictor---especially for every random effect---, convergence warnings appear (Brauer \u0026 Curtin, 2018; Singmann \u0026 Kellen, 2019). In this article, I review the issue of convergence before presenting a new plotting function in R that facilitates the visualisation of the fixed effects fitted by different optimization algorithms (also dubbed optimizers).","tags":["s","R","linear mixed-effects models","data visualisation","statistics"],"title":"A new function to plot convergence diagnostics from lme4::allFit()","type":"post"},{"authors":[],"categories":["R"],"content":"\r\rAs technology and research methods advance, the data sets tend to be larger and the methods more exhaustive. Consequently, the analyses take longer to run. This poses a challenge when the results are to be presented using R Markdown. One has to balance reproducibility and efficiency. On the one hand, it is desirable to keep the R Markdown document as self-contained as possible, so that those who may later examine the document can easily test and edit the code. On the other hand, it would be inefficient to create a document that is very slow to run or very long. The context of the task will determine how how time-consuming and long the code in an Rmd file can be. For instance, one could decide that the knitting can take up to 15 minutes, and each code chunk can span up to 30 lines.\nSeveral methods can be used in each document to accommodate different types of code. Three methods are presented below, ordered from easier-to-reproduce to easier-to-knit.\nFor fast- and concise-enough code: Provide the original code in the Rmd file. The code is run as the document is knitted. Example:\n nrow(myData)\rFor fast-enough but very long code: Store the code in a separate script and source it in the Rmd file. The code is run as the document is knitted. Example:\n source(\u0026#39;analysis/model_diagnostics.R\u0026#39;)\rFor very slow and/or long code: Store the code in a separate script and run it prior to knitting the Rmd file, so that the output from the code (e.g., a model, a plot) is saved and can be read into the Rmd. Example:\n model_1 = readRDS(\u0026#39;results/model_1.rds\u0026#39;)\r\rImportantly, even the third method allows the reproducibility of the code. It just requires a bit of additional documentation to ensure that the end user can also access the script in which the result was produced (e.g., ‘analysis/model_1.R’).\n","date":1636848608,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636848608,"objectID":"c2ff67766be99e135b2d65b6a2f2ff4d","permalink":"https://pablobernabeu.github.io/2021/walking-the-line-between-reproducibility-and-efficiency-in-r-markdown-three-methods/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2021/walking-the-line-between-reproducibility-and-efficiency-in-r-markdown-three-methods/","section":"post","summary":"As technology and research methods advance, the data sets tend to be larger and the methods more exhaustive. Consequently, the analyses take longer to run. This poses a challenge when the results are to be presented using R Markdown. One has to balance reproducibility and efficiency. On the one hand, it is desirable to keep the R Markdown document as self-contained as possible, so that those who may later examine the document can easily test and edit the code.","tags":["s","R","R Markdown"],"title":"Walking the line between reproducibility and efficiency in R Markdown: Three methods","type":"post"},{"authors":[],"categories":["R"],"content":"\r\rWhen knitting an R Markdown document after the first time, errors may sometimes appear. Three tips are recommended below.\n1. Close PDF reader window\rWhen the document is knitted through the ‘Knit’ button, a PDF reader window opens to present the result. Closing this window can help resolve errors.\n\r2. Delete service files\rEvery time the Rmd is knitted, some service files are created. Some of these files have the ‘.tex’ extension (e.g., index.tex, Appendix-B.tex), whereas others do not (e.g., index.log, ZHJhZnQtYXBhLlJtZA==.Rmd, index.synctex.gz). Deleting these files can help resolve errors (see a possible function).\n\r3. Delete code chunks related to the appendices\rWhen knitting papaja documents containing any appendices, some code chunks related to the appendices will automatically appear at the end of the primary Rmd file—e.g.:\n```{r echo = FALSE, results = \u0026#39;asis\u0026#39;, cache = FALSE}\rpapaja::render_appendix(\u0026#39;Appendix-A.Rmd\u0026#39;)\r```\rNormally, these code chunks are automatically removed as the knitting finishes. However, if the chunks remain in place, deleting them manually can help resolve errors.\n\r","date":1636827082,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636827082,"objectID":"55d2dcb6bb3b71357f49cdd9fc452690","permalink":"https://pablobernabeu.github.io/2021/tackling-knitting-errors-in-r-markdown/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2021/tackling-knitting-errors-in-r-markdown/","section":"post","summary":"When knitting an R Markdown document after the first time, errors may sometimes appear. Three tips are recommended below.\n1. Close PDF reader window\rWhen the document is knitted through the ‘Knit’ button, a PDF reader window opens to present the result. Closing this window can help resolve errors.\n\r2. Delete service files\rEvery time the Rmd is knitted, some service files are created. Some of these files have the ‘.","tags":["s","R","R Markdown"],"title":"Tackling knitting errors in R Markdown","type":"post"},{"authors":[],"categories":["R","research methods"],"content":"\r\rThe powercurve function from the simr package in R (Green \u0026amp; MacLeod, 2016) can incur very long running times when the method used for the calculation of p values is Kenward-Roger or Satterthwaite (see Luke, 2017). Here I suggest three ways for cutting down this time.\nWhere possible, use a high-performance (or high-end) computing cluster. This removes the need to use personal computers for these long jobs.\n\rIn case you’re using the fixed() parameter of the powercurve function, and calculating the power for different effects, run these at the same time (‘in parallel’) on different machines, rather than one after another.\n\rParallelize the breaks argument. The breaks argument of the powercurve function allows the calculation of power for different levels of the grouping factor passed to along. Some grouping factors are participant, trial and item. The breaks argument sets the different sample sizes for which power will be calculated. Parallelizing breaks is done by running each number of levels in a separate function. When each has been run and saved, they are combined to allow the plotting. This procedure is demonstrated below.\n\r\rParallelizing breaks\rLet’s do a minimal example using a toy lmer model. A power curve will be craeted for the fixed effect of x along different sample sizes of the grouping factor g.\nNotice that the six sections of the power curve below are serially arranged, one after another. In contrast, to enable parallel processing, each power curve would be placed in a single script, and they would all be run at the same time.\nAlthough the power curves below run in a few minutes, the settings that are often used (e.g., a larger model; fixed('x', 'sa') instead of fixed('x'); nsim = 500 instead of nsim = 50) take far longer. That is where parallel processing becomes useful.1\nlibrary(lme4)\rlibrary(simr)\r# Toy model\rfm = lmer(y ~ x + (x | g), data = simdata)\r# Extend sample size of `g`\rfm_extended_g = extend(fm, along = \u0026#39;g\u0026#39;, n = 12)\r# Parallelize `breaks` by running each number of levels in a separate function.\r# 4 levels of g\rpwcurve_4g = powerCurve(fm_extended_g, fixed(\u0026#39;x\u0026#39;), along = \u0026#39;g\u0026#39;, breaks = 4, nsim = 50, seed = 123, # No progress bar\rprogress = FALSE)\r# 6 levels of g\rpwcurve_6g = powerCurve(fm_extended_g, fixed(\u0026#39;x\u0026#39;), along = \u0026#39;g\u0026#39;, breaks = 6, nsim = 50, seed = 123, # No progress bar\rprogress = FALSE)\r# 8 levels of g\rpwcurve_8g = powerCurve(fm_extended_g, fixed(\u0026#39;x\u0026#39;), along = \u0026#39;g\u0026#39;, breaks = 8, nsim = 50, seed = 123, # No progress bar\rprogress = FALSE)\r# 10 levels of g\rpwcurve_10g = powerCurve(fm_extended_g, fixed(\u0026#39;x\u0026#39;), along = \u0026#39;g\u0026#39;, breaks = 10, nsim = 50, seed = 123, # No progress bar\rprogress = FALSE)\r# 12 levels of g\rpwcurve_12g = powerCurve(fm_extended_g, fixed(\u0026#39;x\u0026#39;), along = \u0026#39;g\u0026#39;, breaks = 12, nsim = 50, seed = 123, # No progress bar\rprogress = FALSE)\r\nHaving saved each section of the power curve, we must now combine them to be able to plot them together.\n# Create a destination object using any of the power curves above.\rall_pwcurve = pwcurve_4g\r# Combine results\rall_pwcurve$ps = c(pwcurve_4g$ps[1], pwcurve_6g$ps[1], pwcurve_8g$ps[1], pwcurve_10g$ps[1], pwcurve_12g$ps[1])\r# Combine the different numbers of levels.\rall_pwcurve$xval = c(pwcurve_4g$nlevels, pwcurve_6g$nlevels, pwcurve_8g$nlevels, pwcurve_10g$nlevels, pwcurve_12g$nlevels)\rprint(all_pwcurve)\r## Power for predictor \u0026#39;x\u0026#39;, (95% confidence interval),\r## by number of levels in g:\r## 4: 46.00% (31.81, 60.68) - 40 rows\r## 6: 74.00% (59.66, 85.37) - 60 rows\r## 8: 92.00% (80.77, 97.78) - 80 rows\r## 10: 98.00% (89.35, 99.95) - 100 rows\r## 12: 100.0% (92.89, 100.0) - 120 rows\r## ## Time elapsed: 0 h 0 m 8 s\rplot(all_pwcurve, xlab = \u0026#39;Levels of g\u0026#39;)\r# For reproducibility purposes\rsessionInfo()\r## R version 4.1.2 (2021-11-01)\r## Platform: x86_64-apple-darwin17.0 (64-bit)\r## Running under: macOS Big Sur 10.16\r## ## Matrix products: default\r## BLAS: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\r## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\r## ## locale:\r## [1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8\r## ## attached base packages:\r## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages:\r## [1] simr_1.0.5 lme4_1.1-28 Matrix_1.4-0 ## [4] knitr_1.37 xaringanExtra_0.5.5\r## ## loaded via a namespace (and not attached):\r## [1] tidyselect_1.1.2 xfun_0.29 bslib_0.3.1 purrr_0.3.4 ## [5] splines_4.1.2 lattice_0.20-45 carData_3.0-5 vctrs_0.3.8 ## [9] generics_0.1.2 htmltools_0.5.2 yaml_2.3.5 mgcv_1.8-38 ## [13] utf8_1.2.2 rlang_1.0.1 jquerylib_0.1.4 nloptr_2.0.0 ## [17] pillar_1.7.0 glue_1.6.1 uuid_1.0-3 plyr_1.8.6 ## [21] binom_1.1-1 lifecycle_1.0.1 stringr_1.4.0 blogdown_1.8 ## [25] evaluate_0.15 fastmap_1.1.0 RLRsim_3.1-6 parallel_4.1.2 ## [29] pbkrtest_0.5.1 fansi_1.0.2 broom_0.7.12 Rcpp_1.0.8 ## [33] backports_1.4.1 plotrix_3.8-2 jsonlite_1.8.0 abind_1.4-5 ## [37] digest_0.6.29 stringi_1.7.6 bookdown_0.24 dplyr_1.0.8 ## [41] grid_4.1.2 cli_3.2.0 tools_4.1.2 magrittr_2.0.2 ## [45] sass_0.4.0 tibble_3.1.6 tidyr_1.2.0 pkgconfig_2.0.3 ## [49] crayon_1.5.0 car_3.0-12 MASS_7.3-55 ellipsis_0.3.2 ## [53] minqa_1.2.4 rmarkdown_2.11 rstudioapi_0.13 iterators_1.0.14\r## [57] R6_2.5.1 boot_1.3-28 nlme_3.1-155 compiler_4.1.2\r\nJust the code\r\rlibrary(lme4)\rlibrary(simr)\r# Toy model\rfm = lmer(y ~ x + (x | g), data = simdata)\r# Extend sample size of `g`\rfm_extended_g = extend(fm, along = \u0026#39;g\u0026#39;, n = 12)\r# Parallelize `breaks` by running each number of levels in a separate function.\r# 4 levels of g\rpwcurve_4g = powerCurve(fm_extended_g, fixed(\u0026#39;x\u0026#39;), along = \u0026#39;g\u0026#39;, breaks = 4, nsim = 50, seed = 123, # No progress bar\rprogress = FALSE)\r# 6 levels of g\rpwcurve_6g = powerCurve(fm_extended_g, fixed(\u0026#39;x\u0026#39;), along = \u0026#39;g\u0026#39;, breaks = 6, nsim = 50, seed = 123, # No progress bar\rprogress = FALSE)\r# 8 levels of g\rpwcurve_8g = powerCurve(fm_extended_g, fixed(\u0026#39;x\u0026#39;), along = \u0026#39;g\u0026#39;, breaks = 8, nsim = 50, seed = 123, # No progress bar\rprogress = FALSE)\r# 10 levels of g\rpwcurve_10g = powerCurve(fm_extended_g, fixed(\u0026#39;x\u0026#39;), along = \u0026#39;g\u0026#39;, breaks = 10, nsim = 50, seed = 123, # No progress bar\rprogress = FALSE)\r# 12 levels of g\rpwcurve_12g = powerCurve(fm_extended_g, fixed(\u0026#39;x\u0026#39;), along = \u0026#39;g\u0026#39;, breaks = 12, nsim = 50, seed = 123, # No progress bar\rprogress = FALSE)\r# Create a destination object using any of the power curves above.\rall_pwcurve = pwcurve_4g\r# Combine results\rall_pwcurve$ps = c(pwcurve_4g$ps[1], pwcurve_6g$ps[1], pwcurve_8g$ps[1], pwcurve_10g$ps[1], pwcurve_12g$ps[1])\r# Combine the different numbers of levels.\rall_pwcurve$xval = c(pwcurve_4g$nlevels, pwcurve_6g$nlevels, pwcurve_8g$nlevels, pwcurve_10g$nlevels, pwcurve_12g$nlevels)\rprint(all_pwcurve)\rplot(all_pwcurve, xlab = \u0026#39;Levels of g\u0026#39;)\r\r\n\r\rReferences\rBrysbaert, M., \u0026amp; Stevens, M. (2018). Power analysis and effect size in mixed effects models: A tutorial. Journal of Cognition, 1(1), 9. http://doi.org/10.5334/joc.10\nGreen, P., \u0026amp; MacLeod, C. J. (2016). SIMR: An R package for power analysis of generalized linear mixed models by simulation. Methods in Ecology and Evolution 7(4), 493–498, https://doi.org/10.1111/2041-210X.12504\nKumle, L., Vo, M. L. H., \u0026amp; Draschkow, D. (2021). Estimating power in (generalized) linear mixed models: An open introduction and tutorial in R. Behavior Research Methods, 1–16. https://doi.org/10.3758/s13428-021-01546-0\nLuke, S. G. (2017). Evaluating significance in linear mixed-effects models in R. Behavior Research Methods, 49(4), 1494–1502. https://doi.org/10.3758/s13428-016-0809-y\n\r\n\r\rThe number of simulations set by nsim should be larger (Brysbaert \u0026amp; Stevens, 2018; Green \u0026amp; MacLeod, 2016). In addition, the effect size for x should be adjusted to the value that best fits with the planned study (Kumle et al., 2021).↩︎\n\r\r\r","date":1627055214,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627055214,"objectID":"b0969a1cecf8eef662e221c5f20b47eb","permalink":"https://pablobernabeu.github.io/2021/parallelizing-simr-powercurve/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2021/parallelizing-simr-powercurve/","section":"post","summary":"The powercurve function from the simr package in R (Green \u0026amp; MacLeod, 2016) can incur very long running times when the method used for the calculation of p values is Kenward-Roger or Satterthwaite (see Luke, 2017). Here I suggest three ways for cutting down this time.\nWhere possible, use a high-performance (or high-end) computing cluster. This removes the need to use personal computers for these long jobs.\n\rIn case you’re using the fixed() parameter of the powercurve function, and calculating the power for different effects, run these at the same time (‘in parallel’) on different machines, rather than one after another.","tags":["s","power analysis"],"title":"Parallelizing simr::powercurve() in R","type":"post"},{"authors":[],"categories":[],"content":"This blog post does not provide a complete overview of the atmosphere at university or in academia, where I think that fairness is far more common than discrimination. Let's keep up the good work and be mindful of the rest.\r\rThere is discrimination at European universities and in the corresponding academia. Minorities are too often oppressed and abused through the use of casual remarks, concerted attacks, unequal respect towards different groups, unfair hiring decisions and other negligent behaviours. Professional and personal negligence is the reason why this discrimination is tolerated and condoned by staff of higher authority. Cases of discrimination are not normally shared by the universities, but they are sometimes registered on the websites of student unions, other student forums, or in the press. Discrimination must not be tolerated either at university or in academia. I hope this blog post provides some useful tips for dealing with discrimination from university students and staff alike.\nHomo homini lupus Xenophobia is one of the types of discrimination that sometimes crop up in halls of residence, departments, panels, committees, offices. Xenophobia exists where you are now, as it does where you come from. It is intrinsic to human societies (see Hinton, 2017). In society at large, who doesn't know the way in which different nationalities are ranked in a certain place? The specifics of these hierarchies vary across regions and individuals (thank goodness); and yet, xenophobia always tends to be underlain by an unfair, puny spirit.\nHierarchies among nationalities In society at large, the locals tend to be first. Some locals sometimes utilise official devices and everyday tricks to ensure that immigrants are second to them. We must all remember that more people should not mean more rights. Each person is a person.\nThe first rank of immigrants is formed of one or two nationalities, smiled at by the fortune of historical or cultural ties. The second rank of immigrants is that of merely-tolerated nationalities. The third rank is formed of immigrants who are despised due to historical, political, cultural factors.\nSadly and dangerously enough, in competitive or onerous circumstances (when is that not the case, though!), homo homini lupus est (people are wolves unto people). Then, discrimination breaks loose (Abbink, \u0026amp; Harris, 2019; Meiring et al., 2014; Mendes \u0026amp; Koslov, 2013).\nPolicies Policy statements are increasingly popular and they give reasons for hope. For instance, the employment webpage of Duke University reads (on 21st July 2021; https://hr.duke.edu/policies/diversity/eeo):\n Duke University prohibits discrimination and harassment, and provides equal employment opportunity without regard to an individual's age, color, disability, gender, gender expression, gender identity, genetic information, national origin, race, religion, sex, sexual orientation, or veteran status.\n Such statements, if indeed observed by the employees, constitute historical\u0026mdash;why, historic!\u0026mdash;changes. They should be celebrated, applauded\u0026mdash;even if they simply adhere to fundamental, human rights.\nDealing with discrimination Discrimination takes many forms. Accordingly, surviving and confronting it requires various approaches. Stoicism, bravery, protection and ingenuity are all valuable against the xenophobia that lurks, and occasionally bites, in halls of residence, informal meetings, committees, offices, and other settings at universities across Europe. Inspiration for such approaches can be found in many places\u0026mdash;even in popular literature. For instance, Harper Lee's To Kill a Mockingbird contains a passage in which a lawyer is mobbed by nostalgic townspeople who do not want a Black citizen to be treated as a human. The strategies for the defence from the lynch mob span from the lawyer's stoicism to his son's adolescent bravery, and from the invisible protection offered by a witness to the disarming ingenuity of the lawyer's eight-year-old daughter, who spots one of her neighbours in the mob.\nCollection and presentation of evidence Ironically, complaint cases are sometimes initiated by those who have exerted discrimination. If you receive a complaint and you feel you have been discriminated against, whether before or through the said complaint, it is important to note this in your response.\nThe discrimination can be well described by producing an extensive document that recounts every relevant episode, with the applicable evidence being attached. Don't fret about reading time: the larger the document, the better.\nName it. Discrimination has precise names\u0026mdash;for instance, xenophobia\u0026mdash;, which must be used where relevant and necessary. People know the meaning: perpetrators are likely know it and so will the authorities or mediators who are in charge of resolving the case. Whether these people like the word to be used or not is their own problem, not yours. Nonetheless, more general, and often more objective, hypernyms are also available, such as \u0026lsquo;discrimination\u0026rsquo; and \u0026lsquo;unfairness\u0026rsquo;.\nCase study This year, I was bullied by some of my flatmates. This bullying was finally underscored when they submitted false accusations about me to the Graduate College Accommodation Office at Lancaster University. The latter office embraced and acted upon these accusations. As a result, a flat meeting was suggested by the Deanery. The Deanery would not initially allow me to receive information in writing and to respond likewise. On the contrary, they required a live meeting. As I repeatedly countered, I thought that a written procedure would cause less stress, yield less bias, and allow a more accurate registration of every input for future reference.\n It is due to three reasons that I would prefer working on the basis of a brief written report, to which I could also respond in writing. These reasons are:\n I think that the written mode helps remain more impartial than the oral mode. I think a written discussion would help in keeping a clear, written record of every part of the report. I think a written discussion causes less stress than an oral one.  I am hopeful that, having presented my preference for responding in writing to a written report, you may acknowledge my personal preference, and perhaps allow me to proceed in that way.\n A written discussion\u0026mdash;e.g., via email\u0026mdash; is a valuable option in situations that could be unfair or abusive.\nFinally, when I was presented with the necessary written information, I replied with a 36-page document, and participated in a one-hour disciplinary hearing. By those means, I described the way in which some of my flatmates had bullied me throughout the year, finally reinforced through the false accusations they had made. They had lied about their own behaviour, too, while they were at it.\nI think this bullying was motivated by the legitimate differences in the way we were each concerned about the Covid-19 pandemic, along with differences in academic stage, age, nationality and physical appearance. As a general rule, people tend to have difficulties to respect or tolerate others who are different from them. In the present instance, this difficulty drove my flatmates to submit false accusations about me while forgetting crucial information about their own behaviour. The time-consuming, 36-page response that I composed helped clarify the state of affairs. Upon a formal review, I was acquitted on all accusations.\nIt is important to express one's feeling of unfairness where it exists. I did so, following the Dean's resolution, through the email below, which I sent to the College (private information replaced with asterisks).\n\rDear All,\nThe Dean has produced a resolution to the disciplinary hearing that was organised due to complaints from some of my flatmates. This resolution considers my flatmates\u0026rsquo; complaints as well as the 36-page response that I submitted and the one-hour hearing that finally took place. In the resolution, I was acquitted of all three alleged violations (my response to the summons and the Dean's resolution are attached to the present email).\nAs part of a fair procedure, I received documents that reflected my flatmates\u0026rsquo; complaints, as well as the treatment of those complaints from the Graduate Accommodation Office. I would be grateful for your consideration of my views on the role of the Graduate Accommodation Office.\nBiased treatment of accusations\nI have considered the response from the Graduate Accommodation Office as the complaints were raised, and until I was summoned to a disciplinary hearing. In my view, the Graduate Accommodation Office failed to demonstrate enough impartiality, which resulted in unfairness towards me. The claims made by some of my flatmates via email were acted upon by the Accommodation Office without an important consideration\u0026ndash;namely, that the accusations could be deliberately false or inaccurate. Indeed, the accusations happened to be so, as I extensively argued in my response to the summons by providing detailed descriptions of the events and documentary evidence.\nDisappointingly, the embracement of my flatmates\u0026rsquo; accusations by the Graduate Accommodation Office contrasts with the caution that is rightly applied when one calls the porters. I have sometimes called over the past three years regarding residents smoking indoors. Every time, they made me aware of the need for them to catch any students as they are smoking indoors. In the absence of the witnessing by the porters, the students can only be informed about the complaint made, but no action can be taken against them. This is a correct precaution against any false or misinformed accusations. In contrast, I think that the Graduate Accommodation Office, in some of their emails with some of my flatmates, embraced the accusations on a one-sided manner, and finally passed on these accusations to the Dean without having had any input from me about the veracity thereof.\nBiased provision of evidence\nAmong the documents included in the disciplinary case, the Graduate Accommodation Office included a complaint that I had raised regarding the persistent failures to prevent a Covid-19 infection by my flatmates and by the housekeeper. My complaint should by no means have been mixed with the complaints against me. Its inclusion is a demonstration of the bias with which I was treated.\nLegitimate differences across flatmates contributing to discrimination\nI think that the biased response from the response from the Graduate Accommodation Office failed to consider the legitimate differences across the residents in these flats. The Graduate Accommodation Office know (as indeed they should) that the seven-person flats in this college are bound to include residents who are legitimately different from each other. In my flat this year, for instance, we have had important differences regarding academic stage, age and country of origin.\n  Academic stage: five master's students, one first-year PhD student and one third-year PhD student (I am the latter one).\n  Age: all my flatmates are under ***, whereas I am ***.\n  Country of origin: three ***, one ***, one ***, one *** and one *** (I am the latter one).\n  These differences must always be considered, as they can contribute to ingroup-outgroup bias in flats. Some of this bias could stem from xenophobia. Unfortunately, Lancaster University has had infamous cases of xenophobia in the past (https://scan.lancastersu.co.uk/2013/05/27/editorial-anonymous-bullying-on-facebook-groups-is-pure-cowardice/).\nIn conclusion, would you please consider the following remarks:\n  I think that the treatment of my flatmates\u0026rsquo; complaints was unfair and biased against me.\n  I think my flatmates aimed to have me expelled from the flat. Their spite was due to me being the only one who has been properly concerned about the virus in this flat, and due to the legitimate differences that existed between me and my flatmates regarding academic stage, age and country of origin, as listed above.\n  I think that this case demonstrates that it is extremely harmful to embrace and act upon accusations in a one-sided manner, as it can result in discrimination. The biased response from the Graduate Accommodation Office has instilled a fear from false accusations in me. As a result, I feel that henceforth I will have to monitor whether I or other residents are discriminated against by the Graduate Accommodation Office.\n  I hope that the Graduate Accommodation Office can acknowledge my experience of the present issue, and consider it to allow a fair treatment towards every resident in the future. As such, henceforth, I will consider the present email as an integral part of this process, and I hope that the Graduate Accommodation Office does too.\nThank you for your attention,\nPablo\n\rWhen I had not received a reply within two weeks, I followed up with another email referring to details in the documents that were suggestive of a bias among the staff. This staff had followed directions from some of my flatmates without requesting my perspective on the issue, my side of things, my feedback, with the same degree of trust. Why? Why had they discriminated against me?\u0026mdash;I asked them. Did I not pay the same accommodation fees as the others? Was I not the same kind of a customer? This time, I did not limit my feedback to the Graduate College: I found it necessary to share my disappointment with some other services of the university as well.\n I sincerely believe that the Graduate College Accommodation Office and the Graduate College Deanery would benefit from some training in dealing with complaints fairly, efficiently and professionally. I believe such a training would benefit all students, especially those\u0026ndash;like myself\u0026ndash;who are more likely to be bullied by flatmates due to having a different profile (country of origin, physical appearance, academic requirements, leisure habits, etc.). My flatmates\u0026rsquo; bullying towards me began in the first week, when they blamed me for placing my food on shelves of the fridge which they had unilaterally chosen. They had assigned me a shelf\u0026mdash;one at the bottom of the fridge. Episodes extended throughout the year, as I extensively described in my 36-page response to the Dean's summons, which I can share with you if that were necessary for the revision of the conditions at Graduate College.\n[\u0026hellip;]\nThe present case has already robbed me of precious time. I wonder if this was part of the intention of some persons involved in this process. In any case, I hope this information will be helpful for the future. Discrimination must be confronted at this university. I pay the same accommodation fees and, logically, I deserve the same trust and care as the most beloved of residents.\n A design feature? Perhaps societies are discriminatory by design. As an illustration, a comment by John Bercow on the xenophobia that lurks in a large party provides an idea of the scale of the problem. Perhaps we are not doing enough about this. This insufficiency might explain, for instance, why the death of an innocent Slovak citizen in police custody at Charleroi Airport (Brussels, Belgium) was barely and poorly covered by the European media. In case that you tolerate graphic footage, please consider the unnecessary, illegal treatment given to Jozef Chovanec. Chovanec's case is but an example. The news archives contain evidence of abuses to minority citizens in many European countries since the 2000s to this day.\nCeaseless awareness Lobbying, whether we like it or not, Activism is one of the methods needed to confront this plight. So, talk about it. Make sure that other potential targets of it know that it is not fair, that they should not have to put up with it, and that they might be able to confront it in one way or another. A ceaseless awareness, fostered by a relentless activism, will also help those who exert discrimination themselves, as arguably, they are often unaware of their own act.\nThe internet The creation of awareness does not have to fully depend on close relationships. Email and social media allow safe activity where anonymity is necessary.\nConclusion The university and the academia are formed by members of society. Logically, therefore, care must taken to ward off discriminatory tendencies that exist in society.\nIf you have suffered discrimination, or you are a potential target for it, please ensure that you are aware of this plague. Regardless of the rank that a given type of discrimination occupies in the social agenda, no discrimination should be tolerated. In addition to being unethical, discrimination breaches human and fundamental rights, national laws and institutional rules.\nIf you are not a target but an ally against discrimination, please continue to confront it where you witness it, and to facilitate a widespread awareness.\nReferences Abbink, K., \u0026amp; Harris, D. (2019). In-group favouritism and out-group discrimination in naturally occurring groups. PloS ONE, 14(9), e0221616. https://doi.org/10.1371/journal.pone.0221616\nHinton, P. (2017). Implicit stereotypes and the predictive brain: cognition and culture in “biased” person perception. Palgrave Communications, 3(1), 1-9. https://doi.org/10.1057/palcomms.2017.86\nMeiring, L., Subramoney, S., Thomas, K. G., Decety, J., \u0026amp; Fourie, M. M. (2014). Empathy and helping: Effects of racial group membership and cognitive load. South African Journal of Psychology, 44(4), 426-438. https://doi.org/10.1177/0081246314530280\nMendes, W. B., \u0026amp; Koslov, K. (2013). Brittle smiles: Positive biases toward stigmatized and outgroup targets. Journal of Experimental Psychology: General, 142(3), 923–933. https://doi.org/10.1037/a0029663\n\r\r\r","date":1626874444,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626874444,"objectID":"84d12412b2d7cbd970ee6e147b8aa916","permalink":"https://pablobernabeu.github.io/2021/surviving-discrimination-and-confronting-it/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2021/surviving-discrimination-and-confronting-it/","section":"post","summary":"This blog post does not provide a complete overview of the atmosphere at university or in academia, where I think that fairness is far more common than discrimination. Let's keep up the good work and be mindful of the rest.\r\rThere is discrimination at European universities and in the corresponding academia. Minorities are too often oppressed and abused through the use of casual remarks, concerted attacks, unequal respect towards different groups, unfair hiring decisions and other negligent behaviours.","tags":[],"title":"Surviving discrimination and confronting it","type":"post"},{"authors":["Pablo Bernabeu, Dermot Lynott and Louise Connell"],"categories":["conceptual processing"],"content":"\r\r\r Video\n Slides\n\n\rfitvids('.shareagain', {players: 'iframe'});\r\r\nAbstract\rResearch in conceptual processing has suggested that the comprehension of words draws on complementary cognitive systems. In the milliseconds during which a word is processed, the linguistic system is activated first. Reading the word entity, for instance, may activate words such as being, thing and object (https://smallworldofwords.org/en/project/explore; De Deyne et al., 2019). Thereupon, the embodied system is activated, incorporating sensorimotor, emotional and social dimensions (Borghi et al., 2019). For instance, entity activates visual, auditory and head-specific meanings (https://embodiedcognitionlab.shinyapps.io/sensorimotor_norms; Lynott et al., 2020). Research has also suggested that the linguistic system is more important for relatively abstract words—e.g., attempt—, whereas the embodied system is more important for more concrete words—e.g., building (Bolognesi \u0026amp; Steen, 2018). The role of individual differences has also been investigated (Dils \u0026amp; Boroditsky, 2010), although to a lesser extent. An individual’s linguistic experience (e.g., larger vocabulary) facilitates word processing and task-relevant attention (Pexman \u0026amp; Yap, 2018; Yap et al., 2017), while greater sensorimotor experience enables more detailed meaning activation within specific conceptual areas (e.g., space: Vukovic \u0026amp; Williams, 2015). The variation across individuals and items, within both the linguistic and embodied systems, is seldom considered simultaneously. We are undertaking this in two studies. The first study (Bernabeu et al., 2021) will merge existing datasets (Lynott et al., 2020; Pexman et al., 2017; Pexman \u0026amp; Yap, 2018; Wingfield \u0026amp; Connell, 2019). The second study will collect novel data to investigate questions such as the unique roles of vocabulary size, sensorimotor experience and attentional control. To determine the sample size for the latter study, two pilot studies with larger-than-average samples were conducted, using the aforementioned datasets and that of Hutchison et al. (2013). Simulation-based, prospective power curves were performed. These pilots revealed important roles for linguistic and embodied information, vocabulary size, and attentional control, as well as statistical power considerations.\n\rReferences\rBernabeu, P., Lynott, D., \u0026amp; Connell, L. (2021). Preregistration: The interplay between linguistic and embodied systems in conceptual processing. OSF. https://osf.io/ftydw/\nBolognesi, M., \u0026amp; Steen, G. (2018). Abstract concepts: Structure, processing, and modeling. Topics in Cognitive Science, 10(3), 490–500. https://doi.org/10.1111/tops.12354\nBorghi, A., M., Barca, L., Binkofski, F., Castelfranchi, C., Pezzulo, G., \u0026amp; Tummolini, L. (2019). Words as social tools: Language, sociality and inner grounding in abstract concepts. Physics of Life Reviews, 29, 120–53. https://doi.org/10.1016/j.plrev.2018.12.001\nDe Deyne, S., Navarro, D. J., Perfors, A., Brysbaert, M., \u0026amp; Storms, G. (2019). The “Small World of Words” English word association norms for over 12,000 cue words. Behavior Research Methods, 51, 987–1006. http://dx.doi.org/10.3758/s13428-018-1115-7\nDils, A. T., \u0026amp; Boroditsky, L. (2010). Visual motion aftereffect from understanding motion language. Proceedings of the National Academy of Sciences, 107(37), 16396-16400. https://doi.org/10.1073/pnas.1009438107\nHutchison, K. A., Balota, D. A., Neely, J. H., Cortese, M. J., Cohen-Shikora, E. R., Tse, C.-S., Yap, M. J., Bengson, J. J., Niemeyer, D., \u0026amp; Buchanan, E. (2013). The semantic priming project. Behavior Research Methods, 45, 1099–1114. https://doi.org/10.3758/s13428-012-0304-z\nLynott, D., Connell, L., Brysbaert, M., Brand, J., \u0026amp; Carney, J. (2020). The Lancaster Sensorimotor Norms: Multidimensional measures of perceptual and action strength for 40,000 English words. Behavior Research Methods, 52, 1-21. https://doi.org/10.3758/s13428-019-01316-z\nPexman, P. M., Heard, A., Lloyd, E., \u0026amp; Yap, M. J. (2017). The Calgary semantic decision project: Concrete/abstract decision data for 10,000 English words. Behavior Research Methods, 49(2), 407–417. https://doi.org/10.3758/s13428-016-0720-6\nPexman, P. M., \u0026amp; Yap, M. J. (2018). Individual differences in semantic processing: Insights from the Calgary semantic decision project. Journal of Experimental Psychology: Learning, Memory, and Cognition, 44(7), 1091–1112.\rhttps://doi.org/10.1037/xlm0000499\nVukovic, N., \u0026amp; Williams, J. N. (2015). Individual differences in spatial cognition influence mental simulation of language. Cognition, 142, 110–122.\rhttps://doi.org/10.1016/j.cognition.2015.05.017\nWingfield, C., \u0026amp; Connell, L. (2019). Understanding the role of linguistic distributional knowledge in cognition. PsyArXiv. https://doi.org/10.31234/osf.io/hpm4z\nYap, M. J., Hutchison, K. A., \u0026amp; Tan, L. C. (2017). Individual differences in semantic priming performance: Insights from the semantic priming project. In M. N. Jones (Ed.), Frontiers of cognitive psychology. Big data in cognitive science (p. 203–226). Routledge/Taylor \u0026amp; Francis Group.\n\r\r","date":1621814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621814400,"objectID":"06718cc28b59a404538568165940872b","permalink":"https://pablobernabeu.github.io/talk/linguistic-and-embodied-systems-in-conceptual-processing-variation-across-individuals-and-items/","publishdate":"2021-05-07T23:01:48+01:00","relpermalink":"/talk/linguistic-and-embodied-systems-in-conceptual-processing-variation-across-individuals-and-items/","section":"talk","summary":"Video\n Slides\n\n\rfitvids('.shareagain', {players: 'iframe'});\r\r\nAbstract\rResearch in conceptual processing has suggested that the comprehension of words draws on complementary cognitive systems. In the milliseconds during which a word is processed, the linguistic system is activated first. Reading the word entity, for instance, may activate words such as being, thing and object (https://smallworldofwords.org/en/project/explore; De Deyne et al., 2019). Thereupon, the embodied system is activated, incorporating sensorimotor, emotional and social dimensions (Borghi et al.","tags":["cognition","conceptual processing","language comprehension","statistics","power analysis"],"title":"Linguistic and embodied systems in conceptual processing: Variation across individuals and items","type":"talk"},{"authors":null,"categories":["open science"],"content":" Video below the slides  Slideshare\rVideo embedded from: https://nuigalway.mediaspace.kaltura.com/media/OSW2021A+OSCG+Open+Scholarship+Prize+-+The+Final!/1_d7ekd3d3/121659351#t=56:08\n\r","date":1620950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620950400,"objectID":"5a81f4095dc1bfc32348bed96eafb45f","permalink":"https://pablobernabeu.github.io/talk/towards-reproducibility-and-maximally-open-data/","publishdate":"2021-05-16T13:01:48+01:00","relpermalink":"/talk/towards-reproducibility-and-maximally-open-data/","section":"talk","summary":" Video below the slides  Slideshare\rVideo embedded from: https://nuigalway.mediaspace.kaltura.com/media/OSW2021A+OSCG+Open+Scholarship+Prize+-+The+Final!/1_d7ekd3d3/121659351#t=56:08\n\r","tags":["open science","open scholarship","open data","reproducibility"],"title":"Towards reproducibility and maximally-open data","type":"talk"},{"authors":[],"categories":["research methods","psycholinguistics"],"content":"Liu et al. (2018) present a study that implements the conceptual modality switch (CMS) paradigm, which has been used to investigate the modality-specific nature of conceptual representations (Pecher et al., 2003). Liu et al.\u0026lsquo;s experiment uses event-related potentials (ERPs; similarly, see Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013). In the design of the switch conditions, the experiment implements a corpus analysis to distinguish between purely-embodied modality switches and switches that are more liable to linguistic bootstrapping (also see Bernabeu et al., 2017; Louwerse \u0026amp; Connell, 2011). The procedure for stimulus selection was novel as well as novel; thus, it could prove useful in future studies, too. In addition, the application of bayesian statistics is an interesting and promising novelty in the present research area.\nIn reviewing the literature, Liu (2018) and Liu et al. (2018) contend that previous studies may be strongly biased due to methodological decisions in the analysis of ERPs. These decisions particularly regard the latency\u0026mdash;i.e., time windows\u0026mdash;and the topographic regions of interest\u0026mdash;i.e., subsets of electrodes. Thus, Liu et al. identify a \u0026lsquo;highly inconsistent\u0026rsquo; (p. 6) landscape in the ERP components that have been ascribed to the CMS effect in previous studies. Similarly, Liu (p. 47) writes:\n Several studies have looked for the ERP manifestations of modality switching costs (Bernabeu, Willems, \u0026amp; Louwerse, 2017; Collins, Pecher, Zeelenberg, \u0026amp; Coulson, 2011; Hald, Hocking, Vernon, Marshall, \u0026amp; Garnham, 2013; Hald, Marshall, Janssen, \u0026amp; Garnham, 2011). However, what they found was not a clear picture. Not only was a significant effect found in the time window for the N400 component, but also a so-called early N400-like effect around 300ms (Bernabeu et al., 2017; Hald et al., 2011), the N1-P2 complex around 200ms (Bernabeu et al., 2017; Hald et al., 2013, 2011), as well as the late positivity component (LPC) after 600ms (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011).\n Drastic conclusions Liu et al. (2018) conclude that a confirmatory research approach is not warranted, and adopt a semi-exploratory approach, creating time windows of 50 ms each, rather than windows linked to known ERP components (Swaab et al., 2012), and using bayesian statistics. Such a drastic conclusion appears to stem from the assumption that, if the CMS were robust enough, it would present in the same guise across studies. Such an assumption, however, may merit further examination, considering the multiplicity of known and unknown variables that may differ across experiments (Barsalou, 2019). This variability influences the replication praxis, as it were. Indeed, Liu et al. themselves allude to one such variable (p. 7).\n These previous studies did not only examine the effect of modality switching but also other linguistic factors such as negated sentences, which could easily distort observed waveforms (Luck, 2005).\n Liu et al.\u0026lsquo;s (2018) conclusions about the \u0026lsquo;inconsistent\u0026rsquo; results do not seem to duly weigh the differences across studies. None of the existing studies is a direct replication of another. The example quoted from Liu et al. above, regarding the presence of negated sentences, is one of the less important differences because one of the corresponding studies implemented the negation as a controlled, experimental condition, contrasting with affirmative sentences (Hald et al., 2013). Yet, other differences exist in virtually every aspect, from the types of modality switch used to the time-locking of ERPs. For instance, the studies vary in the implementation of the modality switches. Whereas Hald et al. (2011) distinguish between a switch and a non-switch condition, Bernabeu et al. (2017) analysed each type of switch separately\u0026mdash;i.e., auditory-to-visual, haptic-to-visual, visual-to-visual. Another difference across the studies is the onset point for ERPs. For instance, Bernabeu et al. time-locked ERPs to the point at which the modality switch is actually elicited in the CMS paradigm\u0026mdash;namely, the first word in target trials. In contrast, the other studies time-locked ERPs to the second word. In addition, these studies vary in their timelines\u0026mdash;i.e., presentation of words and inter-stimulus intervals\u0026mdash;, as well as in the words that were used as stimuli, in the preprocessing of ERPs, in the statistical analysis, and even in the language of testing in one case (all studies using English except Bernabeu et al., 2017, which used Dutch). Moreover, the studies differ in the sample size, ranging from ten finally-analysed participants (Hald et al, 2011) to 46 finally-analysed participants (Bernabeu et al., 2017). Last, the studies differ in the number of items per modality switch condition, ranging from 17 (in one of Liu et al.\u0026lsquo;s, 2018 conditions) to 40 per condition (Hald et al., 2011, 2013). Undoubtedly, seeing larger sample sizes and more stimulus items used in ERP studies is something to promote and celebrate. Last, it may be noted that Liu et al.\u0026lsquo;s stance on the inconsistency of previous results starkly contrasts with their use of a single study\u0026mdash;Hald et al. (2011)\u0026mdash;, with N = 10, as the motivation for their sample size (Albers \u0026amp; Lakens, 2018).\nClarifications Liu et al. (2018) apply bayesian statistics reportedly to help reduce the bias that may exist in the present literature. However, the reviews by Liu (2018) and Liu et al. appear to gloss over some important aspects regarding previous studies. For instance, Liu writes (p. 53):\n The inflation of the probability of Type I error leads to an over-confidence in the interpretation of the results. For example, in the studies on modality switching costs, different time windows were chosen to test the early effect of modality switching costs. While Bernabeu et al. (2017), Hald et al. (2011) and Hald et al. (2013) examined the segment of ERP waveform between 190ms and 300ms or 160ms and 215ms based on visual inspection and found significant effects, Collins et al. (2011) chose a prescribed time window between 100ms and 200ms before the analysis and did not find the effect.\n Liu also refers to the issue of multiple tests related to the multiple time windows and electrodes we find in ERP studies (p. 59):\n It is typical for ERP studies to conduct multiple comparisons (e.g., running the same ANOVA repeatedly on different subsets of data like different time windows and different groups of electrodes). This would massively increase Type I error if no post hoc correction is conducted. However, if Bonferroni or other correction is conducted, it will render the study over-conservative, thus increasing the chance of Type II error. In the present thesis, 90 electrodes will be analysed individually, with 20 time slices in each trial. That results in 1800 NHSTs for each critical variable. A correction of multiple comparison will require a critical level of 2.78 x 10ˆ-5 for each test for a family-wise critical level of .05 (and an uncorrected test will almost definitely lead to false positive results). This stringent criterion could conceivably render it meaningless any p-values we can obtain from a statistical package.\n Arguably, the scenario presented by Liu (2018), in which a researcher could conduct a purely data-driven analysis of ERP data, is extreme. The field of psycholinguistics, in general, does not have a tradition of purely data-driven analysis. Instead, it blends a humanistic background with a scientific methodology. As a result, the hypotheses and methods tend to be largely driven by the available literature. For instance, Bernabeu et al. (2017, quoted below from p. 1632) based their time windows and regions of interest on the most relevant of the preceding studies (also see Bernabeu, 2017).\n Electrodes were divided into an anterior and a posterior area (also done in Hald et al., 2011). Albeit a superficial division, we found it sufficient for the research question. Time windows were selected as in Hald et al., except for the last window, which was extended up to 750 ms post word onset, instead of 700 ms, because the characteristic component of that latency tends to extend until then, as we confirmed by visual inspection of these results.\n The literature-based approach follows the advice from Luck and Gaspelin (2017, p. 149), who wrote: \u0026lsquo;a researcher who wants to avoid significant but bogus effects would be advised to focus on testing a priori predictions without using the observed data to guide the selection of time windows or electrode sites\u0026rsquo; (also see Armstrong, 2014; for a more conservative stance, see Luck \u0026amp; Gaspelin, 2017). In addition, notice that the extension of the last window by 50 ms was informed by Swaab et al. (2012), who report results by which the P600 component (the main component occurring after the N400 in word reading) extended up to 800 ms.\nNext, Liu et al. (2018, pp. 6\u0026ndash;7) write:\n However, the findings of these components have been highly inconsistent. The N400 effect alone was found in the posterior region in some cases (Bernabeu et al., 2017; Hald et al., 2013), while in anterior region in others (Collins et al., 2011, Hald et al. (2011)).\n Yet, Bernabeu et al. (2017, p. 1632) had written:\n In certain parts over the time course, the effect appeared in both anterior and posterior areas\n Liu et al. (2018) continue (p. 7):\n In some cases, it was found in the typical window around 400ms (Collins et al., 2011), while in others an earlier window from 270ms to 370ms (Bernabeu et al., 2017; Hald et al., 2011).\n However, Bernabeu et al. (2017, p. 1632) had written:\n The ERP results revealed a CMS effect from Time Window 1 on, larger after 350 ms.\n Regarding the time-locking of ERPs, Liu (2018, p. 43) writes:\n Because the properties were usually salient for the concepts, the switching costs might have already been incurred when participants were processing the concept word. Bernabeu et al. (2017), in their recent replication of previous ERP studies, reversed the order of concept and property and did not find an immediate effect from the property onset. In future studies, it is recommended to adopt the reverse order, control the concept words so that they do not automatically activate the properties before the words are shown, or analyse epochs after both the concept and property words.\n The above excerpt seems to reveal a misunderstanding of Bernabeu et al.\u0026lsquo;s (2017) method and results (we may assume that, by \u0026lsquo;immediate\u0026rsquo;, Liu (2018) is referring to the 200 ms point or afterwards, since that is about as immediate as it gets; see Amsel et al., 2014; Swaab et al., 2012; Van Dam et al., 2014). Bernabeu et al.\u0026lsquo;s abstract mentioned (p. 1629):\n Event-Related Potentials (ERPs) were time-locked to the onset of the first word (property) in the target trials so as to measure the effect online and to avoid a within-trial confound. A switch effect was found, characterized by more negative ERP amplitudes for modality switches than no-switches. It proved significant in four typical time windows from 160 to 750 milliseconds post word onset, with greater strength in posterior brain regions, and after 350 milliseconds.\n Indeed, time-locking ERPs to the beginning of the trial was one of the principal features of Bernabeu et al.\u0026lsquo;s (2017) experiment. Complementing that, the property words were placed in the first trial because they are more perceptually loaded than concepts (Lynott \u0026amp; Connell, 2013), thus better suiting the main basis of the CMS paradigm. We know that the semantic processing of a word often commences within the first 200 ms (Amsel et al., 2014; Van Dam et al., 2014). Considering the importance of the time course in the grounding of conceptual representations (Hauk, 2016), it seems important to measure the CMS from the moment that it is elicited\u0026mdash;namely, in all experiments, from the first word of the target trial (Bernabeu, 2017; Bernabeu et al., 2017), rather than letting several hundreds of milliseconds elapse. Nonetheless, from a methodological perspective, it would be interesting to compare the two approaches in a dedicated study. This would precisely reveal the speed at which modality-specific meaning becomes activated during conceptual processing.\nOutstanding issues: Random effects and correction for multiple tests A methodological issue affecting the statistical analysis of all the studies hereby considered (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013; Liu et al., 2018) is the absence of some applicable random effects. Some of the studies did not apply any random effects (Collins et al., 2011; Hald et al., 2011, 2013). Even in psycholinguistics, use of linear mixed-effects models is still increasing (Meteyard \u0026amp; Davies, 2020; Yarkoni, 2020). Yet, in those studies that did apply random effects, the corresponding structure was not as exhaustive as it should have been, as they lacked random slopes. In Bernabeu et al. (2017), a model selection approach was applied (Matuschek et al., 2017), whereby each random effect was tested and only kept in the model if it significantly improved the fit. In Liu et al. (2018), random slopes were deemed unfeasible due to computational constraints (for background, see Brauer \u0026amp; Curtin, 2017). Applying a complete random effects structure is important for a robust statistical analysis (Barr et al., 2013; Yarkoni, 2020).\nAnother issue is that of multiple tests. Where a small number of levels is used (e.g., time windows, topographic regions of interest) and these are informed by the literature, the advice has often been ambiguous as to whether a correction should be applied (e.g., Armstrong, 2014; for a more conservative stance, see Luck \u0026amp; Gaspelin, 2017). Indeed, no correction was applied in any of the four studies that used frequentist statistics (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013).\nReanalysis of Bernabeu et al. (2017) The results of Bernabeu et al. (2017) were reanalysed after publication using a more complete random effects structure that incorporated by-participant random slopes for the modality-switch factor\u0026mdash;i.e., (condition | participant). The results were also corrected for multiple tests using the Holm-Bonferroni correction (Holm, 1979). For this purpose, the lowest p-value in the four time windows was multiplied by 4, the next p-value by 3, the next by 2, and the highest p-value was left as it unmodified. In this stepwise correction, if a nonsignificant p-value was reached, all the subsequent p-values became nonsignificant (see analysis script). The results differed from the original, slopes-free models (available script and results) in that the main effect of the modality switch factor became nonsignificant in the second time window (160\u0026ndash;216 ms) and in the fourth one (500\u0026ndash;750 ms), while remaining significant in the third time window (350\u0026ndash;550 ms). Incidentally, note that main effects may not be directly interpretable as the same same variables are present interactions (Kam \u0026amp; Franzese, 2007). Yet, the interactions of modality switch with participant group (quick/slow) and scalp location (anterior/posterior) retained the same significance.\nOpen questions Liu (2018) and Liu et al. (2018) raise interesting and important questions. Firstly, future research may be conducted to investigate what determines the variability of ERP results\u0026mdash;in terms of ERP components, time windows and topographic regions of interest. This research could include a comparison with other measurements, such as response times, to test whether ERPs are less reliable\u0026mdash;i.e., more variable across studies\u0026mdash;than response times. Similarly, future research may investigate whether the ERP literature is more biased than literature employing other measures, such as response times. In addition, future research could investigate whether moving to exploratory, bayesian research designs is a necessary or sufficient condition to reduce bias in research and improve the precision of experimental measurements. Current alternatives to such an approach include direct (or conceptual) replications designed to achieve a higher power than previous studies (e.g., Chen et al., 2019). Arguably, policies determining funding decisions would need to change if we are to fully acknowledge the importance of direct replication (Howe \u0026amp; Perfors, 2018; Kunert, 2016; Simons, 2014; Zwaan et al., 2018). Last, future research may investigate whether \u0026lsquo;clear picture\u0026rsquo; results are realistic, desirable or necessary, and whether unclear-picture results should be eschewed; or whether, on the contrary, clear-picture results may largely be the product of publication bias\u0026mdash;that is, the pressure to hide or misreport those aspects of a study that could challenge its acceptance by peer-reviewers or any other academics.\nReferences Albers, C., \u0026amp; Lakens, D. (2018). When power analyses based on pilot data are biased: Inaccurate effect size estimators and follow-up bias. Journal of Experimental Social Psychology, 74, 187–195. https://doi.org/10.1016/j.jesp.2017.09.004\nAmsel, B. D., Urbach, T. P., \u0026amp; Kutas, M. (2014). Empirically grounding grounded cognition: the case of color. Neuroimage, 99, 149-157. https://doi.org/10.1016/j.neuroimage.2014.05.025\nArmstrong, R. A. (2014). When to use the Bonferroni correction. Ophthalmic and Physiological Optics, 34(5), 502-508. https://doi.org/10.1111/opo.12131\nBarr, D. J., Levy, R., Scheepers, C., \u0026amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68, 255–278. http://dx.doi.org/10.1016/j.jml.2012.11.001\nBarsalou, L. W. (2019). Establishing generalizable mechanisms. Psychological Inquiry, 30(4), 220-230. https://doi.org/10.1080/1047840X.2019.1693857\nBernabeu, P. (2017). Modality switches occur early and extend late in conceptual processing: evidence from ERPs [Master's thesis]. School of Humanities, Tilburg University. https://psyarxiv.com/5gjvk\nBernabeu, P., Willems, R. M., \u0026amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, \u0026amp; E. J. Davelaar (Eds.), Proceedings of the 39th Annual Conference of the Cognitive Science Society (pp. 1629-1634). Austin, TX: Cognitive Science Society. https://doi.org/10.31234/osf.io/a5pcz\nBrauer, M., \u0026amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. Psychological Methods, 23(3), 389–411. https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf\nChen, S., Szabelska, A., Chartier, C. R., Kekecs, Z., Lynott, D., Bernabeu, P., … Schmidt, K. (2018). Investigating object orientation effects across 14 languages. PsyArXiv. https://doi.org/10.31234/osf.io/t2pjv/\nCollins, J., Pecher, D., Zeelenberg, R., \u0026amp; Coulson, S. (2011). Modality switching in a property verification task: an ERP study of what happens when candles flicker after high heels click. Frontiers in Psychology, 2. https://doi.org/10.3389/fpsyg.2011.00010\nHald, L. A., Hocking, I., Vernon, D., Marshall, J.-A., \u0026amp; Garnham, A. (2013). Exploring modality switching effects in negated sentences: further evidence for grounded representations. Frontiers in Psychology, 4, 93. https://doi.org/10.3389/fpsyg.2013.00093\nHald, L. A., Marshall, J.-A., Janssen, D. P., \u0026amp; Garnham, A. (2011). Switching modalities in a sentence verification task: ERP evidence for embodied language processing. Frontiers in Psychology, 2. https://doi.org/10.3389/fpsyg.2011.00045\nHauk, O. (2016). Only time will tell–why temporal information is essential for our neuroscientific understanding of semantics. Psychonomic Bulletin \u0026amp; Review, 23(4), 1072-1079. https://doi.org/10.3758/s13423-015-0873-9\nHolm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 6, 65-70. http://www.jstor.org/stable/4615733.\nHowe, P. D., \u0026amp; Perfors, A. (2018). An argument for how (and why) to incentivise replication. Behavioral and Brain Sciences, 41, e135-e135. http://dx.doi.org/10.1017/S0140525X18000705\nKam, C. D., \u0026amp; Franzese, R. J. (2007). Modeling and interpreting interactive hypotheses in regression analysis. Ann Arbor, MI: University of Michigan Press.\nKunert, R. (2016). Internal conceptual replications do not increase independent replication success. Psychonomic Bulletin \u0026amp; Review, 23(5), 1631-1638. https://doi.org/10.3758/s13423-016-1030-9\nLiu, P. (2018). Embodied-linguistic conceptual representations during metaphor processing. Doctoral thesis, Lancaster University, UK. https://doi.org/10.17635/lancaster/thesis/489\nLiu, P., Lynott, D., \u0026amp; Connell, L. (2018). Continuous neural activations of simulation-linguistic representations in modality switching costs. In P. Liu, Embodied-linguistic conceptual representations during metaphor processing. Doctoral thesis, Lancaster University, UK. https://doi.org/10.17635/lancaster/thesis/489\nLuck, S. J. (2005). Ten simple rules for designing ERP experiments. In T. C. Handy (Ed.), Event-related potentials: A methods handbook. MIT Press.\nLuck, S. J., \u0026amp; Gaspelin, N. (2017). How to get statistically significant effects in any ERP experiment (and why you shouldn't). Psychophysiology, 54(1), 146-157. https://doi.org/10.1111/psyp.12639\nMatuschek, H., Kliegl, R., Vasishth, S., Baayen, H., \u0026amp; Bates, D. (2017). Balancing type 1 error and power in linear mixed models. Journal of Memory and Language, 94, 305–315. https://doi.org/10.1016/j.jml.2017.01.001\nMeteyard, L., \u0026amp; Davies, R. A. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092\nPecher, D., Zeelenberg, R., \u0026amp; Barsalou, L. W. (2003). Verifying different-modality properties for concepts produces switching costs. Psychological Science, 14, 2, 119-24. https://doi.org/10.1111/1467-9280.t01-1-01429\nSimons, D. J. (2014). The value of direct replication. Perspectives on Psychological Science, 9(1), 76–80. https://doi.org/10.1177/1745691613514755\nSwaab, T. Y., Ledoux, K., Camblin, C. C., \u0026amp; Boudewyn, M. A. (2012). Language-related ERP components. In S. J. Luck \u0026amp; E. S. Kappenman (Eds.), Oxford handbook of event-related potential components (pp. 397–440). Oxford University Press. https://doi.org/10.1093/oxfordhb/9780195374148.013.0197\nVan Dam, W. O., Brazil, I. A., Bekkering, H., \u0026amp; Rueschemeyer, S.-A. (2014). Flexibility in embodied language processing: context effects in lexical access. Topics in Cognitive Science, 6(3), 407–424. https://doi.org/10.1111/tops.12100\nYarkoni, T. (2020). The generalizability crisis. Behavioral and Brain Sciences, 1-37. https://doi.org/10.1017/S0140525X20001685\nZwaan, R., Etz, A., Lucas, R., \u0026amp; Donnellan, M. (2018). Making replication mainstream. Behavioral and Brain Sciences, 41, E120. https://doi.org/10.1017/S0140525X17001972\n\r","date":1610150400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610150400,"objectID":"8615e74a11d73aca72b8c255d3ea331e","permalink":"https://pablobernabeu.github.io/2021/brief-clarifications-open-questions-commentary-on-liu-et-al-2018/","publishdate":"2021-01-09T00:00:00Z","relpermalink":"/2021/brief-clarifications-open-questions-commentary-on-liu-et-al-2018/","section":"post","summary":"Liu et al. (2018) present a study that implements the conceptual modality switch (CMS) paradigm, which has been used to investigate the modality-specific nature of conceptual representations (Pecher et al., 2003). Liu et al.\u0026lsquo;s experiment uses event-related potentials (ERPs; similarly, see Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013). In the design of the switch conditions, the experiment implements a corpus analysis to distinguish between purely-embodied modality switches and switches that are more liable to linguistic bootstrapping (also see Bernabeu et al.","tags":["s","conceptual modality switch","conceptual processing","cognition","conceptual replication","word recognition","research methods","event-related potentials","experiment","statistics","bayesian","frequentist","bias","methodology"],"title":"Brief Clarifications, Open Questions: Commentary on Liu et al. (2018)","type":"post"},{"authors":["Bernabeu, P., Lynott, D., \u0026 Connell, L."],"categories":["conceptual processing","individual differences","psycholinguistics"],"content":"\rReference\rBernabeu, P., Lynott, D., \u0026amp; Connell, L. (2021). Preregistration: The interplay between linguistic and embodied systems in conceptual processing. OSF. https://osf.io/ftydw\n\r\r","date":1609804800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609804800,"objectID":"25833132f5c029c6dddbd38d3d82ea6d","permalink":"https://pablobernabeu.github.io/publication/the-interplay-between-linguistic-and-embodied-systems-in-conceptual-processing/","publishdate":"2021-01-05T09:10:27Z","relpermalink":"/publication/the-interplay-between-linguistic-and-embodied-systems-in-conceptual-processing/","section":"publication","summary":"This preregistration outlines a study that will investigate the dynamic nature of conceptual processing by examining the interplay between linguistic distributional systems—comprising word co-occurrence and word association—and embodied systems—comprising sensorimotor and emotional information. A set of confirmatory research questions are addressed using data from the Calgary Semantic Decision project, along with additional measures for the stimuli corresponding to distributional language statistics, embodied information, and individual differences in vocabulary size.","tags":["conceptual processing","word recognition","semantic decision","linguistic distributional knowledge","embodied cognition","statistics","linear mixed-effects models","R","preregistration"],"title":"Preregistration: The interplay between linguistic and embodied systems in conceptual processing","type":"publication"},{"authors":null,"categories":["research and teaching applications","R"],"content":"\rThis open-source, R-based web application allows the conversion of video captions (subtitles) from the Web Video Text Tracks (WebVTT) Format into plain texts. For this purpose, users upload a WebVTT file with the extension .vtt or .txt (examples available here and here). Automatically, metadata such as timestamps are removed, and the text is formatted into a paragraph. The result is displayed on the website, and can be downloaded as .docx and .txt documents. Overall, this application serves to improve the accessibility of video captions.\n🌐 The web application can be launched here.\rThe data is only available to the user, and is deleted when the website is closed.\nQuestions and suggestions can be submitted as issues or emailed to pcbernabeu@gmail.com. The app can be extended via pull requests.\nDeveloper: Pablo Bernabeu (Dept. Psychology, Lancaster University). Licence: Creative Commons Attribution 4.0 International.\n\rCode details\rThe core of the application is in the index.Rmd script, which uses ‘regular expressions’ to process the VTT file. In turn, that script draws on another one to enable the download of .docx documents. Last, the latter script in turn uses a Word template.\n\n\r","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"65d1a22cf3d4b77ba36cd825cf7fe51a","permalink":"https://pablobernabeu.github.io/applications-and-dashboards/vtt-transcription-app/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/applications-and-dashboards/vtt-transcription-app/","section":"applications-and-dashboards","summary":"This open-source, R-based web application allows the conversion of video captions (subtitles) from the Web Video Text Tracks (WebVTT) Format into plain texts. For this purpose, users upload a WebVTT file with the extension of 'vtt' or 'txt'. Automatically, metadata such as timestamps are removed, and the text is formatted.","tags":["subtitles","accessibility","transcription","webvtt","web video text tracks format","web application","data science","regular expressions","stringr","R","R Shiny","Flexdashboard","software"],"title":"WebVTT caption transcription app","type":"applications-and-dashboards"},{"authors":[],"categories":["statistics","linear mixed-effects models"],"content":"\rSlides \r\r\rAbstract\rLinear mixed-effects models (LMEMs) are used to account for variation within factors with multiple observations, such as participants, trials, items, channels, etc (for an earlier approach, see Clark, 1973). This variation is modelled in terms of random intercepts (e.g., overall variation per participant) as well as random slopes for the fixed effects (e.g., treatment effect per participant). These measures help reduce false positives and false negatives (Barr et al., 2013), and the resulting models tend to be robust to violations of assumptions (Schielzeth et al., 2020). The use of LMEMs has grown over the past decade, under various implementation forms (Meteyard \u0026amp; Davies, 2020). In this talk, I will look over the rationale for LMEMs, and demonstrate how to fit them in R (Brauer \u0026amp; Curtin, 2018; Luke, 2017). Challenges will also be covered. For instance, when using the widely-accepted ‘maximal’ approach, based on fitting all possible random effects for each fixed effect, models sometimes fail to find a solution, or ‘convergence’. Advice for the problem of nonconvergence will be demonstrated, based on the progressive lightening of the random effects structure (Singman \u0026amp; Kellen, 2017; for an alternative approach, especially with small samples, see Matuschek et al., 2017). At the end, on a different note, I will present a web application that facilitates data simulation for research and teaching (Bernabeu \u0026amp; Lynott, 2020).\n\rReferences\rBarr, D. J., Levy, R., Scheepers, C., \u0026amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68, 255–278. http://dx.doi.org/10.1016/j.jml.2012.11.001\nBernabeu, P., \u0026amp; Lynott, D. (2020). Web application for the simulation of experimental data (Version 1.2). https://github.com/pablobernabeu/Experimental-data-simulation/\nBrauer, M., \u0026amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. Psychological Methods, 23(3), 389–411. https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf\nClark, H. H. (1973). The language-as-fixed-effect fallacy: A critique of language statistics in psychological research. Journal of Verbal Learning and Verbal Behavior, 12(4), 335-359. https://doi.org/10.1016/S0022-5371(73)80014-3\nLuke, S. G. (2017). Evaluating significance in linear mixed-effects models in R. Behavior Research Methods, 49(4), 1494–1502. https://doi.org/10.3758/s13428-016-0809-y\nMatuschek, H., Kliegl, R., Vasishth, S., Baayen, H., \u0026amp; Bates, D. (2017). Balancing type 1 error and power in linear mixed models. Journal of Memory and Language, 94, 305–315. https://doi.org/10.1016/j.jml.2017.01.001\nMeteyard, L., \u0026amp; Davies, R. A. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092\nSchielzeth, H., Dingemanse, N. J., Nakagawa, S., Westneat, D. F., Allegue, H, Teplitsky, C., Reale, D., Dochtermann, N. A., Garamszegi, L. Z., \u0026amp; Araya-Ajoy, Y. G. (2020). Robustness of linear mixed-effects models to violations of distributional assumptions. Methods in Ecology and Evolution, 00, 1– 12. https://doi.org/10.1111/2041-210X.13434\nSingmann, H., \u0026amp; Kellen, D. (2019). An Introduction to Mixed Models for Experimental Psychology. In D. H. Spieler \u0026amp; E. Schumacher (Eds.), New Methods in Cognitive Psychology (pp. 4–31). Hove, UK: Psychology Press. http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf\n\r\n\r","date":1606348800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606348800,"objectID":"a6b393e7de06f9621834c014056f2de7","permalink":"https://pablobernabeu.github.io/talk/2020-11-26-mixed-effects-models-in-r-and-a-new-tool-for-data-simulation/","publishdate":"2020-10-24T00:00:00Z","relpermalink":"/talk/2020-11-26-mixed-effects-models-in-r-and-a-new-tool-for-data-simulation/","section":"talk","summary":"In this talk, I will look over the rationale for LMEMs, and demonstrate how to fit them in R (Brauer \u0026 Curtin, 2018; Luke, 2017). Challenges will also be covered. For instance, when using the widely-accepted 'maximal' approach, based on fitting all possible random effects for each fixed effect, models sometimes fail to find a solution, or 'convergence'. Advice for the problem of nonconvergence will be demonstrated, based on the progressive lightening of the random effects structure (Singman \u0026 Kellen, 2017; for an alternative approach, especially with small samples, see Matuschek et al., 2017). At the end, on a different note, I will present a web application that facilitates data simulation for research and teaching (Bernabeu \u0026 Lynott, 2020).","tags":["statistics","linear mixed-effects models","regression","R","programming","web application","data simulation","Software Sustainability Institute Fellowship"],"title":"Mixed-effects models in R, and a new tool for data simulation","type":"talk"},{"authors":[],"categories":["research and teaching applications"],"content":"\r\rResumen\rLas aplicaciones web nos ayudan a facilitar el uso de nuestro trabajo, ya que no requieren programación para utilizarlas (ver ejemplos). Crear estas aplicaciones en R, mediante paquetes como “shiny” o “flexdashboard”, ofrece múltiples ventajas. Entre ellas destaca la reproducibilidad, tal como veremos en torno a una aplicación para la simulación de datos. Por un lado, los usuarios pueden exportar un registro de su actividad. Por otro lado, el código utilizado para crear estas aplicaciones se puede compartir, investigar y editar con la facilidad que ofrece un lenguaje de código abierto como R. Esto facilita el uso gratuito, el desarrollo colaborativo y una documentación accesible sobre cualquiera de los paquetes utilizados. Por último, la reproducibilidad se puede maximizar si se facilita a los usuarios que lo deseen la exportación de un código de R ajustado a sus requerimientos (más allá del código de la aplicación en general), lo cual añadiría a la aplicación las ventajas de un paquete de R. Esta última opción (no disponible actualmente en la aplicación de simulación, ni en la mayoría de las aplicaciones) se puede habilitar adaptando el código de la aplicación a funciones básicas de R.\n\rVídeo y filminas\r\r\rSlideshare\r\r\n\r","date":1602115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602115200,"objectID":"027d2496bcad02d5ce3472feb8e10c18","permalink":"https://pablobernabeu.github.io/talk/2020-10-08-reproducibilidad-en-torno-a-una-aplicacion-web/","publishdate":"2020-10-08T00:00:00Z","relpermalink":"/talk/2020-10-08-reproducibilidad-en-torno-a-una-aplicacion-web/","section":"talk","summary":"Las aplicaciones web nos ayudan a facilitar el uso de nuestro trabajo, ya que no requieren programación para utilizarlas. Crear estas aplicaciones en R, mediante paquetes como \"shiny\" o \"flexdashboard\", ofrece múltiples ventajas. Entre ellas destaca la reproducibilidad, tal como veremos en torno a una aplicación para la simulación de datos (https://github.com/pablobernabeu/Experimental-data-simulation).","tags":["programming","programación","web application","aplicación web","data simulation","simulación de datos","statistics","estadística","R","Software Sustainability Institute Fellowship"],"title":"Reproducibilidad en torno a una aplicación web","type":"talk"},{"authors":[],"categories":["research methods"],"content":"In a highly recommendable presentation available on Youtube, Michael Frank walks us through R Markdown. Below, I loosely summarise and partly elaborate on Frank's advice regarding collaboration among colleagues, some of whom may not be used to R Markdown (see relevant time point in Frank's presentation).\n  The first way is using GitHub, which has a great version control system, and even allows the rendering of Markdown text, if the file is given the extension \u0026lsquo;.md\u0026rsquo; on GitHub. Furthermore, GitHub has made private repositories with any number of collaborators free.\n  The second way is copying the text part of the unrendered R Markdown doc (i.e., excluding any long code chunks) to Word or Google Docs, or any other trackable editor. The collaborators would then edit the text, and refrain from editing any of the R or Markdown code (i.e., any inline code, hashes, etc.). Changes would be tracked and accepted (any unwanted edits of the code may be undone), and transferred to the original document.\n  The third way is knitting the document to Word, which allows tracking changes, or otherwise knitting to PDF.\n  ","date":1601683200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601683200,"objectID":"a0ecd1e1e3cce418590be0fb00f25875","permalink":"https://pablobernabeu.github.io/2020/collaboration-while-using-r-markdown/","publishdate":"2020-10-03T00:00:00Z","relpermalink":"/2020/collaboration-while-using-r-markdown/","section":"post","summary":"In a highly recommendable presentation available on Youtube, Michael Frank walks us through R Markdown. Below, I loosely summarise and partly elaborate on Frank's advice regarding collaboration among colleagues, some of whom may not be used to R Markdown (see relevant time point in Frank's presentation).\n  The first way is using GitHub, which has a great version control system, and even allows the rendering of Markdown text, if the file is given the extension \u0026lsquo;.","tags":["s","R","R Markdown"],"title":"Collaboration while using R Markdown","type":"post"},{"authors":[],"categories":["writing"],"content":"When writing formal pieces, some pitfalls in the punctuation are easy to avoid once you know them. Punctuation marks such as the comma, the semi-colon, the colon and the period are useful for organising phrases and clauses, facilitating the reading, and disambiguating. However, these marks are also liable to underuse, as in the case of run-on sentences; misuse, as in the comma splice; and overuse, as it often happens with the Oxford comma.\nRun-on sentences: insufficient punctuation Example (notice the absence of punctuation marks around \u0026lsquo;therefore\u0026rsquo;):\n * Specific agendas in media organisations lead to information being manipulated or hidden therefore individual research efforts are valuable.\n Possible alternative:\n Specific agendas in media organisations lead to information being manipulated or hidden. Therefore, individual research efforts are valuable.\n  More information\nComma splice: misused punctuation A type of run-on sentence, the comma splice is characterised by the use of of too-weak punctuation forms; often, using a comma instead of a semi-colon or a period.\nExample (notice the comma before \u0026lsquo;therefore\u0026rsquo;):\n * Specific agendas in media organisations lead to information being manipulated or hidden, therefore individual research efforts are valuable.\n Possible alternative: see above.\n More information\nOxford comma: sometimes overused The Oxford, or serial, comma (namely, the comma preceding the word \u0026lsquo;and\u0026rsquo; before the last item in a series) has the purpose of facilitating the reading and disambiguating. Where that purpose is not necessary, neither is the comma.\nFind out more:\n Oxford University guidelines\n The Guardian guidelines\n Overuse of Oxford comma\n","date":1601683200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601683200,"objectID":"95c4933f561744da5d3336c780d87794","permalink":"https://pablobernabeu.github.io/2020/formal-punctuation/","publishdate":"2020-10-03T00:00:00Z","relpermalink":"/2020/formal-punctuation/","section":"post","summary":"When writing formal pieces, some pitfalls in the punctuation are easy to avoid once you know them. Punctuation marks such as the comma, the semi-colon, the colon and the period are useful for organising phrases and clauses, facilitating the reading, and disambiguating. However, these marks are also liable to underuse, as in the case of run-on sentences; misuse, as in the comma splice; and overuse, as it often happens with the Oxford comma.","tags":["s","writing","punctuation"],"title":"Notes about punctuation in formal writing","type":"post"},{"authors":[],"categories":["research and teaching applications"],"content":"Unwanted, stranded meetings, overlapping with a general one in a channel, can occur when people click on the Meet (now)/📷 button, instead of clicking on the same Join button in the chat field. This may especially happen to those who reach the channel first, or who cannot see the Join button in the chat field because this field has been taken up by messages.\nTo prevent students from creating or joining stray meetings, the channel lead can direct students to a single meeting link in the chat field, which could be upward ion this field, and to avoid the Meet (now)/📷 button at the top right in channels.\nFurther background “Hi All, in Microsoft Teams, if multiple people click the \u0026lsquo;Meet (now)\u0026rsquo; button in a channel, what happens?”  See Reddit post\nMore advanced tip to have meeting links ready in each channel  Watch tutorial from minute 3:04 to 4:50\n","date":1601683200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601683200,"objectID":"3d55f1b5bc711bbec6ca25d7ffb051a3","permalink":"https://pablobernabeu.github.io/2020/stray-meetings-in-microsoft-teams/","publishdate":"2020-10-03T00:00:00Z","relpermalink":"/2020/stray-meetings-in-microsoft-teams/","section":"post","summary":"Unwanted, stranded meetings, overlapping with a general one in a channel, can occur when people click on the Meet (now)/:camera: button, instead of clicking on the same Join button in the chat field. This may especially happen to those who reach the channel first, or who cannot see the Join button in the chat field because this field has been taken up by messages.","tags":["s","communications"],"title":"Stray meetings in Microsoft Teams","type":"post"},{"authors":[],"categories":["programming","R"],"content":"\r\r\r\r\r\r\r\rThis document is part of teaching materials created for the workshop ‘Open data and reproducibility v2.1: R Markdown, dashboards and Binder’, delivered at the CarpentryCon 2020 conference. The purpose of this specific document is to practise R Markdown, including basic features such as Markdown markup and code chunks, along with more special features such as cross-references for figures, tables, code chunks, etc. The code is on GitHub.\nSince this conference was originally going to take place in Madison, let’s look at some open data from the City of Madison.\nPark types\r[Placeholder text] Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Figure 1 shows the number of parks within each type.\n# Showing the code because echo = TRUE dat = read.csv(\u0026#39;https://opendata.arcgis.com/datasets/9e00ff81868e49b7ba65d4e628b9e14f_6.csv\u0026#39;)\rdat = dat %\u0026gt;%\rgroup_by(Type) %\u0026gt;%\rmutate(parks_number = n())\rggplotly(\rggplot(dat, aes(x=reorder(Type, parks_number), y=parks_number,\rtext = paste(\u0026#39;Number of parks =\u0026#39;, parks_number))) + theme(axis.title.y=element_blank()) + stat_identity(geom=\u0026#39;bar\u0026#39;) + labs(x=\u0026#39;Type\u0026#39;, y=\u0026#39;Number of parks\u0026#39;) + coord_flip(), tooltip = \u0026#39;text\u0026#39;\r)\r\r\r{\"x\":{\"data\":[{\"orientation\":\"h\",\"width\":[0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.9,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.9,0.899999999999999,0.9,0.899999999999999,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.9,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.9,0.899999999999999,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.9,0.9,0.9,0.9,0.899999999999999,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.9,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.9,0.9,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999],\"base\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"x\":[100,100,22,78,78,21,31,100,78,100,78,31,100,78,100,78,78,100,21,31,100,78,100,2,100,100,100,100,78,100,100,100,78,100,100,21,2,78,78,100,21,100,31,100,22,1,78,100,78,78,78,100,100,13,78,100,78,100,22,100,78,100,31,78,100,100,78,31,100,100,100,78,100,100,78,21,100,21,100,78,100,21,21,100,100,100,1,31,100,78,100,78,78,13,21,78,22,100,22,100,31,31,31,78,100,31,31,100,22,31,78,100,78,100,100,100,78,100,100,31,21,22,78,78,78,22,13,100,31,100,78,31,78,21,100,21,31,78,100,78,31,100,78,78,22,78,21,100,100,100,31,78,100,78,100,22,22,78,100,78,78,13,78,100,31,78,100,78,31,100,13,100,22,13,13,78,78,100,78,78,78,100,100,22,78,21,13,21,22,78,21,13,22,100,78,31,31,13,78,78,100,100,100,78,100,100,100,21,100,31,100,31,22,100,78,22,100,100,100,78,31,100,78,31,78,100,78,78,78,100,22,78,100,100,100,78,100,100,100,78,78,21,31,78,78,13,22,78,100,100,100,78,21,100,13,21,22,31,31,31,100,78,100,13,22,21,22,100,78],\"y\":[9,9,6,8,8,5,7,9,8,9,8,7,9,8,9,8,8,9,5,7,9,8,9,3,9,9,9,9,8,9,9,9,8,9,9,5,3,8,8,9,5,9,7,9,6,2,8,9,8,8,8,9,9,4,8,9,8,9,6,9,8,9,7,8,9,9,8,7,9,9,9,8,9,9,8,5,9,5,9,8,9,5,5,9,9,9,1,7,9,8,9,8,8,4,5,8,6,9,6,9,7,7,7,8,9,7,7,9,6,7,8,9,8,9,9,9,8,9,9,7,5,6,8,8,8,6,4,9,7,9,8,7,8,5,9,5,7,8,9,8,7,9,8,8,6,8,5,9,9,9,7,8,9,8,9,6,6,8,9,8,8,4,8,9,7,8,9,8,7,9,4,9,6,4,4,8,8,9,8,8,8,9,9,6,8,5,4,5,6,8,5,4,6,9,8,7,7,4,8,8,9,9,9,8,9,9,9,5,9,7,9,7,6,9,8,6,9,9,9,8,7,9,8,7,8,9,8,8,8,9,6,8,9,9,9,8,9,9,9,8,8,5,7,8,8,4,6,8,9,9,9,8,5,9,4,5,6,7,7,7,9,8,9,4,6,5,6,9,8],\"text\":[\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 22\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 21\",\"Number of parks = 31\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 31\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 21\",\"Number of parks = 31\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 2\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 21\",\"Number of parks = 2\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 21\",\"Number of parks = 100\",\"Number of parks = 31\",\"Number of parks = 100\",\"Number of parks = 22\",\"Number of parks = 1\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 13\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 22\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 31\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 31\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 21\",\"Number of parks = 100\",\"Number of parks = 21\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 21\",\"Number of parks = 21\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 1\",\"Number of parks = 31\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 13\",\"Number of parks = 21\",\"Number of parks = 78\",\"Number of parks = 22\",\"Number of parks = 100\",\"Number of parks = 22\",\"Number of parks = 100\",\"Number of parks = 31\",\"Number of parks = 31\",\"Number of parks = 31\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 31\",\"Number of parks = 31\",\"Number of parks = 100\",\"Number of parks = 22\",\"Number of parks = 31\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 31\",\"Number of parks = 21\",\"Number of parks = 22\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 22\",\"Number of parks = 13\",\"Number of parks = 100\",\"Number of parks = 31\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 31\",\"Number of parks = 78\",\"Number of parks = 21\",\"Number of parks = 100\",\"Number of parks = 21\",\"Number of parks = 31\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 31\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 22\",\"Number of parks = 78\",\"Number of parks = 21\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 31\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 22\",\"Number of parks = 22\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 13\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 31\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 31\",\"Number of parks = 100\",\"Number of parks = 13\",\"Number of parks = 100\",\"Number of parks = 22\",\"Number of parks = 13\",\"Number of parks = 13\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 22\",\"Number of parks = 78\",\"Number of parks = 21\",\"Number of parks = 13\",\"Number of parks = 21\",\"Number of parks = 22\",\"Number of parks = 78\",\"Number of parks = 21\",\"Number of parks = 13\",\"Number of parks = 22\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 31\",\"Number of parks = 31\",\"Number of parks = 13\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 21\",\"Number of parks = 100\",\"Number of parks = 31\",\"Number of parks = 100\",\"Number of parks = 31\",\"Number of parks = 22\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 22\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 31\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 31\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 22\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 21\",\"Number of parks = 31\",\"Number of parks = 78\",\"Number of parks = 78\",\"Number of parks = 13\",\"Number of parks = 22\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 21\",\"Number of parks = 100\",\"Number of parks = 13\",\"Number of parks = 21\",\"Number of parks = 22\",\"Number of parks = 31\",\"Number of parks = 31\",\"Number of parks = 31\",\"Number of parks = 100\",\"Number of parks = 78\",\"Number of parks = 100\",\"Number of parks = 13\",\"Number of parks = 22\",\"Number of parks = 21\",\"Number of parks = 22\",\"Number of parks = 100\",\"Number of parks = 78\"],\"type\":\"bar\",\"textposition\":\"none\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(89,89,89,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":26.2283105022831,\"r\":7.30593607305936,\"b\":40.1826484018265,\"l\":92.7853881278539},\"plot_bgcolor\":\"rgba(235,235,235,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-5,105],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"25\",\"50\",\"75\",\"100\"],\"tickvals\":[0,25,50,75,100],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"25\",\"50\",\"75\",\"100\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"Number of parks\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.4,9.6],\"tickmode\":\"array\",\"ticktext\":[\"OTHER\",\"SPECIAL\",\"SPORTS COMPLEX\",\"TRAFFICWAY\",\"CONSERVATION\",\"OPEN SPACE\",\"COMMUNITY\",\"NEIGHBORHOOD\",\"MINI\"],\"tickvals\":[1,2,3,4,5,6,7,8,9],\"categoryorder\":\"array\",\"categoryarray\":[\"OTHER\",\"SPECIAL\",\"SPORTS COMPLEX\",\"TRAFFICWAY\",\"CONSERVATION\",\"OPEN SPACE\",\"COMMUNITY\",\"NEIGHBORHOOD\",\"MINI\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"\",\"font\":{\"color\":null,\"family\":null,\"size\":0}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"e434646d209\":{\"x\":{},\"y\":{},\"text\":{},\"type\":\"bar\"}},\"cur_data\":\"e434646d209\",\"visdat\":{\"e434646d209\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\rFigure 1: Number of parks within each type.\r\r\rTwenty largest parks\r[Placeholder text] Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Table 1 shows the twenty largest parks types, along with their type and acreage. The code doesn’t show below because echo = FALSE.\n\rTable 1: The twenty largest parks in Madison.\r\rName\rType\rAcreage\r\r\r\rCherokee Marsh - North Unit\rCONSERVATION\r946.58\r\rCherokee Marsh - South Unit (School Road Unit)\rCONSERVATION\r261.27\r\rElver Park\rCOMMUNITY\r250.82\r\rNortheast Park\rCOMMUNITY\r237.76\r\rWarner Park\rCOMMUNITY\r213.49\r\rDoor Creek Park\rCOMMUNITY\r159.97\r\rCherokee Marsh - Mendota Unit\rCONSERVATION\r122.08\r\rOwen Conservation Park\rCONSERVATION\r96.79\r\rReindahl (Amund) Park\rCOMMUNITY\r90.74\r\rOlbrich Park\rCOMMUNITY\r90.01\r\rYahara Hills Park (West)\rCOMMUNITY\r82.20\r\rSycamore Park\rCOMMUNITY\r71.42\r\rTurville Point Conservation Park\rCONSERVATION\r64.28\r\rEdna Taylor Conservation Park\rCONSERVATION\r60.27\r\rQuann Park\rCOMMUNITY\r55.43\r\rDemetral Park\rCOMMUNITY\r49.18\r\rPrairie Ridge Conservation Park\rCONSERVATION\r48.76\r\rOlin Park\rCOMMUNITY\r47.12\r\rHiestand Park\rCOMMUNITY\r46.27\r\rVilas (Henry) Park\rCOMMUNITY\r45.67\r\r\r\rWe could decide to display the code for the table at this point (or any other), which can be done using the parameters ref.label = 'largest-parks', echo = TRUE, eval = FALSE in the chunk options (Xie, Dervieux, \u0026amp; Riederer, 2020).\ndat %\u0026gt;%\rsummarize(Name = Park_Name, Type, Acreage) %\u0026gt;%\rarrange(-Acreage, Name, Type) %\u0026gt;%\rselect(Name, Type, Acreage) %\u0026gt;%\rhead(20) %\u0026gt;%\rkable(caption = \u0026#39;The twenty largest parks in Madison.\u0026#39;)\r\n\r","date":1597363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597363200,"objectID":"bbe7cc7942958a68302a125ad80e07d1","permalink":"https://pablobernabeu.github.io/2020/r-markdown-amidst-madison-parks/","publishdate":"2020-08-14T00:00:00Z","relpermalink":"/2020/r-markdown-amidst-madison-parks/","section":"post","summary":"This document is part of teaching materials created for the workshop 'Open data and reproducibility v2.1: R Markdown, dashboards and Binder', delivered at the CarpentryCon 2020 conference. The purpose of this specific document is to practise R Markdown, including basic features such as Markdown markup and code chunks, along with more special features such as cross-references for figures, tables, code chunks, etc. Since this conference was originally going to take place in Madison, let's look at some open data from the City of Madison.","tags":["s","R","R Markdown","CarpentryCon","workshop","RStudio","bookdown","cross-references"],"title":"R Markdown amidst Madison parks","type":"post"},{"authors":[],"categories":["viewpoint"],"content":"\r\r\nFluke is a popular term for the statistical concepts of false positive or false negative results (more commonly used for false positives). A false positive occurs when a result that does not exist in reality is observed in an analysis due to a methodological error (be it experimental, statistical, or otherwise). Conversely, a false negative occurs when a genuine result is not observed in an analysis, due to the same kind of error. The concept of fluke is defined by its opposite: a truthful, accurate result.\nThe label ‘fluke’ is ubiquitous where statistics is applied, from medicine to psychology and from sociology to politics.\nWhen I won in 2018, many dismissed our victory as a “fluke.”\nOur win was treated as an aberration, or bc my opponent “didn’t try.”\nSo from the start, tonight’s race was important to me.\nTonight we are proving that the people’s movement in NY isn’t an accident. It‘s a mandate.\r— Alexandria Ocasio-Cortez (@AOC) June 24, 2020\r\r\r\nWhat’s in the label ‘fluke’—or what could there be?\rAOC’s tweet points at two main types of reactions to her results in the previous elections. The first type is dismissal, on the grounds of a fluke or a weak opposition. The other reaction is plain ‘aberration’. A few weeks earlier, Mark Leibovich had also delved into this issue in The New York Times (May 4, 2020, https://nyti.ms/2YsJ9fb):\n\rShe believed misconceptions had taken hold about her: that she was angry and strident. That she was naïve. “That I just don’t know how this town works,” she said. “That I’m stupid. Or I’m lucky. That was a big thing the Democrats were saying. That I was a fluke. Which is basically just 10 different ways of saying she’s not supposed to be here.”\n\rThe seemingly objective term ‘fluke’, with its statistical underpinning, seems to be susceptible to biased uses. We may then have to ask: Have other electoral results, comparable to AOC’s, been equally studied for signs of a fluke? Why is a certain result perceived as a fluke in the first place? From perceptual to cognitive and historical biases, people’s judgements are susceptible to visual mistakes (Zamboni, Ledgeway, McGraw, \u0026amp; Schluppeck, 2016), confirmation bias (Rajsic, Taylor, \u0026amp; Pratt, 2018) and biased records (Hug, 2003).\nIn summary, the label ‘fluke’ may in principle be skewed by:\n the eye of the beholder\n the mind of the perceiver\n the availability or lack of data\n\r\n\rTrust and distrust\rProblems associated with the reliance on trust and distrust have become patent even in fields that are relatively regulated against arbitrary decisions, such as higher education and academia (Barber, Hayes, Johnson, Márquez-Magaña, \u0026amp; 10,234 signatories, 2020; Milkman, Akinola, \u0026amp; Chugh, 2012, 2015).\n\n\rReferences\rBarber, P. H., Hayes, T. B., Johnson, T. L., Márquez-Magaña, L., \u0026amp; 10,234 signatories (2020). Systemic racism in higher education. Science, 369, 6510, 1440-1441. https://doi.org/10.1126/science.abd7140\nHug, S. (2003). Selection Bias in Comparative Research: The Case of Incomplete Data Sets. Political Analysis, 11(3), 255-274. https://doi.org/10.1093/pan/mpg014\nMilkman, K. L., Akinola, M., \u0026amp; Chugh, D. (2012). Temporal distance and discrimination: an audit study in academia. Psychological Science, 23(7), 710–717. https://doi.org/10.1177/0956797611434539\n_____ (2015). What happens before? A field experiment exploring how pay and representation differentially shape bias on the pathway into organizations. Journal of Applied Psychology, 100(6), 1678–1712. https://doi.org/10.1037/apl0000022\nRajsic, J., Taylor, J. E. T., \u0026amp; Pratt, J. (2018) Out of sight, out of mind: Matching bias underlies confirmatory visual search. Attention, Perception, \u0026amp; Psychophysics, 79, 498–507. https://doi.org/10.3758/s13414-016-1259-4\nZamboni, E., Ledgeway, T., McGraw, P. V., \u0026amp; Schluppeck, D. (2016). Do perceptual biases emerge early or late in visual processing? Decision-biases in motion perception. Proceedings of the Royal Society B: Biological Sciences, 283(1833), 20160263. https://doi.org/10.1098/rspb.2016.0263\n\r\n\r","date":1593043200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593043200,"objectID":"57e3bb921a728168f4133cdd4dc07187","permalink":"https://pablobernabeu.github.io/2020/whats-in-a-fluke/","publishdate":"2020-06-25T00:00:00Z","relpermalink":"/2020/whats-in-a-fluke/","section":"post","summary":"The label 'fluke' may in principle be skewed by the eye of the beholder, the mind of the perceiver and the availability or lack of data.","tags":["s","fluke","false positive","false negative","perception","bias","discrimination","trust","distrust"],"title":"What's in a fluke? The problem of trust and distrust","type":"post"},{"authors":["Bernabeu, P., \u0026 Lynott, D."],"categories":["research and teaching applications","R"],"content":"\r  Purposes\r\rThis open-source, R-based web application is suitable for educational and research purposes in experimental and quantitative sciences. It allows the creation of varied data sets with specified structures, such as between-group and within-participant variables, that can be categorical or continuous. These parameters can be set throughout the various tabs (sections) from the top menu. In the last tab, the data set can be downloaded. The benefits of this application include time-saving and flexibility in the control of parameters.\nGuidelines\rGeneral guidelines include the following:\n\rIn the names of variables, it’s recommended only to use alphanumeric characters and underscore signs. The latter can be used to separate characters or words (e.g., variable_name). Different names should be used for each variable.\n\rIn the levels of categorical variables, alphanumeric, special characters and spaces are allowed.\n\rIn numeric fields (e.g., ‘Mean’, ‘Standard deviation’, ‘Relative probability [0, 1]’), only numbers and decimal points are allowed.\n\rAs the data set increases, so does the processing time.\n\r\rMore specific guidelines are available in each section.\n\r🌐 The web application can be launched here.\rScreenshot of the Dependent tab (view larger)\r\r\rReference\rBernabeu, P., \u0026amp; Lynott, D. (2020). Web application for the simulation of experimental data (Version 1.4). https://github.com/pablobernabeu/Experiment-simulation-app/\n\rCode\rThis web application was developed in R (R Core Team, 2020). The code is available on Github, where contributions may be made. The initial code for this application was influenced by Section 5.7 (Simulating data for multi-factor designs) in Crump (2017). The R packages used include ‘dplyr’ (Wickham, François, Henry, \u0026amp; Müller, 2018), ‘DT’ (Xie, 2020), ‘flexdashboard’ (Iannone, Allaire, \u0026amp; Borges, 2020), ‘shiny’ (Chang, Cheng, Allaire, Xie, \u0026amp; McPherson, 2020) and ‘stringr’ (Wickham, 2019).\n\rOptions for development and local use of the app\rOption A) Using local R/RStudio or RStudio Cloud project or Binder RStudio environment\r[Step only necessary in R/RStudio] Install the packages in the versions used in the latest release of this application, by running:\ninstall.packages(\u0026#39;devtools\u0026#39;)\rlibrary(devtools)\rinstall_version(\u0026#39;dplyr\u0026#39;, \u0026#39;1.0.2\u0026#39;, \u0026#39;http://cran.us.r-project.org\u0026#39;)\rinstall_version(\u0026#39;DT\u0026#39;, \u0026#39;0.15\u0026#39;, \u0026#39;http://cran.us.r-project.org\u0026#39;)\rinstall_version(\u0026#39;flexdashboard\u0026#39;, \u0026#39;0.5.2\u0026#39;, \u0026#39;http://cran.us.r-project.org\u0026#39;)\rinstall_version(\u0026#39;htmltools\u0026#39;, \u0026#39;0.5.0\u0026#39;, \u0026#39;http://cran.us.r-project.org\u0026#39;)\rinstall_version(\u0026#39;knitr\u0026#39;, \u0026#39;1.30\u0026#39;, \u0026#39;http://cran.us.r-project.org\u0026#39;)\rinstall_version(\u0026#39;ngram\u0026#39;, \u0026#39;3.0.4\u0026#39;, \u0026#39;http://cran.us.r-project.org\u0026#39;)\rinstall_version(\u0026#39;purrr\u0026#39;, \u0026#39;0.3.4\u0026#39;, \u0026#39;http://cran.us.r-project.org\u0026#39;)\rinstall_version(\u0026#39;shiny\u0026#39;, \u0026#39;1.5.0\u0026#39;, \u0026#39;http://cran.us.r-project.org\u0026#39;)\rinstall_version(\u0026#39;stringr\u0026#39;, \u0026#39;1.4.0\u0026#39;, \u0026#39;http://cran.us.r-project.org\u0026#39;)\rinstall_version(\u0026#39;tidyr\u0026#39;, \u0026#39;1.1.2\u0026#39;, \u0026#39;http://cran.us.r-project.org\u0026#39;)\rOpen the index.Rmd script.\n\rRun the application by clicking on ▶️ Run document at the top left, or by running rmarkdown::run('index.Rmd') in the console.\n\rClick on Open in Browser at the top left.\n\r\r\rOption B) Using Dockerfile (see instructions)\r\r\rAcknowledgements\rThank you to RStudio for the free hosting server used by this application, shinyapps.io.\n\rReferences\rChang, W., Cheng, J., Allaire, J., Xie, Y., \u0026amp; McPherson, J. (2020). shiny: Web Application Framework for R. R package version 1.4.0. Available at http://CRAN.R-project.org/package=shiny.\nCrump, M. J. C. (2017). Programming for Psychologists: Data Creation and Analysis (Version 1.1). https://crumplab.github.io/programmingforpsych/.\nIannone, R., Allaire, J. J., \u0026amp; Borges, B. (2020). Flexdashboard: R Markdown Format for Flexible Dashboards. http://rmarkdown.rstudio.com/flexdashboard.\nR Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.\nWickham, H. (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0. https://CRAN.R-project.org/package=stringr.\nWickham, H., François, R., Henry, L., \u0026amp; Müller, K. (2018). dplyr: A Grammar of Data Manipulation. R package version 0.7.6. https://CRAN.R-project.org/package=dplyr.\nXie, Y. (2020). DT: A Wrapper of the JavaScript Library “DataTables”. R package version 0.14. Available at https://CRAN.R-project.org/package=DT.\n\r\rContact\rTo submit any questions or feedback, please post an issue, or email Pablo Bernabeu at p.bernabeu@lancaster.ac.uk.\n\n\r","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"cb99802ba67d5ec447b8a57edf3c7694","permalink":"https://pablobernabeu.github.io/applications-and-dashboards/experimental-data-simulation/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/applications-and-dashboards/experimental-data-simulation/","section":"applications-and-dashboards","summary":"This open-source, R-based web application is suitable for educational or research purposes in experimental sciences. It allows the **creation of varied data sets with specified structures, such as between-group or within-participant variables, that can be categorical or continuous.** These features can be selected along the different tabs. In the penultimate tab, a custom summary of the current data set can be constructed. In the last tab, the list of parameters and the data set can be downloaded.","tags":["web application","data simulation","randomisation","research methods","experiment","statistics","data science","R","Tidyverse","R Shiny","Flexdashboard","HTML","CSS","Software Sustainability Institute Fellowship"],"title":"Web application for the simulation of experimental data","type":"applications-and-dashboards"},{"authors":["Rand et al."],"categories":["research"],"content":"\r\r","date":1590710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590710400,"objectID":"356352fe6162095bd1b10790cbffac29","permalink":"https://pablobernabeu.github.io/2020/how-to-engage-research-group-leaders-in-sustainable-software-practices/","publishdate":"2020-05-29T00:00:00Z","relpermalink":"/2020/how-to-engage-research-group-leaders-in-sustainable-software-practices/","section":"post","summary":"\r\r","tags":["s","research","sustainable practices"],"title":"How to engage Research Group Leaders in sustainable software practices","type":"post"},{"authors":["Cooper et al."],"categories":["software"],"content":"\r\r","date":1590710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590710400,"objectID":"0a69641623857190165304e7c0a35302","permalink":"https://pablobernabeu.github.io/2020/incentives-for-good-research-software-practices/","publishdate":"2020-05-29T00:00:00Z","relpermalink":"/2020/incentives-for-good-research-software-practices/","section":"post","summary":"\r\r","tags":["s","research software","best practices"],"title":"Incentives for good research software practices","type":"post"},{"authors":null,"categories":["data visualisation","R"],"content":"\u0026nbsp; Dashboard\r\r\rThis dashboard presents open data (iNaturalist and BioScan) from Prudic, K.L.; Oliver, J.C.; Brown, B.V.; Long, E.C. Comparisons of Citizen Science Data-Gathering Approaches to Evaluate Urban Butterfly Diversity. Insects 2018, 9, 186. In their study, Prudic and colleagues compared citizen science with traditional methods in the measurement of butterfly populations.\nI developed this dashboard after reproducing the analyses of the original study in a Reprohack session.\nMy coding tasks included transforming the data to a long format,\n# There are pseudovariables, that is, observations entered as variables. Since most R processes # need the tidy format, convert below (see https://r4ds.had.co.nz/tidy-data.html).\r# The specific numbers found through traps and crowdsourcing methods are preserved.\rBioScan = BioScan %\u0026gt;% pivot_longer(\rcols = Anthocharis_sara:Vanessa_cardui, names_to = \u0026quot;Species\u0026quot;,\rvalues_to = \u0026quot;Number\u0026quot;, values_drop_na = TRUE\r)\r# Compare\r#str(BioScan)\r#str(dat)\r# 928 rows now; the result of 29 pseudo-variables being transposed into\r# rows, interacting with 32 previous rows, i.e., 29 * 32 = 928.\r merging three data sets,\n# The iNaturalist data set presents a slightly different challenge from the pseudovariables found above.\r# The number of animals of each species must be computed from repeated entries, per site.\riNaturalist = merge(iNaturalist, iNaturalist %\u0026gt;% count(species, site, name = 'Number'))\r and, as ever, wrangling with the format of the dashboard pages to preserve the format of a table.\nSpecies details {style=\u0026quot;background-color: #FCFCFC;\u0026quot;}\r=======================================================================\rColumn {style=\u0026quot;data-width:100%; position:static; height:1000px;\u0026quot;}\r-----------------------------------------------------------------------\r Reference Bernabeu, P. (2020). Dashboard with data from Prudic, Oliver, Brown, \u0026amp; Long (2018), Comparisons of Citizen Science Data-Gathering Approaches to Evaluate Urban Butterfly Diversity, Insects, 9, 186. https://pablobernabeu.github.io/dashboards/Butterfly-species-richness-in-LA/d.html.\n\r","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"96946d5c2cb3747b0122b8bf0160e852","permalink":"https://pablobernabeu.github.io/applications-and-dashboards/butterfly-species-richness-in-la/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/applications-and-dashboards/butterfly-species-richness-in-la/","section":"applications-and-dashboards","summary":"Dashboard with open data from a study by Prudic et al. (2018), that compares citizen science with traditional methods in butterfly sampling. Coding tasks included long-transforming, merging, and as ever, wrangling with a table.","tags":["data dashboard","R","citizen science","butterflies","nature","open data","BioScan","iNaturalist","HTML","CSS","tidy","merge","Software Sustainability Institute Fellowship"],"title":"Data dashboard: Butterfly species richness in Los Angeles","type":"applications-and-dashboards"},{"authors":[],"categories":["education","programming","R"],"content":"\r\rEnhanced data presentation using reproducible documents and dashboards\rCalendar\r\r\r\r\rDate\rTitle\rEvent and location\rRegistration\r\r\r\r26 Nov 2020\rMixed-effects models in R, and a new tool for data simulation\rNew Tricks Seminars, Dept. Psychology, Lancaster University [online]\r\r\r8 Oct 2020\rReproducibilidad en torno a una aplicación web\rReprohack en español, LatinR Conference 2020 [online]\rLink\r\r13 Aug 2020\rOpen data and reproducibility: R Markdown, data dashboards and Binder v2.1 (co-led with Florencia D’Andrea)\rCarpentryCon, CarpentryCon@Home, The Carpentries [online]\rLink\r\r26 July 2020\rOpen data and reproducibility: R Markdown, data dashboards and Binder (co-led with Eirini Zormpa)\rUK Cognitive Linguistics Conference, University of Birmingham [online]\rLink\r\r6 May 2020\rR Markdown\rLancaster University [online]\r\r\rEvent cancelled\rOpen data and reproducibility 2.0\rSatRday Newcastle upon Tyne, Newcastle University\rLink\r\r\r\r\n\rBackground\rThis project offers free activities to learn and practise reproducible data presentation. Pablo Bernabeu organises these events in the context of a Software Sustainability Institute Fellowship.\nOpen-source software\rProgramming languages such as R and Python offer free, powerful resources for data processing, visualisation and analysis. Experience in these programs is highly valued in data-intensive disciplines.\n\rOpen data\rOriginal data has become a public good in many research fields thanks to cultural and technological advances. On the internet, we can find innumerable data sets from sources such as scientific journals and repositories (e.g., OSF), local and national governments (e.g., London, UK [1, 2]), non-governmental organisations (e.g., data.world), etc. Researchers inside and outside academia nowadays share a lot of their data under attribution licences (e.g., Creative Commons, the UK Open Government Licence, etc.). This allows any external analysts to access these raw data, create (additional) visualisations and analyses, and share these. In society, making data more accessible can demonstrably benefit citizens (despite limitations).\n\r\r\rActivities\rActivities comprise free workshops and datathons.\nWorkshops\rR is a programming language greatly equipped for the creation of reproducible documents and dashboards. Four workshops are offered that cover a suite of interrelated tools—R, R Markdown, data dashboards and Binder environments—, all underlain by reproducible workflows and open-source software.\nEach workshop includes taught and practical sections. The practice provides a chance for participants to experience and address common issues with the code. The level of taught sections is largely tailored to participants; similarly, practice sections are individually adaptable by means of easier and tougher tasks. The duration is also flexible, and some of the workshops can be combined into the same session.\nThe RStudio interface is used in all workshops. Multi-levelled, real code examples are used. Throughout the workshops, and especially in the practice sections, individual questions will be encouraged.\nWorkshop 1: Introduction to R\rThis workshop can serve as an introduction to R or a revision. It demonstrates what can be done in R, and provides resources for individual training. Since the duration is limited, online courses are also recommended (see examples and fee waivers for full content).\n\rData structures\rPackages: general-purpose examples (e.g., tidyverse) and more specific ones (e.g., for statistics or geography)\rLoading and writing data, in native and foreign formats\rWide format (also dubbed ‘untidy’) versus tidy format (also dubbed ‘long’ or ‘narrow’). For most processes in R, data needs to be in a tidy format.\r\r\rImage from Postma and Goedhart (2019; https://doi.org/10.1371/journal.pbio.3000202.g001).\r\rCombining data sets\rData summaries\rPlots with ggplot2::ggplot()\rInteractive plots with plotly::ggplotly()\rStatistics\rLinear mixed-effects models (see also a review of practices)\rHow functions work\rDebugging. Code errors are known as bugs. They can tiresome, but also interesting sometimes! 😅 Some tips for the first many years of experience include: reading and investigating error messages, in both source and console windows; controlling letter case and typos; closing parentheses and inverted commas; ensuring to have the necessary packages installed and loaded; following the format required by each function. To debug, break up code into subcomponents and test each of those to find out the source of the error. Once we act on that, the best outcome is seeing the code work, but sometimes different errors overlap, in which case we may see one error disappearing before another one appears. Debugging soon leads to proficient information seeking. The search process often begins on an internet search engine and extends to user communities, package documentation, tutorials, blogs… (see video explanation). Advanced debugging tools are also available.\rVast availability of free resources on the internet, from Coursera and other MOOC sites, RStudio, University of Glasgow, Carpentries, etc.\rCommunity: StackOverflow, RStudio Community, Github issues (e.g., for R packages), etc. Using and contributing back.\rRStudio Cloud: a personal RStudio environment on the internet\r\r\rWorkshop 2: R Markdown documents\rSet your input and output in stone using R Markdown. The analysis reports may be enriched with website features (HTML/CSS) and published as HTML, PDF or Word documents. Moreover, with R packages such as bookdown, bookdownplus, blogdown and flexdashboard, documents can be formatted as websites, digital papers and books and data dashboards. Other useful packages include rmarkdown, knitr, DT, kableExtra and ggplot2. Further background: presentation by Michael Frank, slides by Ed Berry.\nAs well as facilitating the reproducibility of analyses and results to third parties, R Markdown is helpful during the creation of a report. In particular, it reduces the chances of errors and the number of repetitive tasks. For instance, any part of the data can be inputted in the text directly from the source, rather than manually copying it (e.g., `r mean(dat[dat$location=='Havana', 'measure'])` (expand). Thus, if and when the analysis needs to be changed or updated, the report can be automatically updated at the click of a button. In another area, the captions for figures and tables can be automatised using cross-reference labels (e.g., Table \\@ref(tab:mtcars)). This secures the match between the text and the captions of figures and tables, and it automatically updates the numbering whenever and wherever a new figure or table is introduced.\n\rImage from bookdownplus package (https://bookdownplus.netlify.com/portfolio/).\r\rWorkshop 3: Introduction to data dashboards\rData dashboards are web applications used to visualise data in detail through tables and plots. They assist in explaining and accounting for our data processing and analysis. They don’t require any coding from the end user. While most dashboards and web applications present existing data, a few of them serve the purpose of creating or simulating new data (see example).\n\rThese all-reproducible dashboards are published as websites, and thus, they can include hyperlinks and downloadable files. Some of the R packages used are knitr, DT, kableExtra, reactable, ggplot2, plotly, rmarkdown, flexdashboard and shiny. The aim of this workshop is to practise creating different forms of dashboards—Flexdashboard and Shiny—the latter of which offers greater features, and to practise also with the hosting platforms fitting each type—such as personal websites, RPubs, Binder, Shinyapps and custom servers.\nA great thing about dashboards is that they may be made very simple, but they can also be taken to the next level using some HTML, CSS or Javascript code (on top of the back-end code present in the R packages used), which is addressed in the next workshop.\n\rWorkshop 4: Binder environments and improving data dashboards\rBinder\rBinder is a tool to facilitate public access to software environments—for instance, by publishing an RStudio environment on the internet. Binder can also host Shiny apps. It is generously free for users. After looking at the nuts and bolts of a deployment, participants will be able to deploy their own Binder environments and check the result by the end of the workshop. For this purpose, it’s recommended to have data and R code ready, ideally in a GitHub repository.\n\rImproving data dashboards\rWe will practise how to improve the functionality of dashboards using some HTML, CSS and Javascript code, which is the basis of websites.\n\u0026lt;!-- Javascript function to enable a hovering tooltip --\u0026gt;\r\u0026lt;script\u0026gt;\r$(document).ready(function(){\r$(\u0026#39;[data-toggle=\u0026quot;tooltip1\u0026quot;]\u0026#39;).tooltip();\r});\r\u0026lt;/script\u0026gt;\r\r\rTrade-offs among dashboards\rNext, we will practise with three dashboard types—Flexdashboard, Shiny and Flexdashboard-Shiny—and with the suitable hosting platforms. Firstly, the strength of Flexdashboard (example) is its basis on R Markdown, yielding an unmatched user interface (front-end). Secondly, the strength of Shiny (example) is the input reactivity (back-end) it offers, allowing users to download sections of data they select, in various formats. Last, Flexdashboard-Shiny (example) combines the best of both worlds.\n★  Flexdashboard  ★\r★ ★  Shiny  ★ ★\r★ ★ ★  Flexdashboard-Shiny  ★ ★ ★\rFlexdashboard types are rendered as an HTML document—simple websites—, and can therefore be easily published on personal sites or RPubs. This is convenient because no special hosting is required. In contrast, Shiny and Flexdashboard-Shiny types offer greater features, but require Shiny servers. Fortunately, the shinyapps.io server is available for free, up to some usage limit. This server can host any of the three dashboards mentioned here. Another good option is presented by Binder environments, which can host the Shiny-type dashboards with no (explicit) limit. Yet, the Flexdashboard-Shiny type cannot be hosted in this server (as of January 2020, at least). Consequently, greater functionality may come at a cost for dashboards that have any considerable traffic, whereas dashboards with low traffic may do well on shinyapps.io. Knowing these trade-offs can help navigate usage limits, save on web hosting fees, and increase the availability of our dashboards online, as we can offer fall-back versions on different platforms, as in the example below:\n\r… preferred-dashboard (in case of downtime, please visit this alternative)\n\rTransforming dashboards into the different versions can be as easy as enabling or disabling some features, especially input reactivity. For instance, if we want to downgrade a Flexdashboard-Shiny to a Flexdashboard, to publish it outside of a Shiny server (see example), we must delete runtime:shiny from the header, and disable reactive features, as below.\n\r```r\r# Number of words selected on sidebar\r# reactive(cat(paste0(\u0026#39;Words selected below: \u0026#39;, nrow(selected_props()))))\r```\r\rFree accounts and tips\rHosting sites have specific terms of use. For instance, shinyapps.io has a free starter license with limited use. Free apps can handle a large but limited amount of data, and up to five apps may be created. Beyond this, RStudio offers a wide range of subscriptions starting at $9/month.\nMemory and traffic limits of the free shinyapps.io account can sometimes present problems when heavy data data sets are used, or there are many visits to the app. The memory overload issue is often flagged as Shiny cannot use on-disk bookmarking, whereas excessive traffic may see the app not loading. Fortunately, usage limits need not always require a paid subscription or a custom server, thanks to the following workarounds:\n\rdevelop app locally as far as possible, and only deploy to shinyapps.io only at the last stage;\rprune data set, leaving only the necessary data;\rif necessary, unlink data by splitting it into different sets, reducing computational demands;\rif necessary, use various apps (five are allowed in each free shinyapps.io account);\rif necessary, link from the app to a PDF with visualisations requiring heavy, interlinked data. High-resolution plots can be rendered into a PDF document in a snap, using code such as below.\r\r\rpdf(\u0026#39;List of plots per page\u0026#39;, width = 13, height = 5)\rprint(plot1)\rprint(plot2)\r# ...\rprint(plot150)\rdev.off()\r\rConveniently, all text in a PDF—even in plots—is indexed, so it can be searched [ Ctrl+f / Cmd+f / 🔍 ] (see example). Furthermore, you may also merge the rendered PDF with any other documents.\n\r\rPrerequisites and suggestions for participation in each workshop\rNecessary: laptop or computer with R and RStudio installed, or access to RStudio Cloud; familiarity with the content of the preceding workshops through the web links herein.\nSuggested: having your own data and R code ready (on a Github repository if participating in Workshop 4); participation in some of the preceding workshops.\n\r\rDatathons: creating reproducible documents and dashboards\rIn these coding meetups, participants collaborate to create reproducible documents or dashboards using the data and software they prefer (see examples). Since the work can be split across different people and sections, some nice products may be achieved within a session. Any programming languages may be used.\n\rData used: academic or non-academic data of your own or from open-access sources such as OSF, scientific journals, governments, international institutions, NGOs, etc.\n\rInspired by the great Reprohacks, content suggestions are encouraged. That is, if you’d like to have a reproducible document or dashboard created for a certain, open-access data set, please let us know, and some participants may take it on. Suggestions may be posted as issues or emailed to p.bernabeu@lancaster.ac.uk.\r\r\rPurposes\n\rcollaborating to visualise data in novel ways using reproducible documents or interactive dashboards. For this purpose, participants sometimes draw on additional data to look at a bigger picture;\n\rreflecting on the process by reviewing the techniques applied and challenges encountered.\n\r\rOutput: A key aspect of datathons is the creation of output. Documents and dashboards are (co-)authored by the participants who work on them, who can then publish them on their websites, or on RPubs, Binder, Shinyapps or custom servers. Time constraints notwithstanding, a lot of this output may be very enticing for further development by the same participants, or even by other people if the code is shared online. Just like with data, an attribution licence can be attached to the code.\n\r\rPrerequisites and suggestions for participation in datathons\rNecessary: basic knowledge of reproducible documents or dashboards.\nSuggested: familiarity with the development of reproducible documents or dashboards; an idea about the data you’d like to work with and the kind of document or dashboard you want to create.\n\r\r\rContact\rPlease submit any queries or requests by posting an issue or emailing p.bernabeu@lancaster.ac.uk.\n\r","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"d4c6f6ca4eb0c2665fdbb559c58d2620","permalink":"https://pablobernabeu.github.io/2020/data-is-present-workshops-and-datathons/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/2020/data-is-present-workshops-and-datathons/","section":"post","summary":"This project offers free activities to learn and practise reproducible data presentation. Pablo Bernabeu organises these events in the context of a Software Sustainability Institute Fellowship. Programming languages such as R and Python offer free, powerful resources for data processing, visualisation and analysis. Experience in these programs is highly valued in data-intensive disciplines. Original data has become a public good in many research fields thanks to cultural and technological advances. On the internet, we can find innumerable data sets from sources such as scientific journals and repositories (e.g., OSF), local and national governments, non-governmental organisations (e.g., data.world), etc. Activities comprise free workshops and datathons.","tags":["s","programming","education","workshop","datathon","data presentation","data visualisation","dashboard","reproducibility","open science","open data","statistics","R","R Shiny","Flexdashboard","HTML","CSS","Software Sustainability Institute Fellowship"],"title":"Data is present: Workshops and datathons","type":"post"},{"authors":[],"categories":["event-related potentials"],"content":"Event-related potentials (ERPs) offer a unique insight in the study of human cognition. Let's look at their reason-to-be for the purposes of research, and how they are defined and processed. Most of this content is based on my master's thesis, which I could fortunately conduct at the Max Planck Institute for Psycholinguistics (see thesis or conference paper).\nElectroencephalography The brain produces electrical activity all the time, which can be measured via electrodes on the scalp—a method known as electroencephalography (EEG). These pulses are produced for every one of our states and actions, in a voltage at a micro (µ) scale, typically between 10 µV (0.000010) and 100 µV (0.000100; Aurlien et al., 2004). The overlapping pulses happen at extremely high frequencies; indeed, the signal can be measured once per millisecond. The high frequency of this signal is very interesting for the study of some cognitive processes in particular, for which the time course is (or may be) critical. One such example is conceptual processing, namely, the process of understanding the meaning of words.\nResearch has revealed the relation between certain EEG patterns and cognitive states and functions. Brain activity includes dozens of types, but broadly, it can be divided into neural oscillations and event-related potentials. Specific oscillations (also known as brain waves) are associated with states such as wakefulness, sleep, arousal, relaxation, etc. (Roohi-Azizi, Azimi, Heysieattalab, \u0026amp; Aamidfar, 2017). Event-related potentials instead represent more finite events, such as the presentation of as a stimulus. In cognitive neuroscience, both oscillations and ERPs are studied, whereas in cognitive psychology, ERPs are much more common than oscillations. Let's dive into ERPs below.\nEvent-related potentials In the lab, ERPs are elicited using controlled designs. In each trial, a series of stimuli are presented. At a fixed point therein, an EEG measurement begins and spans for a certain period. In turn, in the analysis, this measurement period is divided into time windows, which often correspond to specific ERP components (e.g., N400 window).\nIn psycholinguistics, for instance, a typical scenario is the presentation of words, and ERPs are systematically time-locked to the same position in consecutive trials, often the onset of a word. By this means, the experimental manipulation is collected, and the non-experimental variation—\u0026lsquo;noise\u0026rsquo;—is largely cancelled out by the aggregation of multiple trials that share the experimental manipulation.\nThe chief reason to employ the ERP method is the measurement of cognitive processes online, that is, precisely as they unfold. This is fitting in the context of language comprehension, where some important processes last for less than a second.\nTime course of word processing Processing a word takes around 800 milliseconds (ms). Within that time, earlier processes (compared to later ones) have been ascribed greater relevance to the core process of understanding a word (Mahon \u0026amp; Caramazza, 2008). This assumes that broader processes start only after more immediate ones have started (but see Lebois, Wilson‐Mendenhall \u0026amp; Barsalou, 2014). The most immediate process is the recognition of a string of letters, which seems to start within 90 ms post word onset in early auditory cortex and the Visual Word Form Area (Willems, Frank, Nijhoff, Hagoort, \u0026amp; van den Bosch, 2016). Then ensue further, fundamental stages known as lexical and semantic processes. Lexical processing is the identification of a string of letters as a known word, and it happens within around 160 ms post word onset. Next, at around 200 ms, we may see the beginning of semantic processing, which denotes a further step in the cognitive analysis of the word that is akin to meaning (Hauk, 2016). These processes may overlap, as indeed suggested by the sensitivity of the N400 ERP (see also next section) to both lexical and semantic tasks (Kutas \u0026amp; Federmeier, 2011). Both processes also likely extend further in the processing timeline (Hauk, 2016). In spite of this overlap, however, lexical and semantic processing have often been linked to different cognitive phenomena. For instance, tasks promoting semantic processing (e.g., semantic decision, whereby participants describe words as concrete or abstract) have been found to engage sensorimotor simulation of the word's meaning (known as embodiment) more strongly than lexical tasks do (Connell \u0026amp; Lynott, 2013; Pexman, Muraki, Sidhu, Siakaluk, \u0026amp; Yap, 2019; Sato, Mengarelli, Riggio, Gallese, \u0026amp; Buccino, 2008).\nOnce the lexical and semantic stages have emerged, post-lexical, post-semantic processes follow (Mahon \u0026amp; Caramazza, 2008). These are mental imagery and episodic memory processes—both with an approximate emergence around 270 ms after word onset. The gradual progression from the identification of a word up to accessing its broadest meaning is an important anchoring point in the current research on the alleged embodiment of meaning comprehension, even if we might hope to count on more definitive threshold points (Hauk, 2016).\nWord processing data are mainly based on written word processing, but spoken words are processed quite similarly, if slightly faster (Leonard, Baud, Sjerps, \u0026amp; Chang, 2016; Pulvermüller, Shtyrov, \u0026amp; Ilmoniemi, 2005; Shtyrov, Hauk, \u0026amp; Pulvermüller, 2004).\nThe bigger take-home messages would be: (1) the processing of meaning might only start at around 160 ms post word onset, and (2) processes outside of meaning comprehension might only start at around 270 ms. These working references must be taken with some caution because particular semantic effects have been found at different stages (e.g., the conceptual modality switch, as in Hald, Marshall, Janssen, \u0026amp; Garnham, 2011; Collins, Pecher, Zeelenberg, \u0026amp; Coulson, 2011). Indeed, in an influential critique of blooming findings on embodiment, Mahon and Caramazza (2008) argued that even early effects might possibly be explained in terms of non-embodied processing. They contended that working memory processes that were ancillary rather than semantic could be quickly engaged with the function of ‘colouring’ a concept, not building it up. To further complicate the matter, we do not have absolute certainty on the later section of the time course. Thus, as Hauk (2016) reviews, the different stages likely overlap at certain points, with different degrees of relevance. For instance, lexical processing may continue even once semantic processing has started, but would naturally become less relevant. Indeed, the relation among these processes is likely more of a continuum than a set of clear-cut modules. In a nutshell, the time course is important with some experimental effects in word processing, and, to that extent, we depend on our knowledge of the basic time course of word processing.\nThe conceptual modality switch paradigm and its time course In demonstrating the relevance of embodied cognition, a sizeable series of studies have shown that reading about different conceptual modalities (e.g., auditory ‘loud bell’ followed by visual ‘pink background’) incurs processing costs (Pecher, Zeelenberg, \u0026amp; Barsalou, 2003). Importantly, this manipulation does not concern the presentation mode of the stimulus, maintained constant, but the intrinsic semantic modality of the stimulus concepts. The conceptual modality switch effect has often been replicated (Pecher, Zeelenberg, \u0026amp; Barsalou, 2004; Solomon \u0026amp; Barsalou, 2004; Marques, 2006; Vermeulen, Niedenthal, \u0026amp; Luminet, 2007; van Dantzig, Pecher, Zeelenberg, \u0026amp; Barsalou, 2008; Lynott \u0026amp; Connell, 2009; Ambrosi, Kalenine, Blaye, \u0026amp; Bonthoux, 2011; Collins et al., 2011; Hald et al., 2011; Hald et al., 2013; Scerrati et al., 2015).\nBernabeu, Willems and Louwerse (2017) addressed a caveat with the time course of the conceptual modality switch paradigm. In previous experiments, trials presented a concept word followed by a property word. ERPs were time-locked to the latter property word. This design may have left uncontrolled a switch produced already at the concept. Indeed, the property word was already supposed to be in the particular modality of the trial. That pitfall could have had two consequences: loss of power and loss of certainty on the time course of the effect. Thus, Bernabeu et al. created a design in which ERPs were time-locked to the first word in target trials (see some early input from researchers online). The purpose of this relocation was not to completely annul the possibility of post-core sensory processes, but to increase the time accuracy by measuring the modality switch from the point at which it is elicited.\nImplementing this design had an ancillary effect on the measurement of response times. A psycholinguistic experiment like this one requires controlling fundamental variables such as word frequency and length, by matching the means of these variables across experimental conditions. This must be controlled in the target words at least. As it is often the case, this control was only possible in the target words—the first one in target trials—, but it was not possible in the second word, which is the crucial one for response times. Response times could still be measured, but comparisons across conditions were not fully warranted. In sum, this was an ERP design.\nERP components When the ERP signal is plotted, it displays multiple wave shapes, or waveforms, each with a peak flanked by falling tails. Each of these waves often corresponds to an ERP component, which is what cognitive scientists are often interested in.\nMultiple components are known, each having been found to consistently peak around specific points in time during a cognitive process. The peak is one of several features characterising each component. A sketch list is shown below (van Hell \u0026amp; Kroll, 2013).\n  Polarity: The component either peaks in the positive or the negative pole of the signal. This polarity is relative to the baseline point that is created in the preprocessing stage (see below);\n  Latency: the time course of the component, encompassing an onset, a peak and an overall duration. Time windows are normally set to match relevant components (e.g., the N400 window, etc.);\n  Amplitude: the voltage reached at a given time (e.g., the peak) or for a certain period (e.g., a time window);\n  Scalp distribution, or topography: the areas on the scalp (the scalp being a reasonable proxy for the brain) in which the component appears;\n  Functional role: the cognitive functions that have been consistently associated with the component.\n  Examples of components in language processing include the N400, consistently linked to semantic processing, that is, seeking the meaning of words or sentences. The N400 is characterised by a large, negative amplitude peaking at around 400 ms post word onset, primarily found in central and posterior sites. N400 effects, which are comparisons of the N400 component in different experimental conditions, have consistently appeared under violations of semantic expectations, i.e., related to meaning and events (Kutas \u0026amp; Federmeier, 2011; Swaab, Ledoux, Camblin, \u0026amp; Boudewyn, 2012). Another well-known component in language is the P600, linked to syntactic processing, which allows the comprehension of sentences (Swaab et al., 2012). Other examples of components include lateralized readiness potentials, signalling motor preparation (Mordkoff \u0026amp; Gianaros, 2000), and the P3b component, which appears in the context of responses (van Vliet et al., 2014). Both of the latter components are relevant to researchers across domains, who often need to ward off contamination from these components in their experiments. In Bernabeu et al.\u0026lsquo;s experiment, for instance, part of the reason why ERPs were time-locked to the first word in target trials was to prevent contamination from these components.\nERP data sets are large, being the product of the number of electrodes times the number of time points, times the number of experimental conditions, times the number of participants. In recent studies, the number of trials often adds to that product, whereas in previous experiments, the trials tended to be aggregated in each condition.\nEEG montage The EEG montage is an important factor. The options are broadly characterised by three parameters of the electrodes (also called channels):\nBrainwaves exposed at an open day.\n  Number: Traditionally, montages with 32, 64 or 128 electrodes have been used. The larger the number, the higher the spatial resolution.\n  Wet / dry: In some montages, the electrical conductance on the electrodes\u0026rsquo; contact point must be increased using some fluid solutions, such as a specific gel (often commercialised by the companies that also make EEG apparatuses). Conversely, other electrodes function in a dry way. Ensuring the proper conductance on wet electrodes has traditionally been very time-consuming for experimenters, often taking over half an hour of wiggling a blunt syringe distributing the saline solution around the tip. Traditionally, wet electrodes produced more reliable data than dry ones, but the times they are a'changing (di Flumeri, 2019).\n  Active / passive: In some wet montages, the conductance-prompting job is much facilitated by the existence of a pilot light on top of each electrode, which signals the conductance level throughout the setup on the participant's head.\n  An EEG/ERP experiment is time-consuming. The preparation (especially conductance-prompting with wet montages) and post-experiment procedures (especially washing the EEG cap) often take four or five times as long as the experiment proper. These procedures are especially long for higher-density, wet, passive montages.\nPreprocessing ERPs ERPs are not the first signal collected in experiments. They are obtained after considerable, systematic preprocessing of the EEG signal.\n\nFor the Bernabeu et al. study, I used Brain Vision software, and followed the tutorials from the well-known ERP Boot Camp of Steve Luck and Emily Kappenman. I applied the following pipeline separately for each participant:\n  labeling channels (64 electrodes);\n  creating channel groups (anterior and posterior);\n  re-referencing the signal offline to the right mastoid (RM), having referenced online to the left mastoid (Ref);\n  \n separating my three experimental conditions;\n  ocular correction for blinks and significant, vertical or horizontal movements of the eyes (seminal method by Gratton, Coles, \u0026amp; Donchin, 1983, which is the default in Brain Vision);\n  baseline correction, which is a standardisation based on a certain period immediately before the onset of the target manipulation;\n  further correction of artifacts such as motor action potentials (or lateralised readiness potentials) resulting from even the subtlest muscle activity.\n  This pipeline is reflected in the scripts exported from Brain Vision.\n \u0026lt;Nodes\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels/positions\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels/positions/rerefRM\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels/positions/rerefRM/TotalMatch\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection/baselinecorr\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection/baselinecorr/baselinecorr_artif\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection/baselinecorr/baselinecorr_artif/minave_TotalMatch\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection/baselinecorr\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection/baselinecorr/baselinecorr_artif\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection/baselinecorr/baselinecorr_artif/minave_EmbodiedMismatch\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection/baselinecorr\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection/baselinecorr/baselinecorr_artif\u0026lt;/string\u0026gt;\r\u0026lt;string\u0026gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection/baselinecorr/baselinecorr_artif/minave_TotalMismatch\u0026lt;/string\u0026gt;\r\u0026lt;/Nodes\r Word reading ERPs can look somewhat like this after the preprocessing (plots made in R).\n\nTo visualise these waveforms throughout the different sections of the data, a dashboard is available.\nStatistical analysis With the myriad repeated measures involved in EEG, linear mixed-effects models are a good option, allowing the registration of electrodes and time points in the error term per participant (and trial, too, if these are not aggregated). The analysis I performed, in R, is available (plots visible by downloading the file from the aforementioned link).\n.embed-responsive{position:relative;height:100%;}.embed-responsive iframe{position:absolute;height:100%;}window.jQuery || document.write('\\x3C/script')  var mfrRender = new mfr.Render(\"mfrIframe\", \"https://mfr.osf.io/render?url=https://osf.io/sx3nw/?direct%26mode=render%26action=download%26mode=render\");\rConclusion Event-related potentials fulfil an important role in cognitive neuroscience and psychology, only surpassed by magnetic electroencephalography (MEG), which unites high temporal and spatial resolution. Learning how to use ERPs is demanding but even more rewarding. It certainly does not make for fast science, but allows the measurement of experimental effects online, that is, as they unfold.\nYou can learn about and overcome multiple challenges. One of the issues I faced once regarded some channels (electrodes) that appeared to be missing from the data. I posted a question on ResearchGate, and emailed Brain Products, the maker of Brain Vision Recorder, which I was using.\n Hi everyone,\nIf you could please give me a hand with this error, I would be very grateful. I have EEG from a psychological experiment, recorded with BrainVision Recorder, and being analyzed with BrainVision Analyzer 2. Most of the recordings are perfectly fine, but a few present a big error. Out of 64 original electrodes, only two appear. These are the right mastoid (RM) and the left eye sensor (LEOG). Both are bipolar electrodes. RM is to be re-referenced to the online reference electrode, while LEOG is to be re-referenced to the right eye electrode.\nI just can't fathom the error because all electrodes worked fine during the recording. Also, the data sets with the error are quite as heavy in terms of bytes as those without the error. Further, why should the RM and LEOG channels remain perfectly well as they do?\nThis issue might seem like a simple zoom I've bypassed, or similar\u0026hellip; But unfortunately the channels are just not there. I've confirmed it as I tried to copy the pipeline from the good data sets onto the faulty ones, where I got the error \u0026lsquo;No channels enabled.\u0026rsquo; In case you had access to the BVA analysis software, please find the raw files for one of the faulty data sets here.\n Thanks to invaluable help from a ResearchGate contributor and the Brain Products team, I could put the pieces back together.\n Update: Problem solved.\nAs Nikolay said, the error originated in Recorder (I had used the workspace from the previous experimenter), and the problem was solved by setting the label and position of each channel.\nI tried editing the .vhdr file in raw (it seemed nice and quick to directly assign the channel names as labels) but i didn't quite find the way. Therefore, with a tip from the Brain Products team, I went about it within the program.\nFirst, I used the transform function \u0026lsquo;Edit channels\u0026rsquo; to rename all labels and set each within their coordinates. I did that for just one subject (it doesn't take as long as it sounds). Afterwards, I created a \u0026lsquo;History template\u0026rsquo; out of that process, and copied it to all other nodes. At any rate, never getting out of the comfort workspace again\u0026hellip; :D\n References Ambrosi, S., Kalenine, S., Blaye, A., \u0026amp; Bonthoux, F. (2011). Modality switching cost during property verification by 7 years of age. International Journal of Behavioral Development, 35, 1, 78-83.\nAurlien, H., Gjerde, I., Aarseth, J., Eldøen, G., Karlsen, B., Skeidsvoll, H., \u0026amp; Gilhus, N. (2003). EEG background activity described by a large computerized database. Clinical Neurophysiology, 115, 665–673.\nBernabeu, P., Willems, R. M., \u0026amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, \u0026amp; E. J. Davelaar (Eds.), Proceedings of the 39th Annual Conference of the Cognitive Science Society (pp. 1629-1634). Austin, TX: Cognitive Science Society.\nCollins, J., Pecher, D., Zeelenberg, R., \u0026amp; Coulson, S. (2011). Modality switching in a property verification task: an ERP study of what happens when candles flicker after high heels click. Frontiers in Psychology, 2.\nConnell, L., \u0026amp; Lynott, D. (2013). Flexible and fast: Linguistic shortcut affects both shallow and deep conceptual processing. Psychonomic Bulletin \u0026amp; Review, 20, 542-550.\nDi Flumeri, G., Aricò, P., Borghini, G., Sciaraffa, N., Di Florio, A., \u0026amp; Babiloni, F. (2019). The Dry Revolution: Evaluation of Three Different EEG Dry Electrode Types in Terms of Signal Spectral Features, Mental States Classification and Usability. Sensors (Basel, Switzerland), 19(6), 1365.\nGratton, G., Coles, M. G., \u0026amp; Donchin, E. (1983). A new method for offline removal of ocular artefact. Electroencephalography and Clinical Neurophysiology, 55, 4, 468-484.\nHald, L. A., Hocking, I., Vernon, D., Marshall, J.-A., \u0026amp; Garnham, A. (2013). Exploring modality switching effects in negated sentences: further evidence for grounded representations. Frontiers in Psychology, 4, 93.\nHald, L. A., Marshall, J.-A., Janssen, D. P., \u0026amp; Garnham, A. (2011). Switching modalities in a sentence verification task: ERP evidence for embodied language processing. Frontiers in Psychology, 2.\nHauk, O. (2016). Only time will tell—Why temporal information is essential for our neuroscientific understanding of semantics. Psychonomic Bulletin \u0026amp; Review, 23, 4, 1072-1079.\nKutas, M., \u0026amp; Federmeier, K. D. (2011). Thirty years and counting: finding meaning in the N400 component of the event-related brain potential (ERP). Annual Review of Psychology, 62, 621–647.\nLebois, L. A. M., Wilson-Mendenhall, C. D., \u0026amp; Barsalou, L. W. (2014). Are automatic conceptual cores the gold standard of semantic processing? The context-dependence of spatial meaning in grounded congruency effects. Cognitive Science, 39, 8, 1764-801.\nLeonard, M. K., Baud, M. O., Sjerps, M. J., \u0026amp; Chang, E. F. (2016). Perceptual restoration of masked speech in human cortex. Nature Communications, 7, 13619.\nLuck, S. J. \u0026amp; Kappenman, E.S. (Eds.), Oxford Handbook of Event-Related Potential Components. New York: Oxford University Press\nMahon, B.Z., \u0026amp; Caramazza, A. (2008). A critical look at the Embodied Cognition Hypothesis and a new proposal for grounding conceptual content. Journal of Physiology - Paris, 102, 59-70.\nMarques, J. F. (2006). Specialization and semantic organization: Evidence for multiple semantics linked to sensory modalities. *Memory \u0026amp; Cognition, 34, 1, 60-67.\nMordkoff, J. T., \u0026amp; Gianaros, P. J. (2000). Detecting the onset of the lateralized readiness potential: A comparison of available methods and procedures. Psychophysiology, 37(3), 347–360.\nPecher, D., Zeelenberg, R., \u0026amp; Barsalou, L. W. (2003). Verifying different-modality properties for concepts produces switching costs. Psychological Science, 14, 2, 119-24.\n____ (2004). Sensorimotor simulations underlie conceptual representations: Modality-specific effects of prior activation. Psychonomic Bulletin \u0026amp; Review, 11, 1, 164-167.\nPexman, P. M., Muraki, E. J., Sidhu, D. M., Siakaluk, P. D., \u0026amp; Yap, M. J. (2019). Quantifying sensorimotor experience: Body-object interaction ratings for more than 9,000 English words. Behavior Research Methods, 51, 453-466.\nPulvermüller, F., Shtyrov, Y., \u0026amp; Hauk, O. (2009). Understanding in an instant: Neurophysiological evidence for mechanistic language circuits in the brain. Brain and Language, 110, 2, 81–94.\nRoohi-Azizi, M., Azimi, L., Heysieattalab, S., \u0026amp; Aamidfar, M. (2017). Changes of the brain's bioelectrical activity in cognition, consciousness, and some mental disorders. Medical journal of the Islamic Republic of Iran, 31, 53.\nSato, M., Mengarelli, M., Riggio, L., Gallese, V., \u0026amp; Buccino, G. (2008). Task related modulation of the motor system during language processing. Brain and Language, 105, 83–90.\nScerrati, E., Baroni, G., Borghi, A. M., Galatolo, R., Lugli, L., \u0026amp; Nicoletti, R. (2015). The modality-switch effect: visually and aurally presented prime sentences activate our senses. Frontiers in Psychology, 6, 1668.\nShtyrov, Y., Hauk, O., \u0026amp; Pulvermüller, F. (2004). Distributed neuronal networks for encoding category-specific semantic information: the mismatch negativity to action words. European Journal of Neuroscience, 1, 4, 1083–1092.\nSolomon, K. O., \u0026amp; Barsalou, L. W. (2004). Perceptual simulation in property verification. Memory \u0026amp; Cognition, 32, 244-259.\nSwaab, T.Y., Ledoux, K., Camblin, C.C., \u0026amp; Boudewyn, M.A. (2012) Language related ERP components. (Book Chapter). In Luck, S. J. \u0026amp; Kappenman, E.S. (Eds.), Oxford Handbook of Event-Related Potential Components (pp. 397-440). New York: Oxford University Press\nVan Dantzig, S., Pecher, D., Zeelenberg, R., \u0026amp; Barsalou, L. W. (2008). Perceptual processing affects conceptual processing. Cognitive Science, 32, 579–590.\nVan Hell, J. G., \u0026amp; Kroll, J. F. (2013). Using electrophysiological measures to track the mapping of words to concepts in the bilingual brain: a focus on translation. In J. Altarriba \u0026amp; L. Isurin (Eds.), Memory, Language, and Bilingualism: Theoretical and Applied Approaches (pp. 126-160). New York: Cambridge University Press.\nVan Vliet, M., Manyakov, N., Storms, G., Fias, W., Wiersema, J., \u0026amp; Van Hulle, M. (2014). Response-Related Potentials during semantic priming: the effect of a speeded button response task on ERPs. PLoS One, 9, 2, e87650.\nVermeulen, N., Niedenthal, P. M., \u0026amp; Luminet, O. (2007). Switching between sensory and affective systems incurs processing costs. Cognitive Science, 31, 1, 183-192.\nWillems, R. M., Frank, S. L., Nijhoff, A. D., Hagoort, P., \u0026amp; Van den Bosch, A. (2016). Prediction during natural language comprehension. Cerebral Cortex, 26, 6, 2506-2516.\n\r","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"6a2774b23fea8ee5fe9d0dac3de1cdfc","permalink":"https://pablobernabeu.github.io/2020/event-related-potentials-why-and-how-i-used-them/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/2020/event-related-potentials-why-and-how-i-used-them/","section":"post","summary":"Event-related potentials (ERPs) offer a unique insight in the study of human cognition. Let's look at their reason-to-be for the purposes of research, and how they are defined and processed. Most of this content is based on my master's thesis, which I could fortunately conduct at the Max Planck Institute for Psycholinguistics (see thesis or conference paper).\nElectroencephalography The brain produces electrical activity all the time, which can be measured via electrodes on the scalp—a method known as electroencephalography (EEG).","tags":["s","event-related potentials","electroencephalography","electrodes","preprocessing","methodology","cognitive neuroscience","Brain Vision","R","statistics","linear mixed-effects models","Max Planck Institute for Psycholinguistics"],"title":"Event-related potentials: Why and how I used them","type":"post"},{"authors":["Chen, S.-C., Szabelzka, A., Chartier, C. R., Kekecs, Z., Lynott, D., Bernabeu, P. ... Schmidt, K."],"categories":["conceptual processing","individual differences","conceptual replication"],"content":"\n General project  Video demonstration of the procedure followed in our lab  Data from our lab Reference Chen, S., Szabelska, A., Chartier, C. R., Kekecs, Z., Lynott, D., Bernabeu, P., … Schmidt, K. (2018). Investigating object orientation effects across 14 languages. PsyArXiv. https://doi.org/10.31234/osf.io/t2pjv/\n\r","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"9e4b60290f7493777dd1254f0a6e8953","permalink":"https://pablobernabeu.github.io/publication/chen-etal-inprep/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/chen-etal-inprep/","section":"publication","summary":"Mental simulation theories of language comprehension propose that people automatically create mental representations of real objects. Evidence from sentence-picture verification tasks has shown that people mentally represent various visual properties such as shape, color, and size. However, the evidence for mental simulations of object orientation is limited. We report a study that investigates the match advantage of object orientation across speakers of different languages. This multi-laboratory project aims to achieve two objectives. First, we examine the replicability of the match advantage of object orientation across multiple languages and laboratories. Second, we will use a mental rotation task to measure participants’ mental imagery after the sentence-picture verification task. The relationship between the participants’ performance of the two tasks will provide a cross-linguistic examination of perceptual simulation processes. With the (broad) evaluation of individual mental imagery ability and potential linguistic moderators, we expect a robust estimation of match advantage of object orientation.","tags":["object orientation effect","conceptual processing","reading","event-related potentials","cognition","crosslinguistic","replication","psycholinguistics","Psychological Science Accelerator"],"title":"Investigating object orientation effects across 14 languages","type":"publication"},{"authors":["Bernabeu, P., \u0026 Tillman, R."],"categories":["linguistic relativity"],"content":"Reference Bernabeu, P., \u0026amp; Tillman, R. (2019). More refined typology and design in linguistic relativity: The case of motion event encoding. Dutch Journal of Applied Linguistics, 8(2), 163-171. http://doi.org/10.1075/dujal.15019.ber\n ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"d966dfb0977cb54bb103ccd6372d0b42","permalink":"https://pablobernabeu.github.io/publication/bernabeu-tillman-2019/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/bernabeu-tillman-2019/","section":"publication","summary":"Linguistic relativity is the influence of language on other realms of cognition. For instance, the way movement is expressed in a person’s native language may influence how they perceive movement. Motion event encoding (MEE) is usually framed as a typological dichotomy. Path-in-verb languages tend to encode path information within the verb (e.g., ‘leave’), whereas manner-in-verb languages encode manner (e.g., ‘jump’). The results of MEE-based linguistic relativity experiments range from no effect to effects on verbal and nonverbal cognition. Seeking a more definitive conclusion, we propose linguistic and experimental enhancements. First, we examine state-of-the-art typology, suggesting how a recent MEE classification across twenty languages (Verkerk, 2014) may enable more powerful analyses. Second, we review procedural challenges such as the influence of verbal thought and second-guessing in experiments. To tackle these challenges, we propose distinguishing verbal and nonverbal subgroups, and having enough filler items. Finally we exemplify this in an experimental design.","tags":["linguistic relativity","psycholinguistics","linguistics","motion events"],"title":"More refined typology and design in linguistic relativity: The case of motion event encoding","type":"publication"},{"authors":["Bernabeu, P."],"categories":["linguistic materials","conceptual replication"],"content":"Reference Bernabeu, P. (2018). Dutch modality exclusivity norms for 336 properties and 411 concepts. PsyArXiv. https://doi.org/10.31234/osf.io/s2c5h\n\r","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"abeb0a72f4f9cafd7395c532b3871bf9","permalink":"https://pablobernabeu.github.io/publication/dutch-modality-exclusivity-norms-for-336-properties-and-411-concepts/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/dutch-modality-exclusivity-norms-for-336-properties-and-411-concepts/","section":"publication","summary":"Part of the toolkit of language researchers is formed of stimuli that have been rated on various dimensions. The current study presents modality exclusivity norms for 336 properties and 411 concepts in Dutch. Forty-two respondents rated the auditory, haptic, and visual strength of these words. Mean scores were then computed, yielding acceptable reliability values. Measures of modality exclusivity and perceptual strength were also computed. Furthermore, the data includes psycholinguistic variables from other corpora, covering length (e.g., number of phonemes), frequency (e.g., contextual diversity), and distinctiveness (e.g., number of orthographic neighbours), along with concreteness and age of acquisition. To test these norms, Lynott and Connell's (2009, 2013) analyses were replicated. First, unimodal, bimodal, and tri-modal words were found. Vision was the most prevalent modality. Vision and touch were relatively related, leaving a more independent auditory modality. Properties were more strongly perceptual than concepts. Last, sound symbolism was investigated using regression, which revealed that auditory strength predicted lexical properties of the words better than the other modalities did, or else with a different direction. All the data and analysis code, including a web application, are available from https://osf.io/brkjw.","tags":["modality exclusivity norms","conceptual modality switch","crosslinguistic","Dutch","dimensionality reduction","language comprehension","linguistics","linguistic norms","open data","principal component analysis","psycholinguistics","replication","statistics","stimuli"],"title":"Dutch modality exclusivity norms for 336 properties and 411 concepts","type":"publication"},{"authors":null,"categories":["statistics","R"],"content":"\r\rPrincipal Component Analysis (PCA) is a technique used to find the core components that underlie different variables. It comes in very useful whenever doubts arise about the true origin of three or more variables. There are two main methods for performing a PCA: naive or less naive. In the naive method, you first check some conditions in your data which will determine the essentials of the analysis. In the less-naive method, you set those yourself based on whatever prior information or purposes you had. The latter method is appropriate when you already have enough information about the intercorrelations, or when you are required to select a specific number of components. I will tackle the naive method, mainly by following the guidelines in Field, Miles, and Field (2012), with updated code where necessary. A manual by Charles M. Friel (Sam Houston State University) was also useful.\nThe ‘naive’ approach is characterized by a first stage that checks whether the PCA should actually be performed with your current variables, or if some should be removed. The variables that are accepted are taken to a second stage which identifies the number of principal components that seem to underlie your set of variables.\n\nSTAGE 1. Determine whether PCA is appropriate at all, considering the variables\r\rVariables should be inter-correlated enough but not too much. Field et al. (2012) provide some thresholds, suggesting that no variable should have many correlations below .30, or any correlation at all above .90. Thus, in the example here, variable Q06 should probably be excluded from the PCA.\n\rBartlett’s test, on the nature of the intercorrelations, should be significant. Significance suggests that the variables are not an ‘identity matrix’ in which correlations are a sampling error.\n\rKMO (Kaiser-Meyer-Olkin), a measure of sampling adequacy based on common variance (so similar purpose as Bartlett’s). As Field et al. review, ‘values between .5 and .7 are mediocre, values between .7 and .8 are good, values between .8 and .9 are great and values above .9 are superb’ (p. 761). There’s a general score as well as one per variable. The general one will often be good, whereas the individual scores may more likely fail. Any variable with a score below .5 should probably be removed, and the test should be run again.\n\rDeterminant: A formula about multicollinearity. The result should preferably fall below .00001.\rNote that some of these tests are run on the dataframe and others on a correlation matrix of the data, as distinguished below.\n\r\r\r# Necessary libraries\rlibrary(ltm)\rlibrary(lattice)\rlibrary(psych)\rlibrary(car)\rlibrary(pastecs)\rlibrary(scales)\rlibrary(ggplot2)\rlibrary(arules)\rlibrary(plyr)\rlibrary(Rmisc)\rlibrary(GPArotation)\rlibrary(gdata)\rlibrary(MASS)\rlibrary(qpcR)\rlibrary(dplyr)\rlibrary(gtools)\rlibrary(Hmisc)\r# Select variables of interest for the PCA\rdataset = mydata[, c(\u0026#39;select_var1\u0026#39;,\u0026#39;select_var1\u0026#39;,\u0026#39;select_var2\u0026#39;,\u0026#39;select_var3\u0026#39;,\u0026#39;select_var4\u0026#39;,\u0026#39;select_var5\u0026#39;,\u0026#39;select_var6\u0026#39;,\u0026#39;select_var7\u0026#39;)]\r# Create matrix: some tests will require it\rdata_matrix = cor(dataset, use = \u0026#39;complete.obs\u0026#39;)\r# See intercorrelations\rround(data_matrix, 2)\r# Bartlett\u0026#39;s\rcortest.bartlett(dataset)\r# KMO (Kaiser-Meyer-Olkin)\rKMO(data_matrix)\r# Determinant\rdet(data_matrix)\r\r\n\rSTAGE 2. Identify number of components (aka factors)\rIn this stage, principal components (formally called ‘factors’ at this stage) are identified among the set of variables.\n\rThe identification is done through a basic, ‘unrotated’ PCA. The number of components set a priori must equal the number of variables that are being tested.\r\r# Start off with unrotated PCA\rpc1 = psych::principal(dataset, nfactors = length(dataset), rotate=\u0026quot;none\u0026quot;)\rpc1\r\rBelow is an example result:\n## Principal Components Analysis\r## Call: psych::principal(r = eng_prop, nfactors = 3, rotate = \u0026quot;none\u0026quot;)\r## Standardized loadings (pattern matrix) based upon correlation matrix\r## PC1 PC2 PC3 h2 u2 com\r## Aud_eng -0.89 0.13 0.44 1 -2.2e-16 1.5\r## Hap_eng 0.64 0.75 0.15 1 1.1e-16 2.0\r## Vis_eng 0.81 -0.46 0.36 1 -4.4e-16 2.0\r## ## PC1 PC2 PC3\r## SS loadings 1.87 0.79 0.34\r## Proportion Var 0.62 0.26 0.11\r## Cumulative Var 0.62 0.89 1.00\r## Proportion Explained 0.62 0.26 0.11\r## Cumulative Proportion 0.62 0.89 1.00\r## ## Mean item complexity = 1.9\r## Test of the hypothesis that 3 components are sufficient.\r## ## The root mean square of the residuals (RMSR) is 0 ## with the empirical chi square 0 with prob \u0026lt; NA ## ## Fit based upon off diagonal values = 1\r\rAmong the columns, there are first the correlations between variables and components, followed by a column (h2) with the ‘communalities’. If less factors than variables had been selected, communality values would be below 1. Then there is the uniqueness column (u2): uniqueness is equal to 1 minus the communality. Next is ‘com’, which reflects the complexity with which a variable relates to the principal components. Those components are precisely found below. The first row contains the sums of squared loadings, or eigenvalues, namely, the total variance explained by each linear component. This value corresponds to the number of units explained out of all possible factors (which were three in the above example). The rows below all cut from the same cloth. Proportion var = variance explained over a total of 1. This is the result of dividing the eigenvalue by the number of components. Multiply by 100 and you get the percentage of total variance explained, which becomes useful. In the example, 99% of the variance has been explained. Aside from the meddling maths, we should actually expect 100% there because the number of factors equaled the number of variables. Cumulative var: variance added consecutively up to the last component. Proportion explained: variance explained over what has actually been explained (only when variables = factors is this the same as Proportion var). Cumulative proportion: the actually explained variance added consecutively up to the last component (Field et al., 2012).\nAccording to Field et al. (2012), two criteria will determine the number of components to select for the next stage:\n\rKaiser’s criterion: components with SS loadings \u0026gt; 1. In our example, only PC1.\r\rA more lenient alternative is Joliffe’s criterion, SS loadings \u0026gt; .7.\n\rScree plot: the number of points after point of inflexion. For this plot, call:\r\rplot(pc1$values, type = \u0026#39;b\u0026#39;)\rImagine a straight line from the first point on the right. Once this line bends considerably, count the points after the bend and up to the last point on the left. The number of points is the number of components to select. The example here is probably the most complicated (two components were finally chosen), but normally it’s not difficult.\nBased on both criteria, go ahead and select the definitive number of components.\n\n\rSTAGE 3. Run definitive PCA\rRun a very similar command as you did before, but now with a more advanced method. The first PCA, a heuristic one, worked essentially on the inter-correlations. The definitive PCA, in contrast, will implement a prior shuffling known as ‘rotation’, to ensure that the result is robust enough (just like cards are shuffled). Explained variance is captured better this way. The go-to rotation method is the orthogonal, or ‘varimax’ (though others may be considered too).\n# Now with varimax rotation, Kaiser-normalized by default:\rpc2 = psych::principal(dataset, nfactors=2, rotate = \u0026quot;varimax\u0026quot;, scores = TRUE)\rpc2\rpc2$loadings\r# Healthcheck\rpc2$residual\rpc2$fit\rpc2$communality\r\rAccording to Field et al. (2012), we would want:\n\rLess than half of residuals with absolute values \u0026gt; 0.05\rModel fit \u0026gt; .9\rAll communalities \u0026gt; .7\r\rIf any of this fails, consider changing the number of factors. Next, the rotated components that have been ‘extracted’ from the core of the set of variables can be added to the dataset. This would enable the use of these components as new variables that might prove powerful and useful (as in this research).\ndataset = cbind(dataset, pc2$scores)\rsummary(dataset$RC1, dataset$RC2)\r\n\rSTAGE 4. Determine ascription of each variable to components\rCheck the main summary by just calling pc2, and see how each variable correlates with the rotated components. This is essential because it reveals how variables load on each component, or in other words, to which component a variable belongs. For instance, the table shown here belongs to a study about the meaning of words (Bernabeu, 2018). These results suggest that the visual and haptic modalities of words are quite related, whereas the auditory modality is relatively unique. When the analysis works out well, a cut-off point of r = .8 may be applied for considering a variable as part of a component.\n\n\rSTAGE 5. Enjoy the plot\rThe plot is perhaps the coolest part about PCA. It really makes an awesome illustration of the power of data analysis.\nggplot(eng_props,\raes(RC1, RC2, label = as.character(main_eng))) + stat_density2d (color = \u0026quot;gray87\u0026quot;) +\rgeom_text(size = ifelse(eng_props$word_eng %in% w_set, 12, 7),\rfontface = ifelse(eng_props$word_eng %in% w_set, \u0026#39;bold\u0026#39;, \u0026#39;plain\u0026#39;)) +\rgeom_point(data=eng_props[eng_props$word_eng %in% w_set,], pch=21, fill=NA, size=14, stroke=2, alpha=.6) +\rlabs(subtitle=\u0026#39;(Data from Lynott \u0026amp; Connell, 2009)\u0026#39;, x = \u0026quot;Varimax-rotated Principal Component 1\u0026quot;, y = \u0026quot;Varimax-rotated Principal Component 2\u0026quot;) + theme_bw() + theme( plot.background = element_blank(), panel.grid.major = element_blank(),\rpanel.grid.minor = element_blank(), panel.border = element_blank(),\raxis.line = element_line(color = \u0026#39;black\u0026#39;),\raxis.title.x = element_text(colour = \u0026#39;black\u0026#39;, size = 23, margin=margin(15,15,15,15)),\raxis.title.y = element_text(colour = \u0026#39;black\u0026#39;, size = 23, margin=margin(15,15,15,15)),\raxis.text.x = element_text(size=16), axis.text.y = element_text(size=16),\rplot.title = element_text(hjust = 0.5, size = 32, face = \u0026quot;bold\u0026quot;, margin=margin(15,15,15,15)),\rplot.subtitle = element_text(hjust = 0.5, size = 20, margin=margin(2,15,15,15)) ) +\rgeom_label_repel(data = eng_props[eng_props$word_eng %in% w_set,], aes(label = word_eng), size = 8, alpha = 0.77, color = \u0026#39;black\u0026#39;, box.padding = 1.5 )\r\rBelow is an example combining PCA plots with code similar to the above. These plots illustrate something further with regard to the relationships among modalities. In property words, the different modalities spread out more clearly than they do in concept words. This makes sense because in language, properties define concepts (Bernabeu, 2018).\nAn example of these analyses is available in available in this RStudio environment, in the norms.R script.\n\n\rReferences\rBernabeu, P. (2018). Dutch modality exclusivity norms for 336 properties and 411 concepts [Unpublished manuscript]. School of Humanities, Tilburg University, the Netherlands. https://psyarxiv.com/s2c5h.\nField, A. P., Miles, J., \u0026amp; Field, Z. (2012). Discovering Statistics Using R. London, UK: Sage.\n\r\n\r","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"52150ffe1f840d9f490798734a157268","permalink":"https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/2018/naive-principal-component-analysis-in-r/","section":"post","summary":"Principal Component Analysis (PCA) is a technique used to find the core components that underlie different variables. It comes in very useful whenever doubts arise about the true origin of three or more variables. There are two main methods for performing a PCA: naive or less naive. In the naive method, you first check some conditions in your data which will determine the essentials of the analysis. In the less-naive method, you set those yourself based on whatever prior information or purposes you had. The 'naive' approach is characterized by a first stage that checks whether the PCA should actually be performed with your current variables, or if some should be removed. The variables that are accepted are taken to a second stage which identifies the number of principal components that seem to underlie your set of variables.","tags":["s","principal component analysis","statistics","dimensionality reduction","R"],"title":"Naive principal component analysis in R","type":"post"},{"authors":[],"categories":["psycholinguistics"],"content":"\r\rThroughout the 1990s, two opposing theories were used to explain how people understand texts, later bridged by the Landscape Model of reading (van den Broek, Young, Tzeng, \u0026amp; Linderholm, 1999). A review is offered below, including a schematic representation of the Landscape Model.\nMemory-based view\rThe memory-based view presented reading as an autonomous, unconscious, effortless process. Readers were purported to achieve an understanding of a text as a whole by combining the concepts, and implications readily afforded, in the text with their own background knowledge (Myers \u0026amp; O’Brien, 1998; O’Brien \u0026amp; Myers, 1999). This memory-based view did not include a conscious (re)activation of meaning. Arguably, this absence raises the question of how readers could recover the meaning if ever distracted while reading.\n\rConstructionist view\rThe constructionist view contended that readers make strategic, time-consuming efforts to access prior text and background knowledge (Graesser, Singer, \u0026amp; Trabasso, 1994; Singer, Graesser, \u0026amp; Trabasso, 1994). A possible challenge for this view is in leisure reading. If reading is effortful, how could it be explained when people become absorbed in leisure reading for hours, and enjoy it? How could this activity become as automatic and fast as it often does?\n\rLandscape Model\rThe memory-based and the constructionist views were bridged in a proposal called the Landscape Model (van den Broek et al., 1999). With a step-based model, van den Broek et al. argued that strategic (re)activations are available to the reader, but need not always be used (see also Converse, 2018). Figure 1 illustrates the dynamic, often cyclical processes.\nFigure 1. Mindmap of van den Broek et al.’s Landscape Model of reading comprehension. Retrieved from https://doi.org/10.6084/m9.figshare.1591215.\n\rThe Landscape Model was applied to the context of discourse analysis by Yeari and van den Broek (2011). The authors noted that discourse analysts may find it useful to adopt a top-down, inductive approach to their task. That is, suppressing the natural, incremental route of meaning making in reading, discourse analysts may want to read through the text to gain a general idea first, before tackling a detailed analysis (see also Bell, 2011).\n\rReferences\rBell, A. (2011). Re-constructing Babel: Discourse analysis, hermeneutics and the Interpretive Arc. Discourse Studies, 13(5), 519–568. https://doi.org/10.1177/1461445611412699.\nConverse, N. E. (2018). The Use of Explicit Comprehension Strategies During Oral Instruction of Informational Text Structures and the Effect on First-graders’ Listening Comprehension (Doctoral dissertation). Retrieved from https://digitalcommons.usu.edu/etd/7305.\nGraesser, A., Singer, M., \u0026amp; Trabasso, T. (1994). Constructing inferences during narrative comprehension. Psychological Review, 101(3), 371–395. https://doi.org/10.1037/0033-295X.101.3.371.\nMyers, J. L., \u0026amp; O’Brien, E. J. (1998). Accessing the discourse representation during reading. Discourse Processes, 26(2-3), 131–157. https://doi.org/10.1080/01638539809545042.\nO’Brien, E. J., \u0026amp; Myers, J. L. (1999). Text comprehension: A view from the bottom up. In S. R. Goldman, A. C. Graesser \u0026amp; P. van den Broek P (Eds.), Narrative Comprehension, Causality, and Coherence: Essays in Honor of Tom Trabasso (pp. 35-53). Mahwah, NJ: Lawrence Erlbaum Associates.\nSinger, M., Graesser, A. C., \u0026amp; Trabasso, T. (1994). Minimal or global inference during reading. Journal of Memory and Language, 33(4), 421–441. https://doi.org/10.1006/jmla.1994.1020.\nvan den Broek, P., Young, M., Tzeng, Y., \u0026amp; Linderholm, T. (1999). The landscape model of reading. In H. van Oostendorp \u0026amp; S. R. Goldman (Eds.), The construction of mental representations during reading (pp. 71-98). Mahwah, NJ: Erlbaum.\nYeari, M., \u0026amp; van den Broek, P. (2011). A cognitive account of discourse understanding and discourse interpretation: The Landscape Model of reading. Discourse Studies, 13(5), 635-643. https://doi.org/10.1177/1461445611412748.\n\r\r","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"356959593f8aaa96e9eb875a8c7783dc","permalink":"https://pablobernabeu.github.io/2018/review-of-the-landscape-model-of-reading-composition-dynamics-and-application/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/2018/review-of-the-landscape-model-of-reading-composition-dynamics-and-application/","section":"post","summary":"Throughout the 1990s, two opposing theories were used to explain how people understand texts, later bridged by the Landscape Model of reading (van den Broek, Young, Tzeng, \u0026amp; Linderholm, 1999). A review is offered below, including a schematic representation of the Landscape Model.\nMemory-based view\rThe memory-based view presented reading as an autonomous, unconscious, effortless process. Readers were purported to achieve an understanding of a text as a whole by combining the concepts, and implications readily afforded, in the text with their own background knowledge (Myers \u0026amp; O’Brien, 1998; O’Brien \u0026amp; Myers, 1999).","tags":["s","reading","psycholinguistics","cognition"],"title":"Review of the Landscape Model of reading: Composition, dynamics and application","type":"post"},{"authors":null,"categories":["linguistic materials","research methods","web-application","R"],"content":"\u0026nbsp; Complete web application Flexdashboard-Shiny \r\u0026nbsp; Reduced dashboard Flexdashboard  \u0026nbsp; This web application presents linguistic data over several tabs. The code combines the great front-end of Flexdashboard—based on R Markdown and yielding an unmatched user interface—, with the great back-end of Shiny—allowing users to download sections of data they select, in various formats.\n  A nice find was the \u0026lsquo;reactable\u0026rsquo; package, which implements Javascript under the hood to allow the use of colours, bar charts, etc.\n Auditory = colDef(header = with_tooltip('Auditory Rating',\r'Mean rating of each word on the auditory modality across participants.'),\rcell = function(value) {\rwidth \u0026lt;- paste0(value / max(table_data$Auditory) * 100, \u0026quot;%\u0026quot;)\rvalue = sprintf(\u0026quot;%.2f\u0026quot;, round(value,2)) # Round to two digits, keeping trailing zeros\rbar_chart(value, width = width, fill = '#ff3030')\r},\ralign = 'left'),\r   One of the hardest nuts to crack was allowing the full functionality of tables—i.e, scaling to screen, frozen header, and vertical and horizontal scrolling—whilst having tweaked the vertical/horizontal orientation of the dashboard sections. Initial clashes were sorted by adjusting the section's CSS styles\nTable {#table style=\u0026quot;background-color:#FCFCFC;\u0026quot;}\r=======================================================================\rInputs {.sidebar style='position:fixed; padding-top: 65px; padding-bottom:30px;'}\r-----------------------------------------------------------------------\r and by also adjusting the reactable settings.\n renderReactable({\rreactable(selected_words(),\rdefaultSorted = list(cat = 'desc', word = 'asc'),\rdefaultColDef = colDef(footerStyle = list(fontWeight = \u0026quot;bold\u0026quot;)),\rheight = 840, striped = TRUE, pagination = FALSE, highlight = TRUE,\r   A nice feature, especially suited to Flexdashboard, was the use of different formats across tabs. Whereas the Info tab presents long text using HTML and CSS styling, along with rmarkdown code output, the other tabs rely more strongly on Javascript features, enabled by R packages such as ‘shiny’ and sweetalert (e.g., allowing modal dialogs—pop-ups), reactable and plotly (e.g., allowing information opened by hovering—tooltips).\n```{r}\r# reactive for the word bar\rhighlighted_properties = reactive(input$highlighted_properties)\rrenderPlotly({\rggplotly(\rggplot( selected_props(), aes(RC1, RC2, label = as.character(word), color = main, # Html tags below used for format. Decimals rounded to two.\rtext = paste0(' ', '\u0026lt;span style=\u0026quot;padding-top:3px; padding-bottom:3px; font-size:2.2em; color:#EEEEEE\u0026quot;\u0026gt;', capitalize(word), '\u0026lt;/span\u0026gt; ', '\u0026lt;br\u0026gt;',\r'\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Dominant modality: \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', main, ' ',\r' ', '\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Modality exclusivity: \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', sprintf(\u0026quot;%.2f\u0026quot;, round(Exclusivity, 2)), '% ',\r'\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Perceptual strength: \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', sprintf(\u0026quot;%.2f\u0026quot;, round(Perceptualstrength, 2)),\r'\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Auditory rating: \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', sprintf(\u0026quot;%.2f\u0026quot;, round(Auditory, 2)), ' ',\r'\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Haptic rating: \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', sprintf(\u0026quot;%.2f\u0026quot;, round(Haptic, 2)), ' ',\r'\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Visual rating: \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', sprintf(\u0026quot;%.2f\u0026quot;, round(Visual, 2)), ' ',\r'\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Concreteness (Brysbaert et al., 2014): \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', sprintf(\u0026quot;%.2f\u0026quot;, round(concrete_Brysbaertetal2014, 2)), ' ',\r'\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Number of letters: \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', letters, ' ',\r'\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Number of phonemes (DutchPOND): \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', round(phonemes_DUTCHPOND, 2), ' ',\r'\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Contextual diversity (lg10CD SUBTLEX-NL): \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;',\rsprintf(\u0026quot;%.2f\u0026quot;, round(freq_lg10CD_SUBTLEXNL, 2)), ' ',\r'\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Word frequency (lg10WF SUBTLEX-NL): \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;',\rsprintf(\u0026quot;%.2f\u0026quot;, round(freq_lg10WF_SUBTLEXNL, 2)), ' ',\r'\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Lemma frequency (CELEX): \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', sprintf(\u0026quot;%.2f\u0026quot;, round(freq_CELEX_lem, 2)), ' ',\r'\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Phonological neighbourhood size (DutchPOND): \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', round(phon_neighbours_DUTCHPOND, 2), ' ',\r'\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Orthographic neighbourhood size (DutchPOND): \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;',\rround(orth_neighbours_DUTCHPOND, 2), ' ',\r'\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Age of acquisition (Brysbaert et al., 2014): \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;',\rsprintf(\u0026quot;%.2f\u0026quot;, round(AoA_Brysbaertetal2014, 2)), ' ', '\u0026lt;br\u0026gt; '\r) ) ) +\rgeom_text(size = ifelse(selected_props()$word %in% highlighted_properties(), 7,\rifelse(is.null(highlighted_properties()), 3, 2.8)),\rfontface = ifelse(selected_props()$word %in% highlighted_properties(), 'bold', 'plain')) +\rgeom_point(alpha = 0) + # This geom_point helps to colour the tooltip according to the dominant modality\rscale_colour_manual(values = colours, drop = FALSE) + theme_bw() + ggtitle('Property words') +\rlabs(x = 'Varimax-rotated Principal Component 1', y = 'Varimax-rotated Principal Component 2') +\rguides(color = guide_legend(title = 'Main\u0026lt;br\u0026gt;modality')) +\rtheme( plot.background = element_blank(), panel.grid.major = element_blank(),\rpanel.grid.minor = element_blank(), panel.border = element_blank(),\raxis.line = element_line(color = 'black'), plot.title = element_text(size = 14, hjust = .5),\raxis.title.x = element_text(colour = 'black', size = 12, margin = margin(15,15,0,15)),\raxis.title.y = element_text(colour = 'black', size = 12, margin = margin(0,15,15,5)),\raxis.text.x = element_text(size = 8), axis.text.y = element_text(size = 8),\rlegend.background = element_rect(size = 2), legend.position = 'none',\rlegend.title = element_blank(),\rlegend.text = element_text(colour = colours, size = 13) ),\rtooltip = 'text'\r)\r})\r# For download, save plot without the interactive 'plotly' part\rproperties_png = reactive({ ggplot(selected_props(), aes(RC1, RC2, color = main, label = as.character(word))) +\rgeom_text(show.legend = FALSE, size = ifelse(selected_props()$word %in% highlighted_properties(), 7,\rifelse(is.null(highlighted_properties()), 3, 2.8)),\rfontface = ifelse(selected_props()$word %in% highlighted_properties(), 'bold', 'plain')) +\rgeom_point(alpha = 0) + scale_colour_manual(values = colours, drop = FALSE) + theme_bw() +\rguides(color = guide_legend(title = 'Main\u0026lt;br\u0026gt;modality', override.aes = list(size = 7, alpha = 1))) +\rggtitle( paste0('Properties', ' (showing ', nrow(selected_props()), ' out of ', nrow(props), ')') ) + labs(x = 'Varimax-rotated Principal Component 1', y = 'Varimax-rotated Principal Component 2') +\rtheme( plot.background = element_blank(), panel.grid.major = element_blank(),\rpanel.grid.minor = element_blank(), panel.border = element_blank(),\raxis.line = element_line(color = 'black'), plot.title = element_text(size = 17, hjust = .5, margin = margin(3,3,7,3)),\raxis.title.x = element_text(colour = 'black', size = 12, margin = margin(10,10,2,10)),\raxis.title.y = element_text(colour = 'black', size = 12, margin = margin(10,10,10,5)),\raxis.text.x = element_text(size = 8), axis.text.y = element_text(size = 8),\rlegend.background = element_rect(size = 2), legend.position = 'right',\rlegend.title = element_blank(), legend.text = element_text(size = 15))\r})\r```\r The only instance in which I drew on javascript code outside R packages was to enable tooltips beyond the packages’ limits—for instance, in the side bar. This javascript feature is created at the top of the script, in the head area.\n\u0026lt;!-- Javascript function to enable a hovering tooltip --\u0026gt;\r\u0026lt;script\u0026gt;\r$(document).ready(function(){\r$('[data-toggle=\u0026quot;tooltip1\u0026quot;]').tooltip();\r});\r\u0026lt;/script\u0026gt;\r   In the side bar, I added a reactive mean for each variable, complementing the range selector.\nreactive(cat(paste0('Mean = ', sprintf(\u0026quot;%.2f\u0026quot;, round(mean(selected_words()$Exclusivity),2)))))\r   Static version published on RPubs A reduced, static version was also created to increase the availability of the content. Removing some reactivity features allows the dashboard to be published as a standard website (i.e., on a personal website, on RPubs, etc.), without the need for a back-end Shiny server. Note that this type of website is dubbed \u0026lsquo;static\u0026rsquo;, but it can retain multiple interactive features thanks to Javascript-based tools under the hood, allowed by R packages such as leaflet for maps, DT for tables, plotly for plots, etc.\nTo create the Flexdashboard-only version departing from the Flexdashboard-Shiny version, I deleted runtime: shiny from the YAML header, and disabled Shiny reactive inputs and objects, as below.\n```{r}\r# Number of words selected on sidebar\r# reactive(cat(paste0('Words selected below: ', nrow(selected_props()))))\r```\r Reference Bernabeu, P. (2018). Dutch modality exclusivity norms for 336 properties and 411 concepts [Data dashboard]. Retrieved from https://pablobernabeu.shinyapps.io/Dutch-Modality-Exclusivity-Norms/.\n\r","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"b6ae96b988a52c16af14e00a29c1b7a6","permalink":"https://pablobernabeu.github.io/applications-and-dashboards/bernabeu-2018-modalitynorms/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/applications-and-dashboards/bernabeu-2018-modalitynorms/","section":"applications-and-dashboards","summary":"This app presents linguistic data over several tabs. The code combines the great front-end of Flexdashboard—based on R Markdown and yielding an unmatched user interface—, with the great back-end of Shiny—allowing users to download sections of data they select, in various formats. The hardest nuts to crack included modifying the rows/columns orientation without affecting the functionality of tables. A cool, recent finding was the reactable package. A nice feature, allowed by Flexdashboard, was the use of quite different formats in different tabs.","tags":["web application","data dashboard","Flexdashboard","R","R Shiny","statistics","regression","principal component analysis","modality exclusivity norms","Dutch","linguistics","HTML","CSS"],"title":"Web application: Dutch modality exclusivity norms","type":"applications-and-dashboards"},{"authors":["Bernabeu, P., Willems, R. M., \u0026 Louwerse, M. M."],"categories":["conceptual processing","conceptual replication"],"content":"Bonus: a conference poster with further analyses  .embed-responsive{position:relative;height:100%;}.embed-responsive iframe{position:absolute;height:100%;}window.jQuery || document.write('\\x3C/script')  var mfrRender = new mfr.Render(\"mfrIframe\", \"https://mfr.osf.io/render?url=https://osf.io/dj52n/?direct%26mode=render%26action=download%26mode=render\");\rReference Bernabeu, P., Willems, R. M., \u0026amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, \u0026amp; E. J. Davelaar (Eds.), Proceedings of the 39th Annual Conference of the Cognitive Science Society (pp. 1629-1634). Austin, TX: Cognitive Science Society. https://doi.org/10.31234/osf.io/a5pcz\n\r","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"54f8c09b786d728c5c440476328c3b7b","permalink":"https://pablobernabeu.github.io/publication/bernabeu-etal-2017/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bernabeu-etal-2017/","section":"publication","summary":"We tested whether conceptual processing is modality-specific by tracking the time course of the Conceptual Modality Switch effect. Forty-six participants verified the relation between property words and concept words. The conceptual modality of consecutive trials was manipulated in order to produce an Auditory-to-visual switch condition, a Haptic-to-visual switch condition, and a Visual-to-visual, no-switch condition. Event-Related Potentials (ERPs) were time-locked to the onset of the first word (property) in the target trials so as to measure the effect online and to avoid a within-trial confound. A switch effect was found, characterized by more negative ERP amplitudes for modality switches than no-switches. It proved significant in four typical time windows from 160 to 750 milliseconds post word onset, with greater strength in the Slow group, in posterior brain regions, and in the N400 window. The earliest switch effect was located in the language brain region, whereas later it was more prominent in the visual region. In the N400 and Late Positive windows, the Quick group presented the effect especially in the language region, whereas the Slow had it rather in the visual region. These results suggest that contextual factors such as time resources modulate the engagement of linguistic and embodied systems in conceptual processing.","tags":["conceptual modality switch","conceptual processing","reading","event-related potentials","cognition","psycholinguistics","statistics","linear mixed-effects models","replication","CogSci"],"title":"Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs","type":"publication"},{"authors":["Bernabeu, P."],"categories":["conceptual processing","psycholinguistics","event-related potentials"],"content":"\nReference Bernabeu, P. (2017). Modality switches occur early and extend late in conceptual processing: Evidence from ERPs. PsyArXiv. https://doi.org/10.31234/osf.io/5gjvk\n\r","date":1490832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490832000,"objectID":"fca4ce877a38106e5fd0c2e5c4383540","permalink":"https://pablobernabeu.github.io/publication/bernabeu-2017-mphil-thesis/","publishdate":"2017-03-31T00:00:00Z","relpermalink":"/publication/bernabeu-2017-mphil-thesis/","section":"publication","summary":"The engagement of sensory brain regions during word recognition is widely documented, yet its precise relevance is less clear. It would constitute perceptual simulation only if it has a functional role in conceptual processing. We investigated this in an Event-Related Potential (ERP) experiment implementing the conceptual modality switch paradigm. In each trial, participants verified the relation between a property word and a concept word. Orthogonally, we manipulated the conceptual modality of successive trials, and tested whether switching modalities incurred any processing costs at different stages of word recognition. Unlike previous studies, we time-locked ERPs to the first word of target trials, in order to measure the modality transitions from the beginning, and also to reduce confounds within the target trial. Further, we included different types of switch\u0026mdash;one from auditory to visual modality, and one from haptic to visual\u0026mdash;, which were compared to the non-switch\u0026mdash;visual to visual. Also, one group of participants was asked to respond quickly (*n* = 21), and another group to respond self-paced (*n* = 21), whilst a few others received no constraints (*n* = 5). We found ERP effects in four typical time windows from 160 to 750 ms post word onset. The overall effect is characterized by a negativity for modality-switching relative to not switching, and it increases over time. Further, the effect arises with both types of switch, and influences both participant groups within anterior and posterior brain regions. The emergence of this effect in the first time window particularly suggests that sensory regions may have a functional role in conceptual processing. The increased effect later on converges with previous studies in supporting the compatibility of distributional and embodied processing. On a less conclusive note, more research may be necessary to ascertain the nature of the effect at late stages.","tags":["Brain Vision","cognition","cognitive neuroscience","conceptual modality switch","conceptual processing","conceptual replication","electrodes","event-related potentials","electroencephalography","language comprehension","linear mixed-effects models","psycholinguistics","reading"],"title":"Modality switches occur early and extend late in conceptual processing: Evidence from ERPs [Master's thesis]","type":"publication"},{"authors":[],"categories":["statistics"],"content":"\r\rThe clock strikes a certain hour, below all the Greg’s teaspoons at play. Results o’clock. The usual, please.\nUsual table. summaryby (having to get the first peek in the cafeteria can only add zest). summaryBy(RT ~ list(Ptp, Group, Cond), behdata, FUN=summary). So, hardly any of the 95% Confidence Intervals contain 0. Does this really mean…?\n\r‘For example, the hypothesis of equality of population means will be rejected at the 0.05 level if and only if a 95% CI for the mean difference does not contain 0.’\n\r— Dallal (2002; http://www.jerrydallal.com/lhsp/pval.htm)\nOf course. The CI just has that and more. The window is showing a chilly 1999 morning. Let’s see the summary again. Wee standard deviations. By card, please.\nMmm, the air outside is worth gingering up…\n\rThe trials!\n\rThe assumption of independence spoils another morning.\nThis new data consisted of response times (RT) that had been collected over several trials. The single dependent variable, RT, was accompanied by other variables which could be analyzed as independent variables. These included Group, Trial Number, and a within-subjects Condition. What had to be done first off, in order to take the usual table? The trials!\nAssumption of independence of observations\rOne must account for any redundant measures below the level of participants (the experimental trials, in this case), so that the sample size (N) used for any summary statistics match the number of participants (or the largest group, n). Why? This is a central assumption in statistics: observations must be independent. We can observe the independence assumption differently, depending on whether we’re summarizing data or performing statistical tests.\n\rFor descriptive tables and plots (involving Standard Error/Deviation, Confidence Intervals, etc), the data ought to be aggregated to the level from which you want to generalize. That level is—in this case and very often—participants. Trials do not normally serve for statistical generalization (they’re good for experimental validity). This realization may come as a bummer if you have first seen the effect sizes in the un-aggregated data. The mirage (see red lines on the left table below) is caused by an inflated N (cf. red lines on the right-hand table). As an illustration, the tables below summarize data with an actual sample n = 23. However, the table on the right includes repeated measures that should have been aggregated, massively inflating n. The inflation of the sample size equals the product of all repeated measures that failed to be aggregated under participants.\r\rMeasures of variance such as the Standard Deviation divide by the sample size. Thus, the larger the sample (N), the smaller the Standard Deviation, Standard Error, Confidence Interval…—that is, the variation or noise.\nAggregating is a snap. For example, with the aggregate() function in R, you just have to include all of your variables except that or those of the repeated measures:\nbehdata_aggreg = aggregate(behdata$RT, list(behdata$Ptp, behdata$Group, behdata$Cond), data=behdata, FUN=mean)\r\rIn statistical tests, repeated measures below the participant level–e.g., trials–normally must be either factored in or aggregated. Barr and colleagues provide an easy, focused guide on this procedure. This is necessary because when the N in the analyses is augmented by unaccounted, redundant observations, the famous assumption of independence of observations is violated, and the results may be invalid, as Vasishth and Nicenboim (2016, p. 3) put it:\r\r\r‘if we were to do a t-test on the unaggregated data, we would violate the independence assumption and the result of the t-test would be invalid.’\n\rNow, usually the repetitions that concern us are the multiple trials or items in experiments, or other sub-participant measures. So what about participants–what are they never aggregated? McCarthy, Whittaker, Boyle, and Eyal (2017, p.10) note:\n\r‘It has also been proposed that researchers aggregate the responses of participants within the same group and use the groups/clusters as the unit of analysis (Stevens, 2007). However, because this would result in losing sample size at the participant level, this approach is not optimal given the already small numbers of groups typically studied in group work research.’\n\r\rDifferent procedure in linear mixed-effects models\rAggregation is no longer necessary, where linear mixed-effects models can be used. These models allow us to account for any clusters (Participants, Trials, Items…) by signing them into the error term (Brauer \u0026amp; Curtin, 2017).\n\r","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"72b40be553d5d3f800e313bb7c15010d","permalink":"https://pablobernabeu.github.io/2017/at-greg-8-am/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/2017/at-greg-8-am/","section":"post","summary":"The single dependent variable, RT, was accompanied by other variables which could be analyzed as independent variables. These included Group, Trial Number, and a within-subjects Condition. What had to be done first off, in order to take the usual table? The trials!","tags":["s","R","statistics","aggregate","trials","repeated measures","assumption","independence of observations","variability","standard error","central assumption"],"title":"At Greg, 8 am","type":"post"},{"authors":[],"categories":["psycholinguistics"],"content":"Research has extensively investigated whether conceptual processing is modality-specific—that is, whether meaning is processed to a large extent on the basis of perceptual and motor affordances (Barsalou, 2016). This possibility challenges long-established theories. It suggests a strong link between physical experience and language which is not borne out of the paradigmatic arbitrariness of words (see Lockwood, Dingemanse, \u0026amp; Hagoort, 2016). Modality-specificity also clashes with models of language that have no link to sensory and motor systems (Barsalou, 2016).\n\u0026nbsp; Conference paper  \u0026nbsp; \u0026nbsp; Master's thesis  \u0026nbsp;  Early discussion on ResearchGate Data and code Data dashboard  In the Conceptual Modality Switch (CMS) paradigm, participants perform a property verification task, deciding whether certain property words can reasonably describe concept words. Covertly, the conceptual modality of consecutive trials is manipulated in order to produce specific switches in conceptual modality. For instance, after the trial Soundless Answer, which is primarily auditory, the following trial may match in modality—Loud Welcome—or mismatch—Fine Selection (visual).\nModality switches incur processing costs, as reflected in brain signals linked to semantic violation, and in longer response times (RTs) (Scerrati, Lugli, Nicoletti, \u0026amp; Borghi, 2016). This effect suggests that perceptual features of concepts are accessed during conceptual processing. More recently, however, the CMS effect was reanalysed using a non-perceptual alternative. Louwerse and Connell (2011) found that language statistics (the co-occurrence of words in a language) were able to approximately predict visual/haptic, olfactory/gustatory, and auditory modalities, but not the subtler differences between visual and haptic and between olfactory and gustatory, which seemed to be reserved for perceptual simulations. Moreover, faster response times (RTs) were best explained by language statistics, whereas slower RTs were best explained by perceptual simulations.\nThe time course of word processing is important. Research suggests that word processing spans one second, during which different processes—semantic and post-semantic—gradually accumulate (Hauk, 2016). The later an effect, the more reasons to question it. Yet, having an early emergence does not either make an effect lexicosemantic, as the meaning encoded could have gone through working memory before activating the actual system of interest, e.g., sensorimotor (Mahon \u0026amp; Caramazza, 2008). Research also suggests that modal systems may contribute to conceptual processing early on—within 200 ms (Vukovic, Feurra, Shpektor, Myachykov, \u0026amp; Shtyrov, 2017). Thus, measuring effects online may prove valuable.\nExperiment Bernabeu, Willems and Louwerse (2017) investigated whether CMS reflects a functionally relevant process of simulation or instead arises only after basic conceptual processing has been attained. We also examined whether different processing systems, amodal and modal, may compatibly operate.\nWe measured CMS online by time-locking Event-Related brain Potentials (ERPs) to the onset of the first word in the target trials, in order to assess how strongly CMS may be influenced by post-semantic processes. Previous research would predict an increase in the CMS effect over time because earlier processing is relatively amodal (Louwerse \u0026amp; Hutchinson, 2012).\nWe tested the compatibility of amodal and modal processing by drawing on Louwerse and Connell’s (2011) findings. In this conceptual replication, we split participants into a Quick and a Slow group based on RT. Maintaining CMS as a within-subjects factor, we predicted that the larger modality switches (e.g., auditory to visual) would be picked up equally by both groups, whereas the subtler switches (e.g., haptic to visual) would be picked up only—or more clearly—by the Slow group.\nMethod The stimuli were normed (Bernabeu, Louwerse, \u0026amp; Willems, in prep.). Three CMS conditions were created—Auditory-to-visual, Haptic-to-visual, Visual-to-visual—, each with 36 target trials. The property verification task was pretested valid (N = 19).\nResults All participants but one responded correctly to over half of the trials, with an overall accuracy of 63%.\nERPs showed a CMS effect from time window 1 on, larger after 350 ms. It appeared with both switch conditions, and was characterized by a more negative amplitude for the switch conditions compared to the no-switch condition. It was generally stronger in the posterior brain regions, and in the Slow group. The results are illustrated in the figure below, which includes 95% Confidence Intervals and time windows.\nThe analysis was done with Linear Mixed Effects models. Final models presented good fits, with R2 ranging from .748 to .862. First, the CMS effect in time window 1 was confirmed significant. Such an early emergence is unprecedented in the CMS literature, and it may have been enabled by the time-locking of ERPs to the first word in target trials. In this time window, the only process not lexicosemantic is possibly working memory (Hauk, 2016), and therefore this early emergence adds support to the possibility that CMS was directly caused by perceptual simulation.\nWhereas in time window 1, the effect was circumscribed to an interaction with Brain Area, by Time Window 2, a main effect of CMS emerged. In Windows 3 and 4, the only experimental effect was CMS.\nBonferroni-corrected, planned ANOVA contrasts into CMS conditions revealed that the no-switch condition differed significantly from the switch conditions. By contrast, the switch conditions (Haptic-to-visual and Auditory-to-visual) hardly differed from each other, underscoring the CMS effect.\nAlthough the interaction of Group and CMS was only significant in Time Windows 1 and 2, Windows 2 to 4 presented a pattern fitting our predictions (Louwerse \u0026amp; Connell, 2011). While the Slow group picked up the switches across all modalities similarly, the Quick group picked up the Auditory-to-visual switch more clearly than the Haptic-to-visual switch.\nStatistical analysis .embed-responsive{position:relative;height:100%;}.embed-responsive iframe{position:absolute;height:100%;}window.jQuery || document.write('\\x3C/script')  var mfrRender = new mfr.Render(\"mfrIframe\", \"https://mfr.osf.io/render?url=https://osf.io/sx3nw/?direct%26mode=render%26action=download%26mode=render\");\rDiscussion Results broadly suggest that cognition may operate on qualitatively different systems for the same task. In conceptual processing, one of these systems appears to be modality-independent, potentially based on linguistic co-occurrences, whereas another system is modality-specific, linked to physical experience.\nA conference poster with further analyses is also available.\nReferences Barsalou, L. W. (2016). On staying grounded and avoiding quixotic dead ends. Psychonomic Bulletin \u0026amp; Review, 23.\nBernabeu, P., Louwerse, M. M., \u0026amp; Willems, R. M. (in prep.). Modality exclusivity norms for 747 properties and concepts in Dutch: a replication of English. Retrieved from https://osf.io/brkjw/\nBernabeu, P., Willems, R. M., \u0026amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, \u0026amp; E. J. Davelaar (Eds.), Proceedings of the 39th Annual Conference of the Cognitive Science Society (pp. 1629-1634). Austin, TX: Cognitive Science Society. https://doi.org/10.31234/osf.io/a5pcz\nHauk, O. (2016). Only time will tell—Why temporal information is essential for our neuroscientific understanding of semantics. Psychonomic Bulletin \u0026amp; Review, 23, 4, 1072-1079.\nLockwood, G., Hagoort, P., \u0026amp; Dingemanse, M. (2016). How iconicity helps people learn new words: neural correlates and individual differences in sound-symbolic bootstrapping. Collabra, 2, 1, 7.\nLouwerse, M., \u0026amp; Connell, L. (2011). A taste of words: linguistic context and perceptual simulation predict the modality of words. Cognitive Science, 35, 2, 381-98.\nLouwerse, M., \u0026amp; Hutchinson, S. (2012). Neurological evidence linguistic processes precede perceptual simulation in conceptual processing. Frontiers in Psychology, 3, 385.\nMahon, B. Z., \u0026amp; Caramazza, A. (2008). A critical look at the Embodied Cognition Hypothesis and a new proposal for grounding conceptual content. Journal of Physiology - Paris, 102, 59-70.\nScerrati, E., Lugli, L., Nicoletti, R., \u0026amp; Borghi, A. M. (2016). The Multilevel Modality-Switch Effect: What Happens When We See the Bees Buzzing and Hear the Diamonds Glistening. Psychonomic Bulletin \u0026amp; Review, doi:10.3758/s13423-016-1150-2.\nVukovic, V., Feurra, M., Shpektor, A., Myachykov, A., \u0026amp; Shtyrov, Y. (2017). Primary motor cortex functionally contributes to language comprehension: An online rTMS study. Neuropsychologia, 96, 222-229.\n\r","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"83a0a5285af23deee88232f66d1ade74","permalink":"https://pablobernabeu.github.io/2017/modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing-evidence-from-erps/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/2017/modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing-evidence-from-erps/","section":"post","summary":"Research has extensively investigated whether conceptual processing is modality-specific—that is, whether meaning is processed to a large extent on the basis of perceptual and motor affordances (Barsalou, 2016). This possibility challenges long-established theories. It suggests a strong link between physical experience and language which is not borne out of the paradigmatic arbitrariness of words (see Lockwood, Dingemanse, \u0026amp; Hagoort, 2016). Modality-specificity also clashes with models of language that have no link to sensory and motor systems (Barsalou, 2016).","tags":["s","psycholinguistics","conceptual processing","experiment","event-related potentials","language comprehension","open data","cognition","conceptual modality switch","modality exclusivity norms","reading","statistics","linear mixed-effects models","ResearchGate"],"title":"Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs","type":"post"},{"authors":[],"categories":["R","data dashboards","open data"],"content":"\r\rDashboards for data visualisation, such as R Shiny and Tableau, allow the interactive exploration of data by means of drop-down lists and checkboxes, with no coding required from the final users. These web applications can be useful for both the data analyst and the public at large.\rVisualisation apps run on internet browsers. This allows for three options: private viewing (useful during analysis), selective sharing (used within work groups), or internet publication. Among the available platforms, R Shiny and Tableau stand out due to being relatively accessible to new users. Apps serve a broad variety of purposes (see this gallery and this one). In science and beyond, these apps allow us to go the extra mile in sharing data. Alongside files and code shared in repositories, we can present the data in a website, in the form of plots or tables. This facilitates the public exploration of each section of the data (groups, participants, trials…) to anyone interested, and allows researchers to account for their proceeding in the analysis.\nPublishers and journals highly encourage authors to make the most of their data by facilitating its easy exploration by the readership–even though they don’t normally offer the possibility of hosting any web applications yet.\nApps can also prove valuable to those analysing the data. For instance, my app helped me to identify the extent of noise in a section of the data. Instead of running through a heavy score of code, the drop-down lists of the app let me seamlessly surf through the different sections.\nAt a certain point, I found a data section that was consistently noisier than the rest, and eventually I had to discard it from further statistical analyses. Yet, instead of removing that from the app, I maintained it with a note attached. This particular trait in the data was rather salient.\nBeyond such a salient feature in the data, a visualisation app may also help to spot subtler patterns such as third variables or individual differences.\nThere are several platforms for creating apps (e.g., Tableau, D3.js, and R Shiny). I focus on R Shiny here for three reasons: it is affordable to use, fairly accessible to new users, and well suited for science as it is based on the R language (see for instance this article).\n\rHow to Shiny\rShiny apps draw on any standard R code that you may already have. This is most commonly plots or tables, but other stuff such as images or Markdown texts are valid too. This is a nice thing to keep in mind when having to create a new app. Part of the job may already be done! The app is distributed among five different areas.\n\rData file(s): These are any data files you’re using (e.g., with csv or rds extensions).\r\r1a. server.R script\rThe server script contains the central processes: plots, tables, etc. Code that existed independently of the app app may be brought into this script by slightly adapting it. At the top, call the shiny library and any others used (e.g., ‘ggplot2’), and also read in the data. The snippet below shows the beginning of an example server.R script.\n\r# server\rlibrary(shiny)\rlibrary(ggplot2)\rEEG.ParticipantAndElectrode = readRDS(\u0026#39;EEG.ParticipantAndElectrode.rds\u0026#39;)\rEEG.ParticipantAndBrainArea = readRDS(\u0026#39;EEG.ParticipantAndBrainArea.rds\u0026#39;)\rEEG.GroupAndElectrode = readRDS(\u0026#39;EEG.GroupAndElectrode.rds\u0026#39;)\rEEG.OLDGroupAndElectrode = readRDS(\u0026#39;EEG.OLDGroupAndElectrode.rds\u0026#39;)\rserver =\rshinyServer(\rfunction(input, output) {\r# plot_GroupAndElectrode:\routput$plot_GroupAndElectrode \u0026lt;- renderPlot({\rdfelectrode \u0026lt;- aggregate(microvolts ~ electrode*time*condition, EEG.GroupAndElectrode[EEG.GroupAndElectrode$RT.based_Groups==input$var.Group,], mean)\rdf2 \u0026lt;- subset(dfelectrode, electrode == input$var.Electrodes.1)\rdf2$condition= as.factor(df2$condition)\rdf2$condition \u0026lt;- gsub(\u0026#39;visual2visual\u0026#39;, \u0026#39; Visual / Visual\u0026#39;, df2$condition)\rdf2$condition \u0026lt;- gsub(\u0026#39;haptic2visual\u0026#39;, \u0026#39; Haptic / Visual\u0026#39;, df2$condition)\rdf2$condition \u0026lt;- gsub(\u0026#39;auditory2visual\u0026#39;, \u0026#39; Auditory / Visual\u0026#39;, df2$condition)\rdf2$time \u0026lt;- as.integer(as.character(df2$time))\rcolours \u0026lt;- c(\u0026#39;firebrick1\u0026#39;, \u0026#39;dodgerblue\u0026#39;, \u0026#39;forestgreen\u0026#39;)\r# green:visual2visual, blue:haptic2visual, red:auditory2visual\rspec_title = paste0(\u0026#39;ERP waveforms for \u0026#39;, input$var.Group, \u0026#39; Group, Electrode \u0026#39;, input$var.Electrodes.1, \u0026#39; (negative values upward; time windows displayed)\u0026#39;)\rplot_GroupAndElectrode = ggplot(df2, aes(x=time, y=-microvolts, color=condition)) +\rgeom_rect(xmin=160, xmax=216, ymin=7.5, ymax=-8, color = \u0026#39;grey75\u0026#39;, fill=\u0026#39;black\u0026#39;, alpha=0, linetype=\u0026#39;longdash\u0026#39;) +\rgeom_rect(xmin=270, xmax=370, ymin=7.5, ymax=-8, color = \u0026#39;grey75\u0026#39;, fill=\u0026#39;black\u0026#39;, alpha=0, linetype=\u0026#39;longdash\u0026#39;) +\rgeom_rect(xmin=350, xmax=550, ymin=8, ymax=-7.5, color = \u0026#39;grey75\u0026#39;, fill=\u0026#39;black\u0026#39;, alpha=0, linetype=\u0026#39;longdash\u0026#39;) +\rgeom_rect(xmin=500, xmax=750, ymin=7.5, ymax=-8, color = \u0026#39;grey75\u0026#39;, fill=\u0026#39;black\u0026#39;, alpha=0, linetype=\u0026#39;longdash\u0026#39;) +\rgeom_line(size=1, alpha = 1) + scale_linetype_manual(values=colours) +\rscale_y_continuous(limits=c(-8.38, 8.3), breaks=seq(-8,8,by=1), expand = c(0,0.1)) +\rscale_x_continuous(limits=c(-208,808),breaks=seq(-200,800,by=100), expand = c(0.005,0), labels= c(\u0026#39;-200\u0026#39;,\u0026#39;-100 ms\u0026#39;,\u0026#39;0\u0026#39;,\u0026#39;100 ms\u0026#39;,\u0026#39;200\u0026#39;,\u0026#39;300 ms\u0026#39;,\u0026#39;400\u0026#39;,\u0026#39;500 ms\u0026#39;,\u0026#39;600\u0026#39;,\u0026#39;700 ms\u0026#39;,\u0026#39;800\u0026#39;)) +\rggtitle(spec_title) + theme_bw() + geom_vline(xintercept=0) +\rannotate(geom=\u0026#39;segment\u0026#39;, y=seq(-8,8,1), yend=seq(-8,8,1), x=-4, xend=8, color=\u0026#39;black\u0026#39;) +\rannotate(geom=\u0026#39;segment\u0026#39;, y=-8.2, yend=-8.38, x=seq(-200,800,100), xend=seq(-200,800,100), color=\u0026#39;black\u0026#39;) +\rgeom_segment(x = -200, y = 0, xend = 800, yend = 0, size=0.5, color=\u0026#39;black\u0026#39;) +\rtheme(legend.position = c(0.100, 0.150), legend.background = element_rect(fill=\u0026#39;#EEEEEE\u0026#39;, size=0),\raxis.title=element_blank(), legend.key.width = unit(1.2,\u0026#39;cm\u0026#39;), legend.text=element_text(size=17),\rlegend.title = element_text(size=17, face=\u0026#39;bold\u0026#39;), plot.title= element_text(size=20, hjust = 0.5, vjust=2),\raxis.text.y = element_blank(), axis.text.x = element_text(size = 14, vjust= 2.12, face=\u0026#39;bold\u0026#39;, color = \u0026#39;grey32\u0026#39;, family=\u0026#39;sans\u0026#39;),\raxis.ticks=element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), plot.margin = unit(c(0.1,0.1,0,0), \u0026#39;cm\u0026#39;)) +\rannotate(\u0026#39;segment\u0026#39;, x=160, xend=216, y=-8, yend=-8, colour = \u0026#39;grey75\u0026#39;, size = 1.5) +\rannotate(\u0026#39;segment\u0026#39;, x=270, xend=370, y=-8, yend=-8, colour = \u0026#39;grey75\u0026#39;, size = 1.5) +\rannotate(\u0026#39;segment\u0026#39;, x=350, xend=550, y=-7.5, yend=-7.5, colour = \u0026#39;grey75\u0026#39;, size = 1.5) +\rannotate(\u0026#39;segment\u0026#39;, x=500, xend=750, y=-8, yend=-8, colour = \u0026#39;grey75\u0026#39;, size = 1.5) +\rscale_fill_manual(name = \u0026#39;Context / Target trial\u0026#39;, values=colours) +\rscale_color_manual(name = \u0026#39;Context / Target trial\u0026#39;, values=colours) +\rguides(linetype=guide_legend(override.aes = list(size=1.2))) +\rguides(color=guide_legend(override.aes = list(size=2.5))) +\r# Print y axis labels within plot area:\rannotate(\u0026#39;text\u0026#39;, label = expression(bold(\u0026#39;\\u2013\u0026#39; * \u0026#39;3 \u0026#39; * \u0026#39;\\u03bc\u0026#39; * \u0026#39;V\u0026#39;)), x = -29, y = 3, size = 4.5, color = \u0026#39;grey32\u0026#39;, family=\u0026#39;sans\u0026#39;) +\rannotate(\u0026#39;text\u0026#39;, label = expression(bold(\u0026#39;+3 \u0026#39; * \u0026#39;\\u03bc\u0026#39; * \u0026#39;V\u0026#39;)), x = -29, y = -3, size = 4.5, color = \u0026#39;grey32\u0026#39;, family=\u0026#39;sans\u0026#39;) +\rannotate(\u0026#39;text\u0026#39;, label = expression(bold(\u0026#39;\\u2013\u0026#39; * \u0026#39;6 \u0026#39; * \u0026#39;\\u03bc\u0026#39; * \u0026#39;V\u0026#39;)), x = -29, y = 6, size = 4.5, color = \u0026#39;grey32\u0026#39;, family=\u0026#39;sans\u0026#39;)\rprint(plot_GroupAndElectrode)\routput$downloadPlot.1 \u0026lt;- downloadHandler(\rfilename \u0026lt;- function(file){\rpaste0(input$var.Group, \u0026#39; group, electrode \u0026#39;, input$var.Electrodes.1, \u0026#39;, \u0026#39;, Sys.Date(), \u0026#39;.png\u0026#39;)},\rcontent \u0026lt;- function(file){\rpng(file, units=\u0026#39;in\u0026#39;, width=13, height=5, res=900)\rprint(plot_GroupAndElectrode)\rdev.off()},\rcontentType = \u0026#39;image/png\u0026#39;)\r} )\r# ...\r\r— Whole script\n\r1b. ui.R script\rThe ui script defines the user interface. For instance, a factor column in the data that has multiple categories may be neatly displayed with a drop-down list on the side bar of the website. The interface may present a central plot before by a legend key below. The snippet below shows the beginning of an example ui.R script.\n\r# UI\rlibrary(shiny)\rlibrary(ggplot2)\rEEG.GroupAndElectrode = readRDS(\u0026#39;EEG.GroupAndElectrode.rds\u0026#39;)\rEEG.ParticipantAndBrainArea = readRDS(\u0026#39;EEG.ParticipantAndBrainArea.rds\u0026#39;)\rEEG.ParticipantAndElectrode = readRDS(\u0026#39;EEG.ParticipantAndElectrode.rds\u0026#39;)\rEEG.OLDGroupAndElectrode = readRDS(\u0026#39;EEG.OLDGroupAndElectrode.rds\u0026#39;)\rui =\rshinyUI(\rfluidPage(\rtags$head(tags$link(rel=\u0026#39;shortcut icon\u0026#39;, href=\u0026#39;https://image.ibb.co/fXUwzb/favic.png\u0026#39;)), # web favicon\rtags$meta(charset=\u0026#39;UTF-8\u0026#39;),\rtags$meta(name=\u0026#39;description\u0026#39;, content=\u0026#39;This R Shiny visualisation dashboard presents data from a psycholinguistic ERP experiment (Bernabeu et al., 2017).\u0026#39;),\rtags$meta(name=\u0026#39;keywords\u0026#39;, content=\u0026#39;R, Shiny, ggplot2, visualisation, data, psycholinguistics, conceptual processing, modality switch, embodied cognition\u0026#39;),\rtags$meta(name=\u0026#39;viewport\u0026#39;, content=\u0026#39;width=device-width, initial-scale=1.0\u0026#39;),\rtitlePanel(h3(strong(\u0026#39;Waveforms in detail from an ERP experiment on the Conceptual Modality Switch\u0026#39;), a(\u0026#39;(Bernabeu et al., 2017)\u0026#39;,\rhref=\u0026#39;https://figshare.com/articles/EEG_study_on_conceptual_modality-switching_Bernabeu_et_al_in_prep_/4210863\u0026#39;, target=\u0026#39;_blank\u0026#39;,\rstyle = \u0026#39;color:#3E454E; text-decoration:underline; font-weight:normal\u0026#39;), align = \u0026#39;center\u0026#39;, style = \u0026#39;color:black\u0026#39;),\rwindowTitle = \u0026#39;Visualization of ERP waveforms from experiment on Conceptual Modality Switch (Bernabeu et al., 2017)\u0026#39;),\rsidebarLayout(\rsidebarPanel(width = 2,\r# Condition 1 for reactivity between tabs and sidebars\rconditionalPanel(\rcondition = \u0026#39;input.tabvals == 1\u0026#39;,\rh5(a(strong(\u0026#39;See paper, statistics, all data.\u0026#39;), \u0026#39;Plots by group and brain area shown in paper.\u0026#39;,\rhref=\u0026#39;https://figshare.com/articles/EEG_study_on_conceptual_modality-switching_Bernabeu_et_al_in_prep_/4210863\u0026#39;,\rtarget=\u0026#39;_blank\u0026#39;), align = \u0026#39;center\u0026#39;),\rbr(),\rselectInput(\u0026#39;var.Group\u0026#39;, label = \u0026#39;Group\u0026#39;, choices = list(\u0026#39;Quick\u0026#39;,\u0026#39;Slow\u0026#39;), selected = \u0026#39;Quick\u0026#39;),\rh6(\u0026#39;Quick G.: 23 participants\u0026#39;),\rh6(\u0026#39;Slow G.: 23 participants\u0026#39;),\rbr(),\rselectInput(\u0026#39;var.Electrodes.1\u0026#39;, label = h5(strong(\u0026#39;Electrode\u0026#39;), br(), \u0026#39;(see montage below)\u0026#39;),\rchoices = list(\u0026#39;1\u0026#39;,\u0026#39;2\u0026#39;,\u0026#39;3\u0026#39;,\u0026#39;4\u0026#39;,\u0026#39;5\u0026#39;,\u0026#39;6\u0026#39;,\u0026#39;7\u0026#39;,\u0026#39;8\u0026#39;,\u0026#39;9\u0026#39;,\u0026#39;10\u0026#39;,\r\u0026#39;11\u0026#39;,\u0026#39;12\u0026#39;,\u0026#39;13\u0026#39;,\u0026#39;14\u0026#39;,\u0026#39;15\u0026#39;,\u0026#39;16\u0026#39;,\u0026#39;17\u0026#39;,\u0026#39;18\u0026#39;,\u0026#39;19\u0026#39;,\u0026#39;20\u0026#39;,\u0026#39;21\u0026#39;,\r\u0026#39;22\u0026#39;,\u0026#39;23\u0026#39;,\u0026#39;24\u0026#39;,\u0026#39;25\u0026#39;,\u0026#39;26\u0026#39;,\u0026#39;27\u0026#39;,\u0026#39;28\u0026#39;,\u0026#39;29\u0026#39;,\u0026#39;30\u0026#39;,\u0026#39;31\u0026#39;,\u0026#39;33\u0026#39;,\r\u0026#39;34\u0026#39;,\u0026#39;35\u0026#39;,\u0026#39;36\u0026#39;,\u0026#39;37\u0026#39;,\u0026#39;38\u0026#39;,\u0026#39;39\u0026#39;,\u0026#39;40\u0026#39;,\u0026#39;41\u0026#39;,\u0026#39;42\u0026#39;,\u0026#39;43\u0026#39;,\u0026#39;44\u0026#39;,\r\u0026#39;45\u0026#39;,\u0026#39;46\u0026#39;,\u0026#39;47\u0026#39;,\u0026#39;48\u0026#39;,\u0026#39;49\u0026#39;,\u0026#39;50\u0026#39;,\u0026#39;51\u0026#39;,\u0026#39;52\u0026#39;,\u0026#39;53\u0026#39;,\u0026#39;54\u0026#39;,\u0026#39;55\u0026#39;,\r\u0026#39;56\u0026#39;,\u0026#39;57\u0026#39;,\u0026#39;58\u0026#39;,\u0026#39;59\u0026#39;,\u0026#39;60\u0026#39;), selected = \u0026#39;30\u0026#39; ),\rbr(), br(),\rh6(\u0026#39;Source code:\u0026#39;),\rh6(strong(\u0026#39;- \u0026#39;), a(\u0026#39;server.R\u0026#39;, href=\u0026#39;https://osf.io/uj8z4/\u0026#39;, target=\u0026#39;_blank\u0026#39;, style = \u0026#39;text-decoration: underline;\u0026#39;)),\rh6(strong(\u0026#39;- \u0026#39;), a(\u0026#39;ui.R\u0026#39;, href=\u0026#39;https://osf.io/8bwcx/\u0026#39;, target=\u0026#39;_blank\u0026#39;, style = \u0026#39;text-decoration: underline;\u0026#39;)),\rbr(),\rh6(a(\u0026#39;CC-By 4.0 License\u0026#39;, href=\u0026#39;https://osf.io/97unm/\u0026#39;, target=\u0026#39;_blank\u0026#39;), align = \u0026#39;center\u0026#39;, style = \u0026#39;text-decoration: underline;\u0026#39;),\rbr(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(),\rbr(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(),\rh5(a(strong(\u0026#39;See paper, statistics, all data.\u0026#39;),\rhref=\u0026#39;https://figshare.com/articles/EEG_study_on_conceptual_modality-switching_Bernabeu_et_al_in_prep_/4210863\u0026#39;,\rtarget=\u0026#39;_blank\u0026#39;), align = \u0026#39;center\u0026#39;),\rbr(), br(), br(), br(), br(), br(), br(), br()\r),\r# ...\r\r— Whole script\n\r2. Deployment and logs\rThis script contains the commands for deploying the app on- or off-line, and for checking the session logs in case of any errors.\n\r3. Automatically created folder\rWhen the app is first deployed on the internet, a subfolder is automatically created with the name ‘rsconnect’. This folder contains a text file which can be used to modify the URL and the title of the webpage.\nSteps to create a Shiny app from scratch:\n1. Tutorials (link). Being open-source software, excellent directions are available through a Google search.\nThe core ideas are:\nAs mentioned above, create a ui.R script for the code containing the user interface, and create a server.R script for the code containing the main content (your plots / tables, etc).\nAt the top of both ui.R and server.R scripts, enter the command library(shiny) and also load any other libraries you’re using (e.g., ggplot2).\nTest your app by deploying it locally, before launching online. For this purpose, first save the ui and server parts independently, as in:\n\rui =\rshinyUI(\rfluidPage(\r# ...\r\rThen deploy locally by running:\nshinyApp(ui, server)\rManaging to run the app locally is a great first step before launching online (which may sometimes prove a bit trickier).\n2. User token (link). Sign up and read in your private key—just to be done once in a computer.\n3. Go for it. After locally testing and saving the two main scripts (ui.R and server.R), run deployApp() to launch the app online.\n4. Bugs and session logs. Most often they won’t be bugs actually, but fancies, as it were. For instance, some special characters have to get even more special (technically, UTF-8 encoding). For a character such as ‘μ’, Shiny prefers ‘Âμ’, or better, the Unicode expression(\"\\u03bc\").\nCling to your logs by calling the line below, which you may keep at hand in your ‘Shiny deployer.R’ script.\nshowLogs(appPath = getwd(), appFile = NULL, appName = NULL, account = NULL,\rentries = 50, streaming = FALSE)\rAt best, the log output will mention any typos and unaccepted characters, pointing to specific lines in your code.\nIt may take a couple of intense days to get a first Shiny app running. Although the usual rabbit holes do exist, years of Shiny have already yielded a sizeable body of free resources online (tutorials, blogs, vlogs). Moreover, there’s also the RStudio Community, and then StackOverflow etc., where you can post any needs/despair. Post your code, log, and explanation, and you’ll be rescued out in a couple of days. Long live those contributors.\nIt’s sometimes enough to upload a bare app, but you might then think it can look better.\n5 (optional). Advance. Use tabs to combine multiple apps on one webpage, use different widgets, include a download option, etc. Tutorials like this one on Youtube can take you there, especially those that provide the code, as in the description of that video. Use those scripts as templates. For example, I made use of tabs on the top of the dashboard in order to keep the side bar from having too many widgets. The appearance of these tabs can be adjusted. More importantly, the inputs in the sidebar can be modified depending on the active tab, by means of ‘reactivity’ conditions.\n\rmainPanel(\rtags$style(HTML(\u0026#39;\r.tabbable \u0026gt; .nav \u0026gt; li \u0026gt; a {background-color:white; color:#3E454E}\r.tabbable \u0026gt; .nav \u0026gt; li \u0026gt; a:hover {background-color:#002555; color:white}\r.tabbable \u0026gt; .nav \u0026gt; li[class=active] \u0026gt; a {background-color:#ECF4FF; color:black}\r.tabbable \u0026gt; .nav \u0026gt; li[class=active] \u0026gt; a:hover {background-color:#E7F1FF; color:black}\r\u0026#39;)),\rtabsetPanel(id=\u0026#39;tabvals\u0026#39;,\rtabPanel(value=1, h4(strong(\u0026#39;Group \u0026amp; Electrode\u0026#39;)), br(), plotOutput(\u0026#39;plot_GroupAndElectrode\u0026#39;),\rh5(a(strong(\u0026#39;See plots with 95% Confidence Intervals\u0026#39;), href=\u0026#39;https://osf.io/2tpxn/\u0026#39;,\rtarget=\u0026#39;_blank\u0026#39;), style=\u0026#39;text-decoration: underline;\u0026#39;), downloadButton(\u0026#39;downloadPlot.1\u0026#39;, \u0026#39;Download HD plot\u0026#39;), br(), br(),\r# EEG montage\rimg(src=\u0026#39;https://preview.ibb.co/n7qiYR/EEG_montage.png\u0026#39;, height=500, width=1000)),\rtabPanel(value=2, h4(strong(\u0026#39;Participant \u0026amp; Area\u0026#39;)), br(), plotOutput(\u0026#39;plot_ParticipantAndLocation\u0026#39;),\rh5(a(strong(\u0026#39;See plots with 95% Confidence Intervals\u0026#39;), href=\u0026#39;https://osf.io/86ch9/\u0026#39;,\rtarget=\u0026#39;_blank\u0026#39;), style=\u0026#39;text-decoration: underline;\u0026#39;), downloadButton(\u0026#39;downloadPlot.2\u0026#39;, \u0026#39;Download HD plot\u0026#39;), br(), br(),\r# EEG montage\rimg(src=\u0026#39;https://preview.ibb.co/n7qiYR/EEG_montage.png\u0026#39;, height=500, width=1000)),\rtabPanel(value=3, h4(strong(\u0026#39;Participant \u0026amp; Electrode\u0026#39;)), br(), plotOutput(\u0026#39;plot_ParticipantAndElectrode\u0026#39;),\rbr(), downloadButton(\u0026#39;downloadPlot.3\u0026#39;, \u0026#39;Download HD plot\u0026#39;), br(), br(),\r# EEG montage\rimg(src=\u0026#39;https://preview.ibb.co/n7qiYR/EEG_montage.png\u0026#39;, height=500, width=1000)),\rtabPanel(value=4, h4(strong(\u0026#39;OLD Group \u0026amp; Electrode\u0026#39;)), br(), plotOutput(\u0026#39;plot_OLDGroupAndElectrode\u0026#39;),\rh5(a(strong(\u0026#39;See plots with 95% Confidence Intervals\u0026#39;), href=\u0026#39;https://osf.io/dvs2z/\u0026#39;,\rtarget=\u0026#39;_blank\u0026#39;), style=\u0026#39;text-decoration: underline;\u0026#39;), downloadButton(\u0026#39;downloadPlot.4\u0026#39;, \u0026#39;Download HD plot\u0026#39;), br(), br(),\r# EEG montage\rimg(src=\u0026#39;https://preview.ibb.co/n7qiYR/EEG_montage.png\u0026#39;, height=500, width=1000))\r),\r\r\nThe official Shiny gallery offers a great array of apps including their code (e.g., basic example). Another feature you may add is the option to download your plots, tables, data…\n\r# In ui.R script\rdownloadButton(\u0026#39;downloadPlot.1\u0026#39;, \u0026#39;Download HD plot\u0026#39;)\r#___________________________________________________\r# In server.R script\rspec_title = paste0(\u0026#39;ERP waveforms for \u0026#39;, input$var.Group, \u0026#39; Group, Electrode \u0026#39;, input$var.Electrodes.1, \u0026#39; (negative values upward; time windows displayed)\u0026#39;)\rplot_GroupAndElectrode = ggplot(df2, aes(x=time, y=-microvolts, color=condition)) +\rgeom_rect(xmin=160, xmax=216, ymin=7.5, ymax=-8, color = \u0026#39;grey75\u0026#39;, fill=\u0026#39;black\u0026#39;, alpha=0, linetype=\u0026#39;longdash\u0026#39;) +\rgeom_rect(xmin=270, xmax=370, ymin=7.5, ymax=-8, color = \u0026#39;grey75\u0026#39;, fill=\u0026#39;black\u0026#39;, alpha=0, linetype=\u0026#39;longdash\u0026#39;) +\rgeom_rect(xmin=350, xmax=550, ymin=8, ymax=-7.5, color = \u0026#39;grey75\u0026#39;, fill=\u0026#39;black\u0026#39;, alpha=0, linetype=\u0026#39;longdash\u0026#39;) +\rgeom_rect(xmin=500, xmax=750, ymin=7.5, ymax=-8, color = \u0026#39;grey75\u0026#39;, fill=\u0026#39;black\u0026#39;, alpha=0, linetype=\u0026#39;longdash\u0026#39;) +\rgeom_line(size=1, alpha = 1) + scale_linetype_manual(values=colours) +\rscale_y_continuous(limits=c(-8.38, 8.3), breaks=seq(-8,8,by=1), expand = c(0,0.1)) +\rscale_x_continuous(limits=c(-208,808),breaks=seq(-200,800,by=100), expand = c(0.005,0), labels= c(\u0026#39;-200\u0026#39;,\u0026#39;-100 ms\u0026#39;,\u0026#39;0\u0026#39;,\u0026#39;100 ms\u0026#39;,\u0026#39;200\u0026#39;,\u0026#39;300 ms\u0026#39;,\u0026#39;400\u0026#39;,\u0026#39;500 ms\u0026#39;,\u0026#39;600\u0026#39;,\u0026#39;700 ms\u0026#39;,\u0026#39;800\u0026#39;)) +\rggtitle(spec_title) + theme_bw() + geom_vline(xintercept=0) +\rannotate(geom=\u0026#39;segment\u0026#39;, y=seq(-8,8,1), yend=seq(-8,8,1), x=-4, xend=8, color=\u0026#39;black\u0026#39;) +\rannotate(geom=\u0026#39;segment\u0026#39;, y=-8.2, yend=-8.38, x=seq(-200,800,100), xend=seq(-200,800,100), color=\u0026#39;black\u0026#39;) +\rgeom_segment(x = -200, y = 0, xend = 800, yend = 0, size=0.5, color=\u0026#39;black\u0026#39;) +\rtheme(legend.position = c(0.100, 0.150), legend.background = element_rect(fill=\u0026#39;#EEEEEE\u0026#39;, size=0),\raxis.title=element_blank(), legend.key.width = unit(1.2,\u0026#39;cm\u0026#39;), legend.text=element_text(size=17),\rlegend.title = element_text(size=17, face=\u0026#39;bold\u0026#39;), plot.title= element_text(size=20, hjust = 0.5, vjust=2),\raxis.text.y = element_blank(), axis.text.x = element_text(size = 14, vjust= 2.12, face=\u0026#39;bold\u0026#39;, color = \u0026#39;grey32\u0026#39;, family=\u0026#39;sans\u0026#39;),\raxis.ticks=element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), plot.margin = unit(c(0.1,0.1,0,0), \u0026#39;cm\u0026#39;)) +\rannotate(\u0026#39;segment\u0026#39;, x=160, xend=216, y=-8, yend=-8, colour = \u0026#39;grey75\u0026#39;, size = 1.5) +\rannotate(\u0026#39;segment\u0026#39;, x=270, xend=370, y=-8, yend=-8, colour = \u0026#39;grey75\u0026#39;, size = 1.5) +\rannotate(\u0026#39;segment\u0026#39;, x=350, xend=550, y=-7.5, yend=-7.5, colour = \u0026#39;grey75\u0026#39;, size = 1.5) +\rannotate(\u0026#39;segment\u0026#39;, x=500, xend=750, y=-8, yend=-8, colour = \u0026#39;grey75\u0026#39;, size = 1.5) +\rscale_fill_manual(name = \u0026#39;Context / Target trial\u0026#39;, values=colours) +\rscale_color_manual(name = \u0026#39;Context / Target trial\u0026#39;, values=colours) +\rguides(linetype=guide_legend(override.aes = list(size=1.2))) +\rguides(color=guide_legend(override.aes = list(size=2.5))) +\r# Print y axis labels within plot area:\rannotate(\u0026#39;text\u0026#39;, label = expression(bold(\u0026#39;\\u2013\u0026#39; * \u0026#39;3 \u0026#39; * \u0026#39;\\u03bc\u0026#39; * \u0026#39;V\u0026#39;)), x = -29, y = 3, size = 4.5, color = \u0026#39;grey32\u0026#39;, family=\u0026#39;sans\u0026#39;) +\rannotate(\u0026#39;text\u0026#39;, label = expression(bold(\u0026#39;+3 \u0026#39; * \u0026#39;\\u03bc\u0026#39; * \u0026#39;V\u0026#39;)), x = -29, y = -3, size = 4.5, color = \u0026#39;grey32\u0026#39;, family=\u0026#39;sans\u0026#39;) +\rannotate(\u0026#39;text\u0026#39;, label = expression(bold(\u0026#39;\\u2013\u0026#39; * \u0026#39;6 \u0026#39; * \u0026#39;\\u03bc\u0026#39; * \u0026#39;V\u0026#39;)), x = -29, y = 6, size = 4.5, color = \u0026#39;grey32\u0026#39;, family=\u0026#39;sans\u0026#39;)\rprint(plot_GroupAndElectrode)\routput$downloadPlot.1 \u0026lt;- downloadHandler(\rfilename \u0026lt;- function(file){\rpaste0(input$var.Group, \u0026#39; group, electrode \u0026#39;, input$var.Electrodes.1, \u0026#39;, \u0026#39;, Sys.Date(), \u0026#39;.png\u0026#39;)},\rcontent \u0026lt;- function(file){\rpng(file, units=\u0026#39;in\u0026#39;, width=13, height=5, res=900)\rprint(plot_GroupAndElectrode)\rdev.off()},\rcontentType = \u0026#39;image/png\u0026#39;)\r} )\r\r\nApps can include any text, such as explanations of any length and web links. For instance, we can link back to the data repository, where the code for the app can be found.\nAn example of a Shiny app is available, which may also be edited and run in this RStudio environment, inside the ‘Shiny-app’ folder.\nThe Shiny server (shinyapps.io) allows publishing dashboards built with various frameworks besides Shiny proper. Flexdashboard and Shinydashboard are two of these frameworks, which have visible advantages over basic Shiny, in terms of layout. An example with Flexdashboard is available.\n★  Flexdashboard  ★\r★ ★  Shiny  ★ ★\r★ ★ ★  Flexdashboard-Shiny  ★ ★ ★\r\r\rLogistics\rMemory capacity can become an issue as you go on, which will be flagged in the error logs as: ‘Shiny cannot use on-disk bookmarking’. This doesn’t necessarily lead you to a paid subscription or to host the website on a custom server. Try pruning the data file, outsourcing data sections across the five available apps.\nApp providers have specific terms of use. To begin, Shiny has a free starter license with limited use, where free apps can handle a certain amount of data, and up to five apps may be created. Beyond that, RStudio offers a wide range of subscriptions starting at $9/month. For its part, Tableau in principle deals only with subscriptions from $35/month on. While they offer 1-year licenses to students and instructors for free, these don’t include web hosting, unlike Shiny’s free plan. Further comparisons of these platforms are available online. Last, I’ll just mention a third language, D3, which is powerful, and may also be used through R.\nIn the case of very heavy data or frequent public use, if you don’t want to host your Shiny app externally, you might consider rendering a PDF with your visualisations instead.\n\rpdf(\u0026quot;List of plots per page\u0026quot;, width=13, height=5)\rprint(plot1)\rprint(plot2)\r# ...\rprint(plot150)\rdev.off()\r\rHigh-resolution plots can be rendered into a PDF document in a snap. Conveniently, all text is indexed, so it can be searched (Ctrl+f / Cmd+f / 🔍) (see example). Furthermore, you may also merge the rendered PDF with any other documents.\n.embed-responsive{position:relative;height:100%;}.embed-responsive iframe{position:absolute;height:100%;}\rwindow.jQuery || document.write('\\x3C/script') \r\r\rvar mfrRender = new mfr.Render(\"mfrIframe\", \"https://mfr.osf.io/render?url=https://osf.io/2tpxn/?direct%26mode=render%26action=download%26mode=render\");\r\n\rSummary in slides\r\r Presenting data interactively online using R Shiny  from Pablo Bernabeu\r\r\r","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"634b1bab4555e068cc9e05367ce3f565","permalink":"https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/","section":"post","summary":"Dashboards for data visualisation, such as R Shiny and Tableau, allow the interactive exploration of data by means of drop-down lists and checkboxes, with no coding required from the final users. These web applications run on internet browsers, allowing for three viewing modes, catered to both analysts and the public at large: (1) private viewing (useful during analysis), (2) selective sharing (used within work groups), and (3) internet publication. Among the available platforms, R Shiny and Tableau stand out due to being relatively accessible to new users. Apps serve a broad variety of purposes. In science and beyond, these apps allow us to go the extra mile in sharing data. Alongside files and code shared in repositories, we can present the data in a website, in the form of plots or tables. This facilitates the public exploration of each section of the data (groups, participants, trials...) to anyone interested, and allows researchers to account for their proceeding in the analysis.","tags":["s","data presentation","dashboard","reproducibility","open science","open data","R","R Shiny","Flexdashboard"],"title":"The case for data dashboards: First steps in R Shiny","type":"post"},{"authors":null,"categories":["conceptual processing","R"],"content":"\u0026nbsp; Dashboard  \u0026nbsp;\rContent\nThe data is from a psychology experiment on the comprehension of words, in which electroencephalographic (EEG) responses were measured. The data are presented in plots spanning 800 milliseconds (the duration of word processing). The aim of this Shiny app is to facilitate the exploration of the data by researchers and the public. Users can delve into the different sections of the data. In a hierarchical order, these sections are groups of participants, individual participants, brain areas, and electrodes.\nShiny apps in science\nBy creating this app, I tried to reach beyond the scope of current open science, which is often confined to files shared on data repositories. I believe that Shiny apps will become general practice in science within a few years (see blog post or slides for more information).\nTechnical details\nI made use of tabs on the top of the dashboard in order to keep the side bar from having too many widgets. I adjusted the appearance of these tabs, and by means of \u0026lsquo;reactivity\u0026rsquo; conditions, also modified the inputs in the sidebar depending on the active tab.\nmainPanel(\rtags$style(HTML('\r.tabbable \u0026gt; .nav \u0026gt; li \u0026gt; a {background-color:white; color:#3E454E}\r.tabbable \u0026gt; .nav \u0026gt; li \u0026gt; a:hover {background-color:#002555; color:white}\r.tabbable \u0026gt; .nav \u0026gt; li[class=active] \u0026gt; a {background-color:#ECF4FF; color:black}\r.tabbable \u0026gt; .nav \u0026gt; li[class=active] \u0026gt; a:hover\t{background-color:#E7F1FF; color:black}\r')),\rtabsetPanel(id='tabvals',\rtabPanel(value=1, h4(strong('Group \u0026amp; Electrode')), br(), plotOutput('plot_GroupAndElectrode'),\rh5(a(strong('See plots with 95% Confidence Intervals'), href='https://osf.io/2tpxn/',\rtarget='_blank'), style='text-decoration: underline;'), downloadButton('downloadPlot.1', 'Download HD plot'), br(), br(),\r# EEG montage\rimg(src='https://preview.ibb.co/n7qiYR/EEG_montage.png', height=500, width=1000)),\rtabPanel(value=2, h4(strong('Participant \u0026amp; Area')), br(), plotOutput('plot_ParticipantAndLocation'),\rh5(a(strong('See plots with 95% Confidence Intervals'), href='https://osf.io/86ch9/',\rtarget='_blank'), style='text-decoration: underline;'), downloadButton('downloadPlot.2', 'Download HD plot'), br(), br(),\r# EEG montage\rimg(src='https://preview.ibb.co/n7qiYR/EEG_montage.png', height=500, width=1000)),\rtabPanel(value=3, h4(strong('Participant \u0026amp; Electrode')), br(), plotOutput('plot_ParticipantAndElectrode'),\rbr(), downloadButton('downloadPlot.3', 'Download HD plot'), br(), br(),\r# EEG montage\rimg(src='https://preview.ibb.co/n7qiYR/EEG_montage.png', height=500, width=1000)),\rtabPanel(value=4, h4(strong('OLD Group \u0026amp; Electrode')), br(), plotOutput('plot_OLDGroupAndElectrode'),\rh5(a(strong('See plots with 95% Confidence Intervals'), href='https://osf.io/dvs2z/',\rtarget='_blank'), style='text-decoration: underline;'), downloadButton('downloadPlot.4', 'Download HD plot'), br(), br(),\r# EEG montage\rimg(src='https://preview.ibb.co/n7qiYR/EEG_montage.png', height=500, width=1000))\r),\r The data set was fairly large, considering the fact that it's hosted with the free plan. In order to lighten the processing, I split the data into various files, reducing the total size. Furthermore, I outsourced a particularly heavy set of plots (those with Confidence Intervals) to PDF files, to which I linked in the app.\nh5(a(strong('See plots with 95% Confidence Intervals'), href='https://osf.io/dvs2z/',\rtarget='_blank'), style='text-decoration: underline;'),\r I also used web links to the published paper and raw data, as well as to the server and ui scripts. These files, along with the data, are publicly available in this repository; they may be accessed within the \u0026ldquo;Files\u0026rdquo; section, by opening the folders \u0026ldquo;ERPs\u0026rdquo; -\u0026gt; \u0026ldquo;Analyses of ERPs averaged across trials\u0026rdquo; -\u0026gt; \u0026ldquo;Shiny app\u0026rdquo;.\nAnother feature I added was the download button.\n# From server.R script\rspec_title = paste0('ERP waveforms for ', input$var.Group, ' Group, Electrode ', input$var.Electrodes.1, ' (negative values upward; time windows displayed)')\rplot_GroupAndElectrode = ggplot(df2, aes(x=time, y=-microvolts, color=condition)) +\rgeom_rect(xmin=160, xmax=216, ymin=7.5, ymax=-8, color = 'grey75', fill='black', alpha=0, linetype='longdash') +\rgeom_rect(xmin=270, xmax=370, ymin=7.5, ymax=-8, color = 'grey75', fill='black', alpha=0, linetype='longdash') +\rgeom_rect(xmin=350, xmax=550, ymin=8, ymax=-7.5, color = 'grey75', fill='black', alpha=0, linetype='longdash') +\rgeom_rect(xmin=500, xmax=750, ymin=7.5, ymax=-8, color = 'grey75', fill='black', alpha=0, linetype='longdash') +\rgeom_line(size=1, alpha = 1) + scale_linetype_manual(values=colours) +\rscale_y_continuous(limits=c(-8.38, 8.3), breaks=seq(-8,8,by=1), expand = c(0,0.1)) +\rscale_x_continuous(limits=c(-208,808),breaks=seq(-200,800,by=100), expand = c(0.005,0), labels= c('-200','-100 ms','0','100 ms','200','300 ms','400','500 ms','600','700 ms','800')) +\rggtitle(spec_title) + theme_bw() + geom_vline(xintercept=0) +\rannotate(geom='segment', y=seq(-8,8,1), yend=seq(-8,8,1), x=-4, xend=8, color='black') +\rannotate(geom='segment', y=-8.2, yend=-8.38, x=seq(-200,800,100), xend=seq(-200,800,100), color='black') +\rgeom_segment(x = -200, y = 0, xend = 800, yend = 0, size=0.5, color='black') +\rtheme(legend.position = c(0.100, 0.150), legend.background = element_rect(fill='#EEEEEE', size=0),\raxis.title=element_blank(), legend.key.width = unit(1.2,'cm'), legend.text=element_text(size=17),\rlegend.title = element_text(size=17, face='bold'), plot.title= element_text(size=20, hjust = 0.5, vjust=2),\raxis.text.y = element_blank(), axis.text.x = element_text(size = 14, vjust= 2.12, face='bold', color = 'grey32', family='sans'),\raxis.ticks=element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), plot.margin = unit(c(0.1,0.1,0,0), 'cm')) +\rannotate('segment', x=160, xend=216, y=-8, yend=-8, colour = 'grey75', size = 1.5) +\rannotate('segment', x=270, xend=370, y=-8, yend=-8, colour = 'grey75', size = 1.5) +\rannotate('segment', x=350, xend=550, y=-7.5, yend=-7.5, colour = 'grey75', size = 1.5) +\rannotate('segment', x=500, xend=750, y=-8, yend=-8, colour = 'grey75', size = 1.5) +\rscale_fill_manual(name = 'Context / Target trial', values=colours) +\rscale_color_manual(name = 'Context / Target trial', values=colours) +\rguides(linetype=guide_legend(override.aes = list(size=1.2))) +\rguides(color=guide_legend(override.aes = list(size=2.5))) +\r# Print y axis labels within plot area:\rannotate('text', label = expression(bold('\\u2013' * '3 ' * '\\u03bc' * 'V')), x = -29, y = 3, size = 4.5, color = 'grey32', family='sans') +\rannotate('text', label = expression(bold('+3 ' * '\\u03bc' * 'V')), x = -29, y = -3, size = 4.5, color = 'grey32', family='sans') +\rannotate('text', label = expression(bold('\\u2013' * '6 ' * '\\u03bc' * 'V')), x = -29, y = 6, size = 4.5, color = 'grey32', family='sans')\rprint(plot_GroupAndElectrode)\routput$downloadPlot.1 \u0026lt;- downloadHandler(\rfilename \u0026lt;- function(file){\rpaste0(input$var.Group, ' group, electrode ', input$var.Electrodes.1, ', ', Sys.Date(), '.png')},\rcontent \u0026lt;- function(file){\rpng(file, units='in', width=13, height=5, res=900)\rprint(plot_GroupAndElectrode)\rdev.off()},\rcontentType = 'image/png')\r} )\r # From ui.R script\rdownloadButton('downloadPlot.1', 'Download HD plot')\r Rising to the challenge\nMy experience with R Shiny has been so good I've been sharing it. Yet, on my first crawling days, I spent an eternity stuck with this elephant in my room: \u0026ldquo;μ\u0026rdquo;. This μ letter (micro-souvenir from hell, as I later knew it), was part of the labels of my plots. All I knew was that I could not deploy the app online, even while I could perfectly launch it locally in my laptop. So, I wondered what use was to deploy locally if I couldn't publish the app?! Eventually, I read about UTF-8 encoding in one forum. Bless them forums. All I had to do was use \u0026ldquo;Âμ\u0026rdquo; instead of the single \u0026ldquo;μ\u0026rdquo;. A better option I found later was: expression(\u0026quot;\\u03bc\u0026quot;).\nBeyond encoding issues, I had a tough time embedding images. You know, the \u0026lsquo;www\u0026rsquo; folder\u0026hellip; To be honest, I still haven't handled the \u0026lsquo;www\u0026rsquo; way\u0026ndash;but where there's a will, there's a way. I managed to include my images by uploading them to a website and then entering their URL in \u0026ldquo;img(src\u0026rdquo;, avoiding the use of folder paths.\nimg(src=\u0026quot;https://preview.ibb.co/n7qiYR/EEG_montage.png 1\u0026quot;, height=500, width=1000)\r Long after I had built the app, I added another image\u0026ndash;the favicon (the little icon on the browser tab).\ntags$head(tags$link(rel=\u0026quot;shortcut icon\u0026quot;, href=\u0026quot;https://image.ibb.co/fXUwzb/favic.png\u0026quot;)), # web favicon\r Reference Bernabeu, P., Willems, R. M., \u0026amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs [Data dashboard]. Retrieved from https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment/.\n\r","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"5d4a9692e0623e887564f71b0c57ff97","permalink":"https://pablobernabeu.github.io/applications-and-dashboards/bernabeu-etal-2017-modalityswitch/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/applications-and-dashboards/bernabeu-etal-2017-modalityswitch/","section":"applications-and-dashboards","summary":"We tested whether conceptual processing is modality-specific by tracking the time course of the Conceptual Modality Switch effect. Forty-six participants verified the relation between property words and concept words. The conceptual modality of consecutive trials was manipulated in order to produce an Auditory-to-visual switch condition, a Haptic-to-visual switch condition, and a Visual-to-visual, no-switch condition. Event-Related Potentials (ERPs) were time-locked to the onset of the first word (property) in the target trials so as to measure the effect online and to avoid a within-trial confound. A switch effect was found, characterized by more negative ERP amplitudes for modality switches than no-switches. It proved significant in four typical time windows from 160 to 750 milliseconds post word onset, with greater strength in the Slow group, in posterior brain regions, and in the N400 window. The earliest switch effect was located in the language brain region, whereas later it was more prominent in the visual region. In the N400 and Late Positive windows, the Quick group presented the effect especially in the language region, whereas the Slow had it rather in the visual region. These results suggest that contextual factors such as time resources modulate the engagement of linguistic and embodied systems in conceptual processing.","tags":["data dashboard","R","R Shiny","conceptual modality switch","conceptual processing","reading","event-related potentials","cognition","psycholinguistics","HTML","CSS"],"title":"Web application: Modality switch effects emerge early and increase throughout conceptual processing","type":"applications-and-dashboards"},{"authors":[],"categories":["linguistic materials"],"content":"\r\r","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"7d26799976b91aa583f2467d84638054","permalink":"https://pablobernabeu.github.io/2016/modality-exclusivity-norms-for-747-properties-and-concepts-in-dutch-a-replication-of-english/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/2016/modality-exclusivity-norms-for-747-properties-and-concepts-in-dutch-a-replication-of-english/","section":"post","summary":"\r\r","tags":["s","research methods","stimuli","linguistic norms","modality exclusivity norms","conceptual modality switch","dimensionality reduction","conceptual replication","Dutch"],"title":"Modality exclusivity norms for 747 properties and concepts in Dutch: A replication of English","type":"post"},{"authors":["Risto Hinno and Pablo Bernabeu"],"categories":["data mining","big data"],"content":"\r\rThis is a slightly revised version of a course project I shared with Risto Hinno.\n\r1. Introduction\rRacism has long been ingrained in human societies. Ancient Greek Aristotle already claimed that non-Greeks were slaves by nature, as they easily submitted to despotic government (Reilly, Kaufman, \u0026amp; Bodino, 2002). This study focuses on racism in the United States, which extends from the foundation of the country, when black people were generally born into slavery, and were at any rate regarded as an inferior people. US racism stands out globally for two reasons. First, the country has played a hegemonic part in the World since soon after its foundation. Second, the US is regarded as the most advanced society technology-wise, as it sets the minutes for the technology sector worldwide. In spite of these advantages, the country has long suffered the plague of widespread racism. Indeed, the abolition of slavery in the mid-nineteenth century did not grant equal citizen rights to the black population. Over time, the black population started to confront this situation. Especially the mid-nineteenth century saw large uprisings and a patent division of different societal sectors, as reflected in literary works such as Ellison’s ‘Invisible Man’ (1952). Inequality and confrontation about racism has extended to date, and the costs thereof have been large in terms of lives and otherwise (Feagin, 2004).\nWith the era of global communication, what happens in the World’s most powerful country is quickly and largely spread overseas—so too with racism matters. The last major such event related to racism happened during the first weeks of July 2016. Within five days, two cases of dubious, lethal police intervention with black citizens were followed by the killing of five policemen by a black youth. The specific course of events was as follows. On July 5th, Alton Sterling was killed by police officers in Louisiana. Next day, Philando Castile was also killed by police. In this case, the presence of Castile’s girlfriend during the tragedy likely determined the following events, because she described the event to the media, underscoring how gratuitous the killing was. During the following hours, outrage escalated within the already-wary population of the US. Yet the crisis would not stop there. During one of the various demonstrations held across the country, a dozen policemen were shot by a sniper, leaving five of them dead. The attacker was a black youth linked to black militant groups which target the Establishment on the grounds of patent racial discrimination. We will refer to this concatenation of events as the Louisiana-Minnesota-Texas (LMT) crisis.1\nIn the welter of events in Louisiana, Minnesota and Texas, some journalists warned of the return of social divisions such as those from the mid-nineteenth century. Such divisions might lead some people to an incomplete perception of the situation, and thus hinder the achievement of any solutions. However, President Obama denied such divisions as he spoke at the funeral for the policemen killed. At the same time, he addressed each of the different groups in the problem, including Establishment institutions and black protesters, advising them all to exercise greater open-mindedness towards the other aspects and bands in the problem (see statement).\nIt must be noted that this small study is primarily a way for us to practise data analysis at a course. Neither the background nor the analyses make a realistic study of racism or the LMT crisis.\n\r2. Goals\rWe wanted to look at these developments from the scope of online data. For this purpose, we scraped online discussions on these developments from a variety of media sources, and within defined time frames in the crisis. We then probed for any noticeable fluctuations in the topics throughout the course of events, and also for any differences across the different media. In this analysis, we took an exploratory approach by means of topic modeling. We wanted to check, first, whether topic modeling would be sensitive and useful at all within such a compact time scale. Were it to allow us, we would analyze how the journalistic and the social media reflected any fluctuations based on the live developments in Louisiana, Minnesota and Texas. As such, the dependent variable (DV) in this study is the overall topic under discussion, which we measured via topic modeling. So, the language we analyze are messages related to the LMT crisis. Two factors are checked as potentially affecting the DV, namely Media and Time.\nThe Media factor regarded the three different sources of information from which we retrieved content, namely: (1) the New York Times (NYT), (2) public tweets related to the NYT, and (3) public comments on the NYT’s Facebook posts.\nThe Time factor was based on the following periods. The first period, from 2 to 4 July, was selected as a baseline during which no remarkable events racism-wise happened. The second period, including 5 and 6 July, contains the days when the two black citizens were killed by policemen. The third period, from 7 to 11 July, contains the aftermath of the crisis overall.\n\r3. Hypotheses\rWe had several hypotheses for our planned analyses. For the Media factor, we hypothesized a greater objectivity and formality overall for the NYT articles compared to the other two sources.\nWe did not have any hypotheses about the Time factor, i.e., the nature of any potential topic changes. In fact, we had considerable reservations as to whether any fluctuations would present, given the fact that the latent feeling of such a crisis might stay negative, critical and fearful from the start, regardless of particular events.\nWith respect to the interaction between the two factors, we hypothesized that the NYT articles would present the lowest degree of thematic variation, due to the fact that such journal pieces require time to investigate and write up—even if they are published online. Comparatively, popular comments on Twitter and Facebook would present more emotionality and subjectivity, and likely they would also present greater influence of immediate events. Furthermore, Twitter should be yet more immediate than Facebook.\nLast, with respect to the DV, we did not actually have any hypotheses about the nature of possible topic fluctuations.\n\r4. Methods\rOnline reactions to the LMT developments were scraped from various online sources. This content was constrained to language, bearing no extensions such as pictures or videos. In order to narrow the scope of the information, all scraped sources were related to The New York Times journal. The sources were, first, the NYT online edition (nytimes.com); second, public tweets related to NYT (@nytimes); and, third, public comments posted on the NYT page (@nytimes). Crucially, these sources are different in nature. Whereas the articles in the journal’s online edition broadly follow the standard article form of mainstream journals, Facebook comments on the page are aligned with the Facebook standards, that is, comparatively informal and outspoken. In accord, tweets referring to @nytimes follow the Twitter conventions, characterized by the 140-character restriction, and the relative immediacy of their information (Oh \u0026amp; Syn, 2015; Wang, He \u0026amp; Zhao, 2014; Josephson \u0026amp; Miller, 2015).\nThe method to scrape content related to the LMT crisis was through keywords. For the three media, the following keywords were entered, such that articles containing any of those words would be returned: ‘black’ OR ‘racism’ OR ‘police’ OR ‘dallas’ OR ‘alton’ OR ‘sterling’ OR ‘philando’ OR ‘castile.’ Further particulars are provided in turn.\nNew York Times. This scraping pipeline started from the official API site for NYT (https://developer.nytimes.com). Metadata was downloaded for 2,000 articles adjusting to the abovementioned keywords. This returned articles dating back to the start of the year. After preprocessing, 29 articles were returned for period 1; 53 for period 2; and 275 for period 3.\nTwitter. This scraping was performed through Geoff Jentry’s R package ‘twitteR’ (https://github.com/geoffjentry/twitteR). Here tweets were selected based on the same keywords, in addition to ‘@nytimes’. Due to the ten-day maximum range of Twitter’s API, no time range was entered. Retweets were removed. With the naked eye we realized that the tweets contained considerable information on Saudi events, we entered the word ‘saudi’ as a negative keyword. After preprocessing, 157 articles were returned for period 1; 489 for period 2; and 5025 for period 3.\nFacebook. The ‘RFacebook’ R package, by Pablo Barbera (https://github.com/pablobarbera/Rfacebook), was used for this scraping. The Facebook API currently allows for the download of all or any posts from one page, with no time restrictions (broader-search functions seem to have been deprecated). We downloaded any comments on the pages’ posts which adjusted to our keywords. After preprocessing, 195 articles were returned for period 1; 1107 for period 2; and 8724 for period 3.\nPreprocessing was performed equally for all sources—as standard in topic modeling, by removing non-relevant (‘stop-words’) and non-linguistic elements. Removed items included the names of the media, as well as numbers, punctuation, links, and technical signs such as @.\n\r5. Results\rOur hypothesis about the Media factor was only partially confirmed. First, sentiment analysis showed that NYT posts were very tempered, with an average sentiment score near 0. In contrast, Twitter presented a rather negative sentiment (Thelwall, Buckley, \u0026amp; Paltoglou, 2011; Saif, He, Fernandez, \u0026amp; Alani, 2016). Yet, to our surprise, Facebook came out with a neutrality close to that of NYT articles, even if there was greater variance among the scores of the Facebook posts. These overall tendencies are illustrated in the plot below. Caution must recommended, however, when considering this sentiment analysis, as this technique is arguably fuzzy generally, and especially so with data under such a tight time frame. This is the case because sentiment analysis, as other big data techniques, capitalizes on the size of data. What it lacks on the precision aspect, compared to null-significance hypothesis testing, for instance, it compensates with the size of the samples, in which the noise is suppressed by thousands of cases. In this case, however, the sampling within only nine days of unusual circumstances calls for circumspection.\nNext, topic modeling was conducted on each source separately. The parameters for topic discovery were entered based on several attempts with different numbers of topics (K) and internal-coherence thresholds (alpha). Finally, topics were selected alike for every source, K = 3, alpha = .2. Below, the first ten words for each topic in each source are shown (note that columns are aligned rightward).\nNew York Times articles\r## Foreign policy Shooting Elections\r## 1 said police new\r## 2 percent said one\r## 3 vote officers people\r## 4 year black can\r## 5 brexit dallas like\r## 6 will officer york\r## 7 since shooting trump\r## 8 european two july\r## 9 british shot even\r## 10 britain says just\r\rFacebook comments\r## Police People Black lives matter\r## 1 police people black\r## 2 gun will lives\r## 3 cops can matter\r## 4 people like white\r## 5 officers get people\r## 6 dont one racist\r## 7 just just blm\r## 8 man need police\r## 9 get dont blacks\r## 10 officer police obama\r\rTweets\r## Racism Dallas shooting Philando shooting\r## 1 black police police\r## 2 police dallas blacks\r## 3 white officers new\r## 4 lives killed shooting\r## 5 people protest philando\r## 6 racism shooting force\r## 7 amp shootings castile\r## 8 matter alton use\r## 9 stop sterling says\r## 10 cops baton violence\rTo start, it may stand out that different topics appear across sources, all the while some are indeed shared. This is perfectly normal for topic modeling on different sources, even when the same topic is being studied. Indeed, it is very relevant for us to remark on the inclusion of foreign affairs and election matters within the NYT articles, but not within people’s tweets and Facebook comments. This makes sense for several reasons. To start, the space a journalist counts on in a NYT article is considerable, compared to tweets, and also compared to ruling conventions of Facebook posts (users may write further, but the average simply will not). Second, the breadth of relation in NYT articles likely responds to the expectations from renown journalists to enrich the news with a broader contextualization. Furthermore, this extension of topics might correspond to the tacit but doubtless alignment of journals to concrete political agendas. While people commenting on Twitter or Facebook are plausibly characterized by just the same virtues and vices, their online reactions could be driven by more emotion and immediacy of focus than those of mass media journalists.\nFor greater visualization, we also provide some captions from the interactive LDAvis tool below. Please click on the figure titles to enjoy the full visualization.\n↑ LDAvis visualization of NYT articles (click to explore in detail)\n\r\n↑ LDAvis visualization of Facebook comments (click to explore in detail)\n\r\n↑ LDAvis visualization of tweets (click to explore in detail)\n\r\nIn order to specifically compare different content sources, we plotted the major language from two sources on the same plot, with an axis spanning from one source to the other, as shown below. The size of the words indicates the frequency of use, and the colour is essentially parallel with the axis, with specific different colours for different corpora, and darker hues for greater association.\n↑ Facebook comments and NYT articles\n\r↑ Facebook comments and tweets\n\r↑ Tweets and NYT articles\n\r\nWe went on to analyze the overlap in topics across journals, in order to quantitatively check whether some topics were indeed shared across sources, even if in different positions (for instance, topic 1 in some source and topic 3 in some other). We did this by means of cosine similarity scores. These scores represent the degree of similarity of two sources on a continuous scale from 0 to 1, where 1 would mean identical. The plots illustrate these comparisons in turn.\n↑ Similarity between Facebook comments and NYT articles\r↑ Similarity between Facebook comments and tweets\r↑ Similarity between tweets and NYT articles\rLast, the interaction of Time and Media was analyzed. As expected, we found differences in the way topics fluctuated over time in the different sources, albeit in unexpected ways. NYT and tweets articles presented great variation, suggesting day-bound sensitivity to the developments. This was to be expected from Twitter, as it is famous for its immediacy. However, the immediacy of NYT articles was rather surprising, as they might have lagged behind due to the necessary investigation and editing for such kind of journalistic pieces. Unlike traditional paper-based NYT articles, this immediacy is now enabled by the publication online. Another unexpected finding was the relative stillness of Facebook posts over time. Since they are published at the minute, and nowadays mostly from mobile, we had thought they would present greater immediacy than NYT articles. We could hypothesize on this, but this would be best analyzed in further research. The plot below illustrates this interaction.\n↑ Topic fluctuations over time for the three content sources\n\r\r\r6. Discussion\rIn this small-scale study, we analyzed the impact of a racism-related crisis in the American society online. This crisis started with the killing of two black citizens by policemen under dubious circumstances, which was followed by massive media attention and street demonstrations, and then continued to the fatal shooting of five policemen by a black militant (the crisis continued yet further after our analyses). The impact of this crisis was large, with a state funeral being organized for the killed policemen, and a presidential address warning of the direction of social tensions, and the need for greater empathy from all social sections involved.\nWe scraped the divided the social reaction to these events from three online sources, namely, the NYT online edition, public comments on the NYT Facebook page, and finally NYT-related tweets. The method was based on keywords highly relevant to this crisis, namely: ‘black’ OR ‘racism’ OR ‘police’ OR ‘dallas’ OR ‘alton’ OR ‘sterling’ OR ‘philando’ OR ‘castile’. We analyzed the Media factor and the Time factor separately, and more interestingly we looked at the interaction between these two factors.\nAs results, we found, first, that NYT articles were the most neutral, closely followed by Facebook posts. In contrast, tweets presented greater negativity overall. Next, we looked at topics within each time frame in each of the three sources. These topics differed across sources, even though there were also considerable overlaps. For instance, NYT articles and related tweets shared the content of their second topics, both of which revolved around ‘shooting.’ We went further to quantitatively measure any such overlaps or otherwise differences across sources. Cosine similarity—which ranges from 1, totally related, to 0, not related at all—confirmed our naked eye feeling. For instance, for the overlap between the aforementioned topics, there was a cosine similarity of .71. In contrast, a cosine of .03 came up for other comparisons, which also makes sense due to the intrinsic differences among these sources.\nLast, the interaction between Time and Source was qualitatively analyzed by means of a plot, and we found that Twitter and NYT articles were most sensitive to live developments in the crisis, whereas Facebook comments lagged behind in this immediacy. All of these findings were discussed within a framework of qualitative big data analysis.\nThe data mass probed in these analyses could be described as medium-sized data in the big data field. This field is relatively recent, and the successful, seminal examples we count on tend to feature larger sizes of data. In particular, for time frames, it is rather uncommon to find such a tight scale as we excerpted. This fact complicates the drawing of assured conclusions from our findings, because we lack well-known precedents along these lines.\n\rAdditional materials\rAll materials are made public on the RRisto GitHub page.\n\rReferences\rEllison, R. (1952). Invisible Man. New York: Random House.\nFeagin, J. R. (2004). Documenting the costs of slavery, segregation, and contemporary racism: Why reparations are in order for African Americans. Harvard BlackLetter Law Journal, 20, 49-81.\nJosephson, S., \u0026amp; Miller, J. S. (2015). Just state the facts on Twitter: Eye tracking shows that readers may ignore questions posted by news organizations on Twitter but not on Facebook. Visual Communication Quarterly, 22(2), 94-105.\nOh, S., \u0026amp; Syn, S. Y. (2015). Motivations for sharing information and social support in social media: A comparative analysis of Facebook, Twitter, Delicious, YouTube, and Flickr. Journal Of The Association For Information Science And Technology, 66(10), 2045-2060.\nReilly, K., Kaufman, S., \u0026amp; Bodino, A. (2003). Racism: A global reader. London: M. E. Sharpe\nSaif, H., He, Y., Fernandez, M., \u0026amp; Alani, H. (2016). Contextual semantics for sentiment analysis of Twitter. Information Processing And Management, 52(1), 5-19.\nThelwall, M., Buckley, K., \u0026amp; Paltoglou, G. (2011). Sentiment in Twitter events. Journal of the American Society for Information Science and Technology, 62(2), 406-418.\nWang, P., He, W., \u0026amp; Zhao, J. (2014). A Tale of Three Social Networks: User Activity Comparisons across Facebook, Twitter, and Foursquare. IEEE Internet Computing, 18(2), 10-15.\n\r\r\rA later update: On July 17, 2016—days after the current analysis—, the LMT crisis was extended with the killing of two policemen in the same Louisiana city where Alton Sterling had been killed.↩︎\n\r\r\r","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"1ed190c11c06905a7e0f3980ec6e9617","permalink":"https://pablobernabeu.github.io/2016/the-louisiana-minnesota-texas-crisis-across-media-and-time-a-big-data-exercise/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/2016/the-louisiana-minnesota-texas-crisis-across-media-and-time-a-big-data-exercise/","section":"post","summary":"Racism has long been ingrained in human societies. Ancient Greek Aristotle already claimed that non-Greeks were slaves by nature, as they easily submitted to despotic government (Reilly, Kaufman, \u0026 Bodino, 2002). This study focuses on racism in the United States, which extends from the foundation of the country, when black people were generally born into slavery, and were at any rate regarded as an inferior people. US racism stands out globally for two reasons. First, the country has played a hegemonic part in the World since soon after its foundation. Second, the US is regarded as the most advanced society technology-wise, as it sets the minutes for the technology sector worldwide. In spite of these advantages, the country has long suffered the plague of widespread racism. Indeed, the abolition of slavery in the mid-nineteenth century did not grant equal citizen rights to the black population. Over time, the black population started to confront this situation. Especially the mid-nineteenth century saw large uprisings and a patent division of different societal sectors, as reflected in literary works such as Ellison’s 'Invisible Man' (1952). Inequality and confrontation about racism has extended to date, and the costs thereof have been large in terms of lives and otherwise (Feagin, 2004).","tags":["s","big data","data mining","racism","web scraping","R","API","social media","news media"],"title":"The Louisiana-Minnesota-Texas crisis across media and time: A big data exercise","type":"post"},{"authors":["Bernabeu, P., \u0026 Vogt, P."],"categories":["evolution of language"],"content":"Reference Bernabeu, P., \u0026amp; Vogt, P. (2015). Language evolution: Current status and future directions. Tenth Language at the University of Essex (LangUE) Postgraduate Conference. https://researchgate.net/publication/280858062/\n\r","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"139129944442d259234f2f3367f68490","permalink":"https://pablobernabeu.github.io/publication/bernabeu_vogt2015/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/bernabeu_vogt2015/","section":"publication","summary":"The topic of language evolution is characterised by the scarcity of records, but also by a large flow  of research  produced within  multiple subtopics  and perspectives.  Over the  past  few decades, significant advancement has been made on the geographical and temporal origins of language, while current work is rather devoted to the underpinnings of language, in brain, genes, body, and culture of humans. Much of this literature is polarized over the crucial dichotomy of nativism  versus  emergentism.  Our  state  of  affairs  report  also  confirms  a  high  degree  of speculation,  albeit  with a  decrease  for modelling. To  tackle the  speculation  and the large research flow, we propose a more impersonal kind of review, focused on the topic’s questions rather than on particular accounts. Another observation is that novel perspectives are on the rise.  One  of  these  highlights  the  importance  of  perceptual  cognition,  often  dubbed ‘embodiment,’ in  the earlier  evolution  of language.  In  following this  lead,  we adapted  a previous experiment which had investigated the correspondence between certain perceptual features of events, and different  grammatical orders arising as participants  acted out those events. That design made a perfect basis for us to put in an additional variable, namely the contrast  between  body-based  communication  (gestures),  and  more  disembodied communication (symbol matching). Albeit tentative, the results of this pilot experiment reveal a greater effect of the embodiment variable on the grammatical preferences, which we see as inviting further exploration of embodied cognition in language evolution.","tags":["language evolution","linguistics"],"title":"Language evolution: Current status and future directions","type":"publication"}]