[{"authors":["admin"],"categories":null,"content":"After doing a research master\u0026rsquo;s in Psycholinguistics, I\u0026rsquo;m now a PhD student and graduate teaching assistant in Cognitive Psychology. I\u0026rsquo;m investigating how conceptual processing is supported by linguistic and sensorimotor brain systems. I use methods such as behavioural and electroencephalographic experiments, corpus analysis, statistics and programming. My CV may be downloaded.\nHow do we understand the meaning of words? My PhD focuses on conceptual processing, and how individual differences affect how people understand the meaning of words.\nThe linguistic and sensorimotor bases of semantic processing: an investigation into the role of individual differences Conceptual processing has been found to draw on two cognitive systems. One is a linguistic system, based on knowledge of words and the relations among them. The other is a sensorimotor system, which draws on our perceptual, motor and affective experience. These two systems contribute to conceptual processing in a sequential manner: the linguistic system is activated first, providing more superficial semantic information, with the sensorimotor system slower to reach peak activation, but providing more refined meaning. We know that the relative importance of each system is modulated by contextual factors (e.g., linguistic context, processing goals, cognitive resources). However, we know much less about how people’s individual cognitive capacities and experience (reading experience, reasoning ability) interact with these systems, which has become a central theme in my work.\nMuch evidence has been found in cognitive psychology for the role of linguistic systems (based on statistical regularities in how words co-occur in language) and sensorimotor systems (based on perceptual, motor, affective experience). The linguistic system is consistently linked to faster, more superficial processing, while the sensorimotor system is linked to slower, more cognitively demanding tasks (Connell \u0026amp; Lynott, 2013; Louwerse, Hutchinson, Tillman, \u0026amp; Recchia, 2015). Recent evidence suggests that individual differences, such as measures of reading experience (e.g., Pexman \u0026amp; Yap, 2018) and of sensorimotor experience (Beilock, et al., 2008; Vukovic \u0026amp; Williams, 2015), provide additional explanatory power for accounts of embodied cognition. For example, linguistic individual differences in vocabulary predict word recognition and information processing speed (Yap et al., 2012). Likewise, sensorimotor individual differences in spatial perspective-taking are also implicated in semantic processing, as participants who tend to naturally adopt an internal perspective in space are more likely to engage in perceptual simulation when reading, by identifying themselves as the protagonist (Vukovic \u0026amp; Williams, 2015).\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"After doing a research master\u0026rsquo;s in Psycholinguistics, I\u0026rsquo;m now a PhD student and graduate teaching assistant in Cognitive Psychology. I\u0026rsquo;m investigating how conceptual processing is supported by linguistic and sensorimotor brain systems. I use methods such as behavioural and electroencephalographic experiments, corpus analysis, statistics and programming. My CV may be downloaded.\nHow do we understand the meaning of words? My PhD focuses on conceptual processing, and how individual differences affect how people understand the meaning of words.","tags":null,"title":"Pablo Bernabeu","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Bernabeu, P."],"categories":null,"content":"Link to dashboard Dashboard with Open data (iNaturalist and BioScan) from Prudic, K.L.; Oliver, J.C.; Brown, B.V.; Long, E.C. Comparisons of Citizen Science Data-Gathering Approaches to Evaluate Urban Butterfly Diversity. Insects 2018, 9, 186. Prudic and colleagues compared citizen science with traditional methods in the measurement of butterfly populations.\nI developed this dashboard independently, after reproducing the analyses of the original study in a Reprohack session.\nMy coding tasks included transforming the data to a long format,\n# There are pseudovariables, that is, observations entered as variables. Since most R processes # need the tidy format, convert below (see https://r4ds.had.co.nz/tidy-data.html). # The specific numbers found through traps and crowdsourcing methods are preserved. BioScan = BioScan %\u0026gt;% pivot_longer( cols = Anthocharis_sara:Vanessa_cardui, names_to = \u0026quot;Species\u0026quot;, values_to = \u0026quot;Number\u0026quot;, values_drop_na = TRUE ) # Compare #str(BioScan) #str(dat) # 928 rows now; the result of 29 pseudo-variables being transposed into # rows, interacting with 32 previous rows, i.e., 29 * 32 = 928.  merging three data sets,\n# The iNaturalist data set presents a challenge slightly different from the pseudovariables found above. # The number of animals of each species must be computed from repeated entries, per site. iNaturalist = merge(iNaturalist, iNaturalist %\u0026gt;% count(species, site, name = 'Number'))  and, as ever, wrangling with the format of the dashboard pages to preserve the format of a table.\nSpecies details {style=\u0026quot;background-color: #FCFCFC;\u0026quot;} ======================================================================= Column {style=\u0026quot;data-width:100%; position:static; height:1000px;\u0026quot;} -----------------------------------------------------------------------  Reference\nBernabeu, P. (2020). Dashboard with data from Prudic, Oliver, Brown, \u0026amp; Long (2018), Comparisons of Citizen Science Data-Gathering Approaches to Evaluate Urban Butterfly Diversity, Insects, 9, 186. Retrieved from https://rpubs.com/pcbernabeu/Butterfly-species-richness-in-LA.\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"c241ee2a921dd32aae269cd114c3b00d","permalink":"/data-dashboards/butterfly-species-richness-in-la/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/data-dashboards/butterfly-species-richness-in-la/","section":"data-dashboards","summary":"Dashboard with open data from a study by Prudic et al. (2018), that compares citizen science with traditional methods in butterfly sampling. Coding tasks included long-transforming, merging, and as ever, wrangling with a table.","tags":["data dashboard","citizen science","butterflies","nature","open data","BioScan","iNaturalist"],"title":"Butterfly species richness in Los Angeles","type":"data-dashboards"},{"authors":[],"categories":["R","data presentation","open data"],"content":" Enhanced data presentation using reproducible documents and dashboards Calendar     Date Activity Event and location Registration Attendance funding Organisation funding    7 May 2020 Workshop University of Manchester Link N8 CIR N8 CIR, SSI Fellowship  10 June 2020 Workshop University of Liverpool Link N8 CIR N8 CIR, SSI Fellowship  27 July 2020 Workshop UK Cognitive Linguistics Conference, University of Birmingham Link  SSI Fellowship  25 Aug 2020 Workshop Lancaster Conference on Infant and Early Child Development, Lancaster University Link    To follow Workshop Durham University Link N8 CIR N8 CIR, SSI Fellowship  To follow Workshop University of York Link N8 CIR N8 CIR, SSI Fellowship  To follow Talk SatRday Newcastle upon Tyne, Newcastle University Link  SSI Fellowship     Background This project offers free activities to learn and practise reproducible data presentation across the UK. Pablo Bernabeu organises these events thanks to a Software Sustainability Institute Fellowship that covers organisation costs on a budget basis, and thanks to further support from the N8 CIR. If you would like to bring any of these events to your institution at no cost, please submit a request (see Contact).\nOpen-source software Programming languages such as R and Python offer free, powerful resources for data processing, visualisation and analysis. Experience in these programs is highly valued in data-intensive disciplines.\n Open data Original data has become a public good in many research fields thanks to cultural and technological advances. On the internet, we can find innumerable data sets from sources such as scientific journals and repositories (e.g., OSF), local and national governments (e.g., London, UK [1, 2]), non-governmental organisations (e.g., data.world), etc. Researchers inside and outside academia nowadays share a lot of their data under attribution licences (e.g., Creative Commons, the UK Open Government Licence, etc.). This allows any external analysts to access these raw data, create (additional) visualisations and analyses, and share these. In society, making data more accessible can demonstrably benefit citizens (despite limitations).\n   Activities Activities comprise free workshops and datathons.\nWorkshops R is a programming language greatly equipped for the creation of reproducible documents and dashboards. Four workshops are offered that cover a suite of interrelated tools—R, R Markdown, data dashboards and Binder environments—, all underlain by reproducible workflows and open-source software.\nEach workshop includes taught and practical sections. The practice provides a chance for participants to experience and address common issues with the code. The level of taught sections is largely tailored to participants; similarly, practice sections are individually adaptable by means of easier and tougher tasks. The duration is also flexible, and some of the workshops can be combined into the same session.\nThe RStudio interface is used in all workshops. Multi-levelled, real code examples are used. Throughout the workshops, and especially in the practice sections, individual questions will be encouraged.\nWorkshop 1: Introduction to R This workshop can serve as an introduction to R or a revision. It demonstrates what can be done in R, and provides resources for individual training. Since the duration is limited, online courses are also recommended (see examples and fee waivers for full content).\n Data structures Packages: general-purpose examples (e.g., tidyverse) and more specific ones (e.g., for statistics or geography) Loading and writing data, in native and foreign formats Wide format (also dubbed ‘untidy’) versus tidy format (also dubbed ‘long’ or ‘narrow’). For most processes in R, data needs to be in a tidy format.   Image from Postma and Goedhart (2019; https://doi.org/10.1371/journal.pbio.3000202.g001).  Combining data sets Data summaries Plots with ggplot2::ggplot() Interactive plots with plotly::ggplotly() Statistics  Linear mixed-effects models (see also a review of practices)  How functions work Debugging. Code errors are known as bugs. They can tiresome, but also interesting sometimes! 😅 Some tips for the first many years of experience include: reading and investigating error messages, in both source and console windows; controlling letter case and typos; closing parentheses and inverted commas; ensuring to have the necessary packages installed and loaded; following the format required by each function. To debug, break up code into subcomponents and test each of those to find out the source of the error. Once we act on that, the best outcome is seeing the code work, but sometimes different errors overlap, in which case we may see one error disappearing before another one appears. Debugging soon leads to proficient information seeking. The search process often begins on an internet search engine and extends to user communities, package documentation, tutorials, blogs… (see video explanation). Advanced debugging tools are also available. Vast availability of free resources on the internet, from Coursera and other MOOC sites, RStudio, University of Glasgow, Carpentries, etc. Community: StackOverflow, RStudio Community, Github issues (e.g., for R packages), etc. Using and contributing back. RStudio Cloud: a personal RStudio environment on the internet   Workshop 2: R Markdown documents Set your input and output in stone with R Markdown. These reports may be enriched with website features (HTML/CSS) and published as websites, PDF, or Word. Moreover, with R packages such as bookdown, bookdownplus, blogdown and flexdashboard, documents can be formatted into websites, digital books and data dashboards. Other useful packages include rmarkdown, knitr, kableExtra and ggplot2.\n Image from bookdownplus package (https://bookdownplus.netlify.com/portfolio/).  Workshop 3: Introduction to data dashboards Data dashboards are web applications used to visualise data in detail through tables and plots. They assist in explaining and accounting for our data processing and analysis. They don’t require any coding from the end user.\n These all-reproducible dashboards are published as websites, and thus, they can include hyperlinks and downloadable files. Some of the R packages used are knitr, kableExtra, reactable, ggplot2, plotly, rmarkdown, flexdashboard and shiny. The aim of this workshop is to practise creating different forms of dashboards—Flexdashboard and Shiny—the latter of which offers greater features, and to practise also with the hosting platforms fitting each type—such as personal websites, RPubs, Binder, Shinyapps and custom servers. A great thing about dashboards is that they may be made very simple, but they can also be taken to the next level using some HTML, CSS or Javascript code (on top of the back-end code present in the R packages used), which is addressed in the next workshop.\n Workshop 4: Binder environments and improving data dashboards Binder Binder is a tool to facilitate public access to software environments—for instance, by publishing an RStudio environment on the internet. Binder can also host Shiny apps. It is generously free for users. After looking at the nuts and bolts of a deployment, participants will be able to deploy their own Binder environments and check the result by the end of the workshop. For this purpose, it’s recommended to have data and R code ready, ideally in a GitHub repository.\n Improving data dashboards We will practise how to improve the functionality of dashboards using some HTML, CSS and Javascript code, which is the basis of websites.\n\u0026lt;!-- Javascript function to enable a hovering tooltip --\u0026gt; \u0026lt;script\u0026gt; $(document).ready(function(){ $(\u0026#39;[data-toggle=\u0026quot;tooltip1\u0026quot;]\u0026#39;).tooltip(); }); \u0026lt;/script\u0026gt;   Trade-offs among dashboards Next, we will practise with three dashboard types—Flexdashboard, Shiny and Flexdashboard-Shiny—and with the suitable hosting platforms. Firstly, the strength of Flexdashboard (example) is its basis on R Markdown, yielding an unmatched user interface (front-end). Secondly, the strength of Shiny (example) is the input reactivity (back-end) it offers, allowing users to download sections of data they select, in various formats. Last, Flexdashboard-Shiny (example) combines the best of both worlds.\n★  Flexdashboard  ★ ★ ★  Shiny  ★ ★ ★ ★ ★  Flexdashboard-Shiny  ★ ★ ★ Flexdashboard types are rendered as an HTML document—simple websites—, and can therefore be easily published on personal sites or RPubs. This is convenient because no special hosting is required. In contrast, Shiny and Flexdashboard-Shiny types offer greater features, but require Shiny servers. Fortunately, the shinyapps.io server is available for free, up to some usage limit. This server can host any of the three dashboards mentioned here. Another good option is presented by Binder environments, which can host the Shiny-type dashboards with no (explicit) limit. Yet, the Flexdashboard-Shiny type cannot be hosted in this server (as of January 2020, at least). Consequently, greater functionality may come at a cost for dashboards that have any considerable traffic, whereas dashboards with low traffic may do well on shinyapps.io. Knowing these trade-offs can help navigate usage limits, save on web hosting fees, and increase the availability of our dashboards online, as we can offer fall-back versions on different platforms, as in the example below:\n … preferred-dashboard (in case of downtime, please visit this alternative)\n Transforming dashboards into the different versions can be as easy as enabling or disabling some features, especially input reactivity. For instance, if we want to downgrade a Flexdashboard-Shiny to a Flexdashboard, to publish it outside of a Shiny server (see example), we must add a setting in the header of the script,\nknit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding) }) and disable reactive features.\n# Number of words selected on sidebar # reactive(cat(paste0(\u0026#39;Words selected below: \u0026#39;, nrow(selected_props()))))  Free accounts and tips App providers have specific terms of use. To begin, shinyapps.io has a free starter license with limited use, where free apps can handle a certain amount of data, and up to five apps may be created. Beyond that, RStudio offers a wide range of subscriptions starting at $9/month.\nMemory and traffic limits of the free shinyapps.io account can sometimes present problems when heavy data data sets are used, or there are many visits to the app. The memory overload issue is often flagged as Shiny cannot use on-disk bookmarking, whereas excessive traffic may see the app not loading. Fortunately, usage limits need not always require a paid subscription or a custom server, thanks to the following workarounds:\n develop app locally as far as possible, and only deploy to shinyapps.io only at the last stage; prune data set, leaving only the necessary data; if necessary, unlink data by splitting it into different sets, reducing computational demands; if necessary, use various apps (five are allowed in each free shinyapps.io account); if necessary, link from the app to a PDF with visualisations requiring heavy, interlinked data. High-resolution plots can be rendered into a PDF document in a snap, using code such as below.\ndf(\u0026#39;List of plots per page\u0026#39;, width = 13, height = 5) print(plot1) print(plot2) # ... print(plot150) dev.off()  Conveniently, all text in a PDF—even in plots—is indexed, so it can be searched [ Ctrl+f / Cmd+f / 🔍 ] (see example). Furthermore, you may also merge the rendered PDF with any other documents.\n  Prerequisites and suggestions for participation in each workshop Necessary: laptop or computer with R and RStudio installed, or access to RStudio Cloud; familiarity with the content of the preceding workshops through the web links herein.\nSuggested: having your own data and R code ready (on a Github repository if participating in Workshop 4); participation in some of the preceding workshops.\n  Datathons: creating reproducible documents and dashboards In these coding meetups, participants collaborate to create reproducible documents or dashboards using the data and software they prefer (see examples). Since the work can be split across different people and sections, some nice products may be achieved within a session. Any programming languages may be used.\n Data used: academic or non-academic data of your own or from open-access sources such as OSF, scientific journals, governments, international institutions, NGOs, etc.\n Inspired by the great Reprohacks, content suggestions are encouraged. That is, if you’d like to have a reproducible document or dashboard created for a certain, open-access data set, please let us know, and some participants may take it on. Suggestions may be posted as issues or emailed to p.bernabeu@lancaster.ac.uk.  Purposes\n collaborating to visualise data in novel ways using reproducible documents or interactive dashboards. For this purpose, participants sometimes draw on additional data to look at a bigger picture;\n reflecting on the process by reviewing the techniques applied and challenges encountered.\n  Output: A key aspect of datathons is the creation of output. Documents and dashboards are (co-)authored by the participants who work on them, who can then publish them on their websites, or on RPubs, Binder, Shinyapps or custom servers. Time constraints notwithstanding, a lot of this output may be very enticing for further development by the same participants, or even by other people if the code is shared online. Just like with data, an attribution licence can be attached to the code.\n  Prerequisites and suggestions for participation in datathons Necessary: basic knowledge of reproducible documents or dashboards.\nSuggested: familiarity with the development of reproducible documents or dashboards; an idea about the data you’d like to work with and the kind of document or dashboard you want to create.\n   Contact Please submit any queries or requests by posting an issue or emailing p.bernabeu@lancaster.ac.uk.\n ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"c8512913ece54a96f28a968061c98dc2","permalink":"/2020/01/01/data-is-present-workshops-and-datathons/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/2020/01/01/data-is-present-workshops-and-datathons/","section":"post","summary":"This project offers free activities to learn and practise reproducible data presentation across the UK. Pablo Bernabeu organises these events thanks to a Software Sustainability Institute Fellowship that covers organisation costs on a budget basis, and thanks to further support from the N8 CIR. Programming languages such as R and Python offer free, powerful resources for data processing, visualisation and analysis. Experience in these programs is highly valued in data-intensive disciplines. Original data has become a public good in many research fields thanks to cultural and technological advances. On the internet, we can find innumerable data sets from sources such as scientific journals and repositories (e.g., OSF), local and national governments, non-governmental organisations (e.g., data.world), etc. Activities comprise free workshops and datathons.","tags":["workshop","datathon","data presentation","dashboard","reproducibility","open science","open data","R","R Shiny","Flexdashboard","Software Sustainability Institute Fellowship","N8 CIR"],"title":"Data is present: workshops and datathons","type":"post"},{"authors":["Chen, S.-C.","Szabelzka, A.","Chartier, C. R.","Kekecs, Z.","Lynott, D.","**Bernabeu, P.**","Schmidt, K.","et al."],"categories":null,"content":"  Video demonstration of the procedure followed in our lab   Reference Chen, S., Szabelska, A., Chartier, C. R., Kekecs, Z., Lynott, D., Bernabeu, P., … Schmidt, K. (conditionally accepted). Investigating object orientation effects across 14 languages. Psychonomic Bulletin and Review. https://doi.org/10.31234/osf.io/t2pjv/\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"9e4b60290f7493777dd1254f0a6e8953","permalink":"/publication/chen-etal-inprep/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/chen-etal-inprep/","section":"publication","summary":"Mental simulation theories of language comprehension propose that people automatically create mental representations of real objects. Evidence from sentence-picture verification tasks has shown that people mentally represent various visual properties such as shape, color, and size. However, the evidence for mental simulations of object orientation is limited. We report a study that investigates the match advantage of object orientation across speakers of different languages. This multi-laboratory project aims to achieve two objectives. First, we examine the replicability of the match advantage of object orientation across multiple languages and laboratories. Second, we will use a mental rotation task to measure participants’ mental imagery after the sentence-picture verification task. The relationship between the participants’ performance of the two tasks will provide a cross-linguistic examination of perceptual simulation processes. With the (broad) evaluation of individual mental imagery ability and potential linguistic moderators, we expect a robust estimation of match advantage of object orientation.","tags":["conceptual modality switch","conceptual processing","reading","event-related potentials","cognition","psycholinguistics","Psychological Science Accelerator"],"title":"Investigating object orientation effects across 14 languages","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["**Bernabeu, P.**","Tillman, R."],"categories":null,"content":"Reference Bernabeu, P., \u0026amp; Tillman, R. (2019). More refined typology and design in linguistic relativity: The case of motion event encoding. Dutch Journal of Applied Linguistics. http://doi.org/10.1075/dujal.15019.ber/\n","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"d966dfb0977cb54bb103ccd6372d0b42","permalink":"/publication/bernabeu-tillman-2019/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/bernabeu-tillman-2019/","section":"publication","summary":"Linguistic relativity is the influence of language on other realms of cognition. For instance, the way movement is expressed in a person’s native language may influence how they perceive movement. Motion event encoding (MEE) is usually framed as a typological dichotomy. Path-in-verb languages tend to encode path information within the verb (e.g., ‘leave’), whereas manner-in-verb languages encode manner (e.g., ‘jump’). The results of MEE-based linguistic relativity experiments range from no effect to effects on verbal and nonverbal cognition. Seeking a more definitive conclusion, we propose linguistic and experimental enhancements. First, we examine state-of-the-art typology, suggesting how a recent MEE classification across twenty languages ( Verkerk, 2014 ) may enable more powerful analyses. Second, we review procedural challenges such as the influence of verbal thought and second-guessing in experiments. To tackle these challenges, we propose distinguishing verbal and nonverbal subgroups, and having enough filler items. Finally we exemplify this in an experimental design.","tags":["linguistic relativity","psycholinguistics","linguistics"],"title":"More refined typology and design in linguistic relativity: the case of motion event encoding","type":"publication"},{"authors":null,"categories":["statistics","R"],"content":" Principal Component Analysis (PCA) is a technique used to find the core components that underlie different variables. It comes in very useful whenever doubts arise about the true origin of three or more variables. There are two main methods for performing a PCA: naive or less naive. In the naive method, you first check some conditions in your data which will determine the essentials of the analysis. In the less-naive method, you set those yourself based on whatever prior information or purposes you had. The latter method is appropriate when you already have enough information about the intercorrelations, or when you are required to select a specific number of components. I will tackle the naive method, mainly by following the guidelines in Field, Miles, and Field (2012), with updated code where necessary. A manual by Charles M. Friel (Sam Houston State University) was also useful.\nThe ‘naive’ approach is characterized by a first stage that checks whether the PCA should actually be performed with your current variables, or if some should be removed. The variables that are accepted are taken to a second stage which identifies the number of principal components that seem to underlie your set of variables.\nSTAGE 1. Determine whether PCA is appropriate at all, considering the variables  Variables should be inter-correlated enough but not too much. Field et al. (2012) provide some thresholds, suggesting that no variable should have many correlations below .30, or any correlation at all above .90. Thus, in the example here, variable Q06 should probably be excluded from the PCA.\n Bartlett’s test, on the nature of the intercorrelations, should be significant. Significance suggests that the variables are not an ‘identity matrix’ in which correlations are a sampling error.\n KMO (Kaiser-Meyer-Olkin), a measure of sampling adequacy based on common variance (so similar purpose as Bartlett’s). As Field et al. review, ‘values between .5 and .7 are mediocre, values between .7 and .8 are good, values between .8 and .9 are great and values above .9 are superb’ (p. 761). There’s a general score as well as one per variable. The general one will often be good, whereas the individual scores may more likely fail. Any variable with a score below .5 should probably be removed, and the test should be run again.\n Determinant: A formula about multicollinearity. The result should preferably fall below .00001. Note that some of these tests are run on the dataframe and others on a correlation matrix of the data, as distinguished below.\n   # Necessary libraries library(ltm) library(lattice) library(psych) library(car) library(pastecs) library(scales) library(ggplot2) library(arules) library(plyr) library(Rmisc) library(GPArotation) library(gdata) library(MASS) library(qpcR) library(dplyr) library(gtools) library(Hmisc) # Select variables of interest for the PCA dataset = mydata[, c(\u0026#39;select_var1\u0026#39;,\u0026#39;select_var1\u0026#39;,\u0026#39;select_var2\u0026#39;,\u0026#39;select_var3\u0026#39;,\u0026#39;select_var4\u0026#39;,\u0026#39;select_var5\u0026#39;,\u0026#39;select_var6\u0026#39;,\u0026#39;select_var7\u0026#39;)] # Create matrix: some tests will require it data_matrix = cor(dataset, use = \u0026#39;complete.obs\u0026#39;) # See intercorrelations round(data_matrix, 2) # Bartlett\u0026#39;s cortest.bartlett(dataset) # KMO (Kaiser-Meyer-Olkin) KMO(data_matrix) # Determinant det(data_matrix)  \n STAGE 2. Identify number of components (aka factors) In this stage, principal components (formally called ‘factors’ at this stage) are identified among the set of variables.\n The identification is done through a basic, ‘unrotated’ PCA. The number of components set a priori must equal the number of variables that are being tested.  # Start off with unrotated PCA pc1 = psych::principal(dataset, nfactors = length(dataset), rotate=\u0026quot;none\u0026quot;) pc1  Below is an example result:\n## Principal Components Analysis ## Call: psych::principal(r = eng_prop, nfactors = 3, rotate = \u0026quot;none\u0026quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PC1 PC2 PC3 h2 u2 com ## Aud_eng -0.89 0.13 0.44 1 -2.2e-16 1.5 ## Hap_eng 0.64 0.75 0.15 1 1.1e-16 2.0 ## Vis_eng 0.81 -0.46 0.36 1 -4.4e-16 2.0 ## ## PC1 PC2 PC3 ## SS loadings 1.87 0.79 0.34 ## Proportion Var 0.62 0.26 0.11 ## Cumulative Var 0.62 0.89 1.00 ## Proportion Explained 0.62 0.26 0.11 ## Cumulative Proportion 0.62 0.89 1.00 ## ## Mean item complexity = 1.9 ## Test of the hypothesis that 3 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0 ## with the empirical chi square 0 with prob \u0026lt; NA ## ## Fit based upon off diagonal values = 1  Among the columns, there are first the correlations between variables and components, followed by a column (h2) with the ‘communalities’. If less factors than variables had been selected, communality values would be below 1. Then there is the uniqueness column (u2): uniqueness is equal to 1 minus the communality. Next is ‘com’, which reflects the complexity with which a variable relates to the principal components. Those components are precisely found below. The first row contains the sums of squared loadings, or eigenvalues, namely, the total variance explained by each linear component. This value corresponds to the number of units explained out of all possible factors (which were three in the above example). The rows below all cut from the same cloth. Proportion var = variance explained over a total of 1. This is the result of dividing the eigenvalue by the number of components. Multiply by 100 and you get the percentage of total variance explained, which becomes useful. In the example, 99% of the variance has been explained. Aside from the meddling maths, we should actually expect 100% there because the number of factors equaled the number of variables. Cumulative var: variance added consecutively up to the last component. Proportion explained: variance explained over what has actually been explained (only when variables = factors is this the same as Proportion var). Cumulative proportion: the actually explained variance added consecutively up to the last component (Field et al., 2012).\nAccording to Field et al. (2012), two criteria will determine the number of components to select for the next stage:\n Kaiser’s criterion: components with SS loadings \u0026gt; 1. In our example, only PC1.  A more lenient alternative is Joliffe’s criterion, SS loadings \u0026gt; .7.\n Scree plot: the number of points after point of inflexion. For this plot, call:  plot(pc1$values, type = \u0026#39;b\u0026#39;) Imagine a straight line from the first point on the right. Once this line bends considerably, count the points after the bend and up to the last point on the left. The number of points is the number of components to select. The example here is probably the most complicated (two components were finally chosen), but normally it’s not difficult.\nBased on both criteria, go ahead and select the definitive number of components.\n STAGE 3. Run definitive PCA Run a very similar command as you did before, but now with a more advanced method. The first PCA, a heuristic one, worked essentially on the inter-correlations. The definitive PCA, in contrast, will implement a prior shuffling known as ‘rotation’, to ensure that the result is robust enough (just like cards are shuffled). Explained variance is captured better this way. The go-to rotation method is the orthogonal, or ‘varimax’ (though others may be considered too).\n# Now with varimax rotation, Kaiser-normalized by default: pc2 = psych::principal(dataset, nfactors=2, rotate = \u0026quot;varimax\u0026quot;, scores = TRUE) pc2 pc2$loadings # Healthcheck pc2$residual pc2$fit pc2$communality  According to Field et al. (2012), we would want:\n Less than half of residuals with absolute values \u0026gt; 0.05 Model fit \u0026gt; .9 All communalities \u0026gt; .7  If any of this fails, consider changing the number of factors. Next, the rotated components that have been ‘extracted’ from the core of the set of variables can be added to the dataset. This would enable the use of these components as new variables that might prove powerful and useful (as in this research).\ndataset = cbind(dataset, pc2$scores) summary(dataset$RC1, dataset$RC2)  STAGE 4. Determine ascription of each variable to components Check the main summary by just calling pc2, and see how each variable correlates with the rotated components. This is essential because it reveals how variables load on each component, or in other words, to which component a variable belongs. For instance, the table shown here belongs to a study about meaning of words. These results suggest that the visual and haptic modalities of words are quite related, whereas the auditory modality is relatively unique. When the analysis works out well, a cut-off point of r = .8 may be applied for considering a variable as part of a component.\n STAGE 5. Enjoy the plot The plot is perhaps the coolest part about PCA. It really makes an awesome illustration of the power of data analysis.\nggplot(eng_props, aes(RC1, RC2, label = as.character(main_eng))) + stat_density2d (color = \u0026quot;gray87\u0026quot;) + geom_text(size = ifelse(eng_props$word_eng %in% w_set, 12, 7), fontface = ifelse(eng_props$word_eng %in% w_set, \u0026#39;bold\u0026#39;, \u0026#39;plain\u0026#39;)) + geom_point(data=eng_props[eng_props$word_eng %in% w_set,], pch=21, fill=NA, size=14, stroke=2, alpha=.6) + labs(subtitle=\u0026#39;(Data from Lynott \u0026amp; Connell, 2009)\u0026#39;, x = \u0026quot;Varimax-rotated Principal Component 1\u0026quot;, y = \u0026quot;Varimax-rotated Principal Component 2\u0026quot;) + theme_bw() + theme( plot.background = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), axis.line = element_line(color = \u0026#39;black\u0026#39;), axis.title.x = element_text(colour = \u0026#39;black\u0026#39;, size = 23, margin=margin(15,15,15,15)), axis.title.y = element_text(colour = \u0026#39;black\u0026#39;, size = 23, margin=margin(15,15,15,15)), axis.text.x = element_text(size=16), axis.text.y = element_text(size=16), plot.title = element_text(hjust = 0.5, size = 32, face = \u0026quot;bold\u0026quot;, margin=margin(15,15,15,15)), plot.subtitle = element_text(hjust = 0.5, size = 20, margin=margin(2,15,15,15)) ) + geom_label_repel(data = eng_props[eng_props$word_eng %in% w_set,], aes(label = word_eng), size = 8, alpha = 0.77, color = \u0026#39;black\u0026#39;, box.padding = 1.5 )  Below is an example combining PCA plots with code similar to the above. These plots illustrate something further with regard to the relationships among modalities. In property words, the different modalities spread out more clearly than they do in concept words. This makes sense because in language, properties define concepts (see more).\nAn example of these analyses is available in available in this RStudio environment, in the norms.R script.\nReferences\nField, A. P., Miles, J., \u0026amp; Field, Z. (2012). Discovering Statistics Using R. London, UK: Sage.\n ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"3eb9409a80519b1c5991840b81ffbf56","permalink":"/2018/01/01/naive-principal-component-analysis-in-r/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/2018/01/01/naive-principal-component-analysis-in-r/","section":"post","summary":"Principal Component Analysis (PCA) is a technique used to find the core components that underlie different variables. It comes in very useful whenever doubts arise about the true origin of three or more variables. There are two main methods for performing a PCA: naive or less naive. In the naive method, you first check some conditions in your data which will determine the essentials of the analysis. In the less-naive method, you set those yourself based on whatever prior information or purposes you had. The 'naive' approach is characterized by a first stage that checks whether the PCA should actually be performed with your current variables, or if some should be removed. The variables that are accepted are taken to a second stage which identifies the number of principal components that seem to underlie your set of variables.","tags":["principal component analysis","statistics","dimensionality reduction","R"],"title":"Naive principal component analysis in R","type":"post"},{"authors":[],"categories":["psycholinguistics"],"content":" Throughout the 1990s, two opposing theories were used to explain how people understand texts, later bridged by the Landscape Model of reading (van den Broek, Young, Tzeng, \u0026amp; Linderholm, 1999). A review is offered below, including a schematic representation of the Landscape Model.\nMemory-based view The memory-based view presented reading as an autonomous, unconscious, effortless process. Readers were purported to achieve an understanding of a text as a whole by combining the concepts, and implications readily afforded, in the text with their own background knowledge (Myers \u0026amp; O’Brien, 1998; O’Brien \u0026amp; Myers, 1999). This memory-based view did not include a conscious (re)activation of meaning. Arguably, this absence raises the question of how readers could recover the meaning if ever distracted while reading.\n Constructionist view The constructionist view contended that readers make strategic, time-consuming efforts to access prior text and background knowledge (Graesser, Singer, \u0026amp; Trabasso, 1994; Singer, Graesser, \u0026amp; Trabasso, 1994). A possible challenge for this view is in leisure reading. If reading is effortful, how could it be explained when people become absorbed in leisure reading for hours, and enjoy it? How could this activity become as automatic and fast as it often does?\n Landscape Model The memory-based and the constructionist views were bridged in a proposal called the Landscape Model (van den Broek et al., 1999). With a step-based model, van den Broek et al. argued that strategic (re)activations are available to the reader, but need not always be used (see also Converse, 2018). Figure 1 illustrates the dynamic, often cyclical processes.\nFigure 1. Mindmap of van den Broek et al.’s Landscape Model of reading comprehension. Retrieved from https://doi.org/10.6084/m9.figshare.1591215.\n The Landscape Model was applied to the context of discourse analysis by Yeari and van den Broek (2011). The authors noted that discourse analysts may find it useful to adopt a top-down, inductive approach to their task. That is, suppressing the natural, incremental route of meaning making in reading, discourse analysts may want to read through the text to gain a general idea first, before tackling a detailed analysis (see also Bell, 2011).\n References Bell, A. (2011). Re-constructing Babel: Discourse analysis, hermeneutics and the Interpretive Arc. Discourse Studies, 13(5), 519–568. https://doi.org/10.1177/1461445611412699.\nConverse, N. E. (2018). The Use of Explicit Comprehension Strategies During Oral Instruction of Informational Text Structures and the Effect on First-graders’ Listening Comprehension (Doctoral dissertation). Retrieved from https://digitalcommons.usu.edu/etd/7305.\nGraesser, A., Singer, M., \u0026amp; Trabasso, T. (1994). Constructing inferences during narrative comprehension. Psychological Review, 101(3), 371–395. https://doi.org/10.1037/0033-295X.101.3.371.\nMyers, J. L., \u0026amp; O’Brien, E. J. (1998). Accessing the discourse representation during reading. Discourse Processes, 26(2-3), 131–157. https://doi.org/10.1080/01638539809545042.\nO’Brien, E. J., \u0026amp; Myers, J. L. (1999). Text comprehension: A view from the bottom up. In S. R. Goldman, A. C. Graesser \u0026amp; P. van den Broek P (Eds.), Narrative Comprehension, Causality, and Coherence: Essays in Honor of Tom Trabasso (pp. 35-53). Mahwah, NJ: Lawrence Erlbaum Associates.\nSinger, M., Graesser, A. C., \u0026amp; Trabasso, T. (1994). Minimal or global inference during reading. Journal of Memory and Language, 33(4), 421–441. https://doi.org/10.1006/jmla.1994.1020.\nvan den Broek, P., Young, M., Tzeng, Y., \u0026amp; Linderholm, T. (1999). The landscape model of reading. In H. van Oostendorp \u0026amp; S. R. Goldman (Eds.), The construction of mental representations during reading (pp. 71-98). Mahwah, NJ: Erlbaum.\nYeari, M., \u0026amp; van den Broek, P. (2011). A cognitive account of discourse understanding and discourse interpretation: The Landscape Model of reading. Discourse Studies, 13(5), 635-643. https://doi.org/10.1177/1461445611412748.\n ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"4ee49038362d35c523c3610202022c17","permalink":"/2018/01/01/review-of-the-landscape-model-of-reading-composition-dynamics-and-application/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/2018/01/01/review-of-the-landscape-model-of-reading-composition-dynamics-and-application/","section":"post","summary":"Throughout the 1990s, two opposing theories were used to explain how people understand texts, later bridged by the Landscape Model of reading (van den Broek, Young, Tzeng, \u0026amp; Linderholm, 1999). A review is offered below, including a schematic representation of the Landscape Model.\nMemory-based view The memory-based view presented reading as an autonomous, unconscious, effortless process. Readers were purported to achieve an understanding of a text as a whole by combining the concepts, and implications readily afforded, in the text with their own background knowledge (Myers \u0026amp; O’Brien, 1998; O’Brien \u0026amp; Myers, 1999).","tags":["reading","psycholinguistics","cognition"],"title":"Review of the Landscape Model of reading: composition, dynamics and application","type":"post"},{"authors":[],"categories":["psycholinguistics"],"content":"Research has extensively investigated whether conceptual processing is modality-specific—that is, whether meaning is processed to a large extent on the basis of perceptual and motor affordances (Barsalou, 2016). This possibility challenges long-established theories. It suggests a strong link between physical experience and language which is not borne out of the paradigmatic arbitrariness of words (see Lockwood, Dingemanse, \u0026amp; Hagoort, 2016). Modality-specificity also clashes with models of language that have no link to sensory and motor systems (Barsalou, 2016).\nDownload article   Download master's thesis   In the Conceptual Modality Switch (CMS) paradigm, participants perform a property verification task, deciding whether certain property words can reasonably describe concept words. Covertly, the conceptual modality of consecutive trials is manipulated in order to produce specific switches in conceptual modality. For instance, after the trial Soundless Answer, which is primarily auditory, the following trial may match in modality—Loud Welcome—or mismatch—Fine Selection (visual).\nModality switches incur processing costs, as reflected in brain signals linked to semantic violation, and in longer response times (RTs) (Scerrati, Lugli, Nicoletti, \u0026amp; Borghi, 2016). This effect suggests that perceptual features of concepts are accessed during conceptual processing. More recently, however, the CMS effect was reanalysed using a non-perceptual alternative. Louwerse and Connell (2011) found that language statistics (the co-occurrence of words in a language) were able to approximately predict visual/haptic, olfactory/gustatory, and auditory modalities, but not the subtler differences between visual and haptic and between olfactory and gustatory, which seemed to be reserved for perceptual simulations. Moreover, faster response times (RTs) were best explained by language statistics, whereas slower RTs were best explained by perceptual simulations.\nThe time course of word processing is important. Research suggests that word processing spans one second, during which different processes—semantic and post-semantic—gradually accumulate (Hauk, 2016). The later an effect, the more reasons to question it. Yet, having an early emergence does not either make an effect lexicosemantic, as the meaning encoded could have gone through working memory before activating the actual system of interest, e.g., sensorimotor (Mahon \u0026amp; Caramazza, 2008). Research also suggests that modal systems may contribute to conceptual processing early on—within 200 ms (Vukovic, Feurra, Shpektor, Myachykov, \u0026amp; Shtyrov, 2017). Thus, measuring effects online may prove valuable.\nExperiment Bernabeu, Willems and Louwerse (2017) investigated whether CMS reflects a functionally relevant process of simulation or instead arises only after basic conceptual processing has been attained. We also examined whether different processing systems, amodal and modal, may compatibly operate.\nWe measured CMS online by time-locking Event-Related brain Potentials (ERPs) to the onset of the first word in the target trials, in order to assess how strongly CMS may be influenced by post-semantic processes. Previous research would predict an increase in the CMS effect over time because earlier processing is relatively amodal (Louwerse \u0026amp; Hutchinson, 2012).\nWe tested the compatibility of amodal and modal processing by drawing on Louwerse and Connell’s (2011) findings. In this conceptual replication, we split participants into a Quick and a Slow group based on RT. Maintaining CMS as a within-subjects factor, we predicted that the larger modality switches (e.g., auditory to visual) would be picked up equally by both groups, whereas the subtler switches (e.g., haptic to visual) would be picked up only—or more clearly—by the Slow group.\nMethod The stimuli were normed (Bernabeu, Louwerse, \u0026amp; Willems, in prep.). Three CMS conditions were created—Auditory-to-visual, Haptic-to-visual, Visual-to-visual—, each with 36 target trials. The property verification task was pretested valid (N = 19).\nResults All participants but one responded correctly to over half of the trials, with an overall accuracy of 63%.\nERPs showed a CMS effect from time window 1 on, larger after 350 ms. It appeared with both switch conditions, and was characterized by a more negative amplitude for the switch conditions compared to the no-switch condition. It was generally stronger in the posterior brain regions, and in the Slow group. The results are illustrated in the figure below, which includes 95% Confidence Intervals and time windows.\nThe analysis was done with Linear Mixed Effects models. Final models presented good fits, with R^2 ranging from .748 to .862. First, the CMS effect in time window 1 was confirmed significant. Such an early emergence is unprecedented in the CMS literature, and it may have been enabled by the time-locking of ERPs to the first word in target trials. In this time window, the only process not lexicosemantic is possibly working memory (Hauk, 2016), and therefore this early emergence adds support to the possibility that CMS was directly caused by perceptual simulation.\nWhereas, in time window 1, the effect was circumscribed to an interaction with Brain Area, by time window 2 a main effect of CMS emerged, and in windows 3 and 4 the only critical effect was CMS.\nBonferroni-corrected, planned ANOVA contrasts into CMS conditions revealed that the no-switch condition differed significantly from the switch conditions. By contrast, the switch conditions (Haptic-to-visual and Auditory-to-visual) hardly differed from each other, underscoring the CMS effect.\nAlthough the interaction of Group and CMS was only significant in time windows 1 and 2, windows 2 to 4 presented a pattern fitting our predictions (Louwerse \u0026amp; Connell, 2011). While the Slow group picked up the switches across all modalities similarly, the Quick group picked up the Auditory-to-visual switch more clearly than the Haptic-to-visual switch.\nDiscussion Results broadly suggest that cognition may operate on qualitatively different systems for the same task. In conceptual processing, one of these systems appears to be modality-independent, potentially based on linguistic co-occurrences, whereas another system is modality-specific, linked to physical experience.\nResources   Paper  Master\u0026rsquo;s thesis  Data and code  Analysis environment in RStudio  References Barsalou, L. W. (2016). On staying grounded and avoiding quixotic dead ends. Psychonomic Bulletin \u0026amp; Review, 23.\nBernabeu, P., Louwerse, M. M., \u0026amp; Willems, R. M. (in prep.). Modality exclusivity norms for 747 properties and concepts in Dutch: a replication of English. Retrieved from https://osf.io/brkjw/\nBernabeu, P., Willems, R. M., \u0026amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, \u0026amp; E. J. Davelaar (Eds.), Proceedings of the 39th Annual Conference of the Cognitive Science Society (pp. 1629-1634). Austin, TX: Cognitive Science Society.\nHauk, O. (2016). Only time will tell—Why temporal information is essential for our neuroscientific understanding of semantics. Psychonomic Bulletin \u0026amp; Review, 23, 4, 1072-1079.\nLockwood, G., Hagoort, P., \u0026amp; Dingemanse, M. (2016). How iconicity helps people learn new words: neural correlates and individual differences in sound-symbolic bootstrapping. Collabra, 2, 1, 7.\nLouwerse, M., \u0026amp; Connell, L. (2011). A taste of words: linguistic context and perceptual simulation predict the modality of words. Cognitive Science, 35, 2, 381-98.\nLouwerse, M., \u0026amp; Hutchinson, S. (2012). Neurological evidence linguistic processes precede perceptual simulation in conceptual processing. Frontiers in Psychology, 3, 385.\nMahon, B. Z., \u0026amp; Caramazza, A. (2008). A critical look at the Embodied Cognition Hypothesis and a new proposal for grounding conceptual content. Journal of Physiology - Paris, 102, 59-70.\nScerrati, E., Lugli, L., Nicoletti, R., \u0026amp; Borghi, A. M. (2016). The Multilevel Modality-Switch Effect: What Happens When We See the Bees Buzzing and Hear the Diamonds Glistening. Psychonomic Bulletin \u0026amp; Review, doi:10.3758/s13423-016-1150-2.\nVukovic, V., Feurra, M., Shpektor, A., Myachykov, A., \u0026amp; Shtyrov, Y. (2017). Primary motor cortex functionally contributes to language comprehension: An online rTMS study. Neuropsychologia, 96, 222-229.\nBonus: A conference poster with a few further analyses  \n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"004e74346bb8923e12a67b0fb0c2fa7e","permalink":"/2017/01/01/modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing-evidence-from-erps/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/2017/01/01/modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing-evidence-from-erps/","section":"post","summary":"Research has extensively investigated whether conceptual processing is modality-specific—that is, whether meaning is processed to a large extent on the basis of perceptual and motor affordances (Barsalou, 2016). This possibility challenges long-established theories. It suggests a strong link between physical experience and language which is not borne out of the paradigmatic arbitrariness of words (see Lockwood, Dingemanse, \u0026amp; Hagoort, 2016). Modality-specificity also clashes with models of language that have no link to sensory and motor systems (Barsalou, 2016).","tags":["psycholinguistics","conceptual processing","experiment","event-related potentials","language comprehension","open data","cognition","conceptual modality switch","modality exclusivity norms","reading"],"title":"[Blog] Modality switch effects emerge early and increase throughout conceptual processing: evidence from ERPs","type":"post"},{"authors":["Bernabeu, P."],"categories":null,"content":"Link to dashboard Content\nThe data is from a psychology experiment on the comprehension of words, in which electroencephalographic (EEG) responses were measured. The data are presented in plots spanning 800 milliseconds (the duration of word processing). The aim of this Shiny app is to facilitate the exploration of the data by researchers and the public. Users can delve into the different sections of the data. In a hierarchical order, these sections are groups of participants, individual participants, brain areas, and electrodes.\nShiny apps in science\nBy creating this app, I tried to reach beyond the scope of current open science, which is often confined to files shared on data repositories. I believe that Shiny apps will become general practice in science within a few years ( see blog post or slides for more information).\nTechnical details\nI made use of tabs on the top of the dashboard in order to keep the side bar from having too many widgets. I adjusted the appearance of these tabs, and by means of \u0026lsquo;reactivity\u0026rsquo; conditions, also modified the inputs in the sidebar depending on the active tab.\nmainPanel( tags$style(HTML(' .tabbable \u0026gt; .nav \u0026gt; li \u0026gt; a {background-color:white; color:#3E454E} .tabbable \u0026gt; .nav \u0026gt; li \u0026gt; a:hover {background-color:#002555; color:white} .tabbable \u0026gt; .nav \u0026gt; li[class=active] \u0026gt; a {background-color:#ECF4FF; color:black} .tabbable \u0026gt; .nav \u0026gt; li[class=active] \u0026gt; a:hover\t{background-color:#E7F1FF; color:black} ')), tabsetPanel(id='tabvals', tabPanel(value=1, h4(strong('Group \u0026amp; Electrode')), br(), plotOutput('plot_GroupAndElectrode'), h5(a(strong('See plots with 95% Confidence Intervals'), href='https://osf.io/2tpxn/', target='_blank'), style='text-decoration: underline;'), downloadButton('downloadPlot.1', 'Download HD plot'), br(), br(), # EEG montage img(src='https://preview.ibb.co/n7qiYR/EEG_montage.png', height=500, width=1000)), tabPanel(value=2, h4(strong('Participant \u0026amp; Area')), br(), plotOutput('plot_ParticipantAndLocation'), h5(a(strong('See plots with 95% Confidence Intervals'), href='https://osf.io/86ch9/', target='_blank'), style='text-decoration: underline;'), downloadButton('downloadPlot.2', 'Download HD plot'), br(), br(), # EEG montage img(src='https://preview.ibb.co/n7qiYR/EEG_montage.png', height=500, width=1000)), tabPanel(value=3, h4(strong('Participant \u0026amp; Electrode')), br(), plotOutput('plot_ParticipantAndElectrode'), br(), downloadButton('downloadPlot.3', 'Download HD plot'), br(), br(), # EEG montage img(src='https://preview.ibb.co/n7qiYR/EEG_montage.png', height=500, width=1000)), tabPanel(value=4, h4(strong('OLD Group \u0026amp; Electrode')), br(), plotOutput('plot_OLDGroupAndElectrode'), h5(a(strong('See plots with 95% Confidence Intervals'), href='https://osf.io/dvs2z/', target='_blank'), style='text-decoration: underline;'), downloadButton('downloadPlot.4', 'Download HD plot'), br(), br(), # EEG montage img(src='https://preview.ibb.co/n7qiYR/EEG_montage.png', height=500, width=1000)) ),  The data set was fairly large, considering the fact that it\u0026rsquo;s hosted with the free plan. In order to lighten the processing, I split the data into various files, reducing the total size. Furthermore, I outsourced a particularly heavy set of plots (those with Confidence Intervals) to PDF files, to which I linked in the app.\nh5(a(strong('See plots with 95% Confidence Intervals'), href='https://osf.io/dvs2z/', target='_blank'), style='text-decoration: underline;'),  I also used web links to the published paper and raw data, as well as to the server and ui scripts. These files, along with the data, are publicly available in this repository; they may be accessed within the \u0026ldquo;Files\u0026rdquo; section, by opening the folders \u0026ldquo;ERPs\u0026rdquo; -\u0026gt; \u0026ldquo;Analyses of ERPs averaged across trials\u0026rdquo; -\u0026gt; \u0026ldquo;Shiny app\u0026rdquo;.\nAnother feature I added was the download button.\n# From server.R script spec_title = paste0('ERP waveforms for ', input$var.Group, ' Group, Electrode ', input$var.Electrodes.1, ' (negative values upward; time windows displayed)') plot_GroupAndElectrode = ggplot(df2, aes(x=time, y=-microvolts, color=condition)) + geom_rect(xmin=160, xmax=216, ymin=7.5, ymax=-8, color = 'grey75', fill='black', alpha=0, linetype='longdash') + geom_rect(xmin=270, xmax=370, ymin=7.5, ymax=-8, color = 'grey75', fill='black', alpha=0, linetype='longdash') + geom_rect(xmin=350, xmax=550, ymin=8, ymax=-7.5, color = 'grey75', fill='black', alpha=0, linetype='longdash') + geom_rect(xmin=500, xmax=750, ymin=7.5, ymax=-8, color = 'grey75', fill='black', alpha=0, linetype='longdash') + geom_line(size=1, alpha = 1) + scale_linetype_manual(values=colours) + scale_y_continuous(limits=c(-8.38, 8.3), breaks=seq(-8,8,by=1), expand = c(0,0.1)) + scale_x_continuous(limits=c(-208,808),breaks=seq(-200,800,by=100), expand = c(0.005,0), labels= c('-200','-100 ms','0','100 ms','200','300 ms','400','500 ms','600','700 ms','800')) + ggtitle(spec_title) + theme_bw() + geom_vline(xintercept=0) + annotate(geom='segment', y=seq(-8,8,1), yend=seq(-8,8,1), x=-4, xend=8, color='black') + annotate(geom='segment', y=-8.2, yend=-8.38, x=seq(-200,800,100), xend=seq(-200,800,100), color='black') + geom_segment(x = -200, y = 0, xend = 800, yend = 0, size=0.5, color='black') + theme(legend.position = c(0.100, 0.150), legend.background = element_rect(fill='#EEEEEE', size=0), axis.title=element_blank(), legend.key.width = unit(1.2,'cm'), legend.text=element_text(size=17), legend.title = element_text(size=17, face='bold'), plot.title= element_text(size=20, hjust = 0.5, vjust=2), axis.text.y = element_blank(), axis.text.x = element_text(size = 14, vjust= 2.12, face='bold', color = 'grey32', family='sans'), axis.ticks=element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), plot.margin = unit(c(0.1,0.1,0,0), 'cm')) + annotate('segment', x=160, xend=216, y=-8, yend=-8, colour = 'grey75', size = 1.5) + annotate('segment', x=270, xend=370, y=-8, yend=-8, colour = 'grey75', size = 1.5) + annotate('segment', x=350, xend=550, y=-7.5, yend=-7.5, colour = 'grey75', size = 1.5) + annotate('segment', x=500, xend=750, y=-8, yend=-8, colour = 'grey75', size = 1.5) + scale_fill_manual(name = 'Context / Target trial', values=colours) + scale_color_manual(name = 'Context / Target trial', values=colours) + guides(linetype=guide_legend(override.aes = list(size=1.2))) + guides(color=guide_legend(override.aes = list(size=2.5))) + # Print y axis labels within plot area: annotate('text', label = expression(bold('\\u2013' * '3 ' * '\\u03bc' * 'V')), x = -29, y = 3, size = 4.5, color = 'grey32', family='sans') + annotate('text', label = expression(bold('+3 ' * '\\u03bc' * 'V')), x = -29, y = -3, size = 4.5, color = 'grey32', family='sans') + annotate('text', label = expression(bold('\\u2013' * '6 ' * '\\u03bc' * 'V')), x = -29, y = 6, size = 4.5, color = 'grey32', family='sans') print(plot_GroupAndElectrode) output$downloadPlot.1 \u0026lt;- downloadHandler( filename \u0026lt;- function(file){ paste0(input$var.Group, ' group, electrode ', input$var.Electrodes.1, ', ', Sys.Date(), '.png')}, content \u0026lt;- function(file){ png(file, units='in', width=13, height=5, res=900) print(plot_GroupAndElectrode) dev.off()}, contentType = 'image/png') } )  # From ui.R script downloadButton('downloadPlot.1', 'Download HD plot')  Easy to laugh now!\nMy experience with Shiny has been so good I\u0026rsquo;ve been sharing my experience. Yet, on my first crawling days, I spent an eternity stuck with this elephant in my room: \u0026ldquo;μ\u0026rdquo;. This μ letter (micro-souvenir from hell, as I later knew it), was part of the labels of my plots. All I knew was that I could not deploy the app online, even while I could perfectly launch it locally in my laptop. So, I wondered what use was to deploy locally if I couldn\u0026rsquo;t publish the app?! Eventually, I read about UTF-8 encoding in one forum. Bless them forums. All I had to do was use \u0026ldquo;Âμ\u0026rdquo; instead of the single \u0026ldquo;μ\u0026rdquo;. A better option I found later was: expression(\u0026quot;\\u03bc\u0026quot;).\nBeyond encoding issues, I had a tough time embedding images. You know, the \u0026lsquo;www\u0026rsquo; folder\u0026hellip; To be honest, I still haven\u0026rsquo;t handled the \u0026lsquo;www\u0026rsquo; way\u0026ndash;but where there\u0026rsquo;s a will there\u0026rsquo;s a way. I managed to include my images by uploading them to a website and then entering their URL in \u0026ldquo;img(src\u0026rdquo;, avoiding the use of folder paths.\nimg(src=\u0026quot;https://preview.ibb.co/n7qiYR/EEG_montage.png 1\u0026quot;, height=500, width=1000)  Long after I had built the app, I added another image\u0026ndash;the favicon (the little icon on the browser tab).\ntags$head(tags$link(rel=\u0026quot;shortcut icon\u0026quot;, href=\u0026quot;https://image.ibb.co/fXUwzb/favic.png\u0026quot;)), # web favicon  Reference Bernabeu, P., Willems, R. M., \u0026amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs [Data dashboard]. Retrieved from https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment/.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"b43f71fa68ea5a4a8268607700bc5854","permalink":"/data-dashboards/bernabeu-etal-2017-modalityswitch/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/data-dashboards/bernabeu-etal-2017-modalityswitch/","section":"data-dashboards","summary":"We tested whether conceptual processing is modality-specific by tracking the time course of the Conceptual Modality Switch effect. Forty-six participants verified the relation between property words and concept words. The conceptual modality of consecutive trials was manipulated in order to produce an Auditory-to-visual switch condition, a Haptic-to-visual switch condition, and a Visual-to-visual, no-switch condition. Event-Related Potentials (ERPs) were time-locked to the onset of the first word (property) in the target trials so as to measure the effect online and to avoid a within-trial confound. A switch effect was found, characterized by more negative ERP amplitudes for modality switches than no-switches. It proved significant in four typical time windows from 160 to 750 milliseconds post word onset, with greater strength in the Slow group, in posterior brain regions, and in the N400 window. The earliest switch effect was located in the language brain region, whereas later it was more prominent in the visual region. In the N400 and Late Positive windows, the Quick group presented the effect especially in the language region, whereas the Slow had it rather in the visual region. These results suggest that contextual factors such as time resources modulate the engagement of linguistic and embodied systems in conceptual processing.","tags":["data dashboard","R Shiny","conceptual modality switch","conceptual processing","reading","event-related potentials","cognition","psycholinguistics"],"title":"[Data dashboard] Modality switch effects emerge early and increase throughout conceptual processing: evidence from ERPs","type":"data-dashboards"},{"authors":["**Bernabeu, P.**","Willems, R. M.","Louwerse, M. M."],"categories":null,"content":"Reference Bernabeu, P., Willems, R. M., \u0026amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, \u0026amp; E. J. Davelaar (Eds.), Proceedings of the 39th Annual Conference of the Cognitive Science Society (pp. 1629-1634). Austin, TX: Cognitive Science Society. https://mindmodeling.org/cogsci2017/papers/0318/\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"54f8c09b786d728c5c440476328c3b7b","permalink":"/publication/bernabeu-etal-2017/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bernabeu-etal-2017/","section":"publication","summary":"We tested whether conceptual processing is modality-specific by tracking the time course of the Conceptual Modality Switch effect. Forty-six participants verified the relation between property words and concept words. The conceptual modality of consecutive trials was manipulated in order to produce an Auditory-to-visual switch condition, a Haptic-to-visual switch condition, and a Visual-to-visual, no-switch condition. Event-Related Potentials (ERPs) were time-locked to the onset of the first word (property) in the target trials so as to measure the effect online and to avoid a within-trial confound. A switch effect was found, characterized by more negative ERP amplitudes for modality switches than no-switches. It proved significant in four typical time windows from 160 to 750 milliseconds post word onset, with greater strength in the Slow group, in posterior brain regions, and in the N400 window. The earliest switch effect was located in the language brain region, whereas later it was more prominent in the visual region. In the N400 and Late Positive windows, the Quick group presented the effect especially in the language region, whereas the Slow had it rather in the visual region. These results suggest that contextual factors such as time resources modulate the engagement of linguistic and embodied systems in conceptual processing.","tags":["conceptual modality switch","conceptual processing","reading","event-related potentials","cognition","psycholinguistics","CogSci"],"title":"[Paper] Modality switch effects emerge early and increase throughout conceptual processing: evidence from ERPs","type":"publication"},{"authors":["Bernabeu, P."],"categories":null,"content":"Link to Flexdashboard version Link to Flexdashboard-Shiny version This Flexdashboard-Shiny app presents linguistic data over several tabs. The code combines the great front-end of Flexdashboard—based on R Markdown and yielding an unmatched user interface—, with the great back-end of Shiny—allowing users to download sections of data they select, in various formats.\n  A cool, recent finding was the reactable package, which puts Javascript into the cells, allowing coloured bars, etc.\n Auditory = colDef(header = with_tooltip('Auditory Rating', 'Mean rating of each word on the auditory modality across participants.'), cell = function(value) { width \u0026lt;- paste0(value / max(table_data$Auditory) * 100, \u0026quot;%\u0026quot;) value = sprintf(\u0026quot;%.2f\u0026quot;, round(value,2)) # Round to two digits, keeping trailing zeros bar_chart(value, width = width, fill = '#ff3030') }, align = 'left'),    One of the hardest nuts to crack was allowing the full functionality of tables—i.e, scaling to screen, frozen header, and vertical and horizontal scrolling—whilst having tweaked the vertical/horizontal orientation of the dashboard sections. Initial clashes were sorted by adjusting the section\u0026rsquo;s CSS styles\nTable {#table style=\u0026quot;background-color:#FCFCFC;\u0026quot;} ======================================================================= Inputs {.sidebar style='position:fixed; padding-top: 65px; padding-bottom:30px;'} -----------------------------------------------------------------------  and by also adjusting the reactable settings.\n renderReactable({ reactable(selected_words(), defaultSorted = list(cat = 'desc', word = 'asc'), defaultColDef = colDef(footerStyle = list(fontWeight = \u0026quot;bold\u0026quot;)), height = 840, striped = TRUE, pagination = FALSE, highlight = TRUE,    A nice feature, especially suited to Flexdashboard, was the use of different formats across tabs. Whereas the Info tab presents long text using HTML and CSS styling, along with rmarkdown code output, the other tabs rely more strongly on Javascript features, enabled by R packages such as ‘shiny’ and sweetalert (e.g., allowing modal dialogs—pop-ups), reactable and plotly (e.g., allowing information opened by hovering—tooltips).\n```{r} # reactive for the word bar highlighted_properties = reactive(input$highlighted_properties) renderPlotly({ ggplotly( ggplot( selected_props(), aes(RC1, RC2, label = as.character(word), color = main, # Html tags below used for format. Decimals rounded to two. text = paste0(' ', '\u0026lt;span style=\u0026quot;padding-top:3px; padding-bottom:3px; font-size:2.2em; color:#EEEEEE\u0026quot;\u0026gt;', capitalize(word), '\u0026lt;/span\u0026gt; ', '\u0026lt;br\u0026gt;', '\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Dominant modality: \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', main, ' ', ' ', '\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Modality exclusivity: \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', sprintf(\u0026quot;%.2f\u0026quot;, round(Exclusivity, 2)), '% ', '\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Perceptual strength: \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', sprintf(\u0026quot;%.2f\u0026quot;, round(Perceptualstrength, 2)), '\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Auditory rating: \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', sprintf(\u0026quot;%.2f\u0026quot;, round(Auditory, 2)), ' ', '\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Haptic rating: \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', sprintf(\u0026quot;%.2f\u0026quot;, round(Haptic, 2)), ' ', '\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Visual rating: \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', sprintf(\u0026quot;%.2f\u0026quot;, round(Visual, 2)), ' ', '\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Concreteness (Brysbaert et al., 2014): \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', sprintf(\u0026quot;%.2f\u0026quot;, round(concrete_Brysbaertetal2014, 2)), ' ', '\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Number of letters: \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', letters, ' ', '\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Number of phonemes (DutchPOND): \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', round(phonemes_DUTCHPOND, 2), ' ', '\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Contextual diversity (lg10CD SUBTLEX-NL): \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', sprintf(\u0026quot;%.2f\u0026quot;, round(freq_lg10CD_SUBTLEXNL, 2)), ' ', '\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Word frequency (lg10WF SUBTLEX-NL): \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', sprintf(\u0026quot;%.2f\u0026quot;, round(freq_lg10WF_SUBTLEXNL, 2)), ' ', '\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Lemma frequency (CELEX): \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', sprintf(\u0026quot;%.2f\u0026quot;, round(freq_CELEX_lem, 2)), ' ', '\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Phonological neighbourhood size (DutchPOND): \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', round(phon_neighbours_DUTCHPOND, 2), ' ', '\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Orthographic neighbourhood size (DutchPOND): \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', round(orth_neighbours_DUTCHPOND, 2), ' ', '\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;span style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt; Age of acquisition (Brysbaert et al., 2014): \u0026lt;/span\u0026gt;\u0026lt;b style=\u0026quot;color:#EEEEEE\u0026quot;\u0026gt;', sprintf(\u0026quot;%.2f\u0026quot;, round(AoA_Brysbaertetal2014, 2)), ' ', '\u0026lt;br\u0026gt; ' ) ) ) + geom_text(size = ifelse(selected_props()$word %in% highlighted_properties(), 7, ifelse(is.null(highlighted_properties()), 3, 2.8)), fontface = ifelse(selected_props()$word %in% highlighted_properties(), 'bold', 'plain')) + geom_point(alpha = 0) + # This geom_point helps to colour the tooltip according to the dominant modality scale_colour_manual(values = colours, drop = FALSE) + theme_bw() + ggtitle('Property words') + labs(x = 'Varimax-rotated Principal Component 1', y = 'Varimax-rotated Principal Component 2') + guides(color = guide_legend(title = 'Main\u0026lt;br\u0026gt;modality')) + theme( plot.background = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), axis.line = element_line(color = 'black'), plot.title = element_text(size = 14, hjust = .5), axis.title.x = element_text(colour = 'black', size = 12, margin = margin(15,15,0,15)), axis.title.y = element_text(colour = 'black', size = 12, margin = margin(0,15,15,5)), axis.text.x = element_text(size = 8), axis.text.y = element_text(size = 8), legend.background = element_rect(size = 2), legend.position = 'none', legend.title = element_blank(), legend.text = element_text(colour = colours, size = 13) ), tooltip = 'text' ) }) # For download, save plot without the interactive 'plotly' part properties_png = reactive({ ggplot(selected_props(), aes(RC1, RC2, color = main, label = as.character(word))) + geom_text(show.legend = FALSE, size = ifelse(selected_props()$word %in% highlighted_properties(), 7, ifelse(is.null(highlighted_properties()), 3, 2.8)), fontface = ifelse(selected_props()$word %in% highlighted_properties(), 'bold', 'plain')) + geom_point(alpha = 0) + scale_colour_manual(values = colours, drop = FALSE) + theme_bw() + guides(color = guide_legend(title = 'Main\u0026lt;br\u0026gt;modality', override.aes = list(size = 7, alpha = 1))) + ggtitle( paste0('Properties', ' (showing ', nrow(selected_props()), ' out of ', nrow(props), ')') ) + labs(x = 'Varimax-rotated Principal Component 1', y = 'Varimax-rotated Principal Component 2') + theme( plot.background = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), axis.line = element_line(color = 'black'), plot.title = element_text(size = 17, hjust = .5, margin = margin(3,3,7,3)), axis.title.x = element_text(colour = 'black', size = 12, margin = margin(10,10,2,10)), axis.title.y = element_text(colour = 'black', size = 12, margin = margin(10,10,10,5)), axis.text.x = element_text(size = 8), axis.text.y = element_text(size = 8), legend.background = element_rect(size = 2), legend.position = 'right', legend.title = element_blank(), legend.text = element_text(size = 15)) }) ```  The only instance in which I drew on javascript code outside R packages was to enable tooltips beyond the packages’ limits—for instance, in the side bar. This javascript feature is created at the top of the script, in the head area.\n\u0026lt;!-- Javascript function to enable a hovering tooltip --\u0026gt; \u0026lt;script\u0026gt; $(document).ready(function(){ $('[data-toggle=\u0026quot;tooltip1\u0026quot;]').tooltip(); }); \u0026lt;/script\u0026gt;    In the side bar, I added a reactive mean for each variable, complementing the range selector.\nreactive(cat(paste0('Mean = ', sprintf(\u0026quot;%.2f\u0026quot;, round(mean(selected_words()$Exclusivity),2)))))    Non-Shiny version published in RPubs A reduced, non-Shiny version was also created to increase the availability of the content. Removing Shiny features allows publication as a simple website. To create the Flexdashboard-only version departing from the Flexdashboard-Shiny version, I added a setting in the header of the script\nknit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding) })  and disabled reactive features.\n```{r} # Number of words selected on sidebar # reactive(cat(paste0('Words selected below: ', nrow(selected_props())))) ```  Category: Research Keywords: science, cognition, language, linguistics, modality, stimulus, experiment, norming Shiny app: https://pablobernabeu.shinyapps.io/dutch-modality-exclusivity-norms/ Repo: https://github.com/pablobernabeu/Modality-exclusivity-norms-Bernabeu-et-al/tree/master/Shiny-app RStudio Cloud: https://rstudio.cloud/project/941860\nReference Bernabeu, P., Willems, R. M., \u0026amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs [Data dashboard for modality exclusivity norms]. Retrieved from https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment/.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"c02b1604521caab20fb2eb75d2972967","permalink":"/data-dashboards/bernabeu-etal-2017-modalitynorms/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/data-dashboards/bernabeu-etal-2017-modalitynorms/","section":"data-dashboards","summary":"This app presents linguistic data over several tabs. The code combines the great front-end of Flexdashboard—based on R Markdown and yielding an unmatched user interface—, with the great back-end of Shiny—allowing users to download sections of data they select, in various formats. The hardest nuts to crack included modifying the rows/columns orientation without affecting the functionality of tables. A cool, recent finding was the reactable package. A nice feature, allowed by Flexdashboard, was the use of quite different formats in different tabs.","tags":["data dashboard","Flexdashboard","R Shiny","modality exclusivity norms","Dutch","linguistics"],"title":"Dutch modality exclusivity norms","type":"data-dashboards"},{"authors":[],"categories":["R","data dashboards","open data"],"content":" Dashboards for data visualisation, such as R Shiny and Tableau, allow the interactive exploration of data by means of drop-down lists and checkboxes, with no coding required from the final users. The apps can be useful for both the data analyst and the public. Visualisation apps run on internet browsers. This allows for three options: private viewing (useful during analysis), selective sharing (used within work groups), or internet publication. Among the available platforms, R Shiny and Tableau stand out due to being relatively accessible to new users. Apps serve a broad variety of purposes (see this gallery and this one). In science and beyond, these apps allow us to go the extra mile in sharing data. Alongside files and code shared in repositories, we can present the data in a website, in the form of plots or tables. This facilitates the public exploration of each section of the data (groups, participants, trials…) to anyone interested, and allows researchers to account for their proceeding in the analysis.\nPublishers and journals highly encourage authors to make the most of their data by facilitating its easy exploration by the readership–even though they don’t normally offer options for hosting web visualisations yet.\nApps can also prove valuable to those analysing the data. For instance, my app helped me a lot in identifying the extent of noise in a section of the data. Instead of running through a heavy score of code, the drop-down lists of the app let me seamlessly surf through the different sections.\nAt a certain point, I found a data section that was consistently noisier than the rest, and eventually I had to discard it from further statistical analyses. Yet, instead of removing that from the app, I maintained it with a note attached. This particular trait in the data was rather salient.\nBeyond such a salient feature in the data, a visualisation app may also help to spot subtler patterns such as third variables or individual differences.\nThere are several platforms for creating apps (e.g., Tableau, D3.js, and R Shiny). I focus on R Shiny here for three reasons: it is affordable to use, fairly accessible to new users, and well suited for science as it is based on the R language (see for instance this article).\n How to Shiny Shiny apps draw on any standard R code that you may already have. This is most commonly plots or tables, but other stuff such as images or Markdown texts are valid too. This is a nice thing to keep in mind when having to create a new app. Part of the job may already be done! The app is distributed among five different areas.\n Data file(s) These are whatever data files you’re using (e.g., with csv or rds extensions).  1a. server.R script The server script contains the central processes: plots, tables, etc. Code that existed independently of the app app may be brought into this script by slightly adapting it. At the top, call the shiny library and any others used (e.g., ‘ggplot2’), and also read in the data. The snippet below shows the beginning of an example server.R script.\n # server library(shiny) library(ggplot2) EEG.ParticipantAndElectrode = readRDS(\u0026#39;EEG.ParticipantAndElectrode.rds\u0026#39;) EEG.ParticipantAndBrainArea = readRDS(\u0026#39;EEG.ParticipantAndBrainArea.rds\u0026#39;) EEG.GroupAndElectrode = readRDS(\u0026#39;EEG.GroupAndElectrode.rds\u0026#39;) EEG.OLDGroupAndElectrode = readRDS(\u0026#39;EEG.OLDGroupAndElectrode.rds\u0026#39;) server = shinyServer( function(input, output) { # plot_GroupAndElectrode: output$plot_GroupAndElectrode \u0026lt;- renderPlot({ dfelectrode \u0026lt;- aggregate(microvolts ~ electrode*time*condition, EEG.GroupAndElectrode[EEG.GroupAndElectrode$RT.based_Groups==input$var.Group,], mean) df2 \u0026lt;- subset(dfelectrode, electrode == input$var.Electrodes.1) df2$condition= as.factor(df2$condition) df2$condition \u0026lt;- gsub(\u0026#39;visual2visual\u0026#39;, \u0026#39; Visual / Visual\u0026#39;, df2$condition) df2$condition \u0026lt;- gsub(\u0026#39;haptic2visual\u0026#39;, \u0026#39; Haptic / Visual\u0026#39;, df2$condition) df2$condition \u0026lt;- gsub(\u0026#39;auditory2visual\u0026#39;, \u0026#39; Auditory / Visual\u0026#39;, df2$condition) df2$time \u0026lt;- as.integer(as.character(df2$time)) colours \u0026lt;- c(\u0026#39;firebrick1\u0026#39;, \u0026#39;dodgerblue\u0026#39;, \u0026#39;forestgreen\u0026#39;) # green:visual2visual, blue:haptic2visual, red:auditory2visual spec_title = paste0(\u0026#39;ERP waveforms for \u0026#39;, input$var.Group, \u0026#39; Group, Electrode \u0026#39;, input$var.Electrodes.1, \u0026#39; (negative values upward; time windows displayed)\u0026#39;) plot_GroupAndElectrode = ggplot(df2, aes(x=time, y=-microvolts, color=condition)) + geom_rect(xmin=160, xmax=216, ymin=7.5, ymax=-8, color = \u0026#39;grey75\u0026#39;, fill=\u0026#39;black\u0026#39;, alpha=0, linetype=\u0026#39;longdash\u0026#39;) + geom_rect(xmin=270, xmax=370, ymin=7.5, ymax=-8, color = \u0026#39;grey75\u0026#39;, fill=\u0026#39;black\u0026#39;, alpha=0, linetype=\u0026#39;longdash\u0026#39;) + geom_rect(xmin=350, xmax=550, ymin=8, ymax=-7.5, color = \u0026#39;grey75\u0026#39;, fill=\u0026#39;black\u0026#39;, alpha=0, linetype=\u0026#39;longdash\u0026#39;) + geom_rect(xmin=500, xmax=750, ymin=7.5, ymax=-8, color = \u0026#39;grey75\u0026#39;, fill=\u0026#39;black\u0026#39;, alpha=0, linetype=\u0026#39;longdash\u0026#39;) + geom_line(size=1, alpha = 1) + scale_linetype_manual(values=colours) + scale_y_continuous(limits=c(-8.38, 8.3), breaks=seq(-8,8,by=1), expand = c(0,0.1)) + scale_x_continuous(limits=c(-208,808),breaks=seq(-200,800,by=100), expand = c(0.005,0), labels= c(\u0026#39;-200\u0026#39;,\u0026#39;-100 ms\u0026#39;,\u0026#39;0\u0026#39;,\u0026#39;100 ms\u0026#39;,\u0026#39;200\u0026#39;,\u0026#39;300 ms\u0026#39;,\u0026#39;400\u0026#39;,\u0026#39;500 ms\u0026#39;,\u0026#39;600\u0026#39;,\u0026#39;700 ms\u0026#39;,\u0026#39;800\u0026#39;)) + ggtitle(spec_title) + theme_bw() + geom_vline(xintercept=0) + annotate(geom=\u0026#39;segment\u0026#39;, y=seq(-8,8,1), yend=seq(-8,8,1), x=-4, xend=8, color=\u0026#39;black\u0026#39;) + annotate(geom=\u0026#39;segment\u0026#39;, y=-8.2, yend=-8.38, x=seq(-200,800,100), xend=seq(-200,800,100), color=\u0026#39;black\u0026#39;) + geom_segment(x = -200, y = 0, xend = 800, yend = 0, size=0.5, color=\u0026#39;black\u0026#39;) + theme(legend.position = c(0.100, 0.150), legend.background = element_rect(fill=\u0026#39;#EEEEEE\u0026#39;, size=0), axis.title=element_blank(), legend.key.width = unit(1.2,\u0026#39;cm\u0026#39;), legend.text=element_text(size=17), legend.title = element_text(size=17, face=\u0026#39;bold\u0026#39;), plot.title= element_text(size=20, hjust = 0.5, vjust=2), axis.text.y = element_blank(), axis.text.x = element_text(size = 14, vjust= 2.12, face=\u0026#39;bold\u0026#39;, color = \u0026#39;grey32\u0026#39;, family=\u0026#39;sans\u0026#39;), axis.ticks=element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), plot.margin = unit(c(0.1,0.1,0,0), \u0026#39;cm\u0026#39;)) + annotate(\u0026#39;segment\u0026#39;, x=160, xend=216, y=-8, yend=-8, colour = \u0026#39;grey75\u0026#39;, size = 1.5) + annotate(\u0026#39;segment\u0026#39;, x=270, xend=370, y=-8, yend=-8, colour = \u0026#39;grey75\u0026#39;, size = 1.5) + annotate(\u0026#39;segment\u0026#39;, x=350, xend=550, y=-7.5, yend=-7.5, colour = \u0026#39;grey75\u0026#39;, size = 1.5) + annotate(\u0026#39;segment\u0026#39;, x=500, xend=750, y=-8, yend=-8, colour = \u0026#39;grey75\u0026#39;, size = 1.5) + scale_fill_manual(name = \u0026#39;Context / Target trial\u0026#39;, values=colours) + scale_color_manual(name = \u0026#39;Context / Target trial\u0026#39;, values=colours) + guides(linetype=guide_legend(override.aes = list(size=1.2))) + guides(color=guide_legend(override.aes = list(size=2.5))) + # Print y axis labels within plot area: annotate(\u0026#39;text\u0026#39;, label = expression(bold(\u0026#39;\\u2013\u0026#39; * \u0026#39;3 \u0026#39; * \u0026#39;\\u03bc\u0026#39; * \u0026#39;V\u0026#39;)), x = -29, y = 3, size = 4.5, color = \u0026#39;grey32\u0026#39;, family=\u0026#39;sans\u0026#39;) + annotate(\u0026#39;text\u0026#39;, label = expression(bold(\u0026#39;+3 \u0026#39; * \u0026#39;\\u03bc\u0026#39; * \u0026#39;V\u0026#39;)), x = -29, y = -3, size = 4.5, color = \u0026#39;grey32\u0026#39;, family=\u0026#39;sans\u0026#39;) + annotate(\u0026#39;text\u0026#39;, label = expression(bold(\u0026#39;\\u2013\u0026#39; * \u0026#39;6 \u0026#39; * \u0026#39;\\u03bc\u0026#39; * \u0026#39;V\u0026#39;)), x = -29, y = 6, size = 4.5, color = \u0026#39;grey32\u0026#39;, family=\u0026#39;sans\u0026#39;) print(plot_GroupAndElectrode) output$downloadPlot.1 \u0026lt;- downloadHandler( filename \u0026lt;- function(file){ paste0(input$var.Group, \u0026#39; group, electrode \u0026#39;, input$var.Electrodes.1, \u0026#39;, \u0026#39;, Sys.Date(), \u0026#39;.png\u0026#39;)}, content \u0026lt;- function(file){ png(file, units=\u0026#39;in\u0026#39;, width=13, height=5, res=900) print(plot_GroupAndElectrode) dev.off()}, contentType = \u0026#39;image/png\u0026#39;) } ) # ...  — Whole script\n 1b. ui.R script The ui script defines the user interface. For instance, a factor column in the data that has multiple categories may be neatly displayed with a drop-down list on the side bar of the website. The interface may present a central plot before by a legend key below. The snippet below shows the beginning of an example ui.R script.\n # UI library(shiny) library(ggplot2) EEG.GroupAndElectrode = readRDS(\u0026#39;EEG.GroupAndElectrode.rds\u0026#39;) EEG.ParticipantAndBrainArea = readRDS(\u0026#39;EEG.ParticipantAndBrainArea.rds\u0026#39;) EEG.ParticipantAndElectrode = readRDS(\u0026#39;EEG.ParticipantAndElectrode.rds\u0026#39;) EEG.OLDGroupAndElectrode = readRDS(\u0026#39;EEG.OLDGroupAndElectrode.rds\u0026#39;) ui = shinyUI( fluidPage( tags$head(includeHTML(\u0026#39;google-analytics.html\u0026#39;)), # Google Analytics tag tags$head(tags$link(rel=\u0026#39;shortcut icon\u0026#39;, href=\u0026#39;https://image.ibb.co/fXUwzb/favic.png\u0026#39;)), # web favicon tags$meta(charset=\u0026#39;UTF-8\u0026#39;), tags$meta(name=\u0026#39;description\u0026#39;, content=\u0026#39;This R Shiny visualisation dashboard presents data from a psycholinguistic ERP experiment (Bernabeu et al., 2017).\u0026#39;), tags$meta(name=\u0026#39;keywords\u0026#39;, content=\u0026#39;R, Shiny, ggplot2, visualisation, data, psycholinguistics, conceptual processing, modality switch, embodied cognition\u0026#39;), tags$meta(name=\u0026#39;viewport\u0026#39;, content=\u0026#39;width=device-width, initial-scale=1.0\u0026#39;), tags$meta(name=\u0026#39;google-site-verification\u0026#39;, content=\u0026#39;HgyhSO1YSk59r3mwlZU9XmouP5oSXmoICeRfH6ytF1k\u0026#39;), # Google Search Console tag titlePanel(h3(strong(\u0026#39;Waveforms in detail from an ERP experiment on the Conceptual Modality Switch\u0026#39;), a(\u0026#39;(Bernabeu et al., 2017)\u0026#39;, href=\u0026#39;https://figshare.com/articles/EEG_study_on_conceptual_modality-switching_Bernabeu_et_al_in_prep_/4210863\u0026#39;, target=\u0026#39;_blank\u0026#39;, style = \u0026#39;color:#3E454E; text-decoration:underline; font-weight:normal\u0026#39;), align = \u0026#39;center\u0026#39;, style = \u0026#39;color:black\u0026#39;), windowTitle = \u0026#39;Visualization of ERP waveforms from experiment on Conceptual Modality Switch (Bernabeu et al., 2017)\u0026#39;), sidebarLayout( sidebarPanel(width = 2, # Condition 1 for reactivity between tabs and sidebars conditionalPanel( condition = \u0026#39;input.tabvals == 1\u0026#39;, h5(a(strong(\u0026#39;See paper, statistics, all data.\u0026#39;), \u0026#39;Plots by group and brain area shown in paper.\u0026#39;, href=\u0026#39;https://figshare.com/articles/EEG_study_on_conceptual_modality-switching_Bernabeu_et_al_in_prep_/4210863\u0026#39;, target=\u0026#39;_blank\u0026#39;), align = \u0026#39;center\u0026#39;), br(), selectInput(\u0026#39;var.Group\u0026#39;, label = \u0026#39;Group\u0026#39;, choices = list(\u0026#39;Quick\u0026#39;,\u0026#39;Slow\u0026#39;), selected = \u0026#39;Quick\u0026#39;), h6(\u0026#39;Quick G.: 23 participants\u0026#39;), h6(\u0026#39;Slow G.: 23 participants\u0026#39;), br(), selectInput(\u0026#39;var.Electrodes.1\u0026#39;, label = h5(strong(\u0026#39;Electrode\u0026#39;), br(), \u0026#39;(see montage below)\u0026#39;), choices = list(\u0026#39;1\u0026#39;,\u0026#39;2\u0026#39;,\u0026#39;3\u0026#39;,\u0026#39;4\u0026#39;,\u0026#39;5\u0026#39;,\u0026#39;6\u0026#39;,\u0026#39;7\u0026#39;,\u0026#39;8\u0026#39;,\u0026#39;9\u0026#39;,\u0026#39;10\u0026#39;, \u0026#39;11\u0026#39;,\u0026#39;12\u0026#39;,\u0026#39;13\u0026#39;,\u0026#39;14\u0026#39;,\u0026#39;15\u0026#39;,\u0026#39;16\u0026#39;,\u0026#39;17\u0026#39;,\u0026#39;18\u0026#39;,\u0026#39;19\u0026#39;,\u0026#39;20\u0026#39;,\u0026#39;21\u0026#39;, \u0026#39;22\u0026#39;,\u0026#39;23\u0026#39;,\u0026#39;24\u0026#39;,\u0026#39;25\u0026#39;,\u0026#39;26\u0026#39;,\u0026#39;27\u0026#39;,\u0026#39;28\u0026#39;,\u0026#39;29\u0026#39;,\u0026#39;30\u0026#39;,\u0026#39;31\u0026#39;,\u0026#39;33\u0026#39;, \u0026#39;34\u0026#39;,\u0026#39;35\u0026#39;,\u0026#39;36\u0026#39;,\u0026#39;37\u0026#39;,\u0026#39;38\u0026#39;,\u0026#39;39\u0026#39;,\u0026#39;40\u0026#39;,\u0026#39;41\u0026#39;,\u0026#39;42\u0026#39;,\u0026#39;43\u0026#39;,\u0026#39;44\u0026#39;, \u0026#39;45\u0026#39;,\u0026#39;46\u0026#39;,\u0026#39;47\u0026#39;,\u0026#39;48\u0026#39;,\u0026#39;49\u0026#39;,\u0026#39;50\u0026#39;,\u0026#39;51\u0026#39;,\u0026#39;52\u0026#39;,\u0026#39;53\u0026#39;,\u0026#39;54\u0026#39;,\u0026#39;55\u0026#39;, \u0026#39;56\u0026#39;,\u0026#39;57\u0026#39;,\u0026#39;58\u0026#39;,\u0026#39;59\u0026#39;,\u0026#39;60\u0026#39;), selected = \u0026#39;30\u0026#39; ), br(), br(), h6(\u0026#39;Source code:\u0026#39;), h6(strong(\u0026#39;- \u0026#39;), a(\u0026#39;server.R\u0026#39;, href=\u0026#39;https://osf.io/uj8z4/\u0026#39;, target=\u0026#39;_blank\u0026#39;, style = \u0026#39;text-decoration: underline;\u0026#39;)), h6(strong(\u0026#39;- \u0026#39;), a(\u0026#39;ui.R\u0026#39;, href=\u0026#39;https://osf.io/8bwcx/\u0026#39;, target=\u0026#39;_blank\u0026#39;, style = \u0026#39;text-decoration: underline;\u0026#39;)), br(), h6(a(\u0026#39;CC-By 4.0 License\u0026#39;, href=\u0026#39;https://osf.io/97unm/\u0026#39;, target=\u0026#39;_blank\u0026#39;), align = \u0026#39;center\u0026#39;, style = \u0026#39;text-decoration: underline;\u0026#39;), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), h5(a(strong(\u0026#39;See paper, statistics, all data.\u0026#39;), href=\u0026#39;https://figshare.com/articles/EEG_study_on_conceptual_modality-switching_Bernabeu_et_al_in_prep_/4210863\u0026#39;, target=\u0026#39;_blank\u0026#39;), align = \u0026#39;center\u0026#39;), br(), br(), br(), br(), br(), br(), br(), br() ), # ...  — Whole script\n 2. Deployment and logs This script contains the commands for deploying the app on- or off-line, and for checking the session logs in case of any errors.\n 3. Automatically created folder When the app is first deployed on the internet, a subfolder is automatically created with the name ‘rsconnect’. This folder contains a text file which can be used to modify the URL and the title of the webpage.\nSteps to create a Shiny app from scratch:\n1. Tutorials (link). Being open-source software, excellent directions are available through a Google search.\nThe core ideas are:\nAs mentioned above, create a ui.R script for the code containing the user interface, and create a server.R script for the code containing the main content (your plots / tables, etc).\nAt the top of both ui.R and server.R scripts, enter the command library(shiny) and also load any other libraries you’re using (e.g., ggplot2).\nTest your app by deploying it locally, before launching online. For this purpose, first save the ui and server parts independently, as in:\n ui = shinyUI( fluidPage( # ...  Then deploy locally by running:\nshinyApp(ui, server) Managing to run the app locally is a great first step before launching online (which may sometimes prove a bit trickier).\n2. User token (link). Sign up and read in your private key—just to be done once in a computer.\n3. Go for it. After locally testing and saving the two main scripts (ui.R and server.R), run deployApp() to launch the app online.\n4. Bugs and session logs. Most often they won’t be bugs actually, but fancies, as it were. For instance, some special characters have to get even more special (technically, UTF-8 encoding). For a character such as ‘μ’, Shiny prefers ‘Âμ’, or better, the Javascript code:\nexpression(\u0026quot;\\u03bc\u0026quot;) Cling to your logs by calling the line below, which you may keep at hand in your ‘Shiny deployer.R’ script.\nshowLogs(appPath = getwd(), appFile = NULL, appName = NULL, account = NULL, entries = 50, streaming = FALSE) At best, the log output will mention any typos and unaccepted characters, pointing to specific lines in your code.\nIt may take a couple of intense days to get a first Shiny app running. Although the usual rabbit holes do exist, years of Shiny have already yielded a sizeable body of free resources online (tutorials, blogs, vlogs). Moreover, there’s also the RStudio Community, and then StackOverflow etc., where you can post any needs/despair. Post your code, log, and explanation, and you’ll be rescued out in a couple of days. Long live those contributors.\nIt’s sometimes enough to upload a bare app, but you might then think it can look better.\n5 (optional). Advance. Use tabs to combine multiple apps on one webpage, use different widgets, include a download option, etc. Tutorials like this one on Youtube can take you there, especially those that provide the code, as in the description of that video. Use those scripts as templates. For example, I made use of tabs on the top of the dashboard in order to keep the side bar from having too many widgets. The appearance of these tabs can be adjusted. More importantly, the inputs in the sidebar can be modified depending on the active tab, by means of ‘reactivity’ conditions.\n mainPanel( tags$style(HTML(\u0026#39; .tabbable \u0026gt; .nav \u0026gt; li \u0026gt; a {background-color:white; color:#3E454E} .tabbable \u0026gt; .nav \u0026gt; li \u0026gt; a:hover {background-color:#002555; color:white} .tabbable \u0026gt; .nav \u0026gt; li[class=active] \u0026gt; a {background-color:#ECF4FF; color:black} .tabbable \u0026gt; .nav \u0026gt; li[class=active] \u0026gt; a:hover {background-color:#E7F1FF; color:black} \u0026#39;)), tabsetPanel(id=\u0026#39;tabvals\u0026#39;, tabPanel(value=1, h4(strong(\u0026#39;Group \u0026amp; Electrode\u0026#39;)), br(), plotOutput(\u0026#39;plot_GroupAndElectrode\u0026#39;), h5(a(strong(\u0026#39;See plots with 95% Confidence Intervals\u0026#39;), href=\u0026#39;https://osf.io/2tpxn/\u0026#39;, target=\u0026#39;_blank\u0026#39;), style=\u0026#39;text-decoration: underline;\u0026#39;), downloadButton(\u0026#39;downloadPlot.1\u0026#39;, \u0026#39;Download HD plot\u0026#39;), br(), br(), # EEG montage img(src=\u0026#39;https://preview.ibb.co/n7qiYR/EEG_montage.png\u0026#39;, height=500, width=1000)), tabPanel(value=2, h4(strong(\u0026#39;Participant \u0026amp; Area\u0026#39;)), br(), plotOutput(\u0026#39;plot_ParticipantAndLocation\u0026#39;), h5(a(strong(\u0026#39;See plots with 95% Confidence Intervals\u0026#39;), href=\u0026#39;https://osf.io/86ch9/\u0026#39;, target=\u0026#39;_blank\u0026#39;), style=\u0026#39;text-decoration: underline;\u0026#39;), downloadButton(\u0026#39;downloadPlot.2\u0026#39;, \u0026#39;Download HD plot\u0026#39;), br(), br(), # EEG montage img(src=\u0026#39;https://preview.ibb.co/n7qiYR/EEG_montage.png\u0026#39;, height=500, width=1000)), tabPanel(value=3, h4(strong(\u0026#39;Participant \u0026amp; Electrode\u0026#39;)), br(), plotOutput(\u0026#39;plot_ParticipantAndElectrode\u0026#39;), br(), downloadButton(\u0026#39;downloadPlot.3\u0026#39;, \u0026#39;Download HD plot\u0026#39;), br(), br(), # EEG montage img(src=\u0026#39;https://preview.ibb.co/n7qiYR/EEG_montage.png\u0026#39;, height=500, width=1000)), tabPanel(value=4, h4(strong(\u0026#39;OLD Group \u0026amp; Electrode\u0026#39;)), br(), plotOutput(\u0026#39;plot_OLDGroupAndElectrode\u0026#39;), h5(a(strong(\u0026#39;See plots with 95% Confidence Intervals\u0026#39;), href=\u0026#39;https://osf.io/dvs2z/\u0026#39;, target=\u0026#39;_blank\u0026#39;), style=\u0026#39;text-decoration: underline;\u0026#39;), downloadButton(\u0026#39;downloadPlot.4\u0026#39;, \u0026#39;Download HD plot\u0026#39;), br(), br(), # EEG montage img(src=\u0026#39;https://preview.ibb.co/n7qiYR/EEG_montage.png\u0026#39;, height=500, width=1000)) ),  \nThe official Shiny gallery offers a great array of apps including their code (e.g., basic example). Another feature you may add is the option to download your plots, tables, data…\n # In ui.R script downloadButton(\u0026#39;downloadPlot.1\u0026#39;, \u0026#39;Download HD plot\u0026#39;) #___________________________________________________ # In server.R script spec_title = paste0(\u0026#39;ERP waveforms for \u0026#39;, input$var.Group, \u0026#39; Group, Electrode \u0026#39;, input$var.Electrodes.1, \u0026#39; (negative values upward; time windows displayed)\u0026#39;) plot_GroupAndElectrode = ggplot(df2, aes(x=time, y=-microvolts, color=condition)) + geom_rect(xmin=160, xmax=216, ymin=7.5, ymax=-8, color = \u0026#39;grey75\u0026#39;, fill=\u0026#39;black\u0026#39;, alpha=0, linetype=\u0026#39;longdash\u0026#39;) + geom_rect(xmin=270, xmax=370, ymin=7.5, ymax=-8, color = \u0026#39;grey75\u0026#39;, fill=\u0026#39;black\u0026#39;, alpha=0, linetype=\u0026#39;longdash\u0026#39;) + geom_rect(xmin=350, xmax=550, ymin=8, ymax=-7.5, color = \u0026#39;grey75\u0026#39;, fill=\u0026#39;black\u0026#39;, alpha=0, linetype=\u0026#39;longdash\u0026#39;) + geom_rect(xmin=500, xmax=750, ymin=7.5, ymax=-8, color = \u0026#39;grey75\u0026#39;, fill=\u0026#39;black\u0026#39;, alpha=0, linetype=\u0026#39;longdash\u0026#39;) + geom_line(size=1, alpha = 1) + scale_linetype_manual(values=colours) + scale_y_continuous(limits=c(-8.38, 8.3), breaks=seq(-8,8,by=1), expand = c(0,0.1)) + scale_x_continuous(limits=c(-208,808),breaks=seq(-200,800,by=100), expand = c(0.005,0), labels= c(\u0026#39;-200\u0026#39;,\u0026#39;-100 ms\u0026#39;,\u0026#39;0\u0026#39;,\u0026#39;100 ms\u0026#39;,\u0026#39;200\u0026#39;,\u0026#39;300 ms\u0026#39;,\u0026#39;400\u0026#39;,\u0026#39;500 ms\u0026#39;,\u0026#39;600\u0026#39;,\u0026#39;700 ms\u0026#39;,\u0026#39;800\u0026#39;)) + ggtitle(spec_title) + theme_bw() + geom_vline(xintercept=0) + annotate(geom=\u0026#39;segment\u0026#39;, y=seq(-8,8,1), yend=seq(-8,8,1), x=-4, xend=8, color=\u0026#39;black\u0026#39;) + annotate(geom=\u0026#39;segment\u0026#39;, y=-8.2, yend=-8.38, x=seq(-200,800,100), xend=seq(-200,800,100), color=\u0026#39;black\u0026#39;) + geom_segment(x = -200, y = 0, xend = 800, yend = 0, size=0.5, color=\u0026#39;black\u0026#39;) + theme(legend.position = c(0.100, 0.150), legend.background = element_rect(fill=\u0026#39;#EEEEEE\u0026#39;, size=0), axis.title=element_blank(), legend.key.width = unit(1.2,\u0026#39;cm\u0026#39;), legend.text=element_text(size=17), legend.title = element_text(size=17, face=\u0026#39;bold\u0026#39;), plot.title= element_text(size=20, hjust = 0.5, vjust=2), axis.text.y = element_blank(), axis.text.x = element_text(size = 14, vjust= 2.12, face=\u0026#39;bold\u0026#39;, color = \u0026#39;grey32\u0026#39;, family=\u0026#39;sans\u0026#39;), axis.ticks=element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), plot.margin = unit(c(0.1,0.1,0,0), \u0026#39;cm\u0026#39;)) + annotate(\u0026#39;segment\u0026#39;, x=160, xend=216, y=-8, yend=-8, colour = \u0026#39;grey75\u0026#39;, size = 1.5) + annotate(\u0026#39;segment\u0026#39;, x=270, xend=370, y=-8, yend=-8, colour = \u0026#39;grey75\u0026#39;, size = 1.5) + annotate(\u0026#39;segment\u0026#39;, x=350, xend=550, y=-7.5, yend=-7.5, colour = \u0026#39;grey75\u0026#39;, size = 1.5) + annotate(\u0026#39;segment\u0026#39;, x=500, xend=750, y=-8, yend=-8, colour = \u0026#39;grey75\u0026#39;, size = 1.5) + scale_fill_manual(name = \u0026#39;Context / Target trial\u0026#39;, values=colours) + scale_color_manual(name = \u0026#39;Context / Target trial\u0026#39;, values=colours) + guides(linetype=guide_legend(override.aes = list(size=1.2))) + guides(color=guide_legend(override.aes = list(size=2.5))) + # Print y axis labels within plot area: annotate(\u0026#39;text\u0026#39;, label = expression(bold(\u0026#39;\\u2013\u0026#39; * \u0026#39;3 \u0026#39; * \u0026#39;\\u03bc\u0026#39; * \u0026#39;V\u0026#39;)), x = -29, y = 3, size = 4.5, color = \u0026#39;grey32\u0026#39;, family=\u0026#39;sans\u0026#39;) + annotate(\u0026#39;text\u0026#39;, label = expression(bold(\u0026#39;+3 \u0026#39; * \u0026#39;\\u03bc\u0026#39; * \u0026#39;V\u0026#39;)), x = -29, y = -3, size = 4.5, color = \u0026#39;grey32\u0026#39;, family=\u0026#39;sans\u0026#39;) + annotate(\u0026#39;text\u0026#39;, label = expression(bold(\u0026#39;\\u2013\u0026#39; * \u0026#39;6 \u0026#39; * \u0026#39;\\u03bc\u0026#39; * \u0026#39;V\u0026#39;)), x = -29, y = 6, size = 4.5, color = \u0026#39;grey32\u0026#39;, family=\u0026#39;sans\u0026#39;) print(plot_GroupAndElectrode) output$downloadPlot.1 \u0026lt;- downloadHandler( filename \u0026lt;- function(file){ paste0(input$var.Group, \u0026#39; group, electrode \u0026#39;, input$var.Electrodes.1, \u0026#39;, \u0026#39;, Sys.Date(), \u0026#39;.png\u0026#39;)}, content \u0026lt;- function(file){ png(file, units=\u0026#39;in\u0026#39;, width=13, height=5, res=900) print(plot_GroupAndElectrode) dev.off()}, contentType = \u0026#39;image/png\u0026#39;) } )  \nApps can include any text, such as explanations of any length and web links. For instance, we can link back to the data repository, where the code for the app can be found.\nAn example of a Shiny app is available, which may also be edited and run in this RStudio environment, inside the ‘Shiny-app’ folder.\nThe Shiny server (shinyapps.io) allows publishing dashboards built with various frameworks besides Shiny proper. Flexdashboard and Shinydashboard are two of these frameworks, which have visible advantages over basic Shiny, in terms of layout. An example with Flexdashboard is available.\n★  Flexdashboard  ★ ★ ★  Shiny  ★ ★ ★ ★ ★  Flexdashboard-Shiny  ★ ★ ★   Logistics Memory capacity can become an issue as you go on, which will be flagged in the error logs as: ‘Shiny cannot use on-disk bookmarking’. This doesn’t necessarily lead you to a paid subscription or to host the website on a custom server. Try pruning the data file, outsourcing data sections across the five available apps.\nApp providers have specific terms of use. To begin, Shiny has a free starter license with limited use, where free apps can handle a certain amount of data, and up to five apps may be created. Beyond that, RStudio offers a wide range of subscriptions starting at $9/month. For its part, Tableau in principle deals only with subscriptions from $35/month on. While they offer 1-year licenses to students and instructors for free, these don’t include web hosting, unlike Shiny’s free plan. Further comparisons of these platforms are available online. Last, I’ll just mention a third language, D3, which is powerful, and may also be used through R.\nIn the case of very heavy data or frequent public use, if you don’t want to host your Shiny app externally, you might consider rendering a PDF with your visualisations instead.\n pdf(\u0026quot;List of plots per page\u0026quot;, width=13, height=5) print(plot1) print(plot2) # ... print(plot150) dev.off()  High-resolution plots can be rendered into a PDF document in a snap. Conveniently, all text is indexed, so it can be searched (Ctrl+f / Cmd+f / 🔍) (see example). Furthermore, you may also merge the rendered PDF with any other documents.\nSummary in slides available.\n  Presenting data interactively online using R Shiny  from Pablo Bernabeu  \nFeel free to share any thoughts or questions in a comment below.\n ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"0e72027452be1d68ee48bba0010c5ceb","permalink":"/2017/01/01/the-case-for-data-dashboards.-first-steps-with-r-shiny/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/2017/01/01/the-case-for-data-dashboards.-first-steps-with-r-shiny/","section":"post","summary":"Dashboards for data visualisation, such as R Shiny and Tableau, allow the interactive exploration of data by means of drop-down lists and checkboxes, with no coding required from the final users. The apps can be useful for both the data analyst and the public. Visualisation apps run on internet browsers. This allows for three options: private viewing (useful during analysis), selective sharing (used within work groups), or internet publication. Among the available platforms, R Shiny and Tableau stand out due to being relatively accessible to new users. Apps serve a broad variety of purposes. In science and beyond, these apps allow us to go the extra mile in sharing data. Alongside files and code shared in repositories, we can present the data in a website, in the form of plots or tables. This facilitates the public exploration of each section of the data (groups, participants, trials...) to anyone interested, and allows researchers to account for their proceeding in the analysis.","tags":["data presentation","dashboard","reproducibility","open science","open data","R Shiny","Flexdashboard"],"title":"The case for data dashboards. First steps with R Shiny","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Hinno","R.","**Bernabeu, P.**"],"categories":["data presentation","R","big data"],"content":" 1. Introduction Racism has long been ingrained in human societies. Ancient Greek Aristotle already claimed that non-Greeks were slaves by nature, as they easily submitted to despotic government (Reilly, Kaufman, \u0026amp; Bodino, 2002). This study focuses on racism in the United States, which extends from the foundation of the country, when black people were generally born into slavery, and were at any rate regarded as an inferior people. US racism stands out globally for two reasons. First, the country has played a hegemonic part in the World since soon after its foundation. Second, the US is regarded as the most advanced society technology-wise, as it sets the minutes for the technology sector worldwide. In spite of these advantages, the country has long suffered the plague of widespread racism. Indeed, the abolition of slavery in the mid-nineteenth century did not grant equal citizen rights to the black population. Over time, the black population started to confront this situation. Especially the mid-nineteenth century saw large uprisings and a patent division of different societal sectors, as reflected in literary works such as Ellison’s ‘Invisible Man’ (1952). Inequality and confrontation about racism has extended to date, and the costs thereof have been large in terms of lives and otherwise (Feagin, 2004).\nWith the era of global communication, what happens in the World’s most powerful country is quickly and largely spread overseas—so too with racism matters. The last major such event related to racism happened during the first weeks of July 2016. Within five days, two cases of dubious, lethal police intervention with black citizens were followed by the killing of five policemen by a black youth. The specific course of events was as follows. On July 5th, Alton Sterling was killed by police officers in Louisiana. Next day, Philando Castile was also killed by police. In this case, the presence of Castile’s girlfriend during the tragedy likely determined the following events, because she described the event to the media, underscoring how gratuitous the killing was. During the following hours, outrage escalated within the already-wary population of the US. Yet the crisis would not stop there. During one of the various demonstrations held across the country, a dozen policemen were shot by a sniper, leaving five of them dead. The attacker was a black youth linked to black militant groups which target the Establishment on the grounds of patent racial discrimination. We will refer to this concatenation of events as the Louisiana-Minnesota-Dallas (LMD) crisis.1\nIn the welter of events in Louisiana, Minnesota and Dallas, some journalists warned of the return of social divisions such as those from the mid-nineteenth century. Such divisions might lead some people to an incomplete perception of the situation, and thus hinder the achievement of any solutions. However, President Obama denied such divisions as he spoke at the funeral for the policemen killed. At the same time, he addressed each of the different groups in the problem, including Establishment institutions and black protesters, advising them all to exercise greater open-mindedness towards the other aspects and bands in the problem (see statement).\nIt must be noted that this small study is primarily a way for us to practise data analysis at a course. Neither the background nor the analyses make a realistic study of racism or the LMD crisis.\n 2. Goals We wanted to look at these developments from the scope of online data. For this purpose, we scraped online discussions on these developments from a variety of media sources, and within defined time frames in the crisis. We then probed for any noticeable fluctuations in the topics throughout the course of events, and also for any differences across the different media. In this analysis, we took an exploratory approach by means of topic modeling. We wanted to check, first, whether topic modeling would be sensitive and useful at all within such a compact time scale. Were it to allow us, we would analyze how the journalistic and the social media reflected any fluctuations based on the live developments in Louisiana, Minnesota and Dallas. As such, the dependent variable (DV) in this study is the overall topic under discussion, which we measured via topic modeling. So, the language we analyze are messages related to the LMD crisis. Two factors are checked as potentially affecting the DV, namely Media and Time.\nThe Media factor regarded the three different sources of information from which we retrieved LMC content. These sources were: (1) the New York Times (NYT), (2) public tweets related to the NYT, and (3) public comments on the NYT’s Facebook posts.\nThe Time factor was based on the following periods. The first period, from 2 to 4 July, was selected as a baseline during which no remarkable events racism-wise happened. The second period, including 5 and 6 July, contains the days when the two black citizens were killed by policemen. The third period, from 7 to 11 July, contains the aftermath of the crisis overall.\n 3. Hypotheses We had several hypotheseses for our planned analyses. For the Media factor, we hypothesized a greater objectivity and formality overall for the NYT articles compared to the other two sources.\nWe did not have any hypotheses about the Time factor, i.e., the nature of any potential topic changes. In fact, we had considerable reservations as to whether any fluctuations would present, given the fact that the latent feeling of such a crisis might stays negative, critical and fearful from the start, regardless of particular events.\nWith respect to the interaction between the two factors, we hypothesized that the NYT articles would present the lowest degree of thematic variation, due to the fact that such journal pieces require time to investigate and write up—even if they are published online. Comparatively, popular comments on Twitter and Facebook would present more emotionality and subjectivity, and likely they would also present greater influence of immediate events. Furthermore, Twitter should be yet more immediate than Facebook.\nLast, with respect to the DV, we did not actually have any hypotheses about the nature of possible topic fluctuations.\n 4. Methods Online reactions to the LMD developments were scraped from various online sources. This content was constrained to language, bearing no extensions such as pictures or videos. In order to narrow the scope of the information, all scraped sources were related to The New York Times journal. The sources were, first, the NYT online edition (nytimes.com); second, public tweets related to NYT (@nytimes); and, third, public comments posted on the NYT page (@nytimes). Crucially, these sources are different in nature. Whereas the articles in the journal’s online edition broadly follow the standard article form of mainstream journals, Facebook comments on the page are aligned with the Facebook standards, that is, comparatively informal and outspoken. In accord, tweets referring to @nytimes follow the Twitter conventions, characterized by the 140-character restriction, and the relative immediacy of their information (Oh \u0026amp; Syn, 2015; Wang, He \u0026amp; Zhao, 2014; Josephson \u0026amp; Miller, 2015).\nThe method to scrape content related to the LMD crisis was through keywords. For the three media, the following keywords were entered, such that articles containing any of those words would be returned: ‘black’ OR ‘racism’ OR ‘police’ OR ‘dallas’ OR ‘alton’ OR ‘sterling’ OR ‘philando’ OR ‘castile.’ Further particulars are provided in turn.\nNew York Times. This scraping pipeline started from the official API site for NYT (https://developer.nytimes.com). Metadata was downloaded for 2,000 articles adjusting to the abovementioned keywords. This returned articles dating back to the start of the year. After preprocessing, 29 articles were returned for period 1; 53 for period 2; and 275 for period 3.\nTwitter. This scraping was performed through Geoff Jentry’s R package ‘twitteR’ (https://github.com/geoffjentry/twitteR). Here tweets were selected based on the same keywords, in addition to ‘@nytimes’. Due to the ten-day maximum range of Twitter’s API, no time range was entered. Retweets were removed. With the naked eye we realized that the tweets contained considerable information on Saudi events, we entered the word ‘saudi’ as a negative keyword. After preprocessing, 157 articles were returned for period 1; 489 for period 2; and 5025 for period 3.\nFacebook. The ‘RFacebook’ R package, by Pablo Barbera (https://github.com/pablobarbera/Rfacebook), was used for this scraping. The Facebook API currently allows for the download of all or any posts from one page, with no time restrictions (broader-search functions seem to have been deprecated). We downloaded any comments on the pages’ posts which adjusted to our keywords. After preprocessing, 195 articles were returned for period 1; 1107 for period 2; and 8724 for period 3.\nPreprocessing was performed equally for all sources—as standard in topic modeling, by removing non-relevant (‘stop-words’) and non-linguistic elements. Removed items included the names of the media, as well as numbers, punctuation, links, and technical signs such as @.\n 5. Results Our hypothesis about the Media factor was only partially confirmed. First, sentiment analysis showed that NYT posts were very tempered, with an average sentiment score near 0. In contrast, Twitter presented a rather negative sentiment (Thelwall, Buckley, \u0026amp; Paltoglou, 2011; Saif, He, Fernandez, \u0026amp; Alani, 2016). Yet, to our surprise, Facebook came out with a neutrality close to that of NYT articles, even if there was greater variance among the scores of the Facebook posts. These overall tendencies are illustrated in the plot below. Caution must recommended, however, when considering this sentiment analysis, as this technique is arguably fuzzy generally, and especially so with data under such a tight time frame. This is the case because sentiment analysis, as other big data techniques, capitalizes on the size of data. What it lacks on the precision aspect, compared to null-significance hypothesis testing, for instance, it compensates with the size of the samples, in which the noise is suppressed by thousands of cases. In this case, however, the sampling within only nine days of unusual circumstances calls for circumspection.\nNext, topic modeling was conducted on each source separately. The parameters for topic discovery were entered based on several attempts with different numbers of topics (K) and internal-coherence thresholds (alpha). Finally, topics were selected alike for every source, K = 3, alpha = .2. Below, the first ten words for each topic in each source are shown (note that columns are aligned rightward).\nNew York Times articles ## Foreign policy Shooting Elections ## 1 said police new ## 2 percent said one ## 3 vote officers people ## 4 year black can ## 5 brexit dallas like ## 6 will officer york ## 7 since shooting trump ## 8 european two july ## 9 british shot even ## 10 britain says just  Facebook comments ## Police People Black lives matter ## 1 police people black ## 2 gun will lives ## 3 cops can matter ## 4 people like white ## 5 officers get people ## 6 dont one racist ## 7 just just blm ## 8 man need police ## 9 get dont blacks ## 10 officer police obama  Tweets ## Racism Dallas shooting Philando shooting ## 1 black police police ## 2 police dallas blacks ## 3 white officers new ## 4 lives killed shooting ## 5 people protest philando ## 6 racism shooting force ## 7 amp shootings castile ## 8 matter alton use ## 9 stop sterling says ## 10 cops baton violence To start, it may stand out that different topics appear across sources, all the while some are indeed shared. This is perfectly normal for topic modeling on different sources, even when the same topic is being studied. Indeed, it is very relevant for us to remark on the inclusion of foreign affairs and election matters within the NYT articles, but not within people’s tweets and Facebook comments. This makes sense for several reasons. To start, the space a journalist counts on in a NYT article is considerable, compared to tweets, and also compared to ruling conventions of Facebook posts (users may write further, but the average simply will not). Second, the breadth of relation in NYT articles likely responds to the expectations from renown journalists to enrich the news with a broader contextualization. Furthermore, this extension of topics might correspond to the tacit but doubtless alignment of journals to concrete political agendas. While people commenting on Twitter or Facebook are plausibly characterized by just the same virtues and vices, their online reactions could be driven by more emotion and immediacy of focus than those of mass media journalists.\nFor greater visualization, we also provide some captions from the interactive LDAvis tool below. Please click on the figure titles to enjoy the full visualization.\n↑ LDAvis visualization of NYT articles (click to explore in detail)\n \n↑ LDAvis visualization of Facebook comments (click to explore in detail)\n \n↑ LDAvis visualization of tweets (click to explore in detail)\n \nIn order to specifically compare different content sources, we plotted the major language from two sources on the same plot, with an axis spanning from one source to the other, as shown below. The size of the words indicates the frequency of use, and the colour is essentially parallel with the axis, with specific different colours for different corpora, and darker hues for greater association.\n↑ Facebook comments and NYT articles\n ↑ Facebook comments and tweets\n ↑ Tweets and NYT articles\n \nWe went on to analyze the overlap in topics across journals, in order to quantitatively check whether some topics were indeed shared across sources, even if in different positions (for instance, topic 1 in some source and topic 3 in some other). We did this by means of cosine similarity scores. This scores represent the degree of similarity of two sources on a continuous scale from 0 to 1, where 1 would mean identical. The plots illustrate this comparisons in turn.\n↑ Similarity between Facebook comments and NYT articles ↑ Similarity between Facebook comments and tweets ↑ Similarity between tweets and NYT articles Last, the interaction of Time and Media was analyzed. As expected, we found differences in the way topics fluctuated over time in the different sources, albeit in unexpected ways. NYT and tweets articles presented great variation, suggesting day-bound sensitivity to the developments. This was to be expected from Twitter, as it is famous for its immediacy. However, the immediacy of NYT articles was rather surprising, as they might have lagged behind due to the necessary investigation and editing for such kind of journalistic pieces. Unlike traditional paper-based NYT articles, this immediacy is now enabled by the publication online. Another unexpected finding was the relative stillness of Facebook posts over time. Since they are published at the minute, and nowadays mostly from mobile, we had thought they would present greater immediacy than NYT articles. We could hypothesize on this, but this would be best analyzed in further research. The plot below illustrates this interaction.\n↑ Topic fluctuations over time for the three content sources\n   6. Discussion In this small-scale study, we analyzed the impact of a racism-related crisis in the American society online. This crisis started with the killing of two black citizens by policemen under dubious circumstances, which was followed by massive media attention and street demonstrations, and then continued to the fatal shooting of five policemen by a black militant (the crisis continued yet further after our analyses). The impact of this crisis was large, with a state funeral being organized for the killed policemen, and a presidential address warning of the direction of social tensions, and the need for greater empathy from all social sections involved.\nWe scraped the divided the social reaction to these events from three online sources, namely, the NYT online edition, public comments on the NYT Facebook page, and finally NYT-related tweets. The method was based on keywords highly relevant to this crisis, namely: ‘black’ OR ‘racism’ OR ‘police’ OR ‘dallas’ OR ‘alton’ OR ‘sterling’ OR ‘philando’ OR ‘castile’. We analyzed the Media factor and the Time factor separately, and more interestingly we looked at the interaction between these two factors.\nAs results, we found, first, that NYT articles were the most neutral, closely followed by Facebook posts. In contrast, tweets presented greater negativity overall. Next, we looked at topics within each time frame in each of the three sources. These topics differed across sources, even though there were also considerable overlaps. For instance, NYT articles and related tweets shared the content of their second topics, both of which revolved around ‘shooting.’ We went further to quantitatively measure any such overlaps or otherwise differences across sources. Cosine similarity—which ranges from 1, totally related, to 0, not related at all—confirmed our naked eye feeling. For instance, for the overlap between the aforementioned topics, there was a cosine similarity of .71. In contrast, a cosine of .03 came up for other comparisons, which also makes sense due to the intrinsic differences among these sources.\nLast, the interaction between Time and Source was qualitatively analyzed by means of a plot, and we found that Twitter and NYT articles were most sensitive to live developments in the crisis, whereas Facebook comments lagged behing in this immediacy. All of these findings were discussed within a framework of qualitative big data analysis.\nThe data mass probed in these analyses could be described as medium-sized data in the big data field. This field is relatively recent, and the sucessful, seminal examples we count on tend to feature larger sizes of data. In particular, for time frames, it is rather uncommon to find such a tight scale as we excerpted. This fact complicates the drawing of assured conclusions from our findings, because we lack well-known precedents along these lines. While the social sciences have developed their tools for small samples, the tools of big data are currently designed for the larger amounts of data.\n Additional materials All materials are made public on the RRisto GitHub page.\n References Ellison, R. (1952). Invisible Man. New York: Random House.\nFeagin, J. R. (2004). Documenting the Costs of Slavery, Segregation, and Contemporary Racism: Why Reparations Are in Order for African Americans Harvard BlackLetter Law Journal, 20, 49-81.\nJosephson, S., \u0026amp; Miller, J. S. (2015). Just State the Facts on Twitter: Eye Tracking Shows That Readers May Ignore Questions Posted by News Organizations On Twitter But Not on Facebook. Visual Communication Quarterly, 22(2), 94-105.\nOh, S., \u0026amp; Syn, S. Y. (2015). Motivations for sharing information and social support in social media: A comparative analysis of Facebook, Twitter, Delicious, YouTube, and Flickr. Journal Of The Association For Information Science And Technology, 66(10), 2045-2060.\nReilly, K., Kaufman, S., \u0026amp; Bodino, A. (2003). Racism: A Global Reader. London: M. E. Sharpe\nSaif, H., He, Y., Fernandez, M., \u0026amp; Alani, H. (2016). Contextual semantics for sentiment analysis of Twitter. Information Processing And Management, 52(1), 5-19.\nThelwall, M., Buckley, K., \u0026amp; Paltoglou, G. (2011). Sentiment in Twitter events. Journal Of The American Society For Information Science And Technology, 62(2), 406-418.\nWang, P., He, W., \u0026amp; Zhao, J. (2014). A Tale of Three Social Networks: User Activity Comparisons across Facebook, Twitter, and Foursquare. IEEE Internet Computing, 18(2), 10-15.\n  A later update: On July 17, 2016—days after the current analysis—, the LMD crisis was extended with the killing of two policemen in the same Louisiana city where Alton Sterling had been killed.↩\n   ","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"0093223f516afac1c9c4f2a2f197164f","permalink":"/2016/01/01/the-louisiana-minnesota-dallas-crisis-across-media-and-time-a-big-data-exercise/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/2016/01/01/the-louisiana-minnesota-dallas-crisis-across-media-and-time-a-big-data-exercise/","section":"post","summary":"Racism has long been ingrained in human societies. Ancient Greek Aristotle already claimed that non-Greeks were slaves by nature, as they easily submitted to despotic government (Reilly, Kaufman, \u0026 Bodino, 2002). This study focuses on racism in the United States, which extends from the foundation of the country, when black people were generally born into slavery, and were at any rate regarded as an inferior people. US racism stands out globally for two reasons. First, the country has played a hegemonic part in the World since soon after its foundation. Second, the US is regarded as the most advanced society technology-wise, as it sets the minutes for the technology sector worldwide. In spite of these advantages, the country has long suffered the plague of widespread racism. Indeed, the abolition of slavery in the mid-nineteenth century did not grant equal citizen rights to the black population. Over time, the black population started to confront this situation. Especially the mid-nineteenth century saw large uprisings and a patent division of different societal sectors, as reflected in literary works such as Ellison’s 'Invisible Man' (1952). Inequality and confrontation about racism has extended to date, and the costs thereof have been large in terms of lives and otherwise (Feagin, 2004).","tags":["big data","racism","web scraping","R","API","social media","news media"],"title":"The Louisiana-Minnesota-Dallas crisis across media and time: a big data exercise","type":"post"},{"authors":["**Bernabeu, P.**","P. Vogt"],"categories":null,"content":"Reference Bernabeu, P., \u0026amp; Vogt, P. (2015). Language evolution: current status and future directions. Tenth Language at the University of Essex (LangUE) Postgraduate Conference. https://researchgate.net/publication/280858062/\n","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"139129944442d259234f2f3367f68490","permalink":"/publication/bernabeu_vogt2015/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/bernabeu_vogt2015/","section":"publication","summary":"The topic of language evolution is characterised by the scarcity of records, but also by a large flow  of research  produced within  multiple subtopics  and perspectives.  Over the  past  few decades, significant advancement has been made on the geographical and temporal origins of language, while current work is rather devoted to the underpinnings of language, in brain, genes, body, and culture of humans. Much of this literature is polarized over the crucial dichotomy of nativism  versus  emergentism.  Our  state  of  affairs  report  also  confirms  a  high  degree  of speculation,  albeit  with a  decrease  for modelling. To  tackle the  speculation  and the large research flow, we propose a more impersonal kind of review, focused on the topic’s questions rather than on particular accounts. Another observation is that novel perspectives are on the rise.  One  of  these  highlights  the  importance  of  perceptual  cognition,  often  dubbed ‘embodiment,’ in  the earlier  evolution  of language.  In  following this  lead,  we adapted  a previous experiment which had investigated the correspondence between certain perceptual features of events, and different  grammatical orders arising as participants  acted out those events. That design made a perfect basis for us to put in an additional variable, namely the contrast  between  body-based  communication  (gestures),  and  more  disembodied communication (symbol matching). Albeit tentative, the results of this pilot experiment reveal a greater effect of the embodiment variable on the grammatical preferences, which we see as inviting further exploration of embodied cognition in language evolution.","tags":["language evolution","linguistics"],"title":"Language evolution: current status and future directions","type":"publication"}]