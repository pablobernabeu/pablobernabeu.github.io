
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>research methods on Pablo Bernabeu</title>
    <link>https://pablobernabeu.github.io/categories/research-methods/</link>
    <description>Recent content in research methods on Pablo Bernabeu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <copyright>Pablo Bernabeu, 2015—{year}. Licence: [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/). Email: pcbernabeu@gmail.com. No cookies operated by this website. Cookies only used by third-party systems such as [Disqus](https://help.disqus.com/en/articles/1717155-use-of-cookies).</copyright>
    <lastBuildDate>Sat, 09 Sep 2023 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://pablobernabeu.github.io/categories/research-methods/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>An inline script for OpenSesame to send EEG triggers via the serial port</title>
      <link>https://pablobernabeu.github.io/2023/an-inline-script-for-opensesame-to-send-eeg-triggers-via-the-serial-port/</link>
      <pubDate>Sat, 09 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2023/an-inline-script-for-opensesame-to-send-eeg-triggers-via-the-serial-port/</guid>
      <description>


&lt;p&gt;The &lt;a href=&#34;https://osdoc.cogsci.nl&#34;&gt;OpenSesame&lt;/a&gt; user base is skyrocketing but—of course—remains small in comparison to many other user bases that we are used to. Therefore, when developing an experiment in OpenSesame, there are still many opportunities to break the mould. When you need to do something beyond the standard operating procedure, it may take longer to find suitable resources than it takes when a more widespread tool is used. So, why would you still want to use OpenSesame? There are many reasons: it is free, open source, based on Python, stable enough thanks to more than a decade of usage and development, &lt;a href=&#34;https://github.com/open-cogsci/OpenSesame&#34;&gt;well maintained&lt;/a&gt;, and has a &lt;a href=&#34;https://forum.cogsci.nl&#34;&gt;community forum&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I recently faced a challenge when developing an EEG experiment that uses event-related potentials. I couldn’t find a way to send triggers via the serial port. Online, I could find several resources, such as a &lt;a href=&#34;https://stonekate.github.io/blog/opensesame&#34;&gt;script by Kate Stone that draws on the parallel port&lt;/a&gt;, an &lt;a href=&#34;https://github.com/esdalmaijer/opensesame_serial_port_trigger&#34;&gt;outdated plugin&lt;/a&gt;, and resources based on Python scripts in &lt;a href=&#34;https://discourse.psychopy.org/t/sending-triggers-to-brainvision-eeg-system-from-psychopy-coderview/11011/11&#34;&gt;PsychoPy&lt;/a&gt; and in &lt;a href=&#34;https://pyserial.readthedocs.io/en/latest/shortintro.html&#34;&gt;base Python&lt;/a&gt;. By standing on these giants’ shoulders, and on even more shoulders from &lt;a href=&#34;https://stackoverflow.com/a/76829646/7050882&#34;&gt;StackOverflow&lt;/a&gt;, I put together the following inline script for OpenSesame. The code must be placed in the &lt;a href=&#34;https://osdoc.cogsci.nl/4.0/manual/python/about/#inline_script-items&#34;&gt;&lt;code&gt;Prepare&lt;/code&gt; phase&lt;/a&gt;. The &lt;code&gt;Run&lt;/code&gt; phase can be empty.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2FEEG-tools-and-tips%2Fblob%2Fmain%2FOpenSesame_inline_script_to_send_EEG_triggers_via_serial_port&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
</description>
      
            <category>electroencephalography</category>
      
            <category>EEG</category>
      
            <category>OpenSesame</category>
      
            <category>opensource</category>
      
            <category>s</category>
      
      
            <category>research methods</category>
      
    </item>
    
    <item>
      <title>How to correctly encode triggers in Python and send them to BrainVision through serial port (useful for OpenSesame and PsychoPy)</title>
      <link>https://pablobernabeu.github.io/2023/encode-triggers-in-python-and-send-them-to-brainvision-through-serial-port-useful-for-opensesame-and-psychopy/</link>
      <pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2023/encode-triggers-in-python-and-send-them-to-brainvision-through-serial-port-useful-for-opensesame-and-psychopy/</guid>
      <description>


</description>
      
            <category>s</category>
      
            <category>electroencephalography</category>
      
            <category>EEG</category>
      
            <category>BrainVision</category>
      
            <category>triggers</category>
      
            <category>serial port</category>
      
            <category>python</category>
      
            <category>OpenSesame</category>
      
            <category>PsychoPy</category>
      
            <category>encoding</category>
      
            <category>binary</category>
      
      
            <category>research methods</category>
      
    </item>
    
    <item>
      <title>Intermixing stimuli from two loops randomly in OpenSesame</title>
      <link>https://pablobernabeu.github.io/2023/intermixing-stimuli-from-two-loops-randomly-in-opensesame/</link>
      <pubDate>Sat, 24 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2023/intermixing-stimuli-from-two-loops-randomly-in-opensesame/</guid>
      <description>


&lt;p&gt;I’m developing a slightly tricky design in &lt;a href=&#34;https://osdoc.cogsci.nl&#34;&gt;OpenSesame&lt;/a&gt; (a Python-based experiment builder).&lt;/p&gt;
&lt;p&gt;My stimuli comprise two kinds of sentences that contain different elements, and different numbers of elements. These sentences must be presented word by word. Furthermore, I need to attach triggers to some words in the first kind of sentences but not in the second kind. Last, these kinds of sentences must be intermixed within a block (or a &lt;code&gt;sequence&lt;/code&gt;) of trials, because the first kind are targets and the second kind are fillers.&lt;/p&gt;
&lt;p&gt;I need to present stimuli randomly from either of two &lt;code&gt;loop&lt;/code&gt;s within a &lt;code&gt;sequence&lt;/code&gt;. Currently, when I try this arrangement using the interface, the resulting script ends up with &lt;code&gt;run loop1&lt;/code&gt; followed by &lt;code&gt;run loop2&lt;/code&gt;, but I can’t find a way to ensure that the overarching sequence pulls randomly from either loop over successive trials. In other words, my goal is to intermix the stimuli from both loops.&lt;/p&gt;
&lt;p&gt;I have searched for clues in the OpenSesame tutorials, in the OpenSesame forum, and elsewhere online, but to no avail.&lt;/p&gt;
&lt;div id=&#34;solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solution&lt;/h2&gt;
&lt;p&gt;Finally, I solved this by first distributing all the words in each sentence across different columns in the stimulus file, yielding ‘word1’, ‘word2’, etc. Next, in OpenSesame, I created a sketchpad for each word. Last, to prevent showing any words that were blank, I entered conditions such as &lt;code&gt;[word7] != NA&lt;/code&gt; in the &lt;kbd&gt;Run if&lt;/kbd&gt; field of the sequence.&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>OpenSesame</category>
      
            <category>s</category>
      
      
            <category>research methods</category>
      
    </item>
    
    <item>
      <title>Simultaneously sampling from two variables in jsPsych</title>
      <link>https://pablobernabeu.github.io/2023/simultaneously-sampling-from-two-variables-in-jspsych/</link>
      <pubDate>Sat, 24 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2023/simultaneously-sampling-from-two-variables-in-jspsych/</guid>
      <description>


&lt;p&gt;I am using &lt;a href=&#34;https://www.jspsych.org&#34;&gt;jsPsych&lt;/a&gt; to create an experiment and I am struggling to sample from two variables simultaneously. Specifically, in each trial, I would like to present a &lt;code&gt;primeWord&lt;/code&gt; and a &lt;code&gt;targetWord&lt;/code&gt; by randomly sampling each of them from its own variable.&lt;/p&gt;
&lt;p&gt;I have looked into several resources—such as &lt;a href=&#34;https://www.jspsych.org/7.0/overview/timeline/index.html#sampling-without-replacement&#34;&gt;sampling without replacement&lt;/a&gt;, &lt;a href=&#34;https://github.com/jspsych/jsPsych/discussions/1076#discussioncomment-87298&#34;&gt;custom sampling&lt;/a&gt; and &lt;a href=&#34;https://github.com/jspsych/jsPsych/discussions/1911#discussioncomment-922511&#34;&gt;position indices&lt;/a&gt;—but to no avail. I’m a beginner at this, so it’s possible that one of these resources was relevant (especially the last one, I think).&lt;/p&gt;
&lt;p&gt;In addition to the parallel sampling, I wonder how I could save the same trial index in the &lt;code&gt;data&lt;/code&gt; of both &lt;code&gt;primeWord&lt;/code&gt; and &lt;code&gt;targetWord&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solution&lt;/h2&gt;
&lt;p&gt;Finally I solved this using position indices in a for-of loop (see below: ‘For loop that creates trial information iteratively over trials (3 steps)’).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;

  &amp;lt;head&amp;gt;

    &amp;lt;!-- jsPsych plugins --&amp;gt;
    &amp;lt;script src=&amp;quot;../jspsych.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src=&amp;quot;../plugins/jspsych-html-keyboard-response.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src=&amp;quot;../plugins/jspsych-html-button-response.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;

    &amp;lt;!-- CSS --&amp;gt;
    &amp;lt;link rel=&amp;quot;stylesheet&amp;quot; href=&amp;quot;../css/jspsych.css&amp;quot;&amp;gt;

    &amp;lt;style&amp;gt;
      body.jspsych-display-element {
        color: #ececec;
        background-color: #2b2b2b;
      }

      #jspsych-html-keyboard-response-stimulus {
        font-size: 32px;
      }

      .fas,
      .far {
        color: #b6b6b6;
      }

    &amp;lt;/style&amp;gt;

  &amp;lt;/head&amp;gt;


  &amp;lt;!-- Beginning of the script that contains the core of the experiment --&amp;gt;
  &amp;lt;script&amp;gt;
    /* Create empty timeline object, which will be sequentially filled in using timeline.push() */
    var timeline = [];

    var instructions = {
      type: &amp;#39;html-button-response&amp;#39;,
      stimulus: [&amp;quot;&amp;lt;p&amp;gt;Each screen will show a word in lower case, such as &amp;#39;target&amp;#39;. Press &amp;lt;b&amp;gt;F&amp;lt;/b&amp;gt; if the word is primarily abstract&amp;lt;/p&amp;gt;&amp;quot; +
        &amp;#39;&amp;lt;p&amp;gt;or &amp;lt;b&amp;gt;J&amp;lt;/b&amp;gt; if it is primarily concrete. Each word is presented for up to five seconds.&amp;lt;/p&amp;gt;&amp;#39;
      ],
      choices: [&amp;#39;Ready to start&amp;#39;]
    }
    /* Add instructions to the timeline */
    timeline.push(instructions)


    /* Stimuli */

    /* Begin with a general list of words, and randomly split the 
    list into a set of prime words and a set of target words. */

    var allWords = [{
        word: &amp;#39;word 1&amp;#39;,
        correct_response: &amp;#39;abstract&amp;#39;
      },
      {
        word: &amp;#39;word 2&amp;#39;,
        correct_response: &amp;#39;abstract&amp;#39;
      },
      {
        word: &amp;#39;word 3&amp;#39;,
        correct_response: &amp;#39;abstract&amp;#39;
      },
      {
        word: &amp;#39;word 4&amp;#39;,
        correct_response: &amp;#39;abstract&amp;#39;
      },
      {
        word: &amp;#39;word 5&amp;#39;,
        correct_response: &amp;#39;abstract&amp;#39;
      },
      {
        word: &amp;#39;word 6&amp;#39;,
        correct_response: &amp;#39;concrete&amp;#39;
      },
      {
        word: &amp;#39;word 7&amp;#39;,
        correct_response: &amp;#39;concrete&amp;#39;
      },
      {
        word: &amp;#39;word 8&amp;#39;,
        correct_response: &amp;#39;concrete&amp;#39;
      },
      {
        word: &amp;#39;word 9&amp;#39;,
        correct_response: &amp;#39;concrete&amp;#39;
      },
      {
        word: &amp;#39;word 10&amp;#39;,
        correct_response: &amp;#39;concrete&amp;#39;
      }

    ]

    /* Shuffle all words */
    var shuffled_allWords = allWords.sort(function() {
      return 0.5 - Math.random()
    });

    /* Split up the list into two sets */
    var midpoint = Math.floor(shuffled_allWords.length / 2);

    /* Set number of trials per participant (must be smaller than half of all words) */
    var number_of_trials = 4;

    /* Create the set of prime words */
    var primeWords = shuffled_allWords.slice(0, number_of_trials);

    /* Make prime words uppercase, as in Hutchison et al.
    (2013; https://doi.org/10.3758/s13428-012-0304-z) */

    var primeWords = primeWords.map(item =&amp;gt; ({
      ...item,
      word: item.word.toUpperCase()
    }))

    /* Create the set of target words */
    var targetWords = shuffled_allWords.slice(midpoint, midpoint + number_of_trials);


    /* Next, create set of interstimulus intervals from a range between 60 and 1200 ms. 
    First, the range is split into as many integers as the number of trials, equally 
    for all participants. Afterwards, the list is shuffled within participants. */

    function makeArr(startValue, stopValue, cardinality) {
      var arr = [];
      var step = (stopValue - startValue) / (cardinality - 1);
      for (var i = 0; i &amp;lt; cardinality; i++) {
        arr.push(startValue + (step * i));
      }
      return arr;
    }

    ordered_interstimulus_intervals = makeArr(60, 1200, number_of_trials);

    interstimulus_interval =
      ordered_interstimulus_intervals.sort(function() {
        return 0.5 - Math.random()
      });


    /* For loop that creates trial information iteratively over trials (3 steps) */

    /* 1. Enable function to create iterable range, to be used in the for loop below */
    const Range = (start, end) =&amp;gt; ({
      *[Symbol.iterator]() {
        while (start &amp;lt; end)
          yield start++;
      }
    })

    /* 2. Initialise stimuli array */
    stimuli = [];

    /* 3. Run loop */
    for (const i of Range(0, number_of_trials)) {
      stimuli.push({
        primeWord: primeWords[i].word,
        targetWord: targetWords[i].word,
        interstimulus_interval: interstimulus_interval[i],
        correct_response: targetWords[i].correct_response,
        trial: i + 1 /* 1 is added because, otherwise, trials would else trials would start from 0 */
      })
    }


    /* Trial content: fixation, primeWord, interstimulus interval, targetWord, feedback.
    This constitutes a unique trial in the semantic priming paradigm. Yet, beware that 
    jsPsych provides a &amp;#39;trial_index&amp;#39; value in the output of the task. That index is 
    assigned to each part of every trial. Thus, in the present experiment, there are 
    five trial_index values per trial--namely, one for each part listed above. */

    /* Fixation cross */
    var fixation = {
      type: &amp;#39;html-keyboard-response&amp;#39;,
      stimulus: &amp;#39;+&amp;#39;,
      response_ends_trial: false,
      trial_duration: function() {
        /* Set fixations with a varying duration to boost participants&amp;#39; attention */
        return jsPsych.randomization.sampleWithoutReplacement([400, 450, 500, 550, 600], 1)[0];
      },
      post_trial_gap: 0,
      data: {
        trial: jsPsych.timelineVariable(&amp;#39;trial&amp;#39;)
      },
      css_classes: [&amp;#39;stimulus&amp;#39;],
      /* Computation run at the end of each trial */
      on_finish: function(data) {
        /* Log key presses, if any, by writing 1 into fixation_keypresses (else, write 0) */
        if (data.key_press == null) {
          var fixation_keypresses = 0;
        } else {
          var fixation_keypresses = 1;
        };
        data.fixation_keypresses = fixation_keypresses
      }
    };

    var primeWord = {
      type: &amp;#39;html-keyboard-response&amp;#39;,
      stimulus: jsPsych.timelineVariable(&amp;#39;primeWord&amp;#39;),
      response_ends_trial: false,
      trial_duration: 150,
      post_trial_gap: 0,
      data: {
        position: &amp;#39;prime&amp;#39;,
        trial: jsPsych.timelineVariable(&amp;#39;trial&amp;#39;)
      },
      css_classes: [&amp;#39;stimulus&amp;#39;],
      /* Computation run at the end of each trial */
      on_finish: function(data) {
        /* Log key presses, if any, by writing 1 into primeWord_keypresses (else, write 0) */
        if (data.key_press == null) {
          var primeWord_keypresses = 0;
        } else {
          var primeWord_keypresses = 1;
        };
        data.primeWord_keypresses = primeWord_keypresses
      }
    };

    var interstimulus_interval = {
      type: &amp;#39;html-keyboard-response&amp;#39;,
      stimulus: &amp;#39; &amp;#39;,
      response_ends_trial: false,
      trial_duration: jsPsych.timelineVariable(&amp;#39;interstimulus_interval&amp;#39;),
      post_trial_gap: 0,
      data: {
        interstimulus_interval: jsPsych.timelineVariable(&amp;#39;interstimulus_interval&amp;#39;),
        trial: jsPsych.timelineVariable(&amp;#39;trial&amp;#39;)
      },
      css_classes: [&amp;#39;stimulus&amp;#39;],
      /* Computation run at the end of each trial */
      on_finish: function(data) {
        /* Log key presses, if any, by writing 1 into interstimulus_interval_keypresses (else, write 0) */
        if (data.key_press == null) {
          var interstimulus_interval_keypresses = 0;
        } else {
          var interstimulus_interval_keypresses = 1;
        };
        data.interstimulus_interval_keypresses = interstimulus_interval_keypresses
      }
    };

    var targetWord = {
      type: &amp;#39;html-keyboard-response&amp;#39;,
      stimulus: jsPsych.timelineVariable(&amp;#39;targetWord&amp;#39;),
      choices: [&amp;#39;f&amp;#39;, &amp;#39;j&amp;#39;],
      trial_duration: 3000,
      post_trial_gap: 0,
      css_classes: [&amp;#39;stimulus&amp;#39;],
      data: {
        position: &amp;#39;target&amp;#39;,
        trial: jsPsych.timelineVariable(&amp;#39;trial&amp;#39;),
        correct_response: jsPsych.timelineVariable(&amp;#39;correct_response&amp;#39;)
      },
      /* Computation run at the end of each trial */
      on_finish: function(data) {
        if (data.key_press !== null) {
          /* Label correct responses */
          if (data.correct_response == &amp;#39;abstract&amp;#39; &amp;amp;&amp;amp; data.key_press == jsPsych.pluginAPI.convertKeyCharacterToKeyCode(&amp;#39;f&amp;#39;) ||
            data.correct_response == &amp;#39;concrete&amp;#39; &amp;amp;&amp;amp; data.key_press == jsPsych.pluginAPI.convertKeyCharacterToKeyCode(&amp;#39;j&amp;#39;)) {
            var accuracy = &amp;#39;correct&amp;#39;;
            /* Label incorrect responses */
          } else if (data.correct_response == &amp;#39;abstract&amp;#39; &amp;amp;&amp;amp; data.key_press == jsPsych.pluginAPI.convertKeyCharacterToKeyCode(&amp;#39;j&amp;#39;) ||
            data.correct_response == &amp;#39;concrete&amp;#39; &amp;amp;&amp;amp; data.key_press == jsPsych.pluginAPI.convertKeyCharacterToKeyCode(&amp;#39;f&amp;#39;)) {
            var accuracy = &amp;#39;incorrect&amp;#39;;
          }
          /* Label unanswered trials */
        } else {
          var accuracy = &amp;#39;unanswered&amp;#39;;
        };
        data.accuracy = accuracy;
        /* Count up premature responses per trial. The command &amp;#39;last(4)&amp;#39; is used below
        to consider only the current part of the &amp;#39;trial&amp;#39; (i.e., targetWord) and the 
        three previous parts (i.e., interstimulus_interval, primeWord and fixation). 
        Notice that the response entered in this part (targetWord) is not added into 
        the sum, as it is appropriate to respond to the target word. */
        data.premature_responses =
          jsPsych.data.get().last(4).filter(&amp;#39;fixation_keypresses&amp;#39; == 1).select(&amp;#39;fixation_keypresses&amp;#39;).sum() +
          jsPsych.data.get().last(4).filter(&amp;#39;primeWord_keypresses&amp;#39; == 1).select(&amp;#39;primeWord_keypresses&amp;#39;).sum() +
          jsPsych.data.get().last(4).filter(&amp;#39;interstimulus_interval_keypresses&amp;#39; == 1).select(&amp;#39;interstimulus_interval_keypresses&amp;#39;).sum();
      }
    };

    feedback = {
      type: &amp;#39;html-keyboard-response&amp;#39;,
      stimulus: function() {
        var last_trial_accuracy = jsPsych.data.getLastTrialData().values()[0].accuracy;
        if (last_trial_accuracy == &amp;#39;incorrect&amp;#39;) {
          return &amp;#39;&amp;lt;p style=&amp;quot;color:red; font-face:bold;&amp;quot;&amp;gt;X&amp;lt;/p&amp;gt;&amp;#39;;
        } else if (last_trial_accuracy == &amp;#39;unanswered&amp;#39;) {
          return &amp;#39;&amp;lt;p style=&amp;quot;color:red; font-face:bold;&amp;quot;&amp;gt;0&amp;lt;/p&amp;gt;&amp;#39;
        } else {
          return &amp;#39;&amp;#39;
        }
      },
      choices: jsPsych.NO_KEYS,
      trial_duration: function() {
        var last_trial_accuracy = jsPsych.data.getLastTrialData().values()[0].accuracy;
        if (last_trial_accuracy == &amp;#39;correct&amp;#39;) {
          return 0
        } else {
          return 800
        }
      }
    };

    var main_procedure = {
      timeline: [fixation, primeWord, interstimulus_interval, targetWord, feedback],
      timeline_variables: stimuli
    };
    timeline.push(main_procedure);


    var debrief = {
      type: &amp;#39;html-keyboard-response&amp;#39;,
      choices: [&amp;#39;c&amp;#39;],
      stimulus: function() {
        var total_correct = jsPsych.data.get().filter({
          accuracy: &amp;#39;correct&amp;#39;
        }).count();
        var total_incorrect = jsPsych.data.get().filter({
          accuracy: &amp;#39;incorrect&amp;#39;
        }).count();
        var total_unanswered = jsPsych.data.get().filter({
          accuracy: &amp;#39;unanswered&amp;#39;
        }).count();
        var accuracy_rate = Math.round(total_correct / (total_correct + total_incorrect + total_unanswered) * 100) + &amp;quot;%&amp;quot;;
        var message = &amp;quot;&amp;lt;div style=&amp;#39;font-size:20px;&amp;#39;&amp;gt;&amp;lt;p&amp;gt;All done!&amp;lt;/p&amp;gt;&amp;quot; +
          &amp;quot;&amp;lt;p&amp;gt;Your accuracy rate was &amp;quot; + accuracy_rate + &amp;quot; (&amp;quot; + total_correct + &amp;quot; correct trials, &amp;quot; + total_incorrect +
          &amp;quot; incorrect and &amp;quot; + total_unanswered + &amp;quot; unanswered).&amp;lt;/p&amp;gt;&amp;quot; +
          &amp;quot;&amp;lt;p&amp;gt;Press C to see the entire set of data generated by this experiment.&amp;lt;/p&amp;gt;&amp;lt;/div&amp;gt;&amp;quot;;
        return message;
      }
    }
    /* Add debrief to the timeline */
    timeline.push(debrief);


    /* Initialize experiment by incorporating the timeline
    and setting the data to be displayed at the end. */
    jsPsych.init({
      timeline: timeline,
      on_finish: function() {
        jsPsych.data.displayData();
      },
      default_iti: function() {
        /* Use varying intertrial intervals to reduce habituation effects */
        return jsPsych.randomization.sampleWithoutReplacement([1300, 1400, 1500, 1600, 1700], 1)[0];
      }
    });

  &amp;lt;/script&amp;gt;

&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
      
            <category>jsPsych</category>
      
            <category>javascript</category>
      
            <category>s</category>
      
      
            <category>research methods</category>
      
    </item>
    
    <item>
      <title>Assigning participant-specific parameters automatically in OpenSesame</title>
      <link>https://pablobernabeu.github.io/2023/assigning-participant-specific-parameters-automatically-in-opensesame/</link>
      <pubDate>Wed, 17 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2023/assigning-participant-specific-parameters-automatically-in-opensesame/</guid>
      <description>


&lt;p&gt;OpenSesame offers options to counterbalance properties of the stimulus across participants. However, in cases of more involved assignments of session parameters across participants, it becomes necessary to write a bit of Python code in an inline script, which should be placed at the top of the timeline. In such a script, the participant-specific parameters are loaded in from a csv file. Below is a minimal example of the csv file.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;17%&#34; /&gt;
&lt;col width=&#34;22%&#34; /&gt;
&lt;col width=&#34;20%&#34; /&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;24%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;participant&lt;/th&gt;
&lt;th&gt;language&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;training_list&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;test_list&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;experiment_list&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;7&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;8&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Below is the corresponding inline script. The code &lt;code&gt;.iloc[0]&lt;/code&gt; at the end of the lines is used to select a cell.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;
# Assigning participant-specific parameters automatically in OpenSesame

import csv  # handle csv file
import pandas as pd  # handle data frames

participant_parameters = pd.read_csv(exp.get_file(&amp;#39;stimuli/parameters per participant.csv&amp;#39;))

var.participant = var.subject_nr

var.language = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;language&amp;#39;].iloc[0]

var.training_list = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;training_list&amp;#39;].iloc[0]

var.test_list = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;test_list&amp;#39;].iloc[0]

var.experiment_list = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;experiment_list&amp;#39;].iloc[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Finally, the variables created in the script can be used in &lt;kbd&gt;Run if&lt;/kbd&gt; conditions (e.g., &lt;code&gt;[language] == &#39;Mini-English&#39;&lt;/code&gt;), as replacements inside file names (e.g., &lt;code&gt;[language] training, List [training_list].csv&lt;/code&gt;), and as input in sketchpads (e.g., &lt;code&gt;Current list: [training_list]&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</description>
      
            <category>research methods</category>
      
            <category>OpenSesame</category>
      
            <category>python</category>
      
            <category>s</category>
      
      
            <category>research methods</category>
      
    </item>
    
    <item>
      <title>Specifying version number in OSF download links</title>
      <link>https://pablobernabeu.github.io/2023/specifying-version-number-in-osf-download-links/</link>
      <pubDate>Tue, 16 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2023/specifying-version-number-in-osf-download-links/</guid>
      <description>


&lt;p&gt;In the preparation of projects, files are often downloaded from OSF. It is good to document the URL addresses that were used for the downloads. These URLs can be provided in a code script (&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/data/semanticpriming_data_preparation.R#L294-L295&#34;&gt;see example&lt;/a&gt;) or in a README file. Better yet, it’s possible to specify the version of each file in the URL. This specification helps reduce the possibility of inaccuracies later, should any files be modified afterwards.&lt;/p&gt;
&lt;p&gt;The versions of files can be consulted on the right-hand side of the file page on OSF, as shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/OSF%20revisions.png&#34; width=&#34;550&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, the appropriate version can be specified in the download link by appending &lt;code&gt;?version=&lt;/code&gt;&lt;strong&gt;X&lt;/strong&gt; at the end. For instance, the seventh version is specified in the link &lt;a href=&#34;https://osf.io/hx6tz/download?version=7&#34;&gt;https://osf.io/hx6tz/download&lt;code&gt;?version=7&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
</description>
      
            <category>research methods</category>
      
            <category>s</category>
      
      
            <category>research methods</category>
      
    </item>
    
    <item>
      <title>Covariates are necessary to validate the variables of interest and to prevent bogus theories</title>
      <link>https://pablobernabeu.github.io/2023/covariates-are-necessary-to-validate-the-variables-of-interest-and-to-prevent-bogus-theories/</link>
      <pubDate>Mon, 15 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2023/covariates-are-necessary-to-validate-the-variables-of-interest-and-to-prevent-bogus-theories/</guid>
      <description>


&lt;p&gt;The need for covariates—or &lt;em&gt;nuisance variables&lt;/em&gt;—in statistical analyses is twofold. The first reason is purely statistical and the second reason is academic.&lt;/p&gt;
&lt;p&gt;First, the use of covariates is often necessary when the variable(s) of interest in a study may be connected to, and affected by, some satellite variables (Bottini et al., 2022; Elze et al., 2017; Sassenhagen &amp;amp; Alday, 2016). This complex scenario is the most common one due to the multivariate, dynamic, interactive nature of the real world.&lt;/p&gt;
&lt;p&gt;Second, the use of covariates is often necessary to prevent the development of bogus, redundant theories. Academics are strongly rewarded for developing theories. As we know, wherever there are strong rewards, there are serious risks. An academic could—consciously or not—produce a theory that is too closely related to an existing theory. So closely related are these theories that the second version might not warrant a name of its own. In such a scenario, covariates are useful and indeed necessary to vet the unique nature of the second version. That is, the first and the second version must be tested in the same model, and the variables corresponding to the first version can be construed as &lt;em&gt;covariates&lt;/em&gt;. This allows both the developers of the theories and the readers to compare the effects corresponding to each version of the theory, and to assess the degree of separation between them.&lt;/p&gt;
&lt;p&gt;The perverted use of covariates (Stefan &amp;amp; Schönbrodt, 2023)—however frequent and harmful—stands completely orthogonal to the correct usage of covariates, in the same way that a stethoscope can be used for good or for bad purposes. It would be poorly informed and misleading to conflate the correct and the incorrect uses, or to reject the use of covariates altogether due to the incorrect uses.&lt;/p&gt;
&lt;p&gt;In conclusion, the effects of interest in correlational/observational studies can be subject to mediation and moderation by satellite variables. These variables cannot be manipulated in correlational/observational studies, but they can—and often should—be included as covariates in the statistical models, to ward off spurious results and to vet similar theories.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bottini, R., Morucci, P., D’Urso, A., Collignon, O., &amp;amp; Crepaldi, D. (2022). The concreteness advantage in lexical decision does not depend on perceptual simulations. &lt;em&gt;Journal of Experimental Psychology: General, 151&lt;/em&gt;(3), 731–738. &lt;a href=&#34;https://doi.org/10.1037/xge0001090&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/xge0001090&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Elze, M. C., Gregson, J., Baber, U., Williamson, E., Sartori, S., Mehran, R., Nichols, M., Stone, G. W., &amp;amp; Pocock, S. J. (2017). Comparison of propensity score methods and covariate adjustment: Evaluation in 4 cardiovascular studies. &lt;em&gt;Journal of the American College of Cardiology, 69&lt;/em&gt;(3), 345-357.&lt;/p&gt;
&lt;p&gt;Sassenhagen, J., &amp;amp; Alday, P. M. (2016). A common misapplication of statistical inference: Nuisance control with null-hypothesis significance tests. &lt;em&gt;Brain and Language, 162&lt;/em&gt;, 42-45. &lt;a href=&#34;https://doi.org/10.1016/j.bandl.2016.08.001&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.bandl.2016.08.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Stefan, A. M., &amp;amp; Schönbrodt, F. D. (2023). Big little lies: A compendium and simulation of p-hacking strategies. &lt;em&gt;Royal Society Open Science, 10&lt;/em&gt;(2), 220346. &lt;a href=&#34;https://doi.org/10.1098/rsos.220346&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1098/rsos.220346&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>research methods</category>
      
            <category>statistics</category>
      
            <category>conflation</category>
      
            <category>s</category>
      
      
            <category>research methods</category>
      
            <category>statistics</category>
      
    </item>
    
    <item>
      <title>Language and vision in conceptual processing: Multilevel analysis and statistical power</title>
      <link>https://pablobernabeu.github.io/publication/language-vision-conceptual-processing/</link>
      <pubDate>Sat, 15 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/publication/language-vision-conceptual-processing/</guid>
      <description>


&lt;div id=&#34;reference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P., Lynott, D., &amp;amp; Connell, L. (2022). &lt;em&gt;Language and vision in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. OSF. &lt;a href=&#34;https://osf.io/dnskh&#34; class=&#34;uri&#34;&gt;https://osf.io/dnskh&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      
            <category>semantic processing</category>
      
            <category>semantic priming</category>
      
            <category>semantic decision</category>
      
            <category>lexical decision</category>
      
            <category>language</category>
      
            <category>vision</category>
      
            <category>visual strength</category>
      
            <category>cognition</category>
      
            <category>psycholinguistics</category>
      
            <category>reading</category>
      
            <category>linear mixed-effects models</category>
      
            <category>frequentist statistics</category>
      
            <category>lme4</category>
      
            <category>lmerTest</category>
      
            <category>Bayesian statistics</category>
      
            <category>brms</category>
      
            <category>power analysis</category>
      
            <category>sample size</category>
      
            <category>simr</category>
      
            <category>rstats</category>
      
            <category>R</category>
      
      
            <category>conceptual processing</category>
      
            <category>embodied cognition</category>
      
            <category>statistical power</category>
      
            <category>research methods</category>
      
            <category>statistics</category>
      
    </item>
    
    <item>
      <title>Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power</title>
      <link>https://pablobernabeu.github.io/publication/pablo-bernabeu-2022-phd-thesis/</link>
      <pubDate>Fri, 14 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/publication/pablo-bernabeu-2022-phd-thesis/</guid>
      <description>&lt;br&gt;
&lt;a href=&#39;https://pablobernabeu.github.io/thesis&#39;&gt;
&lt;button style = &#34;background-color: white; color: black; border: 2px solid #DF2E2E; border-radius: 12px;&#34;&gt;
&lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt;
&lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;i class=&#34;fas fa-mouse-pointer&#34;&gt;&lt;/i&gt;&lt;/span&gt;&amp;nbsp; Online book &lt;/h3&gt;&lt;/button&gt;
&lt;/a&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;div style = &#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;br&gt; 
</description>
      
            <category>object orientation effects</category>
      
            <category>replication</category>
      
            <category>semantic processing</category>
      
            <category>semantic priming</category>
      
            <category>semantic decision</category>
      
            <category>lexical decision</category>
      
            <category>language</category>
      
            <category>vision</category>
      
            <category>visual strength</category>
      
            <category>cognition</category>
      
            <category>psycholinguistics</category>
      
            <category>reading</category>
      
            <category>linear mixed-effects models</category>
      
            <category>frequentist statistics</category>
      
            <category>lme4</category>
      
            <category>lmerTest</category>
      
            <category>Bayesian statistics</category>
      
            <category>brms</category>
      
            <category>power analysis</category>
      
            <category>sample size</category>
      
            <category>simr</category>
      
            <category>rstats</category>
      
            <category>R</category>
      
      
            <category>conceptual processing</category>
      
            <category>embodied cognition</category>
      
            <category>statistical power</category>
      
            <category>research methods</category>
      
            <category>statistics</category>
      
    </item>
    
    <item>
      <title>Parallelizing simr::powercurve() in R</title>
      <link>https://pablobernabeu.github.io/2021/parallelizing-simr-powercurve/</link>
      <pubDate>Fri, 23 Jul 2021 16:46:54 +0100</pubDate>
      
      <guid>https://pablobernabeu.github.io/2021/parallelizing-simr-powercurve/</guid>
      <description>


&lt;p&gt;The &lt;code&gt;powercurve&lt;/code&gt; function from the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/simr/simr.pdf&#34;&gt;‘simr’&lt;/a&gt; (Green &amp;amp; MacLeod, 2016) can incur very long running times when the method used for the calculation of &lt;em&gt;p&lt;/em&gt; values is Kenward-Roger or Satterthwaite (see Luke, 2017). Here I suggest three ways for cutting down this time.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Where possible, use a high-performance (or high-end) computing cluster. This removes the need to use personal computers for these long jobs.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In case you’re using the &lt;code&gt;fixed()&lt;/code&gt; parameter of the &lt;code&gt;powercurve&lt;/code&gt; function, and calculating the power for different effects, run these at the same time (‘in parallel’) on different machines, rather than one after another.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Parallelize&lt;/em&gt; the &lt;code&gt;breaks&lt;/code&gt; argument. The &lt;code&gt;breaks&lt;/code&gt; argument of the &lt;code&gt;powercurve&lt;/code&gt; function allows the calculation of power for different levels of the grouping factor passed to &lt;code&gt;along&lt;/code&gt;. Some grouping factors are &lt;em&gt;participant&lt;/em&gt;, &lt;em&gt;trial&lt;/em&gt; and &lt;em&gt;item&lt;/em&gt;. The &lt;code&gt;breaks&lt;/code&gt; argument sets the different sample sizes for which power will be calculated. Parallelizing &lt;code&gt;breaks&lt;/code&gt; is done by running each number of levels in a separate function. When each has been run and saved, they are &lt;code&gt;c&lt;/code&gt;ombined to allow the plotting. This procedure is demonstrated below.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;parallelizing-breaks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parallelizing &lt;code&gt;breaks&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Let’s do a minimal example using a toy &lt;code&gt;lmer&lt;/code&gt; model. A power curve will be created for the fixed effect of &lt;code&gt;x&lt;/code&gt; along different sample sizes of the grouping factor &lt;code&gt;g&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Notice that the six sections of the power curve below are serially arranged, one after another. In contrast, to enable parallel processing, each power curve would be placed in a single script, and they would all be run at the same time.&lt;/p&gt;
&lt;p&gt;Although the power curves below run in a few minutes, the settings that are often used (e.g., a larger model; &lt;code&gt;fixed(&#39;x&#39;, &#39;sa&#39;)&lt;/code&gt; instead of &lt;code&gt;fixed(&#39;x&#39;)&lt;/code&gt;; &lt;code&gt;nsim = 500&lt;/code&gt; instead of &lt;code&gt;nsim = 50&lt;/code&gt;) take far longer. That is where parallel processing becomes useful.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
library(simr)

# Toy model with data from &amp;#39;simr&amp;#39; package
fm = lmer(y ~ x + (x | g), data = simdata)

# Extend sample size of `g`
fm_extended_g = extend(fm, along = &amp;#39;g&amp;#39;, n = 12)

# Parallelize `breaks` by running each number of levels in a separate function.

# 4 levels of g
pwcurve_4g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 4, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 6 levels of g
pwcurve_6g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 6, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 8 levels of g
pwcurve_8g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 8, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 10 levels of g
pwcurve_10g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 10, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 12 levels of g
pwcurve_12g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 12, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Having saved each section of the power curve, we must now combine them to be able to plot them together (if you wish to automatise this procedure, consider &lt;a href=&#34;https://github.com/pablobernabeu/Language-and-vision-in-conceptual-processing-Multilevel-analysis-and-statistical-power/blob/main/R_functions/combine_powercurve_chunks.R&#34;&gt;this function&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a destination object using any of the power curves above.
all_pwcurve = pwcurve_4g

# Combine results
all_pwcurve$ps = c(pwcurve_4g$ps[1], pwcurve_6g$ps[1], pwcurve_8g$ps[1], 
                   pwcurve_10g$ps[1], pwcurve_12g$ps[1])

# Combine the different numbers of levels.
all_pwcurve$xval = c(pwcurve_4g$nlevels, pwcurve_6g$nlevels, pwcurve_8g$nlevels, 
                     pwcurve_10g$nlevels, pwcurve_12g$nlevels)

print(all_pwcurve)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Power for predictor &amp;#39;x&amp;#39;, (95% confidence interval),
## by number of levels in g:
##       4: 46.00% (31.81, 60.68) - 40 rows
##       6: 74.00% (59.66, 85.37) - 60 rows
##       8: 92.00% (80.77, 97.78) - 80 rows
##      10: 98.00% (89.35, 99.95) - 100 rows
##      12: 100.0% (92.89, 100.0) - 120 rows
## 
## Time elapsed: 0 h 0 m 7 s&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(all_pwcurve, xlab = &amp;#39;Levels of g&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2021/parallelizing-simr-powercurve/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For reproducibility purposes
sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 22621)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.utf8 
## [2] LC_CTYPE=English_United States.utf8   
## [3] LC_MONETARY=English_United States.utf8
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] simr_1.0.7          lme4_1.1-32         Matrix_1.5-3       
## [4] knitr_1.42          xaringanExtra_0.7.0
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.2.0 xfun_0.38        bslib_0.4.2      purrr_1.0.1     
##  [5] splines_4.2.3    lattice_0.20-45  carData_3.0-5    vctrs_0.6.1     
##  [9] generics_0.1.3   htmltools_0.5.5  yaml_2.3.7       mgcv_1.8-42     
## [13] utf8_1.2.3       rlang_1.1.0      jquerylib_0.1.4  nloptr_2.0.3    
## [17] pillar_1.9.0     glue_1.6.2       uuid_1.1-0       plyr_1.8.8      
## [21] binom_1.1-1.1    lifecycle_1.0.3  stringr_1.5.0    blogdown_1.16   
## [25] evaluate_0.21    fastmap_1.1.1    RLRsim_3.1-8     parallel_4.2.3  
## [29] pbkrtest_0.5.2   fansi_1.0.4      broom_1.0.4      Rcpp_1.0.10     
## [33] backports_1.4.1  plotrix_3.8-2    cachem_1.0.7     jsonlite_1.8.4  
## [37] abind_1.4-5      digest_0.6.31    stringi_1.7.12   bookdown_0.33.3 
## [41] dplyr_1.1.1      grid_4.2.3       cli_3.4.1        tools_4.2.3     
## [45] magrittr_2.0.3   sass_0.4.6       tibble_3.2.1     tidyr_1.3.0     
## [49] car_3.1-2        pkgconfig_2.0.3  MASS_7.3-58.3    minqa_1.2.5     
## [53] rmarkdown_2.21   rstudioapi_0.14  iterators_1.0.14 R6_2.5.1        
## [57] boot_1.3-28.1    nlme_3.1-162     compiler_4.2.3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;just-the-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Just the code&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;
library(lme4)
library(simr)

# Toy model
fm = lmer(y ~ x + (x | g), data = simdata)

# Extend sample size of `g`
fm_extended_g = extend(fm, along = &amp;#39;g&amp;#39;, n = 12)

# Parallelize `breaks` by running each number of levels in a separate function.

# 4 levels of g
pwcurve_4g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 4, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 6 levels of g
pwcurve_6g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 6, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 8 levels of g
pwcurve_8g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 8, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 10 levels of g
pwcurve_10g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 10, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 12 levels of g
pwcurve_12g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 12, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)


# Create a destination object using any of the power curves above.
all_pwcurve = pwcurve_4g

# Combine results
all_pwcurve$ps = c(pwcurve_4g$ps[1], pwcurve_6g$ps[1], pwcurve_8g$ps[1], 
                   pwcurve_10g$ps[1], pwcurve_12g$ps[1])

# Combine the different numbers of levels.
all_pwcurve$xval = c(pwcurve_4g$nlevels, pwcurve_6g$nlevels, pwcurve_8g$nlevels, 
                     pwcurve_10g$nlevels, pwcurve_12g$nlevels)


print(all_pwcurve)

plot(all_pwcurve, xlab = &amp;#39;Levels of g&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Brysbaert, M., &amp;amp; Stevens, M. (2018). Power analysis and effect size in mixed effects models: A tutorial. &lt;em&gt;Journal of Cognition, 1&lt;/em&gt;(1), 9. &lt;a href=&#34;http://doi.org/10.5334/joc.10&#34; class=&#34;uri&#34;&gt;http://doi.org/10.5334/joc.10&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Green, P., &amp;amp; MacLeod, C. J. (2016). SIMR: An R package for power analysis of generalized linear mixed models by simulation. &lt;em&gt;Methods in Ecology and Evolution 7&lt;/em&gt;(4), 493–498, &lt;a href=&#34;https://doi.org/10.1111/2041-210X.12504&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/2041-210X.12504&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kumle, L., Vo, M. L. H., &amp;amp; Draschkow, D. (2021). Estimating power in (generalized) linear mixed models: An open introduction and tutorial in R. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, 1–16. &lt;a href=&#34;https://doi.org/10.3758/s13428-021-01546-0&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13428-021-01546-0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Luke, S. G. (2017). Evaluating significance in linear mixed-effects models in R. &lt;em&gt;Behavior Research Methods, 49&lt;/em&gt;(4), 1494–1502. &lt;a href=&#34;https://doi.org/10.3758/s13428-016-0809-y&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13428-016-0809-y&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The number of simulations set by &lt;code&gt;nsim&lt;/code&gt; should be larger (Brysbaert &amp;amp; Stevens, 2018; Green &amp;amp; MacLeod, 2016). In addition, the effect size for &lt;code&gt;x&lt;/code&gt; should be adjusted to the value that best fits with the planned study (Kumle et al., 2021).&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
            <category>s</category>
      
            <category>power analysis</category>
      
      
            <category>R</category>
      
            <category>research methods</category>
      
    </item>
    
    <item>
      <title>Brief Clarifications, Open Questions: Commentary on Liu et al. (2018)</title>
      <link>https://pablobernabeu.github.io/2021/brief-clarifications-open-questions-commentary-on-liu-et-al-2018/</link>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2021/brief-clarifications-open-questions-commentary-on-liu-et-al-2018/</guid>
      <description>&lt;p&gt;Liu et al. (2018) present a study that implements the conceptual modality switch (CMS) paradigm, which has been used to investigate the modality-specific nature of conceptual representations (Pecher et al., 2003). Liu et al.&amp;lsquo;s experiment uses event-related potentials (ERPs; similarly, see Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013). In the design of the switch conditions, the experiment implements a corpus analysis to distinguish between purely-embodied modality switches and switches that are more liable to linguistic bootstrapping (also see Bernabeu et al., 2017; Louwerse &amp;amp; Connell, 2011). The procedure for stimulus selection was novel as well as novel; thus, it could prove useful in future studies, too. In addition, the application of bayesian statistics is an interesting and promising novelty in the present research area.&lt;/p&gt;
&lt;p&gt;In reviewing the literature, Liu (2018) and Liu et al. (2018) contend that previous studies may be strongly biased due to methodological decisions in the analysis of ERPs. These decisions particularly regard the latency&amp;mdash;i.e., time windows&amp;mdash;and the topographic regions of interest&amp;mdash;i.e., subsets of electrodes. Thus, Liu et al. identify a &amp;lsquo;highly inconsistent&amp;rsquo; (p. 6) landscape in the ERP components that have been ascribed to the CMS effect in previous studies. Similarly, Liu (p. 47) writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Several studies have looked for the ERP manifestations of modality switching costs (Bernabeu, Willems, &amp;amp; Louwerse, 2017; Collins, Pecher, Zeelenberg, &amp;amp; Coulson, 2011; Hald, Hocking, Vernon, Marshall, &amp;amp; Garnham, 2013; Hald, Marshall, Janssen, &amp;amp; Garnham, 2011). However, what they found was not a clear picture. Not only was a significant effect found in the time window for the N400 component, but also a so-called early N400-like effect around 300ms (Bernabeu et al., 2017; Hald et al., 2011), the N1-P2 complex around 200ms (Bernabeu et al., 2017; Hald et al., 2013, 2011), as well as the late positivity component (LPC) after 600ms (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;drastic-conclusions&#34;&gt;Drastic conclusions&lt;/h3&gt;
&lt;p&gt;Liu et al. (2018) conclude that a confirmatory research approach is not warranted, and adopt a semi-exploratory approach, creating time windows of 50 ms each, rather than windows linked to known ERP components (Swaab et al., 2012), and using bayesian statistics. Such a drastic conclusion appears to stem from the assumption that, if the CMS were robust enough, it would present in the same guise across studies. Such an assumption, however, may merit further examination, considering the multiplicity of known and unknown variables that may differ across experiments (Barsalou, 2019). This variability influences the &lt;em&gt;replication praxis&lt;/em&gt;, as it were. Indeed, Liu et al. themselves allude to one such variable (p. 7).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;These previous studies did not only examine the effect of modality switching but also other linguistic factors such as negated sentences, which could easily distort observed waveforms (Luck, 2005).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu et al.&amp;lsquo;s (2018) conclusions about the &amp;lsquo;inconsistent&amp;rsquo; results do not seem to duly weigh the differences across studies. None of the existing studies is a direct replication of another. The example quoted from Liu et al. above, regarding the presence of negated sentences, is one of the less important differences because one of the corresponding studies implemented the negation as a controlled, experimental condition, contrasting with affirmative sentences (Hald et al., 2013). Yet, other differences exist in virtually every aspect, from the types of modality switch used to the time-locking of ERPs. For instance, the studies vary in the implementation of the modality switches. Whereas Hald et al. (2011) distinguish between a switch and a non-switch condition, Bernabeu et al. (2017) analysed each type of switch separately&amp;mdash;i.e., auditory-to-visual, haptic-to-visual, visual-to-visual. Another difference across the studies is the onset point for ERPs. For instance, Bernabeu et al. time-locked ERPs to the point at which the modality switch is actually elicited in the CMS paradigm&amp;mdash;namely, the first word in target trials. In contrast, the other studies time-locked ERPs to the second word. In addition, these studies vary in their timelines&amp;mdash;i.e., presentation of words and inter-stimulus intervals&amp;mdash;, as well as in the words that were used as stimuli, in the preprocessing of ERPs, in the statistical analysis, and even in the language of testing in one case (all studies using English except Bernabeu et al., 2017, which used Dutch). Moreover, the studies differ in the sample size, ranging from ten finally-analysed participants (Hald et al, 2011) to 46 finally-analysed participants (Bernabeu et al., 2017). Last, the studies differ in the number of items per modality switch condition, ranging from 17 (in one of Liu et al.&amp;lsquo;s, 2018 conditions) to 40 per condition (Hald et al., 2011, 2013). Undoubtedly, seeing larger sample sizes and more stimulus items used in ERP studies is something to promote and celebrate. Last, it may be noted that Liu et al.&amp;lsquo;s stance on the inconsistency of previous results starkly contrasts with their use of a single study&amp;mdash;Hald et al. (2011)&amp;mdash;, with &lt;em&gt;N&lt;/em&gt; = 10, as the motivation for their sample size (Albers &amp;amp; Lakens, 2018).&lt;/p&gt;
&lt;h3 id=&#34;clarifications&#34;&gt;Clarifications&lt;/h3&gt;
&lt;p&gt;Liu et al. (2018) apply bayesian statistics reportedly to help reduce the bias that may exist in the present literature. However, the reviews by Liu (2018) and Liu et al. appear to gloss over some important aspects regarding previous studies. For instance, Liu writes (p. 53):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The inflation of the probability of Type I error leads to an over-confidence in the interpretation of the results. For example, in the studies on modality switching costs, different time windows were chosen to test the early effect of modality switching costs. While Bernabeu et al. (2017), Hald et al. (2011) and Hald et al. (2013) examined the segment of ERP waveform between 190ms and 300ms or 160ms and 215ms based on visual inspection and found significant effects, Collins et al. (2011) chose a prescribed time window between 100ms and 200ms before the analysis and did not find the effect.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu also refers to the issue of multiple tests related to the multiple time windows and electrodes we find in ERP studies (p. 59):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It is typical for ERP studies to conduct multiple comparisons (e.g., running the same ANOVA repeatedly on different subsets of data like different time windows and different groups of electrodes). This would massively increase Type I error if no post hoc correction is conducted. However, if Bonferroni or other correction is conducted, it will render the study over-conservative, thus increasing the chance of Type II error. In the present thesis, 90 electrodes will be analysed individually, with 20 time slices in each trial. That results in 1800 NHSTs for each critical variable. A correction of multiple comparison will require a critical level of 2.78 x 10ˆ-5 for each test for a family-wise critical level of .05 (and an uncorrected test will almost definitely lead to false positive results). This stringent criterion could conceivably render it meaningless any p-values we can obtain from a statistical package.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Arguably, the scenario presented by Liu (2018), in which a researcher could conduct a purely data-driven analysis of ERP data, is extreme. The field of psycholinguistics, in general, does not have a tradition of purely data-driven analysis. Instead, it blends a humanistic background with a scientific methodology. As a result, the hypotheses and methods tend to be largely driven by the available literature. For instance, Bernabeu et al. (2017, quoted below from p. 1632) based their time windows and regions of interest on the most relevant of the preceding studies (also see Bernabeu, 2017).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Electrodes were divided into an anterior and a posterior area (also done in Hald et al., 2011). Albeit a superficial division, we found it sufficient for the research question. Time windows were selected as in Hald et al., except for the last window, which was extended up to 750 ms post word onset, instead of 700 ms, because the characteristic component of that latency tends to extend until then, as we confirmed by visual inspection of these results.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The literature-based approach follows the advice from Luck and Gaspelin (2017, p. 149), who wrote: &amp;lsquo;a researcher who wants to avoid significant but bogus effects would be advised to focus on testing a priori predictions without using the observed data to guide the selection of time windows or electrode sites&amp;rsquo; (also see Armstrong, 2014; for a more conservative stance, see Luck &amp;amp; Gaspelin, 2017). In addition, notice that the extension of the last window by 50 ms was informed by Swaab et al. (2012), who report results by which the P600 component (the main component occurring after the N400 in word reading) extended up to 800 ms.&lt;/p&gt;
&lt;p&gt;Next, Liu et al. (2018, pp. 6&amp;ndash;7) write:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;However, the findings of these components have been highly inconsistent. The N400 effect alone was found in the posterior region in some cases (Bernabeu et al., 2017; Hald et al., 2013), while in anterior region in others (Collins et al., 2011, Hald et al. (2011)).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yet, Bernabeu et al. (2017, p. 1632) had written:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In certain parts over the time course, the effect appeared in both anterior and posterior areas&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu et al. (2018) continue (p. 7):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In some cases, it was found in the typical window around 400ms (Collins et al., 2011), while in others an earlier window from 270ms to 370ms (Bernabeu et al., 2017; Hald et al., 2011).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, Bernabeu et al. (2017, p. 1632) had written:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The ERP results revealed a CMS effect from Time Window 1 on, larger after 350 ms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Regarding the time-locking of ERPs, Liu (2018, p. 43) writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Because the properties were usually salient for the concepts, the switching costs might have already been incurred when participants were processing the concept word. Bernabeu et al. (2017), in their recent replication of previous ERP studies, reversed the order of concept and property and did not find an immediate effect from the property onset. In future studies, it is recommended to adopt the reverse order, control the concept words so that they do not automatically activate the properties before the words are shown, or analyse epochs after both the concept and property words.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above excerpt seems to reveal a misunderstanding of Bernabeu et al.&amp;lsquo;s (2017) method and results (we may assume that, by &amp;lsquo;immediate&amp;rsquo;, Liu (2018) is referring to the 200 ms point or afterwards, since that is about as immediate as it gets; see Amsel et al., 2014; Swaab et al., 2012; Van Dam et al., 2014). Bernabeu et al.&amp;lsquo;s abstract mentioned (p. 1629):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Event-Related Potentials (ERPs) were time-locked to the onset of the first word (property) in the target trials so as to measure the effect online and to avoid a within-trial confound. A switch effect was found, characterized by more negative ERP amplitudes for modality switches than no-switches. It proved significant in four typical time windows from 160 to 750 milliseconds post word onset, with greater strength in posterior brain regions, and after 350 milliseconds.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Indeed, time-locking ERPs to the beginning of the trial was one of the principal features of Bernabeu et al.&amp;lsquo;s (2017) experiment. Complementing that, the property words were placed in the first trial because they are more perceptually loaded than concepts (Lynott &amp;amp; Connell, 2013), thus better suiting the main basis of the CMS paradigm. We know that the semantic processing of a word often commences within the first 200 ms (Amsel et al., 2014; Van Dam et al., 2014). Considering the importance of the time course in the grounding of conceptual representations (Hauk, 2016), it seems important to measure the CMS from the moment that it is elicited&amp;mdash;namely, in all experiments, from the first word of the target trial (Bernabeu, 2017; Bernabeu et al., 2017), rather than letting several hundreds of milliseconds elapse. Nonetheless, from a methodological perspective, it would be interesting to compare the two approaches in a dedicated study. This would precisely reveal the speed at which modality-specific meaning becomes activated during conceptual processing.&lt;/p&gt;
&lt;h3 id=&#34;outstanding-issues-random-effects-and-correction-for-multiple-tests&#34;&gt;Outstanding issues: Random effects and correction for multiple tests&lt;/h3&gt;
&lt;p&gt;A methodological issue affecting the statistical analysis of all the studies hereby considered (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013; Liu et al., 2018) is the absence of some applicable random effects. Some of the studies did not apply any random effects (Collins et al., 2011; Hald et al., 2011, 2013). Even in psycholinguistics, use of linear mixed-effects models is still increasing (Meteyard &amp;amp; Davies, 2020; Yarkoni, 2020). Yet, in those studies that did apply random effects, the corresponding  structure was not as exhaustive as it should have been, as they lacked random slopes. In Bernabeu et al. (2017), a model selection approach was applied (Matuschek et al., 2017), whereby each random effect was tested and only kept in the model if it significantly improved the fit. In Liu et al. (2018), random slopes were deemed unfeasible due to computational constraints (for background, see Brauer &amp;amp; Curtin, 2017). Applying a complete random effects structure is important for a robust statistical analysis (Barr et al., 2013; Yarkoni, 2020).&lt;/p&gt;
&lt;p&gt;Another issue is that of multiple tests. Where a small number of levels is used (e.g., time windows, topographic regions of interest) and these are informed by the literature, the advice has often been ambiguous as to whether a correction should be applied (e.g., Armstrong, 2014; for a more conservative stance, see Luck &amp;amp; Gaspelin, 2017). Indeed, no correction was applied in any of the four studies that used frequentist statistics (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013).&lt;/p&gt;
&lt;h4 id=&#34;reanalysis-of-bernabeu-et-al-2017&#34;&gt;Reanalysis of Bernabeu et al. (2017)&lt;/h4&gt;
&lt;p&gt;The results of Bernabeu et al. (2017) were reanalysed after publication using a more complete random effects structure that incorporated by-participant random slopes for the modality-switch factor&amp;mdash;i.e., &lt;code&gt;(condition | participant)&lt;/code&gt;. The results were also corrected for multiple tests using the Holm-Bonferroni correction (Holm, 1979). For this purpose, the lowest p-value in the four time windows was multiplied by 4, the next p-value by 3, the next by 2, and the highest p-value was left as it unmodified. In this stepwise correction, if a nonsignificant p-value was reached, all the subsequent p-values became nonsignificant (see &lt;a href=&#34;https://osf.io/unvfs/&#34;&gt;analysis script&lt;/a&gt;). The &lt;a href=&#34;https://osf.io/qhe5s/&#34;&gt;results&lt;/a&gt; differed from the original, slopes-free models (available &lt;a href=&#34;https://osf.io/sx3nw/&#34;&gt;script&lt;/a&gt; and &lt;a href=&#34;https://osf.io/4v38d/&#34;&gt;results&lt;/a&gt;) in that the main effect of the modality switch factor became nonsignificant in the second time window (160&amp;ndash;216 ms) and in the fourth one (500&amp;ndash;750 ms), while remaining significant in the third time window (350&amp;ndash;550 ms). Incidentally, note that main effects may not be directly interpretable as the same same variables are present interactions (Kam &amp;amp; Franzese, 2007). Yet, the interactions of modality switch with participant group (quick/slow) and scalp location (anterior/posterior) retained the same significance.&lt;/p&gt;
&lt;h3 id=&#34;open-questions&#34;&gt;Open questions&lt;/h3&gt;
&lt;p&gt;Liu (2018) and Liu et al. (2018) raise interesting and important questions. Firstly, future research may be conducted to investigate what determines the variability of ERP results&amp;mdash;in terms of ERP components, time windows and topographic regions of interest. This research could include a comparison with other measurements, such as response times, to test whether ERPs are less reliable&amp;mdash;i.e., more variable across studies&amp;mdash;than response times. Similarly, future research may investigate whether the ERP literature is more biased than literature employing other measures, such as response times. In addition, future research could investigate whether moving to exploratory, bayesian research designs is a necessary or sufficient condition to reduce bias in research and improve the precision of experimental measurements. Current alternatives to such an approach include direct (or conceptual) replications designed to achieve a higher power than previous studies (e.g., Chen et al., 2019). Arguably, policies determining funding decisions would need to change if we are to fully acknowledge the importance of &lt;em&gt;direct&lt;/em&gt; replication (Howe &amp;amp; Perfors, 2018; Kunert, 2016; Simons, 2014; Zwaan et al., 2018). Last, future research may investigate whether &amp;lsquo;clear picture&amp;rsquo; results are realistic, desirable or necessary, and whether unclear-picture results should be eschewed; or whether, on the contrary, clear-picture results may largely be the product of publication bias&amp;mdash;that is, the pressure to hide or misreport those aspects of a study that could challenge its acceptance by peer-reviewers or any other academics.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;div style = &#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Albers, C., &amp;amp; Lakens, D. (2018). When power analyses based on pilot data are biased: Inaccurate effect size estimators and follow-up bias. &lt;em&gt;Journal of Experimental Social Psychology, 74&lt;/em&gt;, 187–195. &lt;a href=&#34;https://doi.org/10.1016/j.jesp.2017.09.004&#34;&gt;https://doi.org/10.1016/j.jesp.2017.09.004&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Amsel, B. D., Urbach, T. P., &amp;amp; Kutas, M. (2014). Empirically grounding grounded cognition: the case of color. &lt;em&gt;Neuroimage, 99&lt;/em&gt;, 149-157. &lt;a href=&#34;https://doi.org/10.1016/j.neuroimage.2014.05.025&#34;&gt;https://doi.org/10.1016/j.neuroimage.2014.05.025&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Armstrong, R. A. (2014). When to use the Bonferroni correction. &lt;em&gt;Ophthalmic and Physiological Optics, 34&lt;/em&gt;(5), 502-508. &lt;a href=&#34;https://doi.org/10.1111/opo.12131&#34;&gt;https://doi.org/10.1111/opo.12131&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barr, D. J., Levy, R., Scheepers, C., &amp;amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. &lt;em&gt;Journal of Memory and Language, 68&lt;/em&gt;, 255–278. &lt;a href=&#34;http://dx.doi.org/10.1016/j.jml.2012.11.001&#34;&gt;http://dx.doi.org/10.1016/j.jml.2012.11.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barsalou, L. W. (2019). Establishing generalizable mechanisms. &lt;em&gt;Psychological Inquiry, 30&lt;/em&gt;(4), 220-230. &lt;a href=&#34;https://doi.org/10.1080/1047840X.2019.1693857&#34;&gt;https://doi.org/10.1080/1047840X.2019.1693857&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P. (2017). &lt;em&gt;Modality switches occur early and extend late in conceptual processing: evidence from ERPs&lt;/em&gt; [Master&#39;s thesis]. School of Humanities, Tilburg University. &lt;a href=&#34;https://psyarxiv.com/5gjvk&#34;&gt;https://psyarxiv.com/5gjvk&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P., Willems, R. M., &amp;amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, &amp;amp; E. J. Davelaar (Eds.), &lt;em&gt;Proceedings of the 39th Annual Conference of the Cognitive Science Society&lt;/em&gt; (pp. 1629-1634). Austin, TX: Cognitive Science Society. &lt;a href=&#34;https://doi.org/10.31234/osf.io/a5pcz&#34;&gt;https://doi.org/10.31234/osf.io/a5pcz&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chen, S., Szabelska, A., Chartier, C. R., Kekecs, Z., Lynott, D., Bernabeu, P., … Schmidt, K. (2018). &lt;em&gt;Investigating object orientation effects across 14 languages&lt;/em&gt;. PsyArXiv. &lt;a href=&#34;https://doi.org/10.31234/osf.io/t2pjv/&#34;&gt;https://doi.org/10.31234/osf.io/t2pjv/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Collins, J., Pecher, D., Zeelenberg, R., &amp;amp; Coulson, S. (2011). Modality switching in a property verification task: an ERP study of what happens when candles flicker after high heels click. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2011.00010&#34;&gt;https://doi.org/10.3389/fpsyg.2011.00010&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hald, L. A., Hocking, I., Vernon, D., Marshall, J.-A., &amp;amp; Garnham, A. (2013). Exploring modality switching effects in negated sentences: further evidence for grounded representations. &lt;em&gt;Frontiers in Psychology, 4&lt;/em&gt;, 93. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2013.00093&#34;&gt;https://doi.org/10.3389/fpsyg.2013.00093&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hald, L. A., Marshall, J.-A., Janssen, D. P., &amp;amp; Garnham, A. (2011). Switching modalities in a sentence verification task: ERP evidence for embodied language processing. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2011.00045&#34;&gt;https://doi.org/10.3389/fpsyg.2011.00045&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hauk, O. (2016). Only time will tell–why temporal information is essential for our neuroscientific understanding of semantics. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;(4), 1072-1079. &lt;a href=&#34;https://doi.org/10.3758/s13423-015-0873-9&#34;&gt;https://doi.org/10.3758/s13423-015-0873-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Holm, S. (1979). A simple sequentially rejective multiple test procedure. &lt;em&gt;Scandinavian Journal of Statistics, 6&lt;/em&gt;, 65-70. &lt;a href=&#34;http://www.jstor.org/stable/4615733&#34;&gt;http://www.jstor.org/stable/4615733&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Howe, P. D., &amp;amp; Perfors, A. (2018). An argument for how (and why) to incentivise replication. &lt;em&gt;Behavioral and Brain Sciences, 41&lt;/em&gt;, e135-e135. &lt;a href=&#34;http://dx.doi.org/10.1017/S0140525X18000705&#34;&gt;http://dx.doi.org/10.1017/S0140525X18000705&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kam, C. D., &amp;amp; Franzese, R. J. (2007). &lt;em&gt;Modeling and interpreting interactive hypotheses in regression analysis&lt;/em&gt;. Ann Arbor, MI: University of Michigan Press.&lt;/p&gt;
&lt;p&gt;Kunert, R. (2016). Internal conceptual replications do not increase independent replication success. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;(5), 1631-1638. &lt;a href=&#34;https://doi.org/10.3758/s13423-016-1030-9&#34;&gt;https://doi.org/10.3758/s13423-016-1030-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, P. (2018). &lt;em&gt;Embodied-linguistic conceptual representations during metaphor processing&lt;/em&gt;. Doctoral thesis, Lancaster University, UK. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/489&#34;&gt;https://doi.org/10.17635/lancaster/thesis/489&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, P., Lynott, D., &amp;amp; Connell, L. (2018). Continuous neural activations of simulation-linguistic representations in modality switching costs. In P. Liu, &lt;em&gt;Embodied-linguistic conceptual representations during metaphor processing&lt;/em&gt;. Doctoral thesis, Lancaster University, UK. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/489&#34;&gt;https://doi.org/10.17635/lancaster/thesis/489&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Luck, S. J. (2005). Ten simple rules for designing ERP experiments. In T. C. Handy (Ed.), &lt;em&gt;Event-related potentials: A methods handbook&lt;/em&gt;.
MIT Press.&lt;/p&gt;
&lt;p&gt;Luck, S. J., &amp;amp; Gaspelin, N. (2017). How to get statistically significant effects in any ERP experiment (and why you shouldn&#39;t). &lt;em&gt;Psychophysiology, 54&lt;/em&gt;(1), 146-157. &lt;a href=&#34;https://doi.org/10.1111/psyp.12639&#34;&gt;https://doi.org/10.1111/psyp.12639&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., &amp;amp; Bates, D. (2017). Balancing type 1 error and power in linear mixed models. &lt;em&gt;Journal of Memory and Language, 94&lt;/em&gt;, 305–315. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2017.01.001&#34;&gt;https://doi.org/10.1016/j.jml.2017.01.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meteyard, L., &amp;amp; Davies, R. A. (2020). Best practice guidance for linear mixed-effects models in psychological science. &lt;em&gt;Journal of Memory and Language, 112&lt;/em&gt;, 104092. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2020.104092&#34;&gt;https://doi.org/10.1016/j.jml.2020.104092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pecher, D., Zeelenberg, R., &amp;amp; Barsalou, L. W. (2003). Verifying different-modality properties for concepts produces switching costs. &lt;em&gt;Psychological Science, 14&lt;/em&gt;, 2, 119-24. &lt;a href=&#34;https://doi.org/10.1111/1467-9280.t01-1-01429&#34;&gt;https://doi.org/10.1111/1467-9280.t01-1-01429&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Simons, D. J. (2014). The value of direct replication. &lt;em&gt;Perspectives on Psychological Science, 9&lt;/em&gt;(1), 76–80. &lt;a href=&#34;https://doi.org/10.1177/1745691613514755&#34;&gt;https://doi.org/10.1177/1745691613514755&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Swaab, T. Y., Ledoux, K., Camblin, C. C., &amp;amp; Boudewyn, M. A. (2012). Language-related ERP components. In S. J. Luck &amp;amp; E. S. Kappenman (Eds.), &lt;em&gt;Oxford handbook of event-related potential components&lt;/em&gt; (pp. 397–440). Oxford University Press. &lt;a href=&#34;https://doi.org/10.1093/oxfordhb/9780195374148.013.0197&#34;&gt;https://doi.org/10.1093/oxfordhb/9780195374148.013.0197&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Van Dam, W. O., Brazil, I. A., Bekkering, H., &amp;amp; Rueschemeyer, S.-A. (2014). Flexibility in embodied language processing: context effects in lexical access. &lt;em&gt;Topics in Cognitive Science, 6&lt;/em&gt;(3), 407–424. &lt;a href=&#34;https://doi.org/10.1111/tops.12100&#34;&gt;https://doi.org/10.1111/tops.12100&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yarkoni, T. (2020). The generalizability crisis. &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt;, 1-37. &lt;a href=&#34;https://doi.org/10.1017/S0140525X20001685&#34;&gt;https://doi.org/10.1017/S0140525X20001685&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zwaan, R., Etz, A., Lucas, R., &amp;amp; Donnellan, M. (2018). Making replication mainstream. &lt;em&gt;Behavioral and Brain Sciences, 41&lt;/em&gt;, E120. &lt;a href=&#34;https://doi.org/10.1017/S0140525X17001972&#34;&gt;https://doi.org/10.1017/S0140525X17001972&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>s</category>
      
            <category>conceptual modality switch</category>
      
            <category>conceptual processing</category>
      
            <category>cognition</category>
      
            <category>conceptual replication</category>
      
            <category>word recognition</category>
      
            <category>research methods</category>
      
            <category>event-related potentials</category>
      
            <category>experiment</category>
      
            <category>statistics</category>
      
            <category>bayesian</category>
      
            <category>frequentist</category>
      
            <category>bias</category>
      
            <category>methodology</category>
      
      
            <category>research methods</category>
      
            <category>psycholinguistics</category>
      
    </item>
    
    <item>
      <title>Collaboration while using R Markdown</title>
      <link>https://pablobernabeu.github.io/2020/collaboration-while-using-r-markdown/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2020/collaboration-while-using-r-markdown/</guid>
      <description>&lt;p&gt;In a highly recommendable &lt;a href=&#34;https://www.youtube.com/watch?v=Nj9J5iCSMB0&#34;&gt;presentation available on Youtube&lt;/a&gt;, &lt;strong&gt;Michael Frank&lt;/strong&gt; walks us through R Markdown. Below, I loosely summarise and partly elaborate on Frank&#39;s advice regarding collaboration among colleagues, some of whom may not be used to R Markdown (&lt;a href=&#34;https://www.youtube.com/watch?v=Nj9J5iCSMB0&amp;amp;feature=youtu.be&amp;amp;t=2900&amp;amp;ab_channel=MichaelFrank&#34;&gt;see relevant time point in Frank&#39;s presentation&lt;/a&gt;).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The first way is using GitHub, which has a great version control system, and even allows the rendering of Markdown text, if the file is given the extension &amp;lsquo;.md&amp;rsquo; on GitHub. Furthermore, GitHub has made private repositories with any number of collaborators free.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second way is copying the text part of the unrendered R Markdown doc (i.e., excluding any long code chunks) to Word or Google Docs, or any other trackable editor. The collaborators would then edit the text, and refrain from editing any of the R or Markdown code (i.e., any inline code, hashes, etc.). Changes would be tracked and accepted (any unwanted edits of the code may be undone), and transferred to the original document.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The third way is knitting the document to Word, which allows tracking changes, or otherwise knitting to PDF.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
      
            <category>s</category>
      
            <category>R</category>
      
            <category>R Markdown</category>
      
      
            <category>research methods</category>
      
    </item>
    
    <item>
      <title>Web application: Dutch modality exclusivity norms</title>
      <link>https://pablobernabeu.github.io/applications-and-dashboards/bernabeu-2018-modalitynorms/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/applications-and-dashboards/bernabeu-2018-modalitynorms/</guid>
      <description>&lt;a href=&#39;https://pablobernabeu.shinyapps.io/Dutch-modality-exclusivity-norms&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #196F27; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt; 
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;i class=&#34;fas fa-mouse-pointer&#34;&gt;&lt;/i&gt;&lt;/span&gt;&amp;nbsp; Complete web application &lt;font style=&#39;font-size:60%;&#39;&gt;&lt;i&gt;Flexdashboard-Shiny&lt;/i&gt;&lt;/font&gt; &lt;/h3&gt;&lt;/button&gt;&lt;/a&gt;
&lt;br&gt;
&lt;br&gt;
&lt;a href=&#39;https://pablobernabeu.github.io/dashboards/Dutch-modality-exclusivity-norms/d.html&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #4CAF50; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt; 
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;i class=&#34;fas fa-mouse-pointer&#34;&gt;&lt;/i&gt;&lt;/span&gt;&amp;nbsp; Reduced dashboard &lt;font style=&#39;font-size:60%;&#39;&gt;&lt;i&gt;Flexdashboard&lt;/i&gt;&lt;/font&gt; &lt;/h3&gt;&lt;/button&gt;&lt;/a&gt; &amp;nbsp; 
&lt;br&gt;
&lt;br&gt;
&lt;p&gt;This web application presents linguistic data over several tabs. The code combines the great front-end of Flexdashboard—based on R Markdown and yielding an unmatched user interface—, with the great back-end of Shiny—allowing users to download sections of data they select, in various formats.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A nice find was the &amp;lsquo;reactable&amp;rsquo; package, which implements Javascript under the hood to allow the use of colours, bar charts, etc.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  
Auditory = colDef(header = with_tooltip(&#39;Auditory Rating&#39;,
                                        &#39;Mean rating of each word on the auditory modality across participants.&#39;),
                  cell = function(value) {
                    width &amp;lt;- paste0(value / max(table_data$Auditory) * 100, &amp;quot;%&amp;quot;)
                    value = sprintf(&amp;quot;%.2f&amp;quot;, round(value,2))  # Round to two digits, keeping trailing zeros
                    bar_chart(value, width = width, fill = &#39;#ff3030&#39;)
                    },
                  align = &#39;left&#39;),
  
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One of the hardest nuts to crack was allowing the full functionality of tables—i.e, scaling to screen, frozen header, and vertical and horizontal scrolling—whilst having tweaked the vertical/horizontal orientation of the dashboard sections. Initial clashes were sorted by adjusting the section&#39;s CSS styles&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Table {#table style=&amp;quot;background-color:#FCFCFC;&amp;quot;}
=======================================================================
  
Inputs {.sidebar style=&#39;position:fixed; padding-top: 65px; padding-bottom:30px;&#39;}
-----------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and by also adjusting the reactable settings.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  
renderReactable({
  reactable(selected_words(),
            defaultSorted = list(cat = &#39;desc&#39;, word = &#39;asc&#39;),
            defaultColDef = colDef(footerStyle = list(fontWeight = &amp;quot;bold&amp;quot;)),
            height = 840, striped = TRUE, pagination = FALSE, highlight = TRUE,
  
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A nice feature, especially suited to Flexdashboard, was the use of different formats across tabs. Whereas the Info tab presents long text using HTML and CSS styling, along with rmarkdown code output, the other tabs rely more strongly on Javascript features, enabled by R packages such as ‘shiny’ and sweetalert (e.g., allowing modal dialogs—pop-ups), reactable and plotly (e.g., allowing information opened by hovering—tooltips).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```{r}
  
# reactive for the word bar
highlighted_properties = reactive(input$highlighted_properties)
  
renderPlotly({
 ggplotly(
  ggplot( selected_props(), aes(RC1, RC2, label = as.character(word), color = main, 
    # Html tags below used for format. Decimals rounded to two.
    text = paste0(&#39; &#39;, &#39;&amp;lt;span style=&amp;quot;padding-top:3px; padding-bottom:3px; font-size:2.2em; color:#EEEEEE&amp;quot;&amp;gt;&#39;, capitalize(word), &#39;&amp;lt;/span&amp;gt; &#39;, &#39;&amp;lt;br&amp;gt;&#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Dominant modality: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, main, &#39; &#39;,
     &#39; &#39;, &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Modality exclusivity: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Exclusivity, 2)), &#39;% &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Perceptual strength: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Perceptualstrength, 2)),
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Auditory rating: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Auditory, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Haptic rating: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Haptic, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Visual rating: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Visual, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Concreteness (Brysbaert et al., 2014): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, 
       sprintf(&amp;quot;%.2f&amp;quot;, round(concrete_Brysbaertetal2014, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Number of letters: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, letters, &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Number of phonemes (DutchPOND): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, 
     round(phonemes_DUTCHPOND, 2), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Contextual diversity (lg10CD SUBTLEX-NL): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;,
       sprintf(&amp;quot;%.2f&amp;quot;, round(freq_lg10CD_SUBTLEXNL, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Word frequency (lg10WF SUBTLEX-NL): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;,
       sprintf(&amp;quot;%.2f&amp;quot;, round(freq_lg10WF_SUBTLEXNL, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Lemma frequency (CELEX): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, 
       sprintf(&amp;quot;%.2f&amp;quot;, round(freq_CELEX_lem, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Phonological neighbourhood size (DutchPOND): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, 
     round(phon_neighbours_DUTCHPOND, 2), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Orthographic neighbourhood size (DutchPOND): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;,
     round(orth_neighbours_DUTCHPOND, 2), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Age of acquisition (Brysbaert et al., 2014): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;,
     sprintf(&amp;quot;%.2f&amp;quot;, round(AoA_Brysbaertetal2014, 2)), &#39; &#39;, &#39;&amp;lt;br&amp;gt; &#39;
     ) ) ) +
  geom_text(size = ifelse(selected_props()$word %in% highlighted_properties(), 7,
             ifelse(is.null(highlighted_properties()), 3, 2.8)),
      fontface = ifelse(selected_props()$word %in% highlighted_properties(), &#39;bold&#39;, &#39;plain&#39;)) +
geom_point(alpha = 0) +  # This geom_point helps to colour the tooltip according to the dominant modality
scale_colour_manual(values = colours, drop = FALSE) + theme_bw() + ggtitle(&#39;Property words&#39;) +
labs(x = &#39;Varimax-rotated Principal Component 1&#39;, y = &#39;Varimax-rotated Principal Component 2&#39;) +
guides(color = guide_legend(title = &#39;Main&amp;lt;br&amp;gt;modality&#39;)) +
theme( plot.background = element_blank(), panel.grid.major = element_blank(),
   panel.grid.minor = element_blank(), panel.border = element_blank(),
   axis.line = element_line(color = &#39;black&#39;), plot.title = element_text(size = 14, hjust = .5),
   axis.title.x = element_text(colour = &#39;black&#39;, size = 12, margin = margin(15,15,0,15)),
   axis.title.y = element_text(colour = &#39;black&#39;, size = 12, margin = margin(0,15,15,5)),
   axis.text.x = element_text(size = 8), axis.text.y  = element_text(size = 8),
   legend.background = element_rect(size = 2), legend.position = &#39;none&#39;,
 legend.title = element_blank(),
 legend.text = element_text(colour = colours, size = 13) ),
tooltip = &#39;text&#39;
)
})
  
# For download, save plot without the interactive &#39;plotly&#39; part
  
properties_png = reactive({ ggplot(selected_props(), aes(RC1, RC2, color = main, label = as.character(word))) +
geom_text(show.legend = FALSE, size = ifelse(selected_props()$word %in% highlighted_properties(), 7,
         ifelse(is.null(highlighted_properties()), 3, 2.8)),
      fontface = ifelse(selected_props()$word %in% highlighted_properties(), &#39;bold&#39;, &#39;plain&#39;)) +
geom_point(alpha = 0) + scale_colour_manual(values = colours, drop = FALSE) + theme_bw() +
guides(color = guide_legend(title = &#39;Main&amp;lt;br&amp;gt;modality&#39;, override.aes = list(size = 7, alpha = 1))) +
ggtitle( paste0(&#39;Properties&#39;, &#39; (showing &#39;, nrow(selected_props()), &#39; out of &#39;, nrow(props), &#39;)&#39;) ) + 
labs(x = &#39;Varimax-rotated Principal Component 1&#39;, y = &#39;Varimax-rotated Principal Component 2&#39;) +
theme( plot.background = element_blank(), panel.grid.major = element_blank(),
   panel.grid.minor = element_blank(), panel.border = element_blank(),
   axis.line = element_line(color = &#39;black&#39;), plot.title = element_text(size = 17, hjust = .5, margin = margin(3,3,7,3)),
   axis.title.x = element_text(colour = &#39;black&#39;, size = 12, margin = margin(10,10,2,10)),
   axis.title.y = element_text(colour = &#39;black&#39;, size = 12, margin = margin(10,10,10,5)),
   axis.text.x = element_text(size = 8), axis.text.y  = element_text(size = 8),
   legend.background = element_rect(size = 2), legend.position = &#39;right&#39;,
   legend.title = element_blank(), legend.text = element_text(size = 15))
})
  
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The only instance in which I drew on javascript code outside R packages was to enable tooltips beyond the packages’ limits—for instance, in the side bar. This javascript feature is created at the top of the script, in the head area.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!-- Javascript function to enable a hovering tooltip --&amp;gt;
&amp;lt;script&amp;gt;
$(document).ready(function(){
   $(&#39;[data-toggle=&amp;quot;tooltip1&amp;quot;]&#39;).tooltip();
});
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the side bar, I added a reactive mean for each variable, complementing the range selector.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;reactive(cat(paste0(&#39;Mean = &#39;, 
  sprintf(&amp;quot;%.2f&amp;quot;, round(mean(selected_words()$Exclusivity),2)))))
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2 id=&#34;static-version-published-on-rpubs&#34;&gt;Static version published on RPubs&lt;/h2&gt;
&lt;p&gt;A reduced, &lt;a href=&#34;https://rpubs.com/pcbernabeu/Dutch-modality-exclusivity-norms&#34;&gt;&lt;em&gt;static&lt;/em&gt; version&lt;/a&gt; was also created to increase the availability of the content. Removing some reactivity features allows the dashboard to be published as a standard website (i.e., on a personal website, on &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;, etc.), without the need for a back-end Shiny server. Note that this type of website is dubbed &amp;lsquo;static&amp;rsquo;, but it can retain multiple interactive features thanks to Javascript-based tools under the hood, allowed by R packages such as &lt;code&gt;leaflet&lt;/code&gt; for maps, &lt;code&gt;DT&lt;/code&gt; for tables, &lt;code&gt;plotly&lt;/code&gt; for plots, etc.&lt;/p&gt;
&lt;p&gt;To create the Flexdashboard-only version departing from the Flexdashboard-Shiny version, I deleted &lt;code&gt;runtime: shiny&lt;/code&gt; from the YAML header, and disabled Shiny reactive inputs and objects, as below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```{r}
# Number of words selected on sidebar
# reactive(cat(paste0(&#39;Words selected below: &#39;, nrow(selected_props()))))
```
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;div style = &#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P. (2018). Dutch modality exclusivity norms for 336 properties and 411 concepts [Web application]. Retrieved from &lt;a href=&#34;https://pablobernabeu.shinyapps.io/Dutch-Modality-Exclusivity-Norms/&#34;&gt;https://pablobernabeu.shinyapps.io/Dutch-Modality-Exclusivity-Norms/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;br&gt;
</description>
      
            <category>web application</category>
      
            <category>data dashboard</category>
      
            <category>Flexdashboard</category>
      
            <category>R</category>
      
            <category>R Shiny</category>
      
            <category>statistics</category>
      
            <category>regression</category>
      
            <category>principal component analysis</category>
      
            <category>modality exclusivity norms</category>
      
            <category>Dutch</category>
      
            <category>linguistics</category>
      
            <category>HTML</category>
      
            <category>CSS</category>
      
      
            <category>linguistic materials</category>
      
            <category>research methods</category>
      
            <category>web-application</category>
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>EEG error: datasets missing channels</title>
      <link>https://pablobernabeu.github.io/2016/eeg-error-datasets-missing-channels/</link>
      <pubDate>Tue, 16 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2016/eeg-error-datasets-missing-channels/</guid>
      <description>


</description>
      
            <category>s</category>
      
            <category>event-related potentials</category>
      
            <category>EEG</category>
      
            <category>electroencephalography</category>
      
            <category>BrainVision</category>
      
      
            <category>research-methods</category>
      
    </item>
    
  </channel>
</rss>
