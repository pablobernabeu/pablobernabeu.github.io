<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>statistics | Pablo Bernabeu</title>
    <link>/categories/statistics/</link>
      <atom:link href="/categories/statistics/index.xml" rel="self" type="application/rss+xml" />
    <description>statistics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-uk</language><copyright>© Pablo Bernabeu, 2020. [CC BY Attribution licence](https://creativecommons.org/licenses/by/4.0/). Cookies only used by Disqus to enable comments ([see details](https://help.disqus.com/en/articles/1717155-use-of-cookies)).</copyright><lastBuildDate>Mon, 01 Jan 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/pablobernabeu_image_sharing.png</url>
      <title>statistics</title>
      <link>/categories/statistics/</link>
    </image>
    
    <item>
      <title>Naive principal component analysis in R</title>
      <link>/2018/01/01/naive-principal-component-analysis-in-r/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/2018/01/01/naive-principal-component-analysis-in-r/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Principal_component_analysis&#34;&gt;Principal Component Analysis (PCA)&lt;/a&gt; is a technique used to find the core components that underlie different variables. It comes in very useful whenever doubts arise about the true origin of three or more variables. There are two main methods for performing a PCA: naive or less naive. In the naive method, you first check some conditions in your data which will determine the essentials of the analysis. In the less-naive method, you set those yourself based on whatever prior information or purposes you had. The latter method is appropriate when you already have enough information about the intercorrelations, or when you are required to select a specific number of components. I will tackle the naive method, mainly by following the guidelines in &lt;a href=&#34;https://freethegeogbooks.files.wordpress.com/2016/08/book-for-r-language-stats.pdf&#34;&gt;Field, Miles, and Field (2012)&lt;/a&gt;, with updated code where necessary. A &lt;a href=&#34;https://freethegeogbooks.files.wordpress.com/2016/08/book-for-r-language-stats.pdf&#34;&gt;manual by Charles M. Friel&lt;/a&gt; (Sam Houston State University) was also useful.&lt;/p&gt;
&lt;p&gt;The ‘naive’ approach is characterized by a first stage that checks whether the PCA should actually be performed with your current variables, or if some should be removed. The variables that are accepted are taken to a second stage which identifies the number of principal components that seem to underlie your set of variables.&lt;/p&gt;
&lt;div id=&#34;stage-1.-determine-whether-pca-is-appropriate-at-all-considering-the-variables&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;STAGE 1. Determine whether PCA is appropriate at all, considering the variables&lt;/h4&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;45%&#34; src=&#34;https://media-exp1.licdn.com/dms/image/C5612AQE7vLkOVSIaVQ/article-inline_image-shrink_1500_2232/0?e=1585785600&amp;v=beta&amp;t=ABp_9l8pA-tyTuMANjTv7nCBPKXSBTm4c8X3ocX7yYs&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Variables should be &lt;strong&gt;inter-correlated enough but not too much.&lt;/strong&gt; Field et al. (2012) provide some thresholds, suggesting that no variable should have many correlations below .30, or &lt;em&gt;any&lt;/em&gt; correlation at all above .90. Thus, in the example here, variable Q06 should probably be excluded from the PCA.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Bartlett’s test&lt;/strong&gt;, on the nature of the intercorrelations, should be significant. Significance suggests that the variables are not an ‘identity matrix’ in which correlations are a sampling error.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;KMO&lt;/strong&gt; (Kaiser-Meyer-Olkin), a measure of sampling adequacy based on common variance (so similar purpose as Bartlett’s). As Field et al. review, ‘values between .5 and .7 are mediocre, values between .7 and .8 are good, values between .8 and .9 are great and values above .9 are superb’ (p. 761). There’s a general score as well as one per variable. The general one will often be good, whereas the individual scores may more likely fail. Any variable with a score below .5 should probably be removed, and the test should be run again.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Determinant:&lt;/strong&gt; A formula about multicollinearity. The result should preferably fall below .00001.
Note that some of these tests are run on the dataframe and others on a correlation matrix of the data, as distinguished below.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;
# Necessary libraries
library(ltm)
library(lattice)
library(psych)
library(car)
library(pastecs)
library(scales)
library(ggplot2)
library(arules)
library(plyr)
library(Rmisc)
library(GPArotation)
library(gdata)
library(MASS)
library(qpcR)
library(dplyr)
library(gtools)
library(Hmisc)

# Select variables of interest for the PCA
dataset = mydata[, c(&amp;#39;select_var1&amp;#39;,&amp;#39;select_var1&amp;#39;,&amp;#39;select_var2&amp;#39;,&amp;#39;select_var3&amp;#39;,&amp;#39;select_var4&amp;#39;,&amp;#39;select_var5&amp;#39;,&amp;#39;select_var6&amp;#39;,&amp;#39;select_var7&amp;#39;)]

# Create matrix: some tests will require it
data_matrix = cor(dataset, use = &amp;#39;complete.obs&amp;#39;)

# See intercorrelations
round(data_matrix, 2)

# Bartlett&amp;#39;s
cortest.bartlett(dataset)

# KMO (Kaiser-Meyer-Olkin)
KMO(data_matrix)

# Determinant
det(data_matrix)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-2.-identify-number-of-components-aka-factors&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;STAGE 2. Identify number of components (aka factors)&lt;/h4&gt;
&lt;p&gt;In this stage, principal components (formally called ‘factors’ at this stage) are identified among the set of variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The identification is done through a basic, ‘unrotated’ PCA. The number of components set a priori must equal the number of variables that are being tested.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# Start off with unrotated PCA

pc1 = psych::principal(dataset, nfactors = length(dataset), rotate=&amp;quot;none&amp;quot;)
pc1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is an example result:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Principal Components Analysis
## Call: psych::principal(r = eng_prop, nfactors = 3, rotate = &amp;quot;none&amp;quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##           PC1   PC2  PC3 h2       u2 com
## Aud_eng -0.89  0.13 0.44  1 -2.2e-16 1.5
## Hap_eng  0.64  0.75 0.15  1  1.1e-16 2.0
## Vis_eng  0.81 -0.46 0.36  1 -4.4e-16 2.0
## 
##                        PC1  PC2  PC3
## SS loadings           1.87 0.79 0.34
## Proportion Var        0.62 0.26 0.11
## Cumulative Var        0.62 0.89 1.00
## Proportion Explained  0.62 0.26 0.11
## Cumulative Proportion 0.62 0.89 1.00
## 
## Mean item complexity =  1.9
## Test of the hypothesis that 3 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0 
##  with the empirical chi square  0  with prob &amp;lt;  NA 
## 
## Fit based upon off diagonal values = 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Among the columns, there are first the correlations between variables and components, followed by a column (h2) with the &lt;strong&gt;‘communalities’&lt;/strong&gt;. If less factors than variables had been selected, communality values would be below 1. Then there is the uniqueness column (u2): &lt;strong&gt;uniqueness&lt;/strong&gt; is equal to 1 minus the communality. Next is ‘com’, which reflects the &lt;strong&gt;complexity&lt;/strong&gt; with which a variable relates to the principal components. Those components are precisely found below. The first row contains the sums of squared loadings, or eigenvalues, namely, the total variance explained by each linear component. This value corresponds to the number of units explained out of all possible factors (which were three in the above example). The rows below all cut from the same cloth. &lt;em&gt;Proportion var&lt;/em&gt; = variance explained over a total of 1. This is the result of dividing the eigenvalue by the number of components. Multiply by 100 and you get the percentage of total variance explained, which becomes useful. In the example, 99% of the variance has been explained. Aside from the meddling maths, we should actually expect 100% there because the number of factors equaled the number of variables. &lt;em&gt;Cumulative var:&lt;/em&gt; variance added consecutively up to the last component. &lt;em&gt;Proportion explained:&lt;/em&gt; variance explained over what has actually been explained (only when variables = factors is this the same as Proportion var). &lt;em&gt;Cumulative proportion:&lt;/em&gt; the actually explained variance added consecutively up to the last component (Field et al., 2012).&lt;/p&gt;
&lt;p&gt;According to Field et al. (2012), two criteria will determine the number of components to select for the next stage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kaiser’s criterion: components with SS loadings &amp;gt; 1. In our example, only PC1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A more lenient alternative is Joliffe’s criterion, SS loadings &amp;gt; .7.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scree plot: the number of points after point of inflexion. For this plot, call:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;plot(pc1$values, type = &amp;#39;b&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;35%&#34; src=&#34;https://media-exp1.licdn.com/dms/image/C5612AQF7TVqF5FFS6Q/article-inline_image-shrink_1000_1488/0?e=1585785600&amp;v=beta&amp;t=bEdWqeoT08j0nSiERX2ZPAlEcyPjUhRsEiucZy3wvBM&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Imagine a straight line &lt;strong&gt;from the first point on the right.&lt;/strong&gt; Once this line bends considerably, count the points after the bend and up to the last point on the left. The number of points is the number of components to select. The example here is probably the most complicated (two components were finally chosen), but normally it’s &lt;a href=&#34;https://www.google.nl/search?q=select+principal+components+scree+plot+point+inflexion&amp;amp;source=lnms&amp;amp;tbm=isch&amp;amp;sa=X&amp;amp;ved=0ahUKEwi00ujoto_WAhXJbVAKHbTCBAgQ_AUICigB&amp;amp;biw=1280&amp;amp;bih=619&#34;&gt;not difficult&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Based on both criteria, go ahead and select the definitive number of components.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-3.-run-definitive-pca&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;STAGE 3. Run definitive PCA&lt;/h4&gt;
&lt;p&gt;Run a very similar command as you did before, but now with a more advanced method. The first PCA, a heuristic one, worked essentially on the inter-correlations. The definitive PCA, in contrast, will implement a prior shuffling known as ‘rotation’, to ensure that the result is robust enough (just like cards are shuffled). Explained variance is captured better this way. The go-to rotation method is the orthogonal, or ‘varimax’ (though others may be considered too).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Now with varimax rotation, Kaiser-normalized by default:
pc2 = psych::principal(dataset, nfactors=2, rotate = &amp;quot;varimax&amp;quot;, 
scores = TRUE)
pc2
pc2$loadings

# Healthcheck
pc2$residual
pc2$fit
pc2$communality
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to Field et al. (2012), we would want:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Less than half of &lt;strong&gt;residuals&lt;/strong&gt; with absolute values &amp;gt; 0.05&lt;/li&gt;
&lt;li&gt;Model &lt;strong&gt;fit&lt;/strong&gt; &amp;gt; .9&lt;/li&gt;
&lt;li&gt;All &lt;strong&gt;communalities&lt;/strong&gt; &amp;gt; .7&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If any of this fails, consider changing the number of factors. Next, the rotated components that have been ‘extracted’ from the core of the set of variables can be added to the dataset. This would enable the use of these components as new variables that might prove powerful and useful (as in &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.1551-6709.2010.01157.x/full&#34;&gt;this research&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dataset = cbind(dataset, pc2$scores)
summary(dataset$RC1, dataset$RC2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-4.-determine-ascription-of-each-variable-to-components&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;STAGE 4. Determine ascription of each variable to components&lt;/h4&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;55%&#34; src=&#34;https://media-exp1.licdn.com/dms/image/C5612AQH-p0mz8hnoqw/article-inline_image-shrink_1500_2232/0?e=1585785600&amp;v=beta&amp;t=TMS7L_jyaDzlTZ1nlxmJ5b_3CHbeIkKkQKerQww0DqA&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Check the main summary by just calling pc2, and see how each variable correlates with the rotated components. This is essential because it reveals how variables load on each component, or in other words, to which component a variable belongs. For instance, the table shown here belongs to a &lt;a href=&#34;https://www.linkedin.com/pulse/modality-exclusivity-norms-336-properties-411-dutch-english-bernabeu/?published=t&amp;amp;lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3BRIskxEq5Rgq59xHemwzpdw%3D%3D&#34;&gt;study about meaning of words&lt;/a&gt;. These results suggest that the visual and haptic modalities of words are quite related, whereas the auditory modality is relatively unique. When the analysis works out well, a cut-off point of &lt;em&gt;r&lt;/em&gt; = .8 may be applied for considering a variable as part of a component.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-5.-enjoy-the-plot&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;STAGE 5. Enjoy the plot&lt;/h4&gt;
&lt;p&gt;The plot is perhaps the coolest part about PCA. It really makes an awesome illustration of the power of data analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ggplot(eng_props,
  aes(RC1, RC2, label = as.character(main_eng))) + stat_density2d (color = &amp;quot;gray87&amp;quot;) +
  geom_text(size = ifelse(eng_props$word_eng %in% w_set, 12, 7),
    fontface = ifelse(eng_props$word_eng %in% w_set, &amp;#39;bold&amp;#39;, &amp;#39;plain&amp;#39;)) +
  geom_point(data=eng_props[eng_props$word_eng %in% w_set,], pch=21, fill=NA, size=14, stroke=2, alpha=.6) +
  labs(subtitle=&amp;#39;(Data from Lynott &amp;amp; Connell, 2009)&amp;#39;, x = &amp;quot;Varimax-rotated Principal Component 1&amp;quot;, 
    y = &amp;quot;Varimax-rotated Principal Component 2&amp;quot;) +  theme_bw() +   
  theme( plot.background = element_blank(), panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(), panel.border = element_blank(),
    axis.line = element_line(color = &amp;#39;black&amp;#39;),
    axis.title.x = element_text(colour = &amp;#39;black&amp;#39;, size = 23, margin=margin(15,15,15,15)),
    axis.title.y = element_text(colour = &amp;#39;black&amp;#39;, size = 23, margin=margin(15,15,15,15)),
    axis.text.x = element_text(size=16), axis.text.y  = element_text(size=16),
    plot.title = element_text(hjust = 0.5, size = 32, face = &amp;quot;bold&amp;quot;, margin=margin(15,15,15,15)),
    plot.subtitle = element_text(hjust = 0.5, size = 20, margin=margin(2,15,15,15)) ) +
  geom_label_repel(data = eng_props[eng_props$word_eng %in% w_set,], aes(label = word_eng), size = 8, 
    alpha = 0.77, color = &amp;#39;black&amp;#39;, box.padding = 1.5 )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is an example combining PCA plots with code similar to the above. These plots illustrate something further with regard to the relationships among modalities. In property words, the different modalities spread out more clearly than they do in concept words. This makes sense because in language, properties define concepts (&lt;a href=&#34;https://www.linkedin.com/pulse/modality-exclusivity-norms-336-properties-411-dutch-english-bernabeu?published=t&amp;amp;lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3BRIskxEq5Rgq59xHemwzpdw%3D%3D&#34;&gt;see more&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media-exp1.licdn.com/dms/image/C4D12AQF1aTAK4IAm9w/article-inline_image-shrink_1500_2232/0?e=1585785600&amp;amp;v=beta&amp;amp;t=3iXYQJBTSa0elkK9n0Qcnr9CzUt1xOySVsRqxp-XA9s&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An example of these analyses is &lt;a href=&#34;https://mybinder.org/v2/gh/pablobernabeu/Modality-exclusivity-norms-747-Dutch-English-replication/master?urlpath=rstudio&#34;&gt;available in available in this RStudio environment&lt;/a&gt;, in the &lt;code&gt;norms.R&lt;/code&gt; script.&lt;/p&gt;
&lt;p&gt;References&lt;/p&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Field, A. P., Miles, J., &amp;amp; Field, Z. (2012). &lt;em&gt;Discovering Statistics Using R&lt;/em&gt;. London, UK: Sage.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>At Greg, 8 am</title>
      <link>/2017/01/01/at-greg-8-am/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>/2017/01/01/at-greg-8-am/</guid>
      <description>


&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../../post/at-greg-8-am/at-greg-8-am_files/featured.jpg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Wikimedia Commmons (&lt;a href=&#34;https://commons.wikimedia.org/wiki/File:Charmers_Cafe_on_a_quiet_morning.jpg&#34; class=&#34;uri&#34;&gt;https://commons.wikimedia.org/wiki/File:Charmers_Cafe_on_a_quiet_morning.jpg&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The clock strikes a certain hour, below all the Greg’s teaspoons at play. Results o’clock. The usual, please.&lt;/p&gt;
&lt;p&gt;Usual table. &lt;code&gt;summaryby&lt;/code&gt; (having to get the first peek in the cafeteria can only add zest). &lt;code&gt;summaryBy(RT ~ list(Ptp, Group, Cond), behdata, FUN=summary)&lt;/code&gt;. So, hardly any of the 95% Confidence Intervals contain 0. Does this really mean…?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‘For example, the hypothesis of equality of population means will be rejected at the 0.05 level if and only if a 95% CI for the mean difference does not contain 0.’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;— Dallal (2002; &lt;a href=&#34;http://www.jerrydallal.com/lhsp/pval.htm&#34; class=&#34;uri&#34;&gt;http://www.jerrydallal.com/lhsp/pval.htm&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Of course. The CI just has that and more. The window is showing a chilly 1999 morning. Let’s see the summary again. Wee standard deviations. By card, please.&lt;/p&gt;
&lt;p&gt;Mmm, the air outside is worth gingering up…&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The trials!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The assumption of independence spoils another morning.&lt;/p&gt;
&lt;p&gt;This new data consisted of response times (RT) that had been collected over several trials. The single dependent variable, RT, was accompanied by other variables which could be analyzed as independent variables. These included &lt;em&gt;Group&lt;/em&gt;, &lt;em&gt;Trial Number&lt;/em&gt;, and a within-subjects &lt;em&gt;Condition&lt;/em&gt;. &lt;strong&gt;What had to be done first off, in order to take the usual table?&lt;/strong&gt; &lt;em&gt;The trials!&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;assumption-of-independence-of-observations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assumption of independence of observations&lt;/h2&gt;
&lt;p&gt;One must account for any redundant measures below the level of participants (the experimental trials, in this case), so that the sample size (&lt;em&gt;N&lt;/em&gt;) used for any summary statistics match the number of participants (or the largest group, &lt;em&gt;n&lt;/em&gt;). Why? This is a &lt;a href=&#34;https://stats.stackexchange.com/questions/130019/standard-error-for-aggregated-proportions&#34;&gt;central assumption in statistics&lt;/a&gt;: observations must be independent. We can observe the independence assumption differently, depending on whether we’re summarizing data or performing statistical tests.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For &lt;em&gt;descriptive tables and plots&lt;/em&gt; (involving Standard Error/Deviation, Confidence Intervals, etc), &lt;em&gt;the data ought to be aggregated to the level from which you want to generalize&lt;/em&gt;. That level is—in this case and very often—&lt;em&gt;participants&lt;/em&gt;. Trials do not normally serve for statistical generalization (they’re good for experimental validity). This realization may come as a bummer if you have first seen the effect sizes in the un-aggregated data. The mirage (see red lines on the left table below) is caused by an inflated &lt;em&gt;N&lt;/em&gt; (cf. red lines on the right-hand table). As an illustration, the tables below summarize data with an actual sample &lt;em&gt;n&lt;/em&gt; = 23. However, the table on the right includes repeated measures that should have been aggregated, massively inflating &lt;em&gt;n&lt;/em&gt;. The inflation of the sample size equals the product of all repeated measures that failed to be aggregated under participants.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;../../post/at-greg-8-am/at-greg-8-am_files/inflated.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#39;30%&#39; src=&#39;../../post/at-greg-8-am/at-greg-8-am_files/SD.jpg&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Measures of variance such as the Standard Deviation divide by the sample size. Thus, the larger the sample (N), the smaller the Standard Deviation, Standard Error, Confidence Interval…—that is, the variation or noise.&lt;/p&gt;
&lt;p&gt;Aggregating is a snap. For example, with the aggregate() function in R, you just have to include all of your variables except that or those of the repeated measures:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;behdata_aggreg = aggregate(behdata$RT, list(behdata$Ptp, behdata$Group, behdata$Cond), 
  data=behdata, FUN=mean)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;In statistical tests, repeated measures below the participant level–e.g., trials–normally must be either factored in or aggregated. Barr and colleagues provide an easy, focused &lt;a href=&#34;http://talklab.psy.gla.ac.uk/simgen/faq.html#sec-3&#34;&gt;guide on this procedure&lt;/a&gt;. This is necessary because when the N in the analyses is augmented by unaccounted, redundant observations, &lt;em&gt;the famous assumption of independence of observations is violated&lt;/em&gt;, and the results may be invalid, as &lt;a href=&#34;https://arxiv.org/pdf/1601.01126.pdf&#34;&gt;Vasishth and Nicenboim (2016, p. 3)&lt;/a&gt; put it:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;‘if we were to do a t-test on the unaggregated data, we would violate the independence assumption and the result of the t-test would be invalid.’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now, usually the repetitions that concern us are the multiple trials or items in experiments, or other sub-participant measures. So what about participants–what are they never aggregated? &lt;a href=&#34;http://tandfonline.com.sci-hub.cc/doi/abs/10.1080/01933922.2016.1264520?journalCode=usgw20&#34;&gt;McCarthy, Whittaker, Boyle, and Eyal (2017, p.10)&lt;/a&gt; note:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‘It has also been proposed that researchers aggregate the responses of participants within the same group and use the groups/clusters as the unit of analysis (Stevens, 2007). However, because this would result in losing sample size at the participant level, this approach is not optimal given the already small numbers of groups typically studied in group work research.’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;different-procedure-in-linear-mixed-effects-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Different procedure in linear mixed-effects models&lt;/h2&gt;
&lt;p&gt;Aggregation is no longer necessary, where linear mixed-effects models can be used. These models allow us to &lt;a href=&#34;http://talklab.psy.gla.ac.uk/simgen/faq.html#sec-3&#34;&gt;account for any clusters (Participants, Trials, Items…) by signing them into the error term&lt;/a&gt; (&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer_and_Curtin_LMEMs-2017-Psych_Methods.pdf&#34;&gt;Brauer &amp;amp; Curtin, 2017&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
