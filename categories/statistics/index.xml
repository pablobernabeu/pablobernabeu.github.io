
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>statistics on Pablo Bernabeu</title>
    <link>https://pablobernabeu.github.io/categories/statistics/</link>
    <description>Recent content in statistics on Pablo Bernabeu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <copyright>Pablo Bernabeu, 2015—{year}. Licence: [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/). Email: pcbernabeu@gmail.com. No cookies operated by this website. Cookies only used by third-party systems such as [Disqus](https://help.disqus.com/en/articles/1717155-use-of-cookies).</copyright>
    <lastBuildDate>Thu, 29 Dec 2022 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://pablobernabeu.github.io/categories/statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A table of results for Bayesian mixed-effects models: Grouping variables and specifying random slopes</title>
      <link>https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/</link>
      <pubDate>Thu, 29 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;Here I share the format applied to tables presenting the results of &lt;em&gt;Bayesian&lt;/em&gt; models in Bernabeu (2022; the &lt;a href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes&#34;&gt;table for frequentist models is covered in this other post&lt;/a&gt;). The sample table presents a Bayesian mixed-effects model that was fitted using the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34;&gt;&lt;code&gt;brms&lt;/code&gt;&lt;/a&gt; (Bürkner et al., 2022). The mixed effects were driven by the maximal principle (Brauer &amp;amp; Curtin, 2018). The format of the table resembles one of the examples published by the &lt;a href=&#34;https://apastyle.apa.org/style-grammar-guidelines/tables-figures/sample-tables&#34;&gt;American Psychological Association&lt;/a&gt;. However, there are also deviations from those examples. For instance, in the present table, the effects are grouped under informative labels to facilitate the readers’ comprehension, using the &lt;a href=&#34;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&#34;&gt;&lt;code&gt;kableExtra&lt;/code&gt;&lt;/a&gt; package (Zhu, 2022). Furthermore, the random slopes are specified using superscript letters and a footnote. The table can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;loading-packages-and-the-results-of-the-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading packages and the results of the models&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(knitr)
library(tibble)
library(stringr)
library(lmerTest)
library(kableExtra)

# Load Bayesian results summary

semanticpriming_summary_weaklyinformativepriors_exgaussian = 
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_summary_weaklyinformativepriors_exgaussian.rds?raw=true&amp;#39;)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adjusting-the-names-of-the-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adjusting the names of the effects&lt;/h3&gt;
&lt;p&gt;First, to facilitate the understanding of the results, the original names of the effects will be adjusted in the &lt;code&gt;brms&lt;/code&gt; summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Rename effects in plain language and specify the random slopes
# (if any) for each effect, in the footnote. For this purpose, 
# superscripts are added to the names of the appropriate effects.
# 
# In the interactions below, word-level variables are presented 
# first for the sake of consistency (the order does not affect 
# the results in any way). Also in the interactions, double 
# colons are used to inform the &amp;#39;bayesian_model_table&amp;#39; 
# function that the two terms in the interaction must be split 
# into two lines.

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_attentional_control&amp;#39;] = &amp;#39;Attentional control&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_attentional_control&amp;#39;] = &amp;#39;Attentional control&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_vocabulary_size&amp;#39;] = &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_recoded_participant_gender&amp;#39;] = &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_target_word_frequency&amp;#39;] = &amp;#39;Word frequency&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_target_number_syllables&amp;#39;] = &amp;#39;Number of syllables&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_word_concreteness_diff&amp;#39;] = &amp;#39;Word-concreteness difference&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_cosine_similarity&amp;#39;] = &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_visual_rating_diff&amp;#39;] = &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_recoded_interstimulus_interval&amp;#39;] = &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_word_concreteness_diff:z_vocabulary_size&amp;#39;] = 
  &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Word-concreteness difference : SOA&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_participant_gender&amp;#39;] = 
  &amp;#39;Word-concreteness difference : Gender&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_attentional_control:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Attentional control&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_attentional_control:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Attentional control&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_vocabulary_size:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_vocabulary_size:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_recoded_participant_gender:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : Gender&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_recoded_participant_gender:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : Gender&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian_model_table&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;bayesian_model_table()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;This &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/R_functions/bayesian_model_table.R&#34;&gt;custom function&lt;/a&gt; was used in Bernabeu (2022), with a PDF output. In the current scenario, however, we have an HTML output. In the above function, the code used for the &lt;span class=&#34;math inline&#34;&gt;\(\widehat{R}\)&lt;/span&gt; tailored to the HTML output (&lt;code&gt;&amp;amp;Rcirc;&lt;/code&gt;) does not render properly.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Fbayesian_model_table.R%23L119-L121&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;Instead, the LaTeX code &lt;code&gt;$\\widehat{R}$&lt;/code&gt; must be used. Therefore, we’ll correct this error and load the function below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function used in the manuscript to present summaries from &amp;#39;brms&amp;#39; models 
# in APA-formatted tables. The only obligatory argument to be supplied is 
# a summary of a &amp;#39;brms&amp;#39; model.

bayesian_model_table = 
  
  function(model_summary, show_intercept = TRUE, select_effects = NULL, 
           order_effects = NULL, format = NULL, 
           
           # If interaction_symbol_x = TRUE, replace double colons with 
           # times symbols followed by line breaks and indentation. 
           # Then, replace single colons with times symbols.
           interaction_symbol_x = FALSE,
           
           caption = &amp;#39;Summary of the lmerTest model.&amp;#39;) {
    
    require(dplyr)
    require(knitr)
    require(tibble)
    require(stringr)
    require(lmerTest)
    require(kableExtra)
    
    # Create data frame
    model_summary = 
      data.frame(Effect = rownames(model_summary$fixed), 
                 Estimate = model_summary$fixed$Estimate, 
                 SE = model_summary$fixed$Est.Error, 
                 CrI_2.5 = model_summary$fixed$`l-95% CI`, 
                 CrI_97.5 = model_summary$fixed$`u-95% CI`, 
                 Rhat = model_summary$fixed$Rhat,
                 row.names = NULL)
    
    # Process credible intervals and present both inside square brackets
    
    model_summary$CrI_2.5 = model_summary$CrI_2.5 %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .) %&amp;gt;% 
      # Remove minus sign from pure zeros
      sub(&amp;#39;-0.00&amp;#39;, &amp;#39;0.00&amp;#39;, .)
    
    model_summary$CrI_97.5 = model_summary$CrI_97.5 %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .) %&amp;gt;% 
      # Remove minus sign from pure zeros
      sub(&amp;#39;-0.00&amp;#39;, &amp;#39;0.00&amp;#39;, .)
    
    model_summary$CrI_95 = paste0(&amp;#39;[&amp;#39;, model_summary$CrI_2.5, &amp;#39;, &amp;#39;, 
                                  model_summary$CrI_97.5, &amp;#39;]&amp;#39;)
    
    # If show_intercept = FALSE, remove it
    if(isFALSE(show_intercept)) {
      model_summary = model_summary %&amp;gt;% filter(!grepl(&amp;#39;Intercept&amp;#39;, Effect))
      
      # Put &amp;#39;Intercept&amp;#39; in parentheses
    } else if(!is.null(model_summary[model_summary$Effect == &amp;#39;Intercept&amp;#39;, &amp;#39;Effect&amp;#39;])) {
      model_summary[model_summary$Effect == &amp;#39;Intercept&amp;#39;, &amp;#39;Effect&amp;#39;] = &amp;#39;(Intercept)&amp;#39;
    }
    
    # If select_effects was supplied, apply it and order effects accordingly
    if(!is.null(select_effects)) {
      model_summary = model_summary %&amp;gt;% filter(Effect %in% select_effects) %&amp;gt;%
        arrange(factor(Effect, levels = select_effects))
    }
    
    # If order_effects was supplied, apply order
    if(!is.null(order_effects)) {
      model_summary = model_summary %&amp;gt;%
        arrange(factor(Effect, levels = order_effects))
    }
    
    # Round other values
    
    model_summary$Estimate = model_summary$Estimate %&amp;gt;% as.numeric %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .) %&amp;gt;% 
      # Remove minus sign from pure zeros
      sub(&amp;#39;-0.00&amp;#39;, &amp;#39;0.00&amp;#39;, .)
    
    model_summary$SE = model_summary$SE %&amp;gt;% as.numeric %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .)
    
    model_summary$Rhat = model_summary$Rhat %&amp;gt;% as.numeric %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .)
    
    # Order columns
    model_summary = model_summary %&amp;gt;% select(Effect, Estimate, SE, CrI_95, Rhat)
    
    # Right-align all columns after first one
    align = c(&amp;#39;l&amp;#39;, &amp;#39;r&amp;#39;, &amp;#39;r&amp;#39;, &amp;#39;r&amp;#39;, &amp;#39;r&amp;#39;)
    
    # Establish latex or HTML format: if no format supplied, 
    # try to obtain it from knitr, or apply HTML
    if(missing(format) || is.null(format)) {
      if(knitr::is_latex_output()) {
        format = &amp;#39;latex&amp;#39;
      } else format = &amp;#39;html&amp;#39;
    }
    
    # HTML format
    if(format == &amp;#39;html&amp;#39;) {
      
      # If interaction_symbol_x = TRUE, replace double colons with times 
      # symbols followed by line breaks and indentation. Then, replace 
      # single colons with times symbols.
      if(interaction_symbol_x) {
        model_summary$Effect = model_summary$Effect %&amp;gt;% 
          gsub(&amp;#39;::&amp;#39;, &amp;#39; &amp;amp;times; &amp;lt;br&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;#39;, .) %&amp;gt;%
          gsub(&amp;#39;:&amp;#39;, &amp;#39; &amp;amp;times; &amp;#39;, .)
      }
      
      # Output table
      model_summary %&amp;gt;% 
        
        # Remove header of first column and rename other headers
        rename(&amp;#39; &amp;#39; = &amp;#39;Effect&amp;#39;, &amp;#39;&amp;amp;beta;&amp;#39; = &amp;#39;Estimate&amp;#39;, &amp;#39;&amp;lt;i&amp;gt;SE&amp;lt;/i&amp;gt;&amp;#39; = &amp;#39;SE&amp;#39;, 
               &amp;#39;95% CrI&amp;#39; = &amp;#39;CrI_95&amp;#39;, &amp;#39;$\\widehat{R}$&amp;#39; = &amp;#39;Rhat&amp;#39;) %&amp;gt;%
        
        # Present table
        kbl(digits = 2, booktabs = TRUE, escape = FALSE, align = align,
            
            # Caption of the table (default unless specified)
            caption = caption, 
            
            # Disable occasional line gap (https://stackoverflow.com/a/49018919/7050882)
            linesep = &amp;#39;&amp;#39;) %&amp;gt;%
        
        # Apply nice kableExtra format
        kable_styling() %&amp;gt;%
        
        # Center-align header row
        row_spec(0, align = &amp;#39;c&amp;#39;)
      
      # LaTeX format
    } else {
      
      # If interaction_symbol_x = TRUE, replace double colons with times 
      # symbols followed by line breaks and indentation. Then, replace 
      # single colons with times symbols.
      if(interaction_symbol_x) {
        model_summary$Effect = model_summary$Effect %&amp;gt;% 
          gsub(&amp;#39;::&amp;#39;, &amp;#39; $\\\\times$ \n \\\\hspace{0.3cm}&amp;#39;, .) %&amp;gt;%
          gsub(&amp;#39;:&amp;#39;, &amp;#39; $\\\\times$ &amp;#39;, .)
      }
      
      model_summary$Effect = model_summary$Effect %&amp;gt;%
        
        # Escape underscores to avoid error in table
        str_replace_all(&amp;#39;_&amp;#39;, &amp;#39;\\\\_&amp;#39;) %&amp;gt;%
        
        # Allow line breaks in the names of the effects
        # (used in the interactions)
        kableExtra::linebreak(align = &amp;#39;l&amp;#39;)
      
      # Output table
      model_summary %&amp;gt;% 
        
        # Remove header of first column and rename other headers
        rename(&amp;#39; &amp;#39; = &amp;#39;Effect&amp;#39;, &amp;#39;$\\upbeta$&amp;#39; = &amp;#39;Estimate&amp;#39;, &amp;#39;$SE$&amp;#39; = &amp;#39;SE&amp;#39;, 
               &amp;#39;95\\% CrI&amp;#39; = &amp;#39;CrI_95&amp;#39;, &amp;#39;$\\widehat R$&amp;#39; = &amp;#39;Rhat&amp;#39;) %&amp;gt;%
        
        # Present table
        kbl(digits = 2, booktabs = TRUE, escape = FALSE, align = align,
            
            # Caption of the table (default unless specified)
            caption = caption, 
            
            # Disable occasional line gap (https://stackoverflow.com/a/49018919/7050882)
            linesep = &amp;#39;&amp;#39;) %&amp;gt;%
        
        # Apply nice kableExtra format
        kable_styling() %&amp;gt;%
        
        # Center-align header row
        row_spec(0, align = &amp;#39;c&amp;#39;)
    }
  }&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-function-in-use&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function in use&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create table (using custom function from the &amp;#39;R_functions&amp;#39; folder)
bayesian_model_table(
  semanticpriming_summary_weaklyinformativepriors_exgaussian,
  order_effects = c(&amp;#39;(Intercept)&amp;#39;,
                    &amp;#39;Attentional control&amp;#39;,
                    &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Word frequency&amp;#39;,
                    &amp;#39;Number of syllables&amp;#39;,
                    &amp;#39;Word-concreteness difference&amp;#39;,
                    &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;,
                    &amp;#39;Word-concreteness difference : SOA&amp;#39;,
                    &amp;#39;Word-concreteness difference : Gender&amp;#39;,
                    &amp;#39;Language-based similarity :: Attentional control&amp;#39;,
                    &amp;#39;Visual-strength difference :: Attentional control&amp;#39;,
                    &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;,
                    &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;,
                    &amp;#39;Language-based similarity : Gender&amp;#39;,
                    &amp;#39;Visual-strength difference : Gender&amp;#39;,
                    &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;),
  
  # Replace colons in the names of interactions with times symbols
  interaction_symbol_x = TRUE, 
  
  # No title
  caption = NULL) %&amp;gt;%
  
  # Group predictors under headings
  pack_rows(&amp;#39;Individual differences&amp;#39;, 2, 4) %&amp;gt;% 
  pack_rows(&amp;#39;Target-word lexical covariates&amp;#39;, 5, 6) %&amp;gt;% 
  pack_rows(&amp;#39;Prime--target relationship&amp;#39;, 7, 9) %&amp;gt;% 
  pack_rows(&amp;#39;Task condition&amp;#39;, 10, 10) %&amp;gt;% 
  pack_rows(&amp;#39;Interactions&amp;#39;, 11, 21) %&amp;gt;% 
  
  # Apply white background to override default shading in HTML output
  row_spec(1:21, background = &amp;#39;white&amp;#39;) %&amp;gt;%
  
  # Highlight covariates
  row_spec(c(2, 5:7, 11:15), background = &amp;#39;#FFFFF1&amp;#39;) %&amp;gt;%
  
  # Format
  kable_classic(full_width = FALSE, html_font = &amp;#39;Cambria&amp;#39;) %&amp;gt;%
  
  # Footnote describing abbreviations, random slopes, etc. 
  # LaTeX code used to format the text.
  footnote(escape = FALSE, threeparttable = TRUE, general_title = &amp;#39;&amp;lt;br&amp;gt;&amp;#39;, 
           general = paste(&amp;#39;*Note*. &amp;amp;beta; = Estimate based on $z$-scored predictors; *SE* = standard error;&amp;#39;,
                           &amp;#39;CrI = credible interval. Yellow rows contain covariates. Some interactions are &amp;#39;,
                           &amp;#39;split over two lines, with the second line indented. &amp;lt;br&amp;gt;&amp;#39;, 
                           &amp;#39;&amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt; By-word random slopes were included for this effect.&amp;#39;,
                           &amp;#39;&amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt; By-participant random slopes were included for this effect.&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table lightable-classic&#34; style=&#34;margin-left: auto; margin-right: auto; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;border-bottom: 0;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
β
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;i&gt;SE&lt;/i&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
95% CrI
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\widehat{R}\)&lt;/span&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;background-color: white !important;&#34;&gt;
(Intercept)
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Individual differences&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Vocabulary size &lt;sup&gt;a&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Gender &lt;sup&gt;a&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Target-word lexical covariates&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word frequency
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.12, -0.11]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Number of syllables
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.06, 0.07]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Prime–target relationship&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.07, -0.06]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.01, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;1&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Task condition&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Stimulus onset asynchrony (SOA) &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.02, 0.04]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;11&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Interactions&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × SOA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × &lt;br&gt;    Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × &lt;br&gt;    Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × SOA &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × SOA &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; &#34; colspan=&#34;100%&#34;&gt;
&lt;span style=&#34;font-style: italic;&#34;&gt;&lt;br&gt;&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; &#34; colspan=&#34;100%&#34;&gt;
&lt;sup&gt;&lt;/sup&gt; &lt;em&gt;Note&lt;/em&gt;. β = Estimate based on &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scored predictors; &lt;em&gt;SE&lt;/em&gt; = standard error; CrI = credible interval. Yellow rows contain covariates. Some interactions are split over two lines, with the second line indented. &lt;br&gt; &lt;sup&gt;a&lt;/sup&gt; By-word random slopes were included for this effect. &lt;sup&gt;b&lt;/sup&gt; By-participant random slopes were included for this effect.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
&lt;div id=&#34;shading-specific-rows&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Shading specific rows&lt;/h4&gt;
&lt;p&gt;Shading specific rows is done differently when the output is PDF, as shown below (see p. 170 in &lt;a href=&#34;https://eprints.lancs.ac.uk/id/eprint/177833/6/2022deJuanBernabeuPhD.pdf&#34;&gt;Bernabeu, 2022&lt;/a&gt;).&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fthesis%2Fappendix-E-Bayesian-analysis-results.Rmd%23L164-L165&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Session info&lt;/h3&gt;
&lt;p&gt;If you encounter any blockers while reproducing the table using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;, my current session info may be useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.2 (2022-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19045)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.utf8 
## [2] LC_CTYPE=English_United States.utf8   
## [3] LC_MONETARY=English_United States.utf8
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] kableExtra_1.3.4    lmerTest_3.1-3      lme4_1.1-31        
## [4] Matrix_1.5-1        stringr_1.5.0       tibble_3.1.8       
## [7] dplyr_1.0.10        knitr_1.41          xaringanExtra_0.7.0
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.2.0    xfun_0.36           bslib_0.4.2        
##  [4] splines_4.2.2       lattice_0.20-45     colorspace_2.0-3   
##  [7] vctrs_0.5.1         generics_0.1.3      viridisLite_0.4.1  
## [10] htmltools_0.5.4     yaml_2.3.6          utf8_1.2.2         
## [13] rlang_1.0.6         jquerylib_0.1.4     pillar_1.8.1       
## [16] nloptr_2.0.3        withr_2.5.0         glue_1.6.2         
## [19] DBI_1.1.3           uuid_1.1-0          lifecycle_1.0.3    
## [22] munsell_0.5.0       blogdown_1.16       gtable_0.3.1       
## [25] rvest_1.0.3         evaluate_0.19       fastmap_1.1.0      
## [28] fansi_1.0.3         Rcpp_1.0.9          scales_1.2.1       
## [31] cachem_1.0.6        webshot_0.5.4       jsonlite_1.8.4     
## [34] systemfonts_1.0.4   ggplot2_3.3.5       digest_0.6.31      
## [37] stringi_1.7.8       bookdown_0.31       numDeriv_2016.8-1.1
## [40] grid_4.2.2          cli_3.4.1           tools_4.2.2        
## [43] magrittr_2.0.3      sass_0.4.4          pkgconfig_2.0.3    
## [46] MASS_7.3-58.1       xml2_1.3.3          svglite_2.1.0      
## [49] httr_1.4.4          assertthat_0.2.1    minqa_1.2.5        
## [52] rmarkdown_2.19      rstudioapi_0.14     R6_2.5.1           
## [55] boot_1.3-28         nlme_3.1-160        compiler_4.2.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://doi.org/10.1037/met0000159&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bürkner, P.-C., Gabry, J., Weber, S., Johnson, A., Modrak, M., Badr, H. S., Weber, F., Ben-Shachar, M. S., &amp;amp; Rabel, H. (2022). &lt;em&gt;Package ’brms’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/brms/brms.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhu, H. (2022). &lt;em&gt;Package ’kableExtra’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      
            <category>R</category>
      
            <category>statistics</category>
      
            <category>rstats</category>
      
            <category>brms</category>
      
            <category>credible intervals</category>
      
            <category>s</category>
      
      
            <category>R</category>
      
            <category>statistics</category>
      
    </item>
    
    <item>
      <title>A table of results for frequentist mixed-effects models: Grouping variables and specifying random slopes</title>
      <link>https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/</link>
      <pubDate>Thu, 29 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;Here I share the format applied to tables presenting the results of &lt;em&gt;frequentist&lt;/em&gt; models in Bernabeu (2022; the &lt;a href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes&#34;&gt;table for Bayesian models is covered in this other post&lt;/a&gt;). The sample table presents a mixed-effects model that was fitted using the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; (Kuznetsova et al., 2022). The mixed effects were driven by the maximal principle (Brauer &amp;amp; Curtin, 2018). The format of the table resembles one of the examples published by the &lt;a href=&#34;https://apastyle.apa.org/style-grammar-guidelines/tables-figures/sample-tables&#34;&gt;American Psychological Association&lt;/a&gt;. However, there are also deviations from those examples. For instance, in the present table, the effects are grouped under informative labels to facilitate the readers’ comprehension, using the &lt;a href=&#34;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&#34;&gt;&lt;code&gt;kableExtra&lt;/code&gt;&lt;/a&gt; package (Zhu, 2022). Furthermore, the random slopes are specified using superscript letters and a footnote. The table can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;loading-packages-and-the-results-of-the-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading packages and the results of the models&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(knitr)
library(tibble)
library(stringr)
library(lmerTest)
library(kableExtra)

# Load frequentist coefficients (estimates and confidence intervals)

KR_summary_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/KR_summary_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

confint_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/confint_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adjusting-the-names-of-the-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adjusting the names of the effects&lt;/h3&gt;
&lt;p&gt;First, to facilitate the understanding of the results, the original names of the effects will be adjusted in the &lt;code&gt;lmerTest&lt;/code&gt; summary and in the confidence intervals object.&lt;/p&gt;
&lt;p&gt;Incidentally, the confidence intervals were obtained using the &lt;code&gt;confint.merMod&lt;/code&gt; function from the &lt;code&gt;lme4&lt;/code&gt; package, as neither &lt;code&gt;lmerTest&lt;/code&gt; nor &lt;code&gt;lme4&lt;/code&gt; currently provide confidence intervals in their default results output. However, computing the confidence intervals is uncomplicated (&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/semanticpriming_lmerTest.R#L126-L130&#34;&gt;see code&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Rename effects in plain language and specify the random slopes
# (if any) for each effect, in the footnote. For this purpose, 
# superscripts are added to the names of the appropriate effects.
# 
# In the interactions below, word-level variables are presented 
# first for the sake of consistency (the order does not affect 
# the results in any way). Also in the interactions, double 
# colons are used to inform the &amp;#39;frequentist_model_table&amp;#39; 
# function that the two terms in the interaction must be split 
# into two lines.

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_attentional_control&amp;#39;] = &amp;#39;Attentional control&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_vocabulary_size&amp;#39;] = &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_participant_gender&amp;#39;] = &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_target_word_frequency&amp;#39;] = &amp;#39;Word frequency&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_target_number_syllables&amp;#39;] = &amp;#39;Number of syllables&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_word_concreteness_diff&amp;#39;] = &amp;#39;Word-concreteness difference&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_cosine_similarity&amp;#39;] = &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_visual_rating_diff&amp;#39;] = &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_interstimulus_interval&amp;#39;] = &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_word_concreteness_diff:z_vocabulary_size&amp;#39;] = 
  &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Word-concreteness difference : SOA&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_participant_gender&amp;#39;] = 
  &amp;#39;Word-concreteness difference : Gender&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_attentional_control:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Attentional control&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_attentional_control:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Attentional control&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_vocabulary_size:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_vocabulary_size:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_participant_gender:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : Gender&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_participant_gender:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : Gender&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;


# Next, change the names in the confidence intervals object

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_attentional_control&amp;#39;] = &amp;#39;Attentional control&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_vocabulary_size&amp;#39;] = &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_participant_gender&amp;#39;] = &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_target_word_frequency&amp;#39;] = &amp;#39;Word frequency&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_target_number_syllables&amp;#39;] = &amp;#39;Number of syllables&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_word_concreteness_diff&amp;#39;] = &amp;#39;Word-concreteness difference&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_cosine_similarity&amp;#39;] = &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_visual_rating_diff&amp;#39;] = &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_interstimulus_interval&amp;#39;] = &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_word_concreteness_diff:z_vocabulary_size&amp;#39;] = 
  &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Word-concreteness difference : SOA&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_participant_gender&amp;#39;] = 
  &amp;#39;Word-concreteness difference : Gender&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_attentional_control:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Attentional control&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_attentional_control:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Attentional control&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_vocabulary_size:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_vocabulary_size:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_participant_gender:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : Gender&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_participant_gender:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : Gender&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;frequentist_model_table&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;frequentist_model_table()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The following custom function was used.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Ffrequentist_model_table.R&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;loading-the-function-from-github&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Loading the function from GitHub&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/raw/main/R_functions/frequentist_model_table.R&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-function-in-use&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function in use&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create table (using custom function from the &amp;#39;R_functions&amp;#39; folder)
frequentist_model_table(
  KR_summary_semanticpriming_lmerTest, 
  confint_semanticpriming_lmerTest,
  order_effects = c(&amp;#39;(Intercept)&amp;#39;,
                    &amp;#39;Attentional control&amp;#39;,
                    &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Word frequency&amp;#39;,
                    &amp;#39;Number of syllables&amp;#39;,
                    &amp;#39;Word-concreteness difference&amp;#39;,
                    &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;,
                    &amp;#39;Word-concreteness difference : SOA&amp;#39;,
                    &amp;#39;Word-concreteness difference : Gender&amp;#39;,
                    &amp;#39;Language-based similarity :: Attentional control&amp;#39;,
                    &amp;#39;Visual-strength difference :: Attentional control&amp;#39;,
                    &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;,
                    &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;,
                    &amp;#39;Language-based similarity : Gender&amp;#39;,
                    &amp;#39;Visual-strength difference : Gender&amp;#39;,
                    &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;),
  
  # Replace colons in the names of interactions with times symbols
  interaction_symbol_x = TRUE, 
  
  # No title
  caption = NULL) %&amp;gt;%
  
  # Group predictors under headings
  pack_rows(&amp;#39;Individual differences&amp;#39;, 2, 4) %&amp;gt;% 
  pack_rows(&amp;#39;Target-word lexical covariates&amp;#39;, 5, 6) %&amp;gt;% 
  pack_rows(&amp;#39;Prime--target relationship&amp;#39;, 7, 9) %&amp;gt;% 
  pack_rows(&amp;#39;Task condition&amp;#39;, 10, 10) %&amp;gt;% 
  pack_rows(&amp;#39;Interactions&amp;#39;, 11, 21) %&amp;gt;% 
  
  # Apply white background to override default shading in HTML output
  row_spec(1:21, background = &amp;#39;white&amp;#39;) %&amp;gt;%
  
  # Highlight covariates
  row_spec(c(2, 5:7, 11:15), background = &amp;#39;#FFFFF1&amp;#39;) %&amp;gt;%
  
  # Format
  kable_classic(full_width = FALSE, html_font = &amp;#39;Cambria&amp;#39;) %&amp;gt;%
  
  # Footnote describing abbreviations, random slopes, etc. 
  # LaTeX code used to format the text.
  footnote(escape = FALSE, threeparttable = TRUE, general_title = &amp;#39;&amp;lt;br&amp;gt;&amp;#39;, 
           general = paste(&amp;#39;*Note*. &amp;amp;beta; = Estimate based on $z$-scored predictors; *SE* = standard error;&amp;#39;,
                           &amp;#39;CI = confidence interval. Yellow rows contain covariates. Some interactions are &amp;#39;,
                           &amp;#39;split over two lines, with the second line indented. &amp;lt;br&amp;gt;&amp;#39;, 
                           &amp;#39;&amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt; By-word random slopes were included for this effect.&amp;#39;,
                           &amp;#39;&amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt; By-participant random slopes were included for this effect.&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table lightable-classic&#34; style=&#34;margin-left: auto; margin-right: auto; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;border-bottom: 0;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
β
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;i&gt;SE&lt;/i&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
95% CI
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;i&gt;t&lt;/i&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;i&gt;p&lt;/i&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;background-color: white !important;&#34;&gt;
(Intercept)
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.112
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Individual differences&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.56
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.577
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Vocabulary size &lt;sup&gt;a&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.02
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.987
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Gender &lt;sup&gt;a&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.979
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Target-word lexical covariates&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word frequency
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.16, -0.15]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-49.40
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Number of syllables
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.07, 0.08]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
22.81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Prime–target relationship&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.01, 0.02]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
3.48
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.08, -0.07]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-22.44
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.01, 0.02]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
4.18
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;1&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Task condition&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Stimulus onset asynchrony (SOA) &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.04, 0.07]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
7.47
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;11&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Interactions&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.189
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × SOA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
2.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.010
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.97
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.332
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × &lt;br&gt;    Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-2.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.014
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × &lt;br&gt;    Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.810
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-2.34
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.020
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-1.37
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.172
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.433
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.144
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × SOA &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
3.22
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × SOA &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-2.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.025
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; &#34; colspan=&#34;100%&#34;&gt;
&lt;span style=&#34;font-style: italic;&#34;&gt;&lt;br&gt;&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; &#34; colspan=&#34;100%&#34;&gt;
&lt;sup&gt;&lt;/sup&gt; &lt;em&gt;Note&lt;/em&gt;. β = Estimate based on &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scored predictors; &lt;em&gt;SE&lt;/em&gt; = standard error; CI = confidence interval. Yellow rows contain covariates. Some interactions are split over two lines, with the second line indented. &lt;br&gt; &lt;sup&gt;a&lt;/sup&gt; By-word random slopes were included for this effect. &lt;sup&gt;b&lt;/sup&gt; By-participant random slopes were included for this effect.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
&lt;div id=&#34;shading-specific-rows&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Shading specific rows&lt;/h4&gt;
&lt;p&gt;Shading specific rows is done differently when the output is PDF, as shown below (see p. 62 in &lt;a href=&#34;https://eprints.lancs.ac.uk/id/eprint/177833/6/2022deJuanBernabeuPhD.pdf&#34;&gt;Bernabeu, 2022&lt;/a&gt;).&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fthesis%2FChapter-3-Study-2.Rmd%23L690-L691&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Session info&lt;/h3&gt;
&lt;p&gt;If you encounter any blockers while reproducing the table using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;, my current session info may be useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.2 (2022-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19045)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.utf8 
## [2] LC_CTYPE=English_United States.utf8   
## [3] LC_MONETARY=English_United States.utf8
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] kableExtra_1.3.4    lmerTest_3.1-3      lme4_1.1-31        
## [4] Matrix_1.5-1        stringr_1.5.0       tibble_3.1.8       
## [7] dplyr_1.0.10        knitr_1.41          xaringanExtra_0.7.0
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.2.0    xfun_0.36           bslib_0.4.2        
##  [4] splines_4.2.2       lattice_0.20-45     colorspace_2.0-3   
##  [7] vctrs_0.5.1         generics_0.1.3      viridisLite_0.4.1  
## [10] htmltools_0.5.4     yaml_2.3.6          utf8_1.2.2         
## [13] rlang_1.0.6         jquerylib_0.1.4     pillar_1.8.1       
## [16] nloptr_2.0.3        withr_2.5.0         glue_1.6.2         
## [19] DBI_1.1.3           uuid_1.1-0          lifecycle_1.0.3    
## [22] munsell_0.5.0       blogdown_1.16       gtable_0.3.1       
## [25] rvest_1.0.3         evaluate_0.19       fastmap_1.1.0      
## [28] fansi_1.0.3         Rcpp_1.0.9          scales_1.2.1       
## [31] cachem_1.0.6        webshot_0.5.4       jsonlite_1.8.4     
## [34] systemfonts_1.0.4   ggplot2_3.3.5       digest_0.6.31      
## [37] stringi_1.7.8       bookdown_0.31       numDeriv_2016.8-1.1
## [40] grid_4.2.2          cli_3.4.1           tools_4.2.2        
## [43] magrittr_2.0.3      sass_0.4.4          pkgconfig_2.0.3    
## [46] MASS_7.3-58.1       xml2_1.3.3          svglite_2.1.0      
## [49] httr_1.4.4          assertthat_0.2.1    minqa_1.2.5        
## [52] rmarkdown_2.19      rstudioapi_0.14     R6_2.5.1           
## [55] boot_1.3-28         nlme_3.1-160        compiler_4.2.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://doi.org/10.1037/met0000159&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kuznetsova, A., Brockhoff, P. B., &amp;amp; Christensen, R. H. B. (2022). &lt;em&gt;Package ’lmerTest’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhu, H. (2022). &lt;em&gt;Package ’kableExtra’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      
            <category>R</category>
      
            <category>statistics</category>
      
            <category>rstats</category>
      
            <category>lmerTest</category>
      
            <category>lme4</category>
      
            <category>confidence intervals</category>
      
            <category>s</category>
      
      
            <category>R</category>
      
            <category>statistics</category>
      
    </item>
    
    <item>
      <title>Why can&#39;t we be friends? Plotting frequentist (lmerTest) and Bayesian (brms) mixed-effects models</title>
      <link>https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/</link>
      <pubDate>Fri, 23 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/index.en_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;


&lt;p&gt;Frequentist and Bayesian statistics are sometimes regarded as fundamentally different philosophies. Indeed, can both methods qualify as philosophies, or is one of them just a pointless ritual? Is frequentist statistics about &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; values only? Are frequentist estimates diametrically opposed to Bayesian posterior distributions? Are confidence intervals and credible intervals irreconcilable? Will R crash if &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34;&gt;&lt;code&gt;brms&lt;/code&gt;&lt;/a&gt; are simultaneously loaded? If only we could fit frequentist and Bayesian models to the same data and plot the results together, we might get a glimpse into these puzzles.&lt;/p&gt;
&lt;p&gt;All the analyses shown below can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;. The combination of the frequentist and the Bayesian estimates in the same plot is achieved using the following custom function from &lt;a href=&#34;https://pablobernabeu.github.io/publication/pablo-bernabeu-2022-phd-thesis&#34;&gt;Bernabeu (2022)&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;visualising-frequentist-and-bayesian-estimates-in-one-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualising frequentist and Bayesian estimates in one plot&lt;/h3&gt;
&lt;p&gt;Both frequentist and Bayesian statistics offer the options of hypothesis testing and parameter estimation (Cumming, 2014; Kruschke &amp;amp; Liddell, 2018; Rouder et al., 2018; Schmalz et al., 2022; Tendeiro &amp;amp; Kiers, 2019, 2022; van Ravenzwaaij &amp;amp; Wagenmakers, 2022). In the statistical analyses conducted by Bernabeu (2022), hypothesis testing was performed within the frequentist framework, whereas parameter estimation was performed within both the frequentist and the Bayesian frameworks (for other examples of the &lt;em&gt;estimation&lt;/em&gt; approach, see Milek et al., 2018; Pregla et al., 2021; Rodríguez-Ferreiro et al., 2020).&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Ffrequentist_bayesian_plot.R&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We’ll load the function from GitHub.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/main/R_functions/frequentist_bayesian_plot.R&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;height: 800px; border: 0.5px dotted grey; padding: 10px; resize: both; overflow: auto;&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Presenting the frequentist and the Bayesian estimates in the same plot. 
# For this purpose, the frequentist results are merged into a plot from 
# brms::mcmc_plot()

# install.packages(&amp;#39;devtools&amp;#39;)
# library(devtools)
# install_version(&amp;#39;tidyverse&amp;#39;, &amp;#39;1.3.1&amp;#39;)  # Due to breaking changes, Version 1.3.1 is required.
# install_version(&amp;#39;ggplot2&amp;#39;, &amp;#39;5.3.5&amp;#39;)  # Due to breaking changes, Version 5.3.5 is required.
library(tidyverse)
library(ggplot2)
library(Cairo)

# Load frequentist coefficients (estimates and confidence intervals)

KR_summary_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/KR_summary_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

confint_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/confint_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

# Below are the default names of the effects
# rownames(KR_summary_semanticpriming_lmerTest$coefficients)
# rownames(confint_semanticpriming_lmerTest)

# Load Bayesian posterior distributions

semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian = 
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian.rds?raw=true&amp;#39;)))

# Below are the default names of the effects
# levels(semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter)


# Reorder the components of interactions in the frequentist results to match 
# with the order present in the Bayesian results.

rownames(KR_summary_semanticpriming_lmerTest$coefficients) =
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;, 
              replacement = &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;, 
              replacement = &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;)

rownames(confint_semanticpriming_lmerTest)  = 
  rownames(confint_semanticpriming_lmerTest) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;, 
              replacement = &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;, 
              replacement = &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;)


# Create a vector containing the names of the effects. This vector will be passed 
# to the plotting function.

new_labels = 
  
  semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter %&amp;gt;% 
  unique %&amp;gt;%
  
  # Remove the default &amp;#39;b_&amp;#39; from the beginning of each effect
  str_remove(&amp;#39;^b_&amp;#39;) %&amp;gt;%
  
  # Put Intercept in parentheses
  str_replace(pattern = &amp;#39;Intercept&amp;#39;, replacement = &amp;#39;(Intercept)&amp;#39;) %&amp;gt;%
  
  # First, adjust names of variables (both in main effects and in interactions)
  str_replace(pattern = &amp;#39;z_target_word_frequency&amp;#39;,
              replacement = &amp;#39;Target-word frequency&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_target_number_syllables&amp;#39;,
              replacement = &amp;#39;Number of target-word syllables&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_word_concreteness_diff&amp;#39;,
              replacement = &amp;#39;Word-concreteness difference&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_cosine_similarity&amp;#39;,
              replacement = &amp;#39;Language-based similarity&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_visual_rating_diff&amp;#39;,
              replacement = &amp;#39;Visual-strength difference&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_attentional_control&amp;#39;,
              replacement = &amp;#39;Attentional control&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_vocabulary_size&amp;#39;,
              replacement = &amp;#39;Vocabulary size&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_participant_gender&amp;#39;,
              replacement = &amp;#39;Gender&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval&amp;#39;,
              replacement = &amp;#39;SOA&amp;#39;) %&amp;gt;%
  # Show acronym in main effect of SOA
  str_replace(pattern = &amp;#39;^SOA$&amp;#39;,
              replacement = &amp;#39;Stimulus onset asynchrony (SOA)&amp;#39;) %&amp;gt;%
  
  # Second, adjust order of effects in interactions. In the output from the model, 
  # the word-level variables of interest (i.e., &amp;#39;z_cosine_similarity&amp;#39; and 
  # &amp;#39;z_visual_rating_diff&amp;#39;) sometimes appeared second in their interactions. For 
  # better consistency, the code below moves those word-level variables (with 
  # their new names) to the first position in their interactions. Note that the 
  # order does not affect the results in any way.
  sub(&amp;#39;(\\w+.*):(Language-based similarity|Visual-strength difference)&amp;#39;, 
      &amp;#39;\\2:\\1&amp;#39;, 
      .) %&amp;gt;%
  
  # Replace colons denoting interactions with times symbols
  str_replace(pattern = &amp;#39;:&amp;#39;, replacement = &amp;#39; &amp;amp;times; &amp;#39;)


# Create plot
plot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian =
  frequentist_bayesian_plot(KR_summary_semanticpriming_lmerTest,
                            confint_semanticpriming_lmerTest,
                            semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian,
                            labels = new_labels, interaction_symbol_x = TRUE,
                            vertical_line_at_x = 0, x_title = &amp;#39;Effect size (&amp;amp;beta;)&amp;#39;,
                            legend_ncol = 1) + 
  theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/index.en_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Frequentist and Bayesian estimates are not so polar opposites, are they? What is more, the larger differences between some estimates are the result of the priors that were set on the corresponding effects. With uninformative priors, the frequentist and the Bayesian estimates are virtually identical.&lt;/p&gt;
&lt;script src=&#39;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Fbayesian_analysis%2Fsemanticpriming_brms_weaklyinformativepriors_exgaussian.R%23L16-L35&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#39;&gt;&lt;/script&gt;
&lt;p&gt;Now it’s time to consider in earnest:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Is frequentist statistics about &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; values only? Are frequentist estimates diametrically opposed to Bayesian posterior distributions? Are confidence intervals and credible intervals irreconcilable? Will R crash if &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34;&gt;&lt;code&gt;brms&lt;/code&gt;&lt;/a&gt; are simultaneously loaded?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Session info&lt;/h3&gt;
&lt;p&gt;If you encounter any blockers while reproducing the above analyses using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;, my current session info may be useful. For instance, the legend on the plot may not show if the latest versions of the &lt;code&gt;ggplot2&lt;/code&gt; and the &lt;code&gt;tidyverse&lt;/code&gt; packages are used. Instead, &lt;code&gt;ggplot2 3.3.5&lt;/code&gt; and &lt;code&gt;tidyverse 1.3.1&lt;/code&gt; should be installed using &lt;code&gt;install_version(&#39;tidyverse&#39;, &#39;1.3.1&#39;)&lt;/code&gt; and &lt;code&gt;install_version(&#39;tidyverse&#39;, &#39;1.3.1&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.2 (2022-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19045)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.utf8 
## [2] LC_CTYPE=English_United States.utf8   
## [3] LC_MONETARY=English_United States.utf8
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] ggtext_0.1.2        Cairo_1.6-0         forcats_0.5.2      
##  [4] stringr_1.5.0       dplyr_1.0.10        purrr_1.0.0        
##  [7] readr_2.1.3         tidyr_1.2.1         tibble_3.1.8       
## [10] ggplot2_3.3.5       tidyverse_1.3.1     knitr_1.41         
## [13] xaringanExtra_0.7.0
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.9       lubridate_1.9.0  assertthat_0.2.1 digest_0.6.31   
##  [5] utf8_1.2.2       plyr_1.8.8       R6_2.5.1         cellranger_1.1.0
##  [9] ggridges_0.5.4   backports_1.4.1  reprex_2.0.2     evaluate_0.19   
## [13] highr_0.10       httr_1.4.4       blogdown_1.16    pillar_1.8.1    
## [17] rlang_1.0.6      uuid_1.1-0       readxl_1.4.1     rstudioapi_0.14 
## [21] jquerylib_0.1.4  rmarkdown_2.19   labeling_0.4.2   gridtext_0.1.5  
## [25] munsell_0.5.0    broom_1.0.2      compiler_4.2.2   modelr_0.1.10   
## [29] xfun_0.36        pkgconfig_2.0.3  htmltools_0.5.4  tidyselect_1.2.0
## [33] bookdown_0.31    fansi_1.0.3      crayon_1.5.2     tzdb_0.3.0      
## [37] dbplyr_2.2.1     withr_2.5.0      commonmark_1.8.1 grid_4.2.2      
## [41] jsonlite_1.8.4   gtable_0.3.1     lifecycle_1.0.3  DBI_1.1.3       
## [45] magrittr_2.0.3   scales_1.2.1     cli_3.4.1        stringi_1.7.8   
## [49] cachem_1.0.6     farver_2.1.1     fs_1.5.2         xml2_1.3.3      
## [53] bslib_0.4.2      ellipsis_0.3.2   generics_0.1.3   vctrs_0.5.1     
## [57] tools_4.2.2      glue_1.6.2       markdown_1.4     hms_1.1.2       
## [61] fastmap_1.1.0    yaml_2.3.6       timechange_0.1.1 colorspace_2.0-3
## [65] rvest_1.0.3      haven_2.5.1      sass_0.4.4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cumming, G. (2014). The new statistics: Why and how. &lt;em&gt;Psychological Science, 25&lt;/em&gt;(1), 7–29. &lt;a href=&#34;https://doi.org/10.1177/0956797613504966&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/0956797613504966&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kruschke, J. K., &amp;amp; Liddell, T. M. (2018). The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 25&lt;/em&gt;(1), 178–206.&lt;/p&gt;
&lt;p&gt;Milek, A., Butler, E. A., Tackman, A. M., Kaplan, D. M., Raison, C. L., Sbarra, D. A., Vazire, S., &amp;amp; Mehl, M. R. (2018). “Eavesdropping on happiness” revisited: A pooled, multisample replication of the association between life satisfaction and observed daily conversation quantity and quality. &lt;em&gt;Psychological Science, 29&lt;/em&gt;(9), 1451–1462. &lt;a href=&#34;https://doi.org/10.1177/0956797618774252&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/0956797618774252&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pregla, D., Lissón, P., Vasishth, S., Burchert, F., &amp;amp; Stadie, N. (2021). Variability in sentence comprehension in aphasia in German. &lt;em&gt;Brain and Language, 222&lt;/em&gt;, 105008. &lt;a href=&#34;https://doi.org/10.1016/j.bandl.2021.105008&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.bandl.2021.105008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rodríguez-Ferreiro, J., Aguilera, M., &amp;amp; Davies, R. (2020). Semantic priming and schizotypal personality: Reassessing the link between thought disorder and enhanced spreading of semantic activation. &lt;em&gt;PeerJ, 8&lt;/em&gt;, e9511. &lt;a href=&#34;https://doi.org/10.7717/peerj.9511&#34; class=&#34;uri&#34;&gt;https://doi.org/10.7717/peerj.9511&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rouder, J. N., Haaf, J. M., &amp;amp; Vandekerckhove, J. (2018). Bayesian inference for psychology, part IV: Parameter estimation and Bayes factors. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 25&lt;/em&gt;(1), 102–113. &lt;a href=&#34;https://doi.org/10.3758/s13423-017-1420-7&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13423-017-1420-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Schmalz, X., Biurrun Manresa, J., &amp;amp; Zhang, L. (2021). What is a Bayes factor? &lt;em&gt;Psychological Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1037/met0000421&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000421&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tendeiro, J. N., &amp;amp; Kiers, H. A. L. (2019). A review of issues about null hypothesis Bayesian testing. &lt;em&gt;Psychological Methods, 24&lt;/em&gt;(6), 774–795. &lt;a href=&#34;https://doi.org/10.1037/met0000221&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000221&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tendeiro, J. N., &amp;amp; Kiers, H. A. L. (2022). On the white, the black, and the many shades of gray in between: Our reply to van Ravenzwaaij and Wagenmakers (2021). &lt;em&gt;Psychological Methods, 27&lt;/em&gt;(3), 466–475. &lt;a href=&#34;https://doi.org/10.1037/met0000505&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000505&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;van Ravenzwaaij, D., &amp;amp; Wagenmakers, E.-J. (2022). Advantages masquerading as “issues” in Bayesian hypothesis testing: A commentary on Tendeiro and Kiers (2019). &lt;em&gt;Psychological Methods, 27&lt;/em&gt;(3), 451–465. &lt;a href=&#34;https://doi.org/10.1037/met0000415&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000415&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>statistics</category>
      
            <category>Bayesian statistics</category>
      
            <category>frequentist statistics</category>
      
            <category>linear-mixed effects models</category>
      
            <category>lmerTest</category>
      
            <category>brms</category>
      
            <category>plotting</category>
      
            <category>data visualisation</category>
      
            <category>s</category>
      
      
            <category>R</category>
      
            <category>statistics</category>
      
    </item>
    
    <item>
      <title>Bayesian workflow: Prior determination, predictive checks and sensitivity analyses</title>
      <link>https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/</link>
      <pubDate>Thu, 22 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/index.en_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
library(ggridges)
library(ggtext)
library(patchwork)
library(papaja)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This post presents a code-through of a Bayesian workflow in R, which can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;. The content is &lt;em&gt;closely&lt;/em&gt; based on &lt;span class=&#34;citation&#34;&gt;Bernabeu (&lt;a href=&#34;#ref-bernabeu2022a&#34; role=&#34;doc-biblioref&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;, which was in turn based on lots of other references. In addition to those, you may wish to consider &lt;span class=&#34;citation&#34;&gt;Nicenboim et al. (&lt;a href=&#34;#ref-nicenboim2023&#34; role=&#34;doc-biblioref&#34;&gt;2023&lt;/a&gt;)&lt;/span&gt;, a book in preparation that is already available online (&lt;a href=&#34;https://vasishth.github.io/bayescogsci/book&#34; class=&#34;uri&#34;&gt;https://vasishth.github.io/bayescogsci/book&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In &lt;span class=&#34;citation&#34;&gt;Bernabeu (&lt;a href=&#34;#ref-bernabeu2022a&#34; role=&#34;doc-biblioref&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;, a Bayesian analysis was performed to complement the estimates that had been obtained in the frequentist analysis. Whereas the goal of the frequentist analysis had been hypothesis testing, for which &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; values were used, the goal of the Bayesian analysis was parameter estimation. Accordingly, we estimated the posterior distribution of every effect, without calculating Bayes factors &lt;span class=&#34;citation&#34;&gt;(for other examples of the same &lt;em&gt;estimation approach&lt;/em&gt;, see &lt;a href=&#34;#ref-milekEavesdroppingHappinessRevisited2018&#34; role=&#34;doc-biblioref&#34;&gt;Milek et al., 2018&lt;/a&gt;; &lt;a href=&#34;#ref-preglaVariabilitySentenceComprehension2021&#34; role=&#34;doc-biblioref&#34;&gt;Pregla et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34; role=&#34;doc-biblioref&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;; for comparisons between estimation and hypothesis testing, see &lt;a href=&#34;#ref-cummingNewStatisticsWhy2014&#34; role=&#34;doc-biblioref&#34;&gt;Cumming, 2014&lt;/a&gt;; &lt;a href=&#34;#ref-kruschkeBayesianNewStatistics2018&#34; role=&#34;doc-biblioref&#34;&gt;Kruschke &amp;amp; Liddell, 2018&lt;/a&gt;; &lt;a href=&#34;#ref-rouderBayesianInferencePsychology2018&#34; role=&#34;doc-biblioref&#34;&gt;Rouder et al., 2018&lt;/a&gt;; &lt;a href=&#34;#ref-schmalzWhatBayesFactor2021&#34; role=&#34;doc-biblioref&#34;&gt;Schmalz et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-tendeiroReviewIssuesNull2019&#34; role=&#34;doc-biblioref&#34;&gt;Tendeiro &amp;amp; Kiers, 2019&lt;/a&gt;, &lt;a href=&#34;#ref-tendeiroOnTheWhite2022&#34; role=&#34;doc-biblioref&#34;&gt;in press&lt;/a&gt;; &lt;a href=&#34;#ref-vanravenzwaaijAdvantagesMasqueradingIssues2021&#34; role=&#34;doc-biblioref&#34;&gt;van Ravenzwaaij &amp;amp; Wagenmakers, 2021&lt;/a&gt;)&lt;/span&gt;. In the estimation approach, the estimates are interpreted by considering the position of their credible intervals in relation to the expected effect size. That is, the closer an interval is to an effect size of 0, the smaller the effect of that predictor. For instance, an interval that is symmetrically centred on 0 indicates a very small effect, whereas—in comparison—an interval that does not include 0 at all indicates a far larger effect.&lt;/p&gt;
&lt;p&gt;This analysis served two purposes: first, to ascertain the interpretation of the smaller effects—which were identified as unreliable in the power analyses—, and second, to complement the estimates obtained in the frequentist analysis. The latter purpose was pertinent because the frequentist models presented convergence warnings—even though it must be noted that a previous study found that frequentist and Bayesian estimates were similar despite convergence warnings appearing in the frequentist analysis &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34; role=&#34;doc-biblioref&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;)&lt;/span&gt;. Furthermore, the complementary analysis was pertinent because the frequentist models presented residual errors that deviated from normality—even though mixed-effects models are fairly robust to such a deviation &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kniefViolatingNormalityAssumption2021&#34; role=&#34;doc-biblioref&#34;&gt;Knief &amp;amp; Forstmeier, 2021&lt;/a&gt;; &lt;a href=&#34;#ref-schielzethRobustnessLinearMixed2020&#34; role=&#34;doc-biblioref&#34;&gt;Schielzeth et al., 2020&lt;/a&gt;)&lt;/span&gt;. Owing to these precedents, we expected to find broadly similar estimates in the frequentist analyses and in the Bayesian ones. Across studies, each frequentist model has a Bayesian counterpart, with the exception of the secondary analysis performed in Study 2.1 (semantic priming) that included &lt;code&gt;vision-based similarity&lt;/code&gt; as a predictor. The R package ‘brms’, Version 2.17.0, was used for the Bayesian analysis &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner, 2018&lt;/a&gt;; &lt;a href=&#34;#ref-burknerPackageBrms2022&#34; role=&#34;doc-biblioref&#34;&gt;Bürkner et al., 2022&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Priors&lt;/h2&gt;
&lt;p&gt;Priors are one of the hardest nuts to crack in Bayesian statistics. First, it can be useful to inspect what priors can be set in the model. Second, it is important to visualise a reasonable set of priors based on the available literature or any other available sources. Third, just before fitting the model, the adequacy of a range of priors should be assessed using prior predictive checks. Fourth, posterior predictive checks were performed to assess the consistency between the observed data and new data predicted by the posterior distributions. Fifth, the influence of the priors on the results should be assessed through a prior sensitivity analysis &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-leeBayesianCognitiveModeling2014&#34; role=&#34;doc-biblioref&#34;&gt;Lee &amp;amp; Wagenmakers, 2014&lt;/a&gt;; &lt;a href=&#34;#ref-schootBayesianStatisticsModelling2021&#34; role=&#34;doc-biblioref&#34;&gt;Van de Schoot et al., 2021&lt;/a&gt;; also see &lt;a href=&#34;#ref-bernabeu2022a&#34; role=&#34;doc-biblioref&#34;&gt;Bernabeu, 2022&lt;/a&gt;; &lt;a href=&#34;#ref-preglaVariabilitySentenceComprehension2021&#34; role=&#34;doc-biblioref&#34;&gt;Pregla et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34; role=&#34;doc-biblioref&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneInteractionGrammaticallyDistinct2021&#34; role=&#34;doc-biblioref&#34;&gt;Stone et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34; role=&#34;doc-biblioref&#34;&gt;Stone et al., 2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-what-priors-can-be-set&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Checking what priors can be set&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;brms::get_prior&lt;/code&gt; function can be used to check what effects in the model can be assigned a prior. The output (see &lt;a href=&#34;http://paul-buerkner.github.io/brms/reference/get_prior.html&#34;&gt;example&lt;/a&gt;) will include the current (perhaps default) prior on each effect.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Fbayesian_analysis%2Fprior_predictive_checks%2Fsemanticpriming_priorpredictivecheck_informativepriors.R%23L28-L100&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;determining-the-priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Determining the priors&lt;/h2&gt;
&lt;p&gt;The priors were established by inspecting the effect sizes obtained in previous studies as well as the effect sizes obtained in our frequentist analyses of the present data (reported in Studies 2.1, 2.2 and 2.3 below). In the first regard, the previous studies that were considered were selected because the experimental paradigms, variables and analytical procedures they had used were similar to those used in our current studies. Specifically, regarding paradigms, we sought studies that implemented: (I) semantic priming with a lexical decision task—as in Study 2.1—, (II) semantic decision—as in Study 2.2—, or (III) lexical decision—as in Study 2.3. Regarding analytical procedures, we sought studies in which both the dependent and the independent variables were &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scored. We found two studies that broadly matched these criteria: &lt;span class=&#34;citation&#34;&gt;Lim et al. (&lt;a href=&#34;#ref-lim2020a&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; (see Table 5 therein) and &lt;span class=&#34;citation&#34;&gt;Pexman &amp;amp; Yap (&lt;a href=&#34;#ref-pexman2018a&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; (see Tables 6 and 7 therein). Out of these studies, &lt;span class=&#34;citation&#34;&gt;Pexman &amp;amp; Yap (&lt;a href=&#34;#ref-pexman2018a&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; contained the variables that were most similar to ours, which included vocabulary size (labelled ‘NAART’) and word frequency.&lt;/p&gt;
&lt;p&gt;Based on both these studies and on the frequentist analyses reported below, a range of effect sizes was identified that spanned between β = -0.30 and β = 0.30. This range was centred around 0 as the variables were &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scored. The bounds of this range were determined by the largest effects, which appeared in &lt;span class=&#34;citation&#34;&gt;Pexman &amp;amp; Yap (&lt;a href=&#34;#ref-pexman2018a&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;. Pexman et al. conducted a semantic decision study, and split the data set into abstract and concrete words. The two largest effects they found were—first—a word concreteness effect in the concrete-words analysis of β = -0.41, and—second—a word concreteness effect in the abstract-words analysis of β = 0.20. Unlike Pexman et al., we did not split the data set into abstract and concrete words, but analysed these sets together. Therefore, we averaged between the aforementioned values, obtaining a range between β = -0.30 and β = 0.30.&lt;/p&gt;
&lt;p&gt;In the results of &lt;span class=&#34;citation&#34;&gt;Lim et al. (&lt;a href=&#34;#ref-lim2020a&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Pexman &amp;amp; Yap (&lt;a href=&#34;#ref-pexman2018a&#34; role=&#34;doc-biblioref&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;, and in our frequentist results, some effects consistently presented a negative polarity (i.e., leading to shorter response times), whereas some other effects were consistently positive. We incorporated the direction of effects into the priors only in cases of large effects that had presented a consistent direction (either positive or negative) in previous studies and in our frequentist analyses in the present studies. These criteria were matched by the following variables: word frequency—with a negative direction, as higher word frequency leads to shorter RTs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-brysbaertWordFrequencyEffect2018a&#34; role=&#34;doc-biblioref&#34;&gt;Brysbaert et al., 2018&lt;/a&gt;; &lt;a href=&#34;#ref-brysbaertImpactWordPrevalence2016&#34; role=&#34;doc-biblioref&#34;&gt;Brysbaert et al., 2016&lt;/a&gt;; &lt;a href=&#34;#ref-lim2020a&#34; role=&#34;doc-biblioref&#34;&gt;Lim et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-mendesPervasiveEffectWord2021&#34; role=&#34;doc-biblioref&#34;&gt;Mendes &amp;amp; Undorf, 2021&lt;/a&gt;; &lt;a href=&#34;#ref-pexman2018a&#34; role=&#34;doc-biblioref&#34;&gt;Pexman &amp;amp; Yap, 2018&lt;/a&gt;)&lt;/span&gt;—, number of letters and number of syllables—both with positive directions &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bartonWordlengthEffectReading2014&#34; role=&#34;doc-biblioref&#34;&gt;Barton et al., 2014&lt;/a&gt;; &lt;a href=&#34;#ref-beyersmannEvidenceEmbeddedWord2020&#34; role=&#34;doc-biblioref&#34;&gt;Beyersmann et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-pexman2018a&#34; role=&#34;doc-biblioref&#34;&gt;Pexman &amp;amp; Yap, 2018&lt;/a&gt;)&lt;/span&gt;—, and orthographic Levenshtein distance—with a positive direction &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cerniMotorExpertiseTyping2016&#34; role=&#34;doc-biblioref&#34;&gt;Cerni et al., 2016&lt;/a&gt;; &lt;a href=&#34;#ref-dijkstraMultilinkComputationalModel2019&#34; role=&#34;doc-biblioref&#34;&gt;Dijkstra et al., 2019&lt;/a&gt;; &lt;a href=&#34;#ref-kimEffectsLexicalFeatures2018&#34; role=&#34;doc-biblioref&#34;&gt;Kim et al., 2018&lt;/a&gt;; &lt;a href=&#34;#ref-yarkoniMovingColtheartNew2008&#34; role=&#34;doc-biblioref&#34;&gt;Yarkoni et al., 2008&lt;/a&gt;)&lt;/span&gt;. We did not incorporate information about the direction of the word concreteness effect, as this effect can follow different directions in abstract and concrete words &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-brysbaert2014a&#34; role=&#34;doc-biblioref&#34;&gt;Brysbaert et al., 2014&lt;/a&gt;; &lt;a href=&#34;#ref-pexman2018a&#34; role=&#34;doc-biblioref&#34;&gt;Pexman &amp;amp; Yap, 2018&lt;/a&gt;)&lt;/span&gt;, and we analysed both sets of words together. In conclusion, the four predictors that had directional priors were covariates. All the other predictors had priors centred on 0. Last, as a methodological matter, it is noteworthy that most of the psycholinguistic studies applying Bayesian analysis have not incorporated any directional information in priors &lt;span class=&#34;citation&#34;&gt;(e.g., &lt;a href=&#34;#ref-preglaVariabilitySentenceComprehension2021&#34; role=&#34;doc-biblioref&#34;&gt;Pregla et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34; role=&#34;doc-biblioref&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34; role=&#34;doc-biblioref&#34;&gt;Stone et al., 2020&lt;/a&gt;; cf. &lt;a href=&#34;#ref-stoneInteractionGrammaticallyDistinct2021&#34; role=&#34;doc-biblioref&#34;&gt;Stone et al., 2021&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;prior-distributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Prior distributions&lt;/h3&gt;
&lt;p&gt;The choice of priors can influence the results in consequential ways. To assess the extent of this influence, &lt;em&gt;prior sensitivity analyses&lt;/em&gt; have been recommended. These analyses are performed by comparing the effect of more and less strict priors—or, in other words, priors varying in their degree of informativeness. The degree of variation is adjusted through the standard deviation, and the means are not varied &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-leeBayesianCognitiveModeling2014&#34; role=&#34;doc-biblioref&#34;&gt;Lee &amp;amp; Wagenmakers, 2014&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34; role=&#34;doc-biblioref&#34;&gt;Stone et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-schootBayesianStatisticsModelling2021&#34; role=&#34;doc-biblioref&#34;&gt;Van de Schoot et al., 2021&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In this way, we compared the results obtained using ‘informative’ priors (&lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; = 0.1), ‘weakly-informative’ priors (&lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; = 0.2) and ‘diffuse’ priors (&lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; = 0.3). These standard deviations were chosen so that around 95% of values in the informative priors would fall within our initial range of effect sizes that spanned from -0.30 to 0.30. All priors are illustrated in the figure below. These priors resembled others from previous psycholinguistic studies &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-preglaVariabilitySentenceComprehension2021&#34; role=&#34;doc-biblioref&#34;&gt;Pregla et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-stoneInteractionGrammaticallyDistinct2021&#34; role=&#34;doc-biblioref&#34;&gt;Stone et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34; role=&#34;doc-biblioref&#34;&gt;Stone et al., 2020&lt;/a&gt;)&lt;/span&gt;. For instance, &lt;span class=&#34;citation&#34;&gt;Stone et al. (&lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; used the following priors: &lt;span class=&#34;math inline&#34;&gt;\(Normal\)&lt;/span&gt;(0, 0.1), &lt;span class=&#34;math inline&#34;&gt;\(Normal\)&lt;/span&gt;(0, 0.3) and &lt;span class=&#34;math inline&#34;&gt;\(Normal\)&lt;/span&gt;(0, 1). The range of standard deviations we used—i.e., 0.1, 0.2 and 0.3—was narrower than those of previous studies because our dependent variable and our predictors were &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scored, resulting in small estimates and small &lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt;s &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-lim2020a&#34; role=&#34;doc-biblioref&#34;&gt;Lim et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-pexman2018a&#34; role=&#34;doc-biblioref&#34;&gt;Pexman &amp;amp; Yap, 2018&lt;/a&gt;)&lt;/span&gt;. These priors were used on the fixed effects and on the standard deviation parameters of the fixed effects. For the correlations among the random effects, an &lt;span class=&#34;math inline&#34;&gt;\(LKJ\)&lt;/span&gt;(2) prior was used &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-lewandowskiGeneratingRandomCorrelation2009&#34; role=&#34;doc-biblioref&#34;&gt;Lewandowski et al., 2009&lt;/a&gt;)&lt;/span&gt;. This is a ‘regularising’ prior, as it assumes that high correlations among random effects are rare &lt;span class=&#34;citation&#34;&gt;(also used in &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34; role=&#34;doc-biblioref&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneInteractionGrammaticallyDistinct2021&#34; role=&#34;doc-biblioref&#34;&gt;Stone et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34; role=&#34;doc-biblioref&#34;&gt;Stone et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-vasishthBayesianDataAnalysis2018&#34; role=&#34;doc-biblioref&#34;&gt;Vasishth et al., 2018&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set seed number to ensure exact reproducibility 
# of the random distributions
set.seed(123)

# The code below plots all our types of priors. Each distribution 
# contains 10,000 simulations, resulting in 90,000 rows.

# The green vertical rectangle shows the range of plausible effect 
# sizes based on previous studies that applied a similar analysis 
# (Lim et al., 2020, https://doi.org/10.1177/1747021820906566; 
# Pexman &amp;amp; Yap, 2018, https://doi.org/10.1037/xlm0000499) as 
# well as on the frequentist analyses of the current data.

priors = data.frame(
  
  informativeness = 
    as.factor(c(rep(&amp;#39;Informative priors (*SD* = 0.1)&amp;#39;, 30000),
                rep(&amp;#39;Weakly-informative priors (*SD* = 0.2)&amp;#39;, 30000),
                rep(&amp;#39;Diffuse priors (*SD* = 0.3)&amp;#39;, 30000))), 
  
  direction = as.factor(c(rep(&amp;#39;negative&amp;#39;, 10000), 
                          rep(&amp;#39;neutral&amp;#39;, 10000),
                          rep(&amp;#39;positive&amp;#39;, 10000),
                          rep(&amp;#39;negative&amp;#39;, 10000), 
                          rep(&amp;#39;neutral&amp;#39;, 10000),
                          rep(&amp;#39;positive&amp;#39;, 10000),
                          rep(&amp;#39;negative&amp;#39;, 10000), 
                          rep(&amp;#39;neutral&amp;#39;, 10000),
                          rep(&amp;#39;positive&amp;#39;, 10000))),
  
  direction_and_distribution = 
    as.factor(c(rep(&amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.1)&amp;#39;, 10000), 
                rep(&amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.1)&amp;#39;, 10000),
                rep(&amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.1)&amp;#39;, 10000),
                rep(&amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.2)&amp;#39;, 10000),
                rep(&amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.2)&amp;#39;, 10000),
                rep(&amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.2)&amp;#39;, 10000),
                rep(&amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.3)&amp;#39;, 10000), 
                rep(&amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.3)&amp;#39;, 10000),
                rep(&amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.3)&amp;#39;, 10000))),
  
  estimate = c(rnorm(10000, m = -0.1, sd = 0.1),
               rnorm(10000, m = 0, sd = 0.1),
               rnorm(10000, m = 0.1, sd = 0.1),
               rnorm(10000, m = -0.1, sd = 0.2),
               rnorm(10000, m = 0, sd = 0.2),
               rnorm(10000, m = 0.1, sd = 0.2),
               rnorm(10000, m = -0.1, sd = 0.3),
               rnorm(10000, m = 0, sd = 0.3),
               rnorm(10000, m = 0.1, sd = 0.3))
)

# Order factor levels

priors$informativeness = 
  ordered(priors$informativeness, 
          levels = c(&amp;#39;Informative priors (*SD* = 0.1)&amp;#39;, 
                     &amp;#39;Weakly-informative priors (*SD* = 0.2)&amp;#39;, 
                     &amp;#39;Diffuse priors (*SD* = 0.3)&amp;#39;))

priors$direction = 
  ordered(priors$direction, 
          levels = c(&amp;#39;negative&amp;#39;, &amp;#39;neutral&amp;#39;, &amp;#39;positive&amp;#39;))

priors$direction_and_distribution =
  ordered(priors$direction_and_distribution,
          levels = c(&amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.1)&amp;#39;, 
                     &amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.1)&amp;#39;,
                     &amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.1)&amp;#39;,
                     &amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.2)&amp;#39;, 
                     &amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.2)&amp;#39;,
                     &amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.2)&amp;#39;,
                     &amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.3)&amp;#39;, 
                     &amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.3)&amp;#39;,
                     &amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.3)&amp;#39;))


# PLOT zone

colours = c(&amp;#39;#7276A2&amp;#39;, &amp;#39;black&amp;#39;, &amp;#39;#A27272&amp;#39;)
fill_colours = c(&amp;#39;#CCCBE7&amp;#39;, &amp;#39;#D7D7D7&amp;#39;, &amp;#39;#E7CBCB&amp;#39;)

# Initialise plot (`aes` specified separately to allow 
# use of `geom_rect` at the end)
ggplot() +
  
  # Turn to the distributions
  stat_density_ridges(data = priors, 
                      aes(x = estimate, y = direction_and_distribution, 
                          color = direction, fill = direction),
                      geom = &amp;#39;density_ridges_gradient&amp;#39;, alpha = 0.7, 
                      jittered_points = TRUE, quantile_lines = TRUE, 
                      quantiles = c(0.025, 0.975), show.legend = F) +
  scale_color_manual(values = colours) + 
  scale_fill_manual(values = fill_colours) + 
  # Adjust X axis to the random distributions obtained
  scale_x_continuous(limits = c(min(priors$estimate), 
                                max(priors$estimate)), 
                     n.breaks = 6, expand = c(0.04, 0.04)) +
  scale_y_discrete(expand = expansion(add = c(0.18, 1.9))) +
  # Facets containing the three models varying in informativeness
  facet_wrap(vars(informativeness), scales = &amp;#39;free&amp;#39;, dir = &amp;#39;v&amp;#39;) +
  # Vertical line at x = 0
  geom_vline(xintercept = 0, linetype = &amp;#39;dashed&amp;#39;, color = &amp;#39;grey50&amp;#39;) +
  xlab(&amp;#39;Effect size (&amp;amp;beta;)&amp;#39;) + 
  ylab(&amp;#39;Direction of the prior and corresponding distribution&amp;#39;) +
  theme_minimal() +
  theme(axis.title.x = ggtext::element_markdown(size = 12, margin = margin(t = 9)),
        axis.text.x = ggtext::element_markdown(size = 11, margin = margin(t = 4)),
        axis.title.y = ggtext::element_markdown(size = 12, margin = margin(r = 9)),
        axis.text.y = ggtext::element_markdown(lineheight = 1.6, colour = colours),
        strip.background = element_rect(fill = &amp;#39;grey98&amp;#39;, colour = &amp;#39;grey90&amp;#39;,
                                        linetype = &amp;#39;solid&amp;#39;),
        strip.text = element_markdown(size = 11, margin = margin(t = 7, b = 7)),
        panel.spacing.y = unit(9, &amp;#39;pt&amp;#39;), panel.grid.minor = element_blank(), 
        plot.margin = margin(8, 8, 9, 8)
  ) +
  
  # Shaded rectangle containing range of previous effects
  geom_rect(data = data.frame(x = 1), xmin = -0.3, xmax = 0.3, 
            ymin = -Inf, ymax = Inf, fill = &amp;#39;darkgreen&amp;#39;, alpha = .3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/index.en_files/figure-html/bayesian-priors-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Priors used in the three studies. The green vertical rectangle shows the range of plausible effect sizes based on previous studies and on our frequentist analyses. In the informative priors, around 95% of the values fall within the range.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;prior-predictive-checks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Prior predictive checks&lt;/h2&gt;
&lt;p&gt;The adequacy of each of these priors was assessed by performing prior predictive checks, in which we compared the observed data to the predictions of the model &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-schootBayesianStatisticsModelling2021&#34; role=&#34;doc-biblioref&#34;&gt;Van de Schoot et al., 2021&lt;/a&gt;)&lt;/span&gt;. Furthermore, in these checks we also tested the adequacy of two model-wide distributions: the traditional Gaussian distribution (default in most analyses) and an exponentially modified Gaussian—dubbed ‘ex-Gaussian’—distribution &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-matzkePsychologicalInterpretationExGaussian2009&#34; role=&#34;doc-biblioref&#34;&gt;Matzke &amp;amp; Wagenmakers, 2009&lt;/a&gt;)&lt;/span&gt;. The ex-Gaussian distribution was considered because the residual errors of the frequentist models were not normally distributed &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-loTransformNotTransform2015&#34; role=&#34;doc-biblioref&#34;&gt;Lo &amp;amp; Andrews, 2015&lt;/a&gt;)&lt;/span&gt;, and because this distribution was found to be more appropriate than the Gaussian one in a previous, related study &lt;span class=&#34;citation&#34;&gt;(see supplementary materials of &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34; role=&#34;doc-biblioref&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;)&lt;/span&gt;. The ex-Gaussian distribution had an identity link function, which preserves the interpretability of the coefficients, as opposed to a transformation applied directly to the dependent variable &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-loTransformNotTransform2015&#34; role=&#34;doc-biblioref&#34;&gt;Lo &amp;amp; Andrews, 2015&lt;/a&gt;)&lt;/span&gt;. The results of these prior predictive checks revealed that the priors were adequate, and that the ex-Gaussian distribution was more appropriate than the Gaussian one, converging with &lt;span class=&#34;citation&#34;&gt;Rodríguez-Ferreiro et al. (&lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. Therefore, the ex-Gaussian distribution was used in the final models.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Fbayesian_analysis%2Fprior_predictive_checks%2Fsemanticpriming_priorpredictivecheck_informativepriors.R%23L105-L235&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;models-with-a-gaussian-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Models with a Gaussian distribution&lt;/h3&gt;
&lt;p&gt;The figures below show the prior predictive checks for the Gaussian models. These plots show the maximum, mean and minimum values of the observed data (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) and those of the predicted distribution (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;, which stands for &lt;em&gt;rep&lt;/em&gt;lications of the outcome). The way of interpreting these plots is by comparing the observed data to the predicted distribution. The specifics of this comparison vary across the three plots. First, in the upper plot, which shows the maximum values, the ideal scenario would show the observed maximum value (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) overlapping with the maximum value of the predicted distribution (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;). Second, in the middle plot, showing the mean values, the ideal scenario would show the observed mean value (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) overlapping with the mean value of the predicted distribution (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;). Last, in the lower plot, which shows the minimum values, the ideal scenario would have the observed minimum value (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) overlapping with the minimum value of the predicted distribution (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;). While the overlap need not be absolute, the closer the observed and the predicted values are on the X axis, the better. As such, the three predictive checks below—corresponding to models that used the default Gaussian distribution—show that the priors fitted the data acceptably but not very well.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_informativepriors.pdf&#34;&gt;See plot on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the Gaussian, informative prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_weaklyinformativepriors.pdf&#34;&gt;See plot on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the Gaussian, weakly-informative prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_diffusepriors.pdf&#34;&gt;See plot on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the Gaussian, diffuse prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;models-with-an-exponentially-modified-gaussian-i.e.-ex-gaussian-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Models with an exponentially-modified Gaussian (i.e., ex-Gaussian) distribution&lt;/h3&gt;
&lt;p&gt;In contrast to the above results, the figures below demonstrate that, when an ex-Gaussian distribution was used, the priors fitted the data far better, which converged with the results of a similar comparison performed by Rodríguez-Ferreiro et al. (2020; see supplementary materials of the latter study).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_informativepriors_exgaussian.pdf&#34;&gt;See plot on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the ex-Gaussian, informative prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_weaklyinformativepriors_exgaussian.pdf&#34;&gt;See plot on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the ex-Gaussian, weakly-informative prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_diffusepriors_exgaussian.pdf&#34;&gt;See plot on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the ex-Gaussian, diffuse prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-predictive-checks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Posterior predictive checks&lt;/h2&gt;
&lt;p&gt;Based on the results from the prior predictive checks, the ex-Gaussian distribution was used in the final models. Next, posterior predictive checks were performed to assess the consistency between the observed data and new data predicted by the posterior distributions &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-schootBayesianStatisticsModelling2021&#34; role=&#34;doc-biblioref&#34;&gt;Van de Schoot et al., 2021&lt;/a&gt;)&lt;/span&gt;. The figure below presents the posterior predictive checks for the latter models. The interpretation of these plots is simple: the distributions of the observed (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) and the predicted data (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;) should be as similar as possible. As such, the plots below suggest that the results are trustworthy.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Fbayesian_analysis%2Fposterior_predictive_checks%2Fsemanticpriming_posteriorpredictivechecks.R&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/posterior_predictive_checks/plots/semanticpriming_posteriorpredictivechecks_allpriors_exgaussian.pdf&#34;&gt;See plot on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Posterior predictive checks for the (ex-Gaussian) models from the semantic priming study. The observed data (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) and the predicted data (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;) almost entirely overlap with each other, demonstrating a very good fit.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prior-sensitivity-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5. Prior sensitivity analysis&lt;/h2&gt;
&lt;p&gt;In the main analysis, the informative, weakly-informative and diffuse priors were used in separate models. In other words, in each model, all priors had the same degree of informativeness &lt;span class=&#34;citation&#34;&gt;(as done in &lt;a href=&#34;#ref-preglaVariabilitySentenceComprehension2021&#34; role=&#34;doc-biblioref&#34;&gt;Pregla et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34; role=&#34;doc-biblioref&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneInteractionGrammaticallyDistinct2021&#34; role=&#34;doc-biblioref&#34;&gt;Stone et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34; role=&#34;doc-biblioref&#34;&gt;Stone et al., 2020&lt;/a&gt;)&lt;/span&gt;. In this way, a prior sensitivity analysis was performed to acknowledge the likely influence of the priors on the posterior distributions—that is, on the results &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-leeBayesianCognitiveModeling2014&#34; role=&#34;doc-biblioref&#34;&gt;Lee &amp;amp; Wagenmakers, 2014&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34; role=&#34;doc-biblioref&#34;&gt;Stone et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-schootBayesianStatisticsModelling2021&#34; role=&#34;doc-biblioref&#34;&gt;Van de Schoot et al., 2021&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We’ll first load a &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/R_functions/frequentist_bayesian_plot.R&#34;&gt;custom function (&lt;code&gt;frequentist_bayesian_plot&lt;/code&gt;)&lt;/a&gt; from GitHub.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/main/R_functions/frequentist_bayesian_plot.R&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;height: 800px; border: 0.5px dotted grey; padding: 10px; resize: both; overflow: auto;&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Presenting the frequentist and the Bayesian estimates in the same plot. 
# For this purpose, the frequentist results are merged into a plot from 
# brms::mcmc_plot()

# install.packages(&amp;#39;devtools&amp;#39;)
# library(devtools)
# install_version(&amp;#39;tidyverse&amp;#39;, &amp;#39;1.3.1&amp;#39;)  # Due to breaking changes, Version 1.3.1 is required.
# install_version(&amp;#39;ggplot2&amp;#39;, &amp;#39;5.3.5&amp;#39;)  # Due to breaking changes, Version 5.3.5 is required.
library(tidyverse)
library(ggplot2)
library(Cairo)

# Load frequentist coefficients (estimates and confidence intervals)

KR_summary_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/KR_summary_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

confint_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/confint_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

# Below are the default names of the effects
# rownames(KR_summary_semanticpriming_lmerTest$coefficients)
# rownames(confint_semanticpriming_lmerTest)

# Load Bayesian posterior distributions

semanticpriming_posteriordistributions_informativepriors_exgaussian = 
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_posteriordistributions_informativepriors_exgaussian.rds?raw=true&amp;#39;)))

semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian.rds?raw=true&amp;#39;)))

semanticpriming_posteriordistributions_diffusepriors_exgaussian = 
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_posteriordistributions_diffusepriors_exgaussian.rds?raw=true&amp;#39;)))

# Below are the default names of the effects
# levels(semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter)


# Reorder the components of interactions in the frequentist results to match 
# with the order present in the Bayesian results.

rownames(KR_summary_semanticpriming_lmerTest$coefficients) =
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;, 
              replacement = &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;, 
              replacement = &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;)

rownames(confint_semanticpriming_lmerTest)  = 
  rownames(confint_semanticpriming_lmerTest) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;, 
              replacement = &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;, 
              replacement = &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;)


# Create a vector containing the names of the effects. This vector will be passed 
# to the plotting function.

new_labels = 
  
  semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter %&amp;gt;% 
  unique %&amp;gt;%
  
  # Remove the default &amp;#39;b_&amp;#39; from the beginning of each effect
  str_remove(&amp;#39;^b_&amp;#39;) %&amp;gt;%
  
  # Put Intercept in parentheses
  str_replace(pattern = &amp;#39;Intercept&amp;#39;, replacement = &amp;#39;(Intercept)&amp;#39;) %&amp;gt;%
  
  # First, adjust names of variables (both in main effects and in interactions)
  str_replace(pattern = &amp;#39;z_target_word_frequency&amp;#39;,
              replacement = &amp;#39;Target-word frequency&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_target_number_syllables&amp;#39;,
              replacement = &amp;#39;Number of target-word syllables&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_word_concreteness_diff&amp;#39;,
              replacement = &amp;#39;Word-concreteness difference&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_cosine_similarity&amp;#39;,
              replacement = &amp;#39;Language-based similarity&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_visual_rating_diff&amp;#39;, 
              replacement = &amp;#39;Visual-strength difference&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_attentional_control&amp;#39;,
              replacement = &amp;#39;Attentional control&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_vocabulary_size&amp;#39;,
              replacement = &amp;#39;Vocabulary size&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_participant_gender&amp;#39;,
              replacement = &amp;#39;Gender&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval&amp;#39;,
              replacement = &amp;#39;SOA&amp;#39;) %&amp;gt;%
  # Show acronym in main effect of SOA
  str_replace(pattern = &amp;#39;^SOA$&amp;#39;,
              replacement = &amp;#39;Stimulus onset asynchrony (SOA)&amp;#39;) %&amp;gt;%
  
  # Second, adjust order of effects in interactions. In the output from the model, 
  # the word-level variables of interest (i.e., &amp;#39;z_cosine_similarity&amp;#39; and 
  # &amp;#39;z_visual_rating_diff&amp;#39;) sometimes appeared second in their interactions. For 
  # better consistency, the code below moves those word-level variables (with 
  # their new names) to the first position in their interactions. Note that the 
  # order does not affect the results in any way.
  sub(&amp;#39;(\\w+.*):(Language-based similarity|Visual-strength difference)&amp;#39;, 
      &amp;#39;\\2:\\1&amp;#39;, 
      .) %&amp;gt;%
  
  # Replace colons denoting interactions with times symbols
  str_replace(pattern = &amp;#39;:&amp;#39;, replacement = &amp;#39; &amp;amp;times; &amp;#39;) 


# Create plots, beginning with the informative-prior model

plot_semanticpriming_frequentist_bayesian_plot_informativepriors_exgaussian =
  frequentist_bayesian_plot(KR_summary_semanticpriming_lmerTest,
                            confint_semanticpriming_lmerTest,
                            semanticpriming_posteriordistributions_informativepriors_exgaussian,
                            labels = new_labels, interaction_symbol_x = TRUE,
                            vertical_line_at_x = 0, x_title = &amp;#39;Effect size (&amp;amp;beta;)&amp;#39;, 
                            x_axis_labels = 3, note_frequentist_no_prior = TRUE) +
  ggtitle(&amp;#39;Prior *SD* = 0.1&amp;#39;)

#####

plot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian =
  frequentist_bayesian_plot(KR_summary_semanticpriming_lmerTest,
                            confint_semanticpriming_lmerTest,
                            semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian,
                            labels = new_labels, interaction_symbol_x = TRUE,
                            vertical_line_at_x = 0, x_title = &amp;#39;Effect size (&amp;amp;beta;)&amp;#39;,
                            x_axis_labels = 3, note_frequentist_no_prior = TRUE) +
  ggtitle(&amp;#39;Prior *SD* = 0.2&amp;#39;) +
  theme(axis.text.y = element_blank())

#####

plot_semanticpriming_frequentist_bayesian_plot_diffusepriors_exgaussian =
  frequentist_bayesian_plot(KR_summary_semanticpriming_lmerTest,
                            confint_semanticpriming_lmerTest,
                            semanticpriming_posteriordistributions_diffusepriors_exgaussian,
                            labels = new_labels, interaction_symbol_x = TRUE,
                            vertical_line_at_x = 0, x_title = &amp;#39;Effect size (&amp;amp;beta;)&amp;#39;, 
                            x_axis_labels = 3, note_frequentist_no_prior = TRUE) +
  ggtitle(&amp;#39;Prior *SD* = 0.3&amp;#39;) + 
  theme(axis.text.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The figure below presents the posterior distribution of each effect in each model. The frequentist estimates are also shown to facilitate the comparison.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_semanticpriming_frequentist_bayesian_plot_informativepriors_exgaussian +
    plot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian +
    plot_semanticpriming_frequentist_bayesian_plot_diffusepriors_exgaussian +
    
    plot_layout(ncol = 3, guides = &amp;#39;collect&amp;#39;) &amp;amp; theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/index.en_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Estimates from the frequentist analysis (in red) and from the Bayesian analysis (in blue) for the semantic priming study, in each model. The frequentist means (represented by points) are flanked by 95% confidence intervals. The Bayesian means (represented by vertical lines) are flanked by 95% credible intervals in light blue (in some cases, the interval is occluded by the bar of the mean)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;A blog post on the &lt;a href=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lme4-and-bayesian-brms-mixed-effects-models&#34;&gt;frequentist-Bayesian plots is also available&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Session info&lt;/h3&gt;
&lt;p&gt;If you encounter any blockers while reproduce the above analyses using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;, my current session info may be useful. For instance, the legend on the last plot may not show if the latest versions of the &lt;code&gt;ggplot2&lt;/code&gt; and the &lt;code&gt;tidyverse&lt;/code&gt; packages are used. Instead, &lt;code&gt;ggplot2 3.3.5&lt;/code&gt; and &lt;code&gt;tidyverse 1.3.1&lt;/code&gt; should be installed using &lt;code&gt;install_version(&#39;tidyverse&#39;, &#39;1.3.1&#39;)&lt;/code&gt; and &lt;code&gt;install_version(&#39;tidyverse&#39;, &#39;1.3.1&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.2 (2022-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19045)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.utf8 
## [2] LC_CTYPE=English_United States.utf8   
## [3] LC_MONETARY=English_United States.utf8
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] Cairo_1.6-0         forcats_0.5.2       stringr_1.5.0      
##  [4] purrr_1.0.0         readr_2.1.3         tidyr_1.2.1        
##  [7] tibble_3.1.8        tidyverse_1.3.1     papaja_0.1.1       
## [10] tinylabels_0.2.3    patchwork_1.1.2     ggtext_0.1.2       
## [13] ggridges_0.5.4      ggplot2_3.3.5       dplyr_1.0.10       
## [16] knitr_1.41          xaringanExtra_0.7.0
## 
## loaded via a namespace (and not attached):
##  [1] httr_1.4.4       sass_0.4.4       jsonlite_1.8.4   modelr_0.1.10   
##  [5] bslib_0.4.2      assertthat_0.2.1 highr_0.10       cellranger_1.1.0
##  [9] yaml_2.3.6       pillar_1.8.1     backports_1.4.1  glue_1.6.2      
## [13] uuid_1.1-0       digest_0.6.31    gridtext_0.1.5   rvest_1.0.3     
## [17] colorspace_2.0-3 htmltools_0.5.4  plyr_1.8.8       pkgconfig_2.0.3 
## [21] broom_1.0.2      haven_2.5.1      bookdown_0.31    scales_1.2.1    
## [25] tzdb_0.3.0       timechange_0.1.1 generics_0.1.3   farver_2.1.1    
## [29] ellipsis_0.3.2   cachem_1.0.6     withr_2.5.0      cli_3.4.1       
## [33] magrittr_2.0.3   crayon_1.5.2     readxl_1.4.1     evaluate_0.19   
## [37] fs_1.5.2         fansi_1.0.3      xml2_1.3.3       blogdown_1.16   
## [41] tools_4.2.2      hms_1.1.2        lifecycle_1.0.3  munsell_0.5.0   
## [45] reprex_2.0.2     compiler_4.2.2   jquerylib_0.1.4  rlang_1.0.6     
## [49] grid_4.2.2       rstudioapi_0.14  labeling_0.4.2   rmarkdown_2.19  
## [53] gtable_0.3.1     DBI_1.1.3        markdown_1.4     R6_2.5.1        
## [57] lubridate_1.9.0  fastmap_1.1.0    utf8_1.2.2       commonmark_1.8.1
## [61] stringi_1.7.8    Rcpp_1.0.9       vctrs_0.5.1      dbplyr_2.2.1    
## [65] tidyselect_1.2.0 xfun_0.36&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-bartonWordlengthEffectReading2014&#34; class=&#34;csl-entry&#34;&gt;
Barton, J. J. S., Hanif, H. M., Eklinder Björnström, L., &amp;amp; Hills, C. (2014). The word-length effect in reading: &lt;span&gt;A&lt;/span&gt; review. &lt;em&gt;Cognitive Neuropsychology&lt;/em&gt;, &lt;em&gt;31&lt;/em&gt;(5-6), 378–412. &lt;a href=&#34;https://doi.org/10.1080/02643294.2014.895314&#34;&gt;https://doi.org/10.1080/02643294.2014.895314&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bernabeu2022a&#34; class=&#34;csl-entry&#34;&gt;
Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: &lt;span&gt;Multilevel&lt;/span&gt; analysis and statistical power&lt;/em&gt;. &lt;span&gt;Lancaster University&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-beyersmannEvidenceEmbeddedWord2020&#34; class=&#34;csl-entry&#34;&gt;
Beyersmann, E., Grainger, J., &amp;amp; Taft, M. (2020). Evidence for embedded word length effects in complex nonwords. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;35&lt;/em&gt;(2), 235–245. &lt;a href=&#34;https://doi.org/10.1080/23273798.2019.1659989&#34;&gt;https://doi.org/10.1080/23273798.2019.1659989&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brysbaertWordFrequencyEffect2018a&#34; class=&#34;csl-entry&#34;&gt;
Brysbaert, M., Mandera, P., &amp;amp; Keuleers, E. (2018). The word frequency effect in word processing: &lt;span&gt;An&lt;/span&gt; updated review. &lt;em&gt;Current Directions in Psychological Science&lt;/em&gt;, &lt;em&gt;27&lt;/em&gt;(1), 45–50. &lt;a href=&#34;https://doi.org/10.1177/0963721417727521&#34;&gt;https://doi.org/10.1177/0963721417727521&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brysbaertImpactWordPrevalence2016&#34; class=&#34;csl-entry&#34;&gt;
Brysbaert, M., Stevens, M., Mandera, P., &amp;amp; Keuleers, E. (2016). The impact of word prevalence on lexical decision times: &lt;span&gt;Evidence&lt;/span&gt; from the &lt;span&gt;Dutch Lexicon Project&lt;/span&gt; 2. &lt;em&gt;Journal of Experimental Psychology: Human Perception and Performance&lt;/em&gt;, &lt;em&gt;42&lt;/em&gt;(3), 441–458. &lt;a href=&#34;https://doi.org/10.1037/xhp0000159&#34;&gt;https://doi.org/10.1037/xhp0000159&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brysbaert2014a&#34; class=&#34;csl-entry&#34;&gt;
Brysbaert, M., Warriner, A. B., &amp;amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known &lt;span&gt;English&lt;/span&gt; word lemmas. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;46&lt;/em&gt;, 904–911. &lt;a href=&#34;https://doi.org/10.3758/s13428-013-0403-5&#34;&gt;https://doi.org/10.3758/s13428-013-0403-5&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://journal.r-project.org/archive/2018/RJ-2018-017/index.html&#34;&gt;https://journal.r-project.org/archive/2018/RJ-2018-017/index.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerPackageBrms2022&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C., Gabry, J., Weber, S., Johnson, A., Modrak, M., Badr, H. S., Weber, F., Ben-Shachar, M. S., &amp;amp; Rabel, H. (2022). &lt;em&gt;Package ’&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;’&lt;/em&gt;. &lt;span&gt;CRAN&lt;/span&gt;. &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34;&gt;https://cran.r-project.org/web/packages/brms/brms.pdf&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cerniMotorExpertiseTyping2016&#34; class=&#34;csl-entry&#34;&gt;
Cerni, T., Velay, J.-L., Alario, F.-X., Vaugoyeau, M., &amp;amp; Longcamp, M. (2016). Motor expertise for typing impacts lexical decision performance. &lt;em&gt;Trends in Neuroscience and Education&lt;/em&gt;, &lt;em&gt;5&lt;/em&gt;(3), 130–138. &lt;a href=&#34;https://doi.org/10.1016/j.tine.2016.07.007&#34;&gt;https://doi.org/10.1016/j.tine.2016.07.007&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cummingNewStatisticsWhy2014&#34; class=&#34;csl-entry&#34;&gt;
Cumming, G. (2014). The new statistics: &lt;span&gt;Why&lt;/span&gt; and how. &lt;em&gt;Psychological Science&lt;/em&gt;, &lt;em&gt;25&lt;/em&gt;(1), 7–29. &lt;a href=&#34;https://doi.org/10.1177/0956797613504966&#34;&gt;https://doi.org/10.1177/0956797613504966&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-dijkstraMultilinkComputationalModel2019&#34; class=&#34;csl-entry&#34;&gt;
Dijkstra, T., Wahl, A., Buytenhuijs, F., Halem, N. V., Al-Jibouri, Z., Korte, M. D., &amp;amp; Rekké, S. (2019). Multilink: &lt;span&gt;A&lt;/span&gt; computational model for bilingual word recognition and word translation. &lt;em&gt;Bilingualism: Language and Cognition&lt;/em&gt;, &lt;em&gt;22&lt;/em&gt;(4), 657–679. &lt;a href=&#34;https://doi.org/10.1017/S1366728918000287&#34;&gt;https://doi.org/10.1017/S1366728918000287&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kimEffectsLexicalFeatures2018&#34; class=&#34;csl-entry&#34;&gt;
Kim, M., Crossley, S. A., &amp;amp; Skalicky, S. (2018). Effects of lexical features, textual properties, and individual differences on word processing times during second language reading comprehension. &lt;em&gt;Reading and Writing&lt;/em&gt;, &lt;em&gt;31&lt;/em&gt;(5), 1155–1180. &lt;a href=&#34;https://doi.org/10.1007/s11145-018-9833-x&#34;&gt;https://doi.org/10.1007/s11145-018-9833-x&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kniefViolatingNormalityAssumption2021&#34; class=&#34;csl-entry&#34;&gt;
Knief, U., &amp;amp; Forstmeier, W. (2021). Violating the normality assumption may be the lesser of two evils. &lt;em&gt;Behavior Research Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3758/s13428-021-01587-5&#34;&gt;https://doi.org/10.3758/s13428-021-01587-5&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeBayesianNewStatistics2018&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K., &amp;amp; Liddell, T. M. (2018). The &lt;span&gt;Bayesian New Statistics&lt;/span&gt;: &lt;span&gt;Hypothesis&lt;/span&gt; testing, estimation, meta-analysis, and power analysis from a &lt;span&gt;Bayesian&lt;/span&gt; perspective. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, &lt;em&gt;25&lt;/em&gt;(1), 178–206. &lt;a href=&#34;https://doi.org/10.3758/s13423-016-1221-4&#34;&gt;https://doi.org/10.3758/s13423-016-1221-4&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-leeBayesianCognitiveModeling2014&#34; class=&#34;csl-entry&#34;&gt;
Lee, M. D., &amp;amp; Wagenmakers, E.-J. (2014). &lt;em&gt;Bayesian cognitive modeling: &lt;span&gt;A&lt;/span&gt; practical course&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/CBO9781139087759&#34;&gt;https://doi.org/10.1017/CBO9781139087759&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lewandowskiGeneratingRandomCorrelation2009&#34; class=&#34;csl-entry&#34;&gt;
Lewandowski, D., Kurowicka, D., &amp;amp; Joe, H. (2009). Generating random correlation matrices based on vines and extended onion method. &lt;em&gt;Journal of Multivariate Analysis&lt;/em&gt;, &lt;em&gt;100&lt;/em&gt;(9), 1989–2001. &lt;a href=&#34;https://doi.org/10.1016/j.jmva.2009.04.008&#34;&gt;https://doi.org/10.1016/j.jmva.2009.04.008&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lim2020a&#34; class=&#34;csl-entry&#34;&gt;
Lim, R. Y., Yap, M. J., &amp;amp; Tse, C.-S. (2020). Individual differences in &lt;span&gt;Cantonese Chinese&lt;/span&gt; word recognition: &lt;span&gt;Insights&lt;/span&gt; from the &lt;span&gt;Chinese Lexicon Project&lt;/span&gt;. &lt;em&gt;Quarterly Journal of Experimental Psychology&lt;/em&gt;, &lt;em&gt;73&lt;/em&gt;(4), 504–518. &lt;a href=&#34;https://doi.org/10.1177/1747021820906566&#34;&gt;https://doi.org/10.1177/1747021820906566&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-loTransformNotTransform2015&#34; class=&#34;csl-entry&#34;&gt;
Lo, S., &amp;amp; Andrews, S. (2015). To transform or not to transform: Using generalized linear mixed models to analyse reaction time data. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;, 1171. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2015.01171&#34;&gt;https://doi.org/10.3389/fpsyg.2015.01171&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-matzkePsychologicalInterpretationExGaussian2009&#34; class=&#34;csl-entry&#34;&gt;
Matzke, D., &amp;amp; Wagenmakers, E.-J. (2009). Psychological interpretation of the ex-&lt;span&gt;Gaussian&lt;/span&gt; and shifted &lt;span&gt;Wald&lt;/span&gt; parameters: &lt;span&gt;A&lt;/span&gt; diffusion model analysis. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, &lt;em&gt;16&lt;/em&gt;(5), 798–817. &lt;a href=&#34;https://doi.org/10.3758/PBR.16.5.798&#34;&gt;https://doi.org/10.3758/PBR.16.5.798&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mendesPervasiveEffectWord2021&#34; class=&#34;csl-entry&#34;&gt;
Mendes, P. S., &amp;amp; Undorf, M. (2021). On the pervasive effect of word frequency in metamemory. &lt;em&gt;Quarterly Journal of Experimental Psychology&lt;/em&gt;, 17470218211053329. &lt;a href=&#34;https://doi.org/10.1177/17470218211053329&#34;&gt;https://doi.org/10.1177/17470218211053329&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-milekEavesdroppingHappinessRevisited2018&#34; class=&#34;csl-entry&#34;&gt;
Milek, A., Butler, E. A., Tackman, A. M., Kaplan, D. M., Raison, C. L., Sbarra, D. A., Vazire, S., &amp;amp; Mehl, M. R. (2018). &lt;span&gt;“&lt;span&gt;Eavesdropping&lt;/span&gt; on happiness”&lt;/span&gt; revisited: &lt;span&gt;A&lt;/span&gt; pooled, multisample replication of the association between life satisfaction and observed daily conversation quantity and quality. &lt;em&gt;Psychological Science&lt;/em&gt;, &lt;em&gt;29&lt;/em&gt;(9), 1451–1462. &lt;a href=&#34;https://doi.org/10.1177/0956797618774252&#34;&gt;https://doi.org/10.1177/0956797618774252&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-nicenboim2023&#34; class=&#34;csl-entry&#34;&gt;
Nicenboim, B., Schad, D., &amp;amp; Vasishth, S. (2023). &lt;em&gt;An introduction to &lt;span&gt;Bayesian&lt;/span&gt; data analysis for cognitive science&lt;/em&gt;. &lt;span&gt;Chapman and Hall/CRC Statistics in the Social and Behavioral Sciences Series&lt;/span&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-pexman2018a&#34; class=&#34;csl-entry&#34;&gt;
Pexman, P. M., &amp;amp; Yap, M. J. (2018). Individual differences in semantic processing: &lt;span&gt;Insights&lt;/span&gt; from the &lt;span&gt;Calgary&lt;/span&gt; semantic decision project. &lt;em&gt;Journal of Experimental Psychology: Learning, Memory, and Cognition&lt;/em&gt;, &lt;em&gt;44&lt;/em&gt;(7), 1091–1112. &lt;a href=&#34;https://doi.org/10.1037/xlm0000499&#34;&gt;https://doi.org/10.1037/xlm0000499&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-preglaVariabilitySentenceComprehension2021&#34; class=&#34;csl-entry&#34;&gt;
Pregla, D., Lissón, P., Vasishth, S., Burchert, F., &amp;amp; Stadie, N. (2021). Variability in sentence comprehension in aphasia in &lt;span&gt;German&lt;/span&gt;. &lt;em&gt;Brain and Language&lt;/em&gt;, &lt;em&gt;222&lt;/em&gt;, 105008. &lt;a href=&#34;https://doi.org/10.1016/j.bandl.2021.105008&#34;&gt;https://doi.org/10.1016/j.bandl.2021.105008&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34; class=&#34;csl-entry&#34;&gt;
Rodríguez-Ferreiro, J., Aguilera, M., &amp;amp; Davies, R. (2020). Semantic priming and schizotypal personality: Reassessing the link between thought disorder and enhanced spreading of semantic activation. &lt;em&gt;PeerJ&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;, e9511. &lt;a href=&#34;https://doi.org/10.7717/peerj.9511&#34;&gt;https://doi.org/10.7717/peerj.9511&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rouderBayesianInferencePsychology2018&#34; class=&#34;csl-entry&#34;&gt;
Rouder, J. N., Haaf, J. M., &amp;amp; Vandekerckhove, J. (2018). Bayesian inference for psychology, part &lt;span&gt;IV&lt;/span&gt;: &lt;span&gt;Parameter&lt;/span&gt; estimation and &lt;span&gt;Bayes&lt;/span&gt; factors. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, &lt;em&gt;25&lt;/em&gt;(1), 102–113. &lt;a href=&#34;https://doi.org/10.3758/s13423-017-1420-7&#34;&gt;https://doi.org/10.3758/s13423-017-1420-7&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schielzethRobustnessLinearMixed2020&#34; class=&#34;csl-entry&#34;&gt;
Schielzeth, H., Dingemanse, N. J., Nakagawa, S., Westneat, D. F., Allegue, H., Teplitsky, C., Réale, D., Dochtermann, N. A., Garamszegi, L. Z., &amp;amp; Araya‐Ajoy, Y. G. (2020). Robustness of linear mixed‐effects models to violations of distributional assumptions. &lt;em&gt;Methods in Ecology and Evolution&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt;(9), 1141–1152. &lt;a href=&#34;https://doi.org/10.1111/2041-210X.13434&#34;&gt;https://doi.org/10.1111/2041-210X.13434&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schmalzWhatBayesFactor2021&#34; class=&#34;csl-entry&#34;&gt;
Schmalz, X., Biurrun Manresa, J., &amp;amp; Zhang, L. (2021). What is a &lt;span&gt;Bayes&lt;/span&gt; factor? &lt;em&gt;Psychological Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1037/met0000421&#34;&gt;https://doi.org/10.1037/met0000421&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-stoneEffectDecayLexical2020&#34; class=&#34;csl-entry&#34;&gt;
Stone, K., Malsburg, T. von der, &amp;amp; Vasishth, S. (2020). The effect of decay and lexical uncertainty on processing long-distance dependencies in reading. &lt;em&gt;PeerJ&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;, e10438. &lt;a href=&#34;https://doi.org/10.7717/peerj.10438&#34;&gt;https://doi.org/10.7717/peerj.10438&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-stoneInteractionGrammaticallyDistinct2021&#34; class=&#34;csl-entry&#34;&gt;
Stone, K., Veríssimo, J., Schad, D. J., Oltrogge, E., Vasishth, S., &amp;amp; Lago, S. (2021). The interaction of grammatically distinct agreement dependencies in predictive processing. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;36&lt;/em&gt;(9), 1159–1179. &lt;a href=&#34;https://doi.org/10.1080/23273798.2021.1921816&#34;&gt;https://doi.org/10.1080/23273798.2021.1921816&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-tendeiroReviewIssuesNull2019&#34; class=&#34;csl-entry&#34;&gt;
Tendeiro, J. N., &amp;amp; Kiers, H. A. L. (2019). A review of issues about null hypothesis &lt;span&gt;Bayesian&lt;/span&gt; testing. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;24&lt;/em&gt;(6), 774–795. &lt;a href=&#34;https://doi.org/10.1037/met0000221&#34;&gt;https://doi.org/10.1037/met0000221&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-tendeiroOnTheWhite2022&#34; class=&#34;csl-entry&#34;&gt;
Tendeiro, J. N., &amp;amp; Kiers, H. A. L. (in press). On the white, the black, and the many shades of gray in between: &lt;span&gt;Our&lt;/span&gt; reply to van &lt;span&gt;Ravenzwaaij&lt;/span&gt; and &lt;span&gt;Wagenmakers&lt;/span&gt; (2021). &lt;em&gt;Psychological Methods&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-schootBayesianStatisticsModelling2021&#34; class=&#34;csl-entry&#34;&gt;
Van de Schoot, R., Depaoli, S., Gelman, A., King, R., Kramer, B., Märtens, K., Tadesse, M. G., Vannucci, M., Willemsen, J., &amp;amp; Yau, C. (2021). Bayesian statistics and modelling. &lt;em&gt;Nature Reviews Methods Primers&lt;/em&gt;, &lt;em&gt;1&lt;/em&gt;, 3. &lt;a href=&#34;https://doi.org/10.1038/s43586-020-00003-0&#34;&gt;https://doi.org/10.1038/s43586-020-00003-0&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vanravenzwaaijAdvantagesMasqueradingIssues2021&#34; class=&#34;csl-entry&#34;&gt;
van Ravenzwaaij, D., &amp;amp; Wagenmakers, E.-J. (2021). Advantages masquerading as &lt;span&gt;“issues”&lt;/span&gt; in &lt;span&gt;Bayesian&lt;/span&gt; hypothesis testing: &lt;span&gt;A&lt;/span&gt; commentary on &lt;span&gt;Tendeiro&lt;/span&gt; and &lt;span&gt;Kiers&lt;/span&gt; (2019). &lt;em&gt;Psychological Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1037/met0000415&#34;&gt;https://doi.org/10.1037/met0000415&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vasishthBayesianDataAnalysis2018&#34; class=&#34;csl-entry&#34;&gt;
Vasishth, S., Nicenboim, B., Beckman, M. E., Li, F., &amp;amp; Kong, E. J. (2018). Bayesian data analysis in the phonetic sciences: &lt;span&gt;A&lt;/span&gt; tutorial introduction. &lt;em&gt;Journal of Phonetics&lt;/em&gt;, &lt;em&gt;71&lt;/em&gt;, 147–161. &lt;a href=&#34;https://doi.org/10.1016/j.wocn.2018.07.008&#34;&gt;https://doi.org/10.1016/j.wocn.2018.07.008&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-yarkoniMovingColtheartNew2008&#34; class=&#34;csl-entry&#34;&gt;
Yarkoni, T., Balota, D., &amp;amp; Yap, M. J. (2008). Moving beyond &lt;span&gt;Coltheart&lt;/span&gt;’s &lt;span&gt;N&lt;/span&gt;: &lt;span&gt;A&lt;/span&gt; new measure of orthographic similarity. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, &lt;em&gt;15&lt;/em&gt;(5), 971–979. &lt;a href=&#34;https://doi.org/10.3758/PBR.15.5.971&#34;&gt;https://doi.org/10.3758/PBR.15.5.971&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      
            <category>Bayesian statistics</category>
      
            <category>linear mixed-effects models</category>
      
            <category>priors</category>
      
            <category>predictive checks</category>
      
            <category>sensitivity analysis</category>
      
            <category>R</category>
      
            <category>visualisation</category>
      
            <category>brms</category>
      
            <category>s</category>
      
      
            <category>R</category>
      
            <category>statistics</category>
      
    </item>
    
    <item>
      <title>Language and vision in conceptual processing: Multilevel analysis and statistical power</title>
      <link>https://pablobernabeu.github.io/publication/language-vision-conceptual-processing/</link>
      <pubDate>Sat, 15 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/publication/language-vision-conceptual-processing/</guid>
      <description>


&lt;div id=&#34;reference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P., Lynott, D., &amp;amp; Connell, L. (2022). &lt;em&gt;Language and vision in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. OSF. &lt;a href=&#34;https://osf.io/dnskh&#34; class=&#34;uri&#34;&gt;https://osf.io/dnskh&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      
            <category>semantic processing</category>
      
            <category>semantic priming</category>
      
            <category>semantic decision</category>
      
            <category>lexical decision</category>
      
            <category>language</category>
      
            <category>vision</category>
      
            <category>visual strength</category>
      
            <category>cognition</category>
      
            <category>psycholinguistics</category>
      
            <category>reading</category>
      
            <category>linear mixed-effects models</category>
      
            <category>frequentist statistics</category>
      
            <category>lme4</category>
      
            <category>lmerTest</category>
      
            <category>Bayesian statistics</category>
      
            <category>brms</category>
      
            <category>power analysis</category>
      
            <category>sample size</category>
      
            <category>simr</category>
      
            <category>rstats</category>
      
            <category>R</category>
      
      
            <category>conceptual processing</category>
      
            <category>embodied cognition</category>
      
            <category>statistical power</category>
      
            <category>research methods</category>
      
            <category>statistics</category>
      
    </item>
    
    <item>
      <title>Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power</title>
      <link>https://pablobernabeu.github.io/publication/pablo-bernabeu-2022-phd-thesis/</link>
      <pubDate>Fri, 14 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/publication/pablo-bernabeu-2022-phd-thesis/</guid>
      <description>&lt;br&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;div style = &#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;br&gt;
</description>
      
            <category>object orientation effects</category>
      
            <category>replication</category>
      
            <category>semantic processing</category>
      
            <category>semantic priming</category>
      
            <category>semantic decision</category>
      
            <category>lexical decision</category>
      
            <category>language</category>
      
            <category>vision</category>
      
            <category>visual strength</category>
      
            <category>cognition</category>
      
            <category>psycholinguistics</category>
      
            <category>reading</category>
      
            <category>linear mixed-effects models</category>
      
            <category>frequentist statistics</category>
      
            <category>lme4</category>
      
            <category>lmerTest</category>
      
            <category>Bayesian statistics</category>
      
            <category>brms</category>
      
            <category>power analysis</category>
      
            <category>sample size</category>
      
            <category>simr</category>
      
            <category>rstats</category>
      
            <category>R</category>
      
      
            <category>conceptual processing</category>
      
            <category>embodied cognition</category>
      
            <category>statistical power</category>
      
            <category>research methods</category>
      
            <category>statistics</category>
      
    </item>
    
    <item>
      <title>Mixed-effects models in R, and a new tool for data simulation</title>
      <link>https://pablobernabeu.github.io/talk/2020-11-26-mixed-effects-models-in-r-and-a-new-tool-for-data-simulation/</link>
      <pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/talk/2020-11-26-mixed-effects-models-in-r-and-a-new-tool-for-data-simulation/</guid>
      <description>


&lt;div id=&#34;slides&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Slides   &lt;a href=&#34;https://hackmd.io/@pablobernabeu/SkRyLbaqw&#34;&gt;&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;500&#34; src=&#34;https://hackmd.io/@pablobernabeu/SkRyLbaqw&#34; frameborder=&#34;0&#34; style=&#34;padding-top:5px&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Linear mixed-effects models (LMEMs) are used to account for variation within factors with multiple observations, such as participants, trials, items, channels, etc (for an earlier approach, see Clark, 1973). This variation is modelled in terms of random intercepts (e.g., overall variation per participant) as well as random slopes for the fixed effects (e.g., treatment effect per participant). These measures help reduce false positives and false negatives (Barr et al., 2013), and the resulting models tend to be robust to violations of assumptions (Schielzeth et al., 2020). The use of LMEMs has grown over the past decade, under various implementation forms (Meteyard &amp;amp; Davies, 2020). In this talk, I will look over the rationale for LMEMs, and demonstrate how to fit them in R (Brauer &amp;amp; Curtin, 2018; Luke, 2017). Challenges will also be covered. For instance, when using the widely-accepted ‘maximal’ approach, based on fitting all possible random effects for each fixed effect, models sometimes fail to find a solution, or ‘convergence’. Advice for the problem of nonconvergence will be demonstrated, based on the progressive lightening of the random effects structure (Singman &amp;amp; Kellen, 2017; for an alternative approach, especially with small samples, see Matuschek et al., 2017). At the end, on a different note, I will present a web application that facilitates data simulation for research and teaching (Bernabeu &amp;amp; Lynott, 2020).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Barr, D. J., Levy, R., Scheepers, C., &amp;amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. &lt;em&gt;Journal of Memory and Language, 68&lt;/em&gt;, 255–278. &lt;a href=&#34;http://dx.doi.org/10.1016/j.jml.2012.11.001&#34; class=&#34;uri&#34;&gt;http://dx.doi.org/10.1016/j.jml.2012.11.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P., &amp;amp; Lynott, D. (2020). &lt;em&gt;Web application for the simulation of experimental data&lt;/em&gt; (Version 1.2). &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/&#34; class=&#34;uri&#34;&gt;https://github.com/pablobernabeu/Experimental-data-simulation/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34; class=&#34;uri&#34;&gt;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Clark, H. H. (1973). The language-as-fixed-effect fallacy: A critique of language statistics in psychological research. &lt;em&gt;Journal of Verbal Learning and Verbal Behavior, 12&lt;/em&gt;(4), 335-359. &lt;a href=&#34;https://doi.org/10.1016/S0022-5371(73)80014-3&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/S0022-5371(73)80014-3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Luke, S. G. (2017). Evaluating significance in linear mixed-effects models in R. &lt;em&gt;Behavior Research Methods, 49&lt;/em&gt;(4), 1494–1502. &lt;a href=&#34;https://doi.org/10.3758/s13428-016-0809-y&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13428-016-0809-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., &amp;amp; Bates, D. (2017). Balancing type 1 error and power in linear mixed models. &lt;em&gt;Journal of Memory and Language, 94&lt;/em&gt;, 305–315. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2017.01.001&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.jml.2017.01.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meteyard, L., &amp;amp; Davies, R. A. (2020). Best practice guidance for linear mixed-effects models in psychological science. &lt;em&gt;Journal of Memory and Language, 112&lt;/em&gt;, 104092. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2020.104092&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.jml.2020.104092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Schielzeth, H., Dingemanse, N. J., Nakagawa, S., Westneat, D. F., Allegue, H, Teplitsky, C., Reale, D., Dochtermann, N. A., Garamszegi, L. Z., &amp;amp; Araya-Ajoy, Y. G. (2020). Robustness of linear mixed-effects models to violations of distributional assumptions. &lt;em&gt;Methods in Ecology and Evolution, 00&lt;/em&gt;, 1– 12. &lt;a href=&#34;https://doi.org/10.1111/2041-210X.13434&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/2041-210X.13434&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Singmann, H., &amp;amp; Kellen, D. (2019). An Introduction to Mixed Models for Experimental Psychology. In D. H. Spieler &amp;amp; E. Schumacher (Eds.), &lt;em&gt;New Methods in Cognitive Psychology&lt;/em&gt; (pp. 4–31). Hove, UK: Psychology Press. &lt;a href=&#34;http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf&#34; class=&#34;uri&#34;&gt;http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>statistics</category>
      
            <category>linear mixed-effects models</category>
      
            <category>regression</category>
      
            <category>R</category>
      
            <category>programming</category>
      
            <category>web application</category>
      
            <category>data simulation</category>
      
            <category>Software Sustainability Institute Fellowship</category>
      
      
            <category>statistics</category>
      
            <category>linear mixed-effects models</category>
      
    </item>
    
    <item>
      <title>Naive principal component analysis in R</title>
      <link>https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Principal_component_analysis&#34;&gt;Principal Component Analysis (PCA)&lt;/a&gt; is a technique used to find the core components that underlie different variables. It comes in very useful whenever doubts arise about the true origin of three or more variables. There are two main methods for performing a PCA: naive or less naive. In the naive method, you first check some conditions in your data which will determine the essentials of the analysis. In the less-naive method, you set those yourself based on whatever prior information or purposes you had. The latter method is appropriate when you already have enough information about the intercorrelations, or when you are required to select a specific number of components. I will tackle the naive method, mainly by following the guidelines in &lt;a href=&#34;https://freethegeogbooks.files.wordpress.com/2016/08/book-for-r-language-stats.pdf&#34;&gt;Field, Miles, and Field (2012)&lt;/a&gt;, with updated code where necessary. A &lt;a href=&#34;https://freethegeogbooks.files.wordpress.com/2016/08/book-for-r-language-stats.pdf&#34;&gt;manual by Charles M. Friel&lt;/a&gt; (Sam Houston State University) was also useful.&lt;/p&gt;
&lt;p&gt;The ‘naive’ approach is characterized by a first stage that checks whether the PCA should actually be performed with your current variables, or if some should be removed. The variables that are accepted are taken to a second stage which identifies the number of principal components that seem to underlie your set of variables.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;stage-1.-determine-whether-pca-is-appropriate-at-all-considering-the-variables&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;STAGE 1. Determine whether PCA is appropriate at all, considering the variables&lt;/h4&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;45%&#34; src=&#34;https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/1.jpg&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Variables should be &lt;strong&gt;inter-correlated enough but not too much.&lt;/strong&gt; Field et al. (2012) provide some thresholds, suggesting that no variable should have many correlations below .30, or &lt;em&gt;any&lt;/em&gt; correlation at all above .90. Thus, in the example here, variable Q06 should probably be excluded from the PCA.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Bartlett’s test&lt;/strong&gt;, on the nature of the intercorrelations, should be significant. Significance suggests that the variables are not an ‘identity matrix’ in which correlations are a sampling error.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;KMO&lt;/strong&gt; (Kaiser-Meyer-Olkin), a measure of sampling adequacy based on common variance (so similar purpose as Bartlett’s). As Field et al. review, ‘values between .5 and .7 are mediocre, values between .7 and .8 are good, values between .8 and .9 are great and values above .9 are superb’ (p. 761). There’s a general score as well as one per variable. The general one will often be good, whereas the individual scores may more likely fail. Any variable with a score below .5 should probably be removed, and the test should be run again.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Determinant:&lt;/strong&gt; A formula about multicollinearity. The result should preferably fall below .00001.
Note that some of these tests are run on the dataframe and others on a correlation matrix of the data, as distinguished below.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;
# Necessary libraries
library(ltm)
library(lattice)
library(psych)
library(car)
library(pastecs)
library(scales)
library(ggplot2)
library(arules)
library(plyr)
library(Rmisc)
library(GPArotation)
library(gdata)
library(MASS)
library(qpcR)
library(dplyr)
library(gtools)
library(Hmisc)

# Select variables of interest for the PCA
dataset = mydata[, c(&amp;#39;select_var1&amp;#39;,&amp;#39;select_var1&amp;#39;,&amp;#39;select_var2&amp;#39;,&amp;#39;select_var3&amp;#39;,&amp;#39;select_var4&amp;#39;,&amp;#39;select_var5&amp;#39;,&amp;#39;select_var6&amp;#39;,&amp;#39;select_var7&amp;#39;)]

# Create matrix: some tests will require it
data_matrix = cor(dataset, use = &amp;#39;complete.obs&amp;#39;)

# See intercorrelations
round(data_matrix, 2)

# Bartlett&amp;#39;s
cortest.bartlett(dataset)

# KMO (Kaiser-Meyer-Olkin)
KMO(data_matrix)

# Determinant
det(data_matrix)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-2.-identify-number-of-components-aka-factors&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;STAGE 2. Identify number of components (aka factors)&lt;/h4&gt;
&lt;p&gt;In this stage, principal components (formally called ‘factors’ at this stage) are identified among the set of variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The identification is done through a basic, ‘unrotated’ PCA. The number of components set a priori must equal the number of variables that are being tested.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# Start off with unrotated PCA

pc1 = psych::principal(dataset, nfactors = length(dataset), rotate=&amp;quot;none&amp;quot;)
pc1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is an example result:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Principal Components Analysis
## Call: psych::principal(r = eng_prop, nfactors = 3, rotate = &amp;quot;none&amp;quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##           PC1   PC2  PC3 h2       u2 com
## Aud_eng -0.89  0.13 0.44  1 -2.2e-16 1.5
## Hap_eng  0.64  0.75 0.15  1  1.1e-16 2.0
## Vis_eng  0.81 -0.46 0.36  1 -4.4e-16 2.0
## 
##                        PC1  PC2  PC3
## SS loadings           1.87 0.79 0.34
## Proportion Var        0.62 0.26 0.11
## Cumulative Var        0.62 0.89 1.00
## Proportion Explained  0.62 0.26 0.11
## Cumulative Proportion 0.62 0.89 1.00
## 
## Mean item complexity =  1.9
## Test of the hypothesis that 3 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0 
##  with the empirical chi square  0  with prob &amp;lt;  NA 
## 
## Fit based upon off diagonal values = 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Among the columns, there are first the correlations between variables and components, followed by a column (h2) with the &lt;strong&gt;‘communalities’&lt;/strong&gt;. If less factors than variables had been selected, communality values would be below 1. Then there is the uniqueness column (u2): &lt;strong&gt;uniqueness&lt;/strong&gt; is equal to 1 minus the communality. Next is ‘com’, which reflects the &lt;strong&gt;complexity&lt;/strong&gt; with which a variable relates to the principal components. Those components are precisely found below. The first row contains the sums of squared loadings, or eigenvalues, namely, the total variance explained by each linear component. This value corresponds to the number of units explained out of all possible factors (which were three in the above example). The rows below all cut from the same cloth. &lt;em&gt;Proportion var&lt;/em&gt; = variance explained over a total of 1. This is the result of dividing the eigenvalue by the number of components. Multiply by 100 and you get the percentage of total variance explained, which becomes useful. In the example, 99% of the variance has been explained. Aside from the meddling maths, we should actually expect 100% there because the number of factors equaled the number of variables. &lt;em&gt;Cumulative var:&lt;/em&gt; variance added consecutively up to the last component. &lt;em&gt;Proportion explained:&lt;/em&gt; variance explained over what has actually been explained (only when variables = factors is this the same as Proportion var). &lt;em&gt;Cumulative proportion:&lt;/em&gt; the actually explained variance added consecutively up to the last component (Field et al., 2012).&lt;/p&gt;
&lt;p&gt;According to Field et al. (2012), two criteria will determine the number of components to select for the next stage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kaiser’s criterion: components with SS loadings &amp;gt; 1. In our example, only PC1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A more lenient alternative is Joliffe’s criterion, SS loadings &amp;gt; .7.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scree plot: the number of points after point of inflexion. For this plot, call:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;plot(pc1$values, type = &amp;#39;b&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;35%&#34; src=&#34;https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/2.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Imagine a straight line &lt;strong&gt;from the first point on the right.&lt;/strong&gt; Once this line bends considerably, count the points after the bend and up to the last point on the left. The number of points is the number of components to select. The example here is probably the most complicated (two components were finally chosen), but normally it’s &lt;a href=&#34;https://www.google.nl/search?q=select+principal+components+scree+plot+point+inflexion&amp;amp;source=lnms&amp;amp;tbm=isch&amp;amp;sa=X&amp;amp;ved=0ahUKEwi00ujoto_WAhXJbVAKHbTCBAgQ_AUICigB&amp;amp;biw=1280&amp;amp;bih=619&#34;&gt;not difficult&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Based on both criteria, go ahead and select the definitive number of components.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-3.-run-definitive-pca&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;STAGE 3. Run definitive PCA&lt;/h4&gt;
&lt;p&gt;Run a very similar command as you did before, but now with a more advanced method. The first PCA, a heuristic one, worked essentially on the inter-correlations. The definitive PCA, in contrast, will implement a prior shuffling known as ‘rotation’, to ensure that the result is robust enough (just like cards are shuffled). Explained variance is captured better this way. The go-to rotation method is the orthogonal, or ‘varimax’ (though others may be considered too).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Now with varimax rotation, Kaiser-normalized by default:
pc2 = psych::principal(dataset, nfactors=2, rotate = &amp;quot;varimax&amp;quot;, 
scores = TRUE)
pc2
pc2$loadings

# Healthcheck
pc2$residual
pc2$fit
pc2$communality
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to Field et al. (2012), we would want:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Less than half of &lt;strong&gt;residuals&lt;/strong&gt; with absolute values &amp;gt; 0.05&lt;/li&gt;
&lt;li&gt;Model &lt;strong&gt;fit&lt;/strong&gt; &amp;gt; .9&lt;/li&gt;
&lt;li&gt;All &lt;strong&gt;communalities&lt;/strong&gt; &amp;gt; .7&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If any of this fails, consider changing the number of factors. Next, the rotated components that have been ‘extracted’ from the core of the set of variables can be added to the dataset. This would enable the use of these components as new variables that might prove powerful and useful (as in &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.1551-6709.2010.01157.x/full&#34;&gt;this research&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dataset = cbind(dataset, pc2$scores)
summary(dataset$RC1, dataset$RC2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-4.-determine-ascription-of-each-variable-to-components&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;STAGE 4. Determine ascription of each variable to components&lt;/h4&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;55%&#34; src=&#34;https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/3.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Check the main summary by just calling pc2, and see how each variable correlates with the rotated components. This is essential because it reveals how variables load on each component, or in other words, to which component a variable belongs. For instance, the table shown here belongs to a study about the meaning of words (Bernabeu, 2018). These results suggest that the visual and haptic modalities of words are quite related, whereas the auditory modality is relatively unique. When the analysis works out well, a cut-off point of &lt;em&gt;r&lt;/em&gt; = .8 may be applied for considering a variable as part of a component.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-5.-enjoy-the-plot&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;STAGE 5. Enjoy the plot&lt;/h4&gt;
&lt;p&gt;The plot is perhaps the coolest part about PCA. It really makes an awesome illustration of the power of data analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ggplot(eng_props,
  aes(RC1, RC2, label = as.character(main_eng))) + stat_density2d (color = &amp;quot;gray87&amp;quot;) +
  geom_text(size = ifelse(eng_props$word_eng %in% w_set, 12, 7),
    fontface = ifelse(eng_props$word_eng %in% w_set, &amp;#39;bold&amp;#39;, &amp;#39;plain&amp;#39;)) +
  geom_point(data=eng_props[eng_props$word_eng %in% w_set,], pch=21, fill=NA, size=14, stroke=2, alpha=.6) +
  labs(subtitle=&amp;#39;(Data from Lynott &amp;amp; Connell, 2009)&amp;#39;, x = &amp;quot;Varimax-rotated Principal Component 1&amp;quot;, 
    y = &amp;quot;Varimax-rotated Principal Component 2&amp;quot;) +  theme_bw() +   
  theme( plot.background = element_blank(), panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(), panel.border = element_blank(),
    axis.line = element_line(color = &amp;#39;black&amp;#39;),
    axis.title.x = element_text(colour = &amp;#39;black&amp;#39;, size = 23, margin=margin(15,15,15,15)),
    axis.title.y = element_text(colour = &amp;#39;black&amp;#39;, size = 23, margin=margin(15,15,15,15)),
    axis.text.x = element_text(size=16), axis.text.y  = element_text(size=16),
    plot.title = element_text(hjust = 0.5, size = 32, face = &amp;quot;bold&amp;quot;, margin=margin(15,15,15,15)),
    plot.subtitle = element_text(hjust = 0.5, size = 20, margin=margin(2,15,15,15)) ) +
  geom_label_repel(data = eng_props[eng_props$word_eng %in% w_set,], aes(label = word_eng), size = 8, 
    alpha = 0.77, color = &amp;#39;black&amp;#39;, box.padding = 1.5 )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is an example combining PCA plots with code similar to the above. These plots illustrate something further with regard to the relationships among modalities. In property words, the different modalities spread out more clearly than they do in concept words. This makes sense because in language, properties define concepts (Bernabeu, 2018).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An example of these analyses is &lt;a href=&#34;https://mybinder.org/v2/gh/pablobernabeu/Modality-exclusivity-norms-747-Dutch-English-replication/master?urlpath=rstudio&#34;&gt;available in available in this RStudio environment&lt;/a&gt;, in the &lt;code&gt;norms.R&lt;/code&gt; script.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P. (2018). &lt;em&gt;Dutch modality exclusivity norms for 336 properties and 411 concepts&lt;/em&gt; [Unpublished manuscript]. School of Humanities, Tilburg University, the Netherlands. &lt;a href=&#34;https://psyarxiv.com/s2c5h&#34; class=&#34;uri&#34;&gt;https://psyarxiv.com/s2c5h&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Field, A. P., Miles, J., &amp;amp; Field, Z. (2012). &lt;em&gt;Discovering Statistics Using R&lt;/em&gt;. London, UK: Sage.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>s</category>
      
            <category>principal component analysis</category>
      
            <category>statistics</category>
      
            <category>dimensionality reduction</category>
      
            <category>R</category>
      
      
            <category>statistics</category>
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>At Greg, 8 am</title>
      <link>https://pablobernabeu.github.io/2017/at-greg-8-am/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2017/at-greg-8-am/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2017/at-greg-8-am/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The clock strikes a certain hour, below all the Greg’s teaspoons at play. Results o’clock. The usual, please.&lt;/p&gt;
&lt;p&gt;Usual table. &lt;code&gt;summaryby&lt;/code&gt; (having to get the first peek in the cafeteria can only add zest). &lt;code&gt;summaryBy(RT ~ list(Ptp, Group, Cond), behdata, FUN=summary)&lt;/code&gt;. So, hardly any of the 95% Confidence Intervals contain 0. Does this really mean…?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‘For example, the hypothesis of equality of population means will be rejected at the 0.05 level if and only if a 95% CI for the mean difference does not contain 0.’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;— Dallal (2002; &lt;a href=&#34;http://www.jerrydallal.com/lhsp/pval.htm&#34; class=&#34;uri&#34;&gt;http://www.jerrydallal.com/lhsp/pval.htm&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Of course. The CI just has that and more. The window is showing a chilly 1999 morning. Let’s see the summary again. Wee standard deviations. By card, please.&lt;/p&gt;
&lt;p&gt;Mmm, the air outside is worth gingering up…&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The trials!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The assumption of independence spoils another morning.&lt;/p&gt;
&lt;p&gt;This new data consisted of response times (RT) that had been collected over several trials. The single dependent variable, RT, was accompanied by other variables which could be analyzed as independent variables. These included &lt;em&gt;Group&lt;/em&gt;, &lt;em&gt;Trial Number&lt;/em&gt;, and a within-subjects &lt;em&gt;Condition&lt;/em&gt;. &lt;strong&gt;What had to be done first off, in order to take the usual table?&lt;/strong&gt; &lt;em&gt;The trials!&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;assumption-of-independence-of-observations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assumption of independence of observations&lt;/h2&gt;
&lt;p&gt;One must account for any redundant measures below the level of participants (the experimental trials, in this case), so that the sample size (&lt;em&gt;N&lt;/em&gt;) used for any summary statistics match the number of participants (or the largest group, &lt;em&gt;n&lt;/em&gt;). Why? This is a &lt;a href=&#34;https://stats.stackexchange.com/questions/130019/standard-error-for-aggregated-proportions&#34;&gt;central assumption in statistics&lt;/a&gt;: observations must be independent. We can observe the independence assumption differently, depending on whether we’re summarizing data or performing statistical tests.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For &lt;em&gt;descriptive tables and plots&lt;/em&gt; (involving Standard Error/Deviation, Confidence Intervals, etc), &lt;em&gt;the data ought to be aggregated to the level from which you want to generalize&lt;/em&gt;. That level is—in this case and very often—&lt;em&gt;participants&lt;/em&gt;. Trials do not normally serve for statistical generalization (they’re good for experimental validity). This realization may come as a bummer if you have first seen the effect sizes in the un-aggregated data. The mirage (see red lines on the left table below) is caused by an inflated &lt;em&gt;N&lt;/em&gt; (cf. red lines on the right-hand table). As an illustration, the tables below summarize data with an actual sample &lt;em&gt;n&lt;/em&gt; = 23. However, the table on the right includes repeated measures that should have been aggregated, massively inflating &lt;em&gt;n&lt;/em&gt;. The inflation of the sample size equals the product of all repeated measures that failed to be aggregated under participants.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;inflated.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#39;30%&#39; src=&#39;SD.jpg&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Measures of variance such as the Standard Deviation divide by the sample size. Thus, the larger the sample (N), the smaller the Standard Deviation, Standard Error, Confidence Interval…—that is, the variation or noise.&lt;/p&gt;
&lt;p&gt;Aggregating is a snap. For example, with the aggregate() function in R, you just have to include all of your variables except that or those of the repeated measures:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;behdata_aggreg = aggregate(behdata$RT, list(behdata$Ptp, behdata$Group, behdata$Cond), 
  data=behdata, FUN=mean)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;In statistical tests, repeated measures below the participant level–e.g., trials–normally must be either factored in or aggregated. Barr and colleagues provide an easy, focused &lt;a href=&#34;http://talklab.psy.gla.ac.uk/simgen/faq.html#sec-3&#34;&gt;guide on this procedure&lt;/a&gt;. This is necessary because when the N in the analyses is augmented by unaccounted, redundant observations, &lt;em&gt;the famous assumption of independence of observations is violated&lt;/em&gt;, and the results may be invalid, as &lt;a href=&#34;https://arxiv.org/pdf/1601.01126.pdf&#34;&gt;Vasishth and Nicenboim (2016, p. 3)&lt;/a&gt; put it:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;‘if we were to do a t-test on the unaggregated data, we would violate the independence assumption and the result of the t-test would be invalid.’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now, usually the repetitions that concern us are the multiple trials or items in experiments, or other sub-participant measures. So what about participants–what are they never aggregated? &lt;a href=&#34;http://tandfonline.com.sci-hub.cc/doi/abs/10.1080/01933922.2016.1264520?journalCode=usgw20&#34;&gt;McCarthy, Whittaker, Boyle, and Eyal (2017, p.10)&lt;/a&gt; note:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‘It has also been proposed that researchers aggregate the responses of participants within the same group and use the groups/clusters as the unit of analysis (Stevens, 2007). However, because this would result in losing sample size at the participant level, this approach is not optimal given the already small numbers of groups typically studied in group work research.’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;different-procedure-in-linear-mixed-effects-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Different procedure in linear mixed-effects models&lt;/h2&gt;
&lt;p&gt;Aggregation is no longer necessary, where linear mixed-effects models can be used. These models allow us to &lt;a href=&#34;http://talklab.psy.gla.ac.uk/simgen/faq.html#sec-3&#34;&gt;account for any clusters (Participants, Trials, Items…) by signing them into the error term&lt;/a&gt; (&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer_and_Curtin_LMEMs-2017-Psych_Methods.pdf&#34;&gt;Brauer &amp;amp; Curtin, 2017&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>s</category>
      
            <category>R</category>
      
            <category>statistics</category>
      
            <category>aggregate</category>
      
            <category>trials</category>
      
            <category>repeated measures</category>
      
            <category>assumption</category>
      
            <category>independence of observations</category>
      
            <category>variability</category>
      
            <category>standard error</category>
      
            <category>central assumption</category>
      
      
            <category>statistics</category>
      
    </item>
    
  </channel>
</rss>
