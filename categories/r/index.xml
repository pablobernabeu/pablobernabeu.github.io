
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Pablo Bernabeu</title>
    <link>https://pablobernabeu.github.io/categories/r/</link>
    <description>Recent content in R on Pablo Bernabeu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <copyright>Pablo Bernabeu, 2015—{year}. Licence: [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/). Email: pcbernabeu@gmail.com. No cookies operated by this website. Cookies only used by third-party systems such as [Disqus](https://help.disqus.com/en/articles/1717155-use-of-cookies).</copyright>
    <lastBuildDate>Sat, 04 Nov 2023 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://pablobernabeu.github.io/categories/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>FAQs on mixed-effects models</title>
      <link>https://pablobernabeu.github.io/2023/faqs-on-mixed-effects-models/</link>
      <pubDate>Sat, 04 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2023/faqs-on-mixed-effects-models/</guid>
      <description>


&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;I am dealing with nested data, and I remember from an article by &lt;a href=&#34;https://doi.org/10.1016/S0022-5371(73)80014-3&#34;&gt;Clark (1973)&lt;/a&gt; that nested should be analysed using special models. I’ve looked into mixed-effects models, and I’ve reached a structure with random intercepts by subjects and by items. Is this fine?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;In early days, researchers would aggregate the data across these repeated measures to prevent the violation of the assumption of independence of observations, which is one of the most important assumptions in statistics. With the advent of mixed-effects models, researchers began accounting for these repeated measures using random intercepts and slopes. However, problems of convergence led many researchers to remove random slopes. This became widespread until, over the past few years, we have realised that random slopes are necessary to prevent an inflation of the Type I error due to the violation of the assumption of independence (&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;Brauer &amp;amp; Curtin, 2018&lt;/a&gt;; &lt;a href=&#34;http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf&#34;&gt;Singmann &amp;amp; Kellen, 2019&lt;/a&gt;). Please see Table 17 in Brauer and Curtin (2018). Due to the present reasons, the models in the current article are anti-conservative. To redress this problem, please consider the inclusion of random slopes by participant for all between-items variables [e.g., &lt;code&gt;(stimulus_condition | participant)&lt;/code&gt;], and random slopes by item for all between-participants variables [e.g., &lt;code&gt;(extraversion | item)&lt;/code&gt;]. Interaction terms should also have the corresponding slopes, except when the variables in the interaction vary within different units, that is, one between participants and one between items (&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;Brauer &amp;amp; Curtin, 2018&lt;/a&gt;). Each of the random intercepts and random slopes included in the model should be noted in the main text, for instance using footnotes in the results table (see &lt;a href=&#34;https://bookdown.org/pablobernabeu/language-sensorimotor-conceptual-processing-statistical-power/study-2.1-semantic-priming.html#semanticpriming-results&#34;&gt;example&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;I calculated the &lt;em&gt;p&lt;/em&gt; values by comparing minimally-different models using the &lt;code&gt;anova&lt;/code&gt; function. Is this fine?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.3758/s13428-016-0809-y&#34;&gt;Luke (2017)&lt;/a&gt; warns that the &lt;em&gt;p&lt;/em&gt; values calculated by model comparison—which are based on likelihood ratio tests—can be anti-conservative. Therefore, the Kenward-Roger and the Satterthwaite methods are recommended instead (both available in other packages, such as &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;lmerTest&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/afex/afex.pdf&#34;&gt;afex&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;The lme4 package only runs on one thread (CPU) but the computer has 8. Do you have any advice on making the model run using more of the threads? It’s taking a very long time. I’ve seen these two possible solutions online from 2018 (&lt;a href=&#34;https://stackoverflow.com/questions/48315268/how-can-i-make-r-using-more-than-1-core-8-available-on-a-ubuntu-rstudio-server&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://stat.ethz.ch/pipermail/r-sig-mixed-models/2018q3/027170.html&#34;&gt;here&lt;/a&gt;) but would like some advice if they have any or have attempted either of these solutions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;From the information I have seen in the past as well as right now, parallelising (g)lmer intentionally would be very involved. There is certainly interest in it, as your resources show (also see &lt;a href=&#34;https://github.com/lme4/lme4/issues?q=is%3Aissue+parallel&#34;&gt;here&lt;/a&gt;). However, the current information suggests to me that it is not possible.&lt;/p&gt;
&lt;p&gt;Interestingly, some isolated cases of unintentional parallelisation have been documented, and the developers of the &lt;a href=&#34;https://cran.r-project.org/web/packages/lme4/lme4.pdf&#34;&gt;lme4&lt;/a&gt; package were &lt;a href=&#34;&#34;&gt;surprised about them&lt;/a&gt; because they have not created this feature (see &lt;a href=&#34;https://github.com/lme4/lme4/issues/492&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://github.com/lme4/lme4/issues/627&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I think the best approach may be running your model(s) in a high-performance computing (HPC) cluster. Although this would not reduce the amount of time required for each model, it would have two advantages. First, your own computers wouldn’t be busy for days, and second, you could even run several models at the same time without exhausting your own computers. I still have access to the HPC at my previous university, and it would be fine for me to send your model(s) there if that would help you. Feel free to let me know. Otherwise I can see that your university has this facility too.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;We took your advice and ran the model on a supercomputer - it took roughly 2.5 days, which is what it took for the model to run on my iMac and a gaming laptop Vivienne has.&lt;/p&gt;
&lt;p&gt;The model, however, didn’t converge. We have read that you can use &lt;code&gt;allFit()&lt;/code&gt; to try the fit with all available optimizers. Do you have any experience using this? If you did, I wondered where this would sit in the code for the model? How and where do I add this in to check all available optimizers, please?&lt;/p&gt;
&lt;p&gt;I have attached my code in a txt file and the data in excel for you to see, in case it is of any use.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;The multi-optimizer check is indeed a way (albeit tentative) to probe into the convergence. Convergence has long been a fuzzy subject, as there are different standpoints depending on the degree of conservativeness that is sought after by the analysts.&lt;/p&gt;
&lt;p&gt;On Page 124 in my thesis (&lt;a href=&#34;https://osf.io/97u5c&#34; class=&#34;uri&#34;&gt;https://osf.io/97u5c&lt;/a&gt;), you can find this multi-optimizer check (also see this &lt;a href=&#34;https://pablobernabeu.github.io/2021/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit&#34;&gt;blog post&lt;/a&gt;). All the code is available on OSF. More generally, I discuss the issue of convergence throughout the thesis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;I have run the model with &lt;code&gt;optimizer=&#34;nloptwrap&#34;&lt;/code&gt; and &lt;code&gt;algorithm=&#34;NLOPT_LN_BOBYQA&#34;&lt;/code&gt; and received the following warning message (once the model ran) -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;In optwrap(optimizer, devfun, start, rho$lower, control = control, :
convergence code 5 from nloptwrap: NLOPT_MAXEVAL_REACHED: optimization stopped becasue maxeval (above) was reached.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does this mean that the model didn’t converge? I’m only asking because I wasn’t given a statement saying it didn’t converge, as it did with Nelder_Mead. It was stated (at the end of summary table)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Optimizer (Nelder_Mead) convergence code: 4 (failure to converge in 10000 evaluations)
failure to converge in 10000 evaluations&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;Please try &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/semanticpriming_lmerTest.R#L109&#34;&gt;increasing the max number of iterations&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;We increased the max number of iterations to 1e6 and then 1e7, and the model didn’t converge. But it has converged with &lt;code&gt;maxeval=1e8&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I wanted to ask please, do you know of any issues with the max iterations being this high and effecting the interpretability of the model? Or is it completely fine?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;There are no side-effects to increasing the number of iterations (see Remedy 6 in &lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;Brauer &amp;amp; Curtin, 2018&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
</description>
      
            <category>linear-mixed effects models</category>
      
            <category>R</category>
      
            <category>research methods</category>
      
            <category>s</category>
      
      
            <category>linear mixed-effects models</category>
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>FAIR standards for the creation of research materials, with examples</title>
      <link>https://pablobernabeu.github.io/2023/fair-standards-for-the-creation-of-research-materials-with-examples/</link>
      <pubDate>Thu, 02 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2023/fair-standards-for-the-creation-of-research-materials-with-examples/</guid>
      <description>


&lt;p&gt;Notwithstanding the need for speed in most scientific projects, what should be the &lt;strong&gt;minimum acceptable standards&lt;/strong&gt; in the &lt;strong&gt;creation of research materials&lt;/strong&gt; such as stimuli, custom software for data collection (e.g., experiment in jsPsych, OpenSesame or psychoPy), or scripts for statistical analysis?&lt;/p&gt;
&lt;p&gt;The answer to this question is contingent upon the field of research, the purpose and the duration of the project, and many other contextual factors. So, to narrow down the scope and come at a general answer, let’s suppose we asked a researcher in the cognitive sciences (e.g., a linguist, a psychologist or a neuroscientist) who values open science. Perhaps, such a researchers would be satisfied with a method for the creation of materials that &lt;strong&gt;allows the creators of the materials, as well as their collaborators and any other stakeholders (e.g., any fellow scientists working in the same field), to explore, understand, reproduce, modify, and reuse the materials following their completion and thereafter&lt;/strong&gt;. Let’s review some of the implements that can help fulfil these standards.&lt;/p&gt;
&lt;div id=&#34;fairness&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;FAIRness&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.go-fair.org/fair-principles&#34;&gt;FAIR Guiding Principles for scientific data management and stewardship&lt;/a&gt; exhaustively describe a protocol for making materials &lt;strong&gt;F&lt;/strong&gt;indable, &lt;strong&gt;A&lt;/strong&gt;ccessible, &lt;strong&gt;I&lt;/strong&gt;nteroperable and &lt;strong&gt;R&lt;/strong&gt;eusable. These terms cover the five allowances listed above, along with other important aspects.&lt;/p&gt;
&lt;p&gt;Let’s look at some instantiations of the FAIR principles.&lt;/p&gt;
&lt;div id=&#34;sharing-the-materials&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sharing the materials&lt;/h2&gt;
&lt;p&gt;A condition sine qua non is to share the materials publicly online, as far as possible. Repositories on servers such as OSF or GitHub are often sufficient to this end. Unfortunately, most studies in the cognitive sciences still do not share the complete materials.&lt;/p&gt;
&lt;p&gt;One of the reasons why sharing is so important is to prevent wrong assumptions by the audience that will consider the research. That is, when the materials of a study are not publicly shared online, the readers of the papers are left with two options: to assume that there were no errors or to assume that were some errors of an uncertain degree. The method followed in the creation of the materials should free the readers of this tribulation, by allowing them to consult the materials and their preparation in full, or as completely as possible.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reproducibility&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reproducibility&lt;/h2&gt;
&lt;p&gt;It is convenient to allow others, and our future selves, to reproduce the materials throughout their preparation and at any time thereafter. For this purpose, &lt;strong&gt;R&lt;/strong&gt; can be used to register in scripts as many as possible of the steps followed throughout the preparation of the materials. Far from being only a software for data analysis, R allows the preparation of texts, images, audios, etc. Humans err, by definition. That can be counted on. Conveniently, registering the steps followed during weeks or months of preparation allows us to offload part of the documentation efforts. It’s a way of video-recording, as it were, all the additions, subtractions, replacements, transformations and calculations performed with the raw materials.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generous-documentation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generous documentation&lt;/h2&gt;
&lt;p&gt;Under the curse of knowledge, the creators of research materials may believe that their materials are self-explanatory. Often they are more obscure than they think. To allow any other stakeholders, including their future selves, to exercise the five allowances listed above—i.e., explore, understand, reproduce, modify, and reuse the materials—, the preparation process and the end materials should be documented with enough detail. This can be done using README.txt files throughout the project. Using the &lt;code&gt;.txt&lt;/code&gt; format/extension is recommended because other formats, such as Microsoft Word, may not be (fully) available in some computers. To exemplify the format and the content of readme files, below is an excerpt from a longitudinal study on which I’ve been working.&lt;/p&gt;
&lt;textarea readonly style=&#39;border-color: lightgrey; overflow: auto; color: darkblue; font-size: 90%; min-width: 125%; height: 4600px; white-space: pre; overflow-wrap: normal; padding-right: 0.5em; padding-left: 1em;&#39;&gt;

-- Post-training test --

In Sessions 2, 3, 4 and 6, if the test is failed in the first attempt, the training and the test are 
repeated (following González Alonso et al., 2020). In such cases, the result is shown at the end 
of the second attempt. The session advances if the accuracy achieved in the second attempt exceeds 
80%, whereas the session stops if the accuracy is lower. In the latter situation, an &#39;End of session&#39; 
message is presented, flanked by two orange circles, and followed by an acknowledgement for the 
participant. Once the participant has read this screen, the experimenter quits the session by 
pressing &#39;ESC&#39; and then &#39;Q&#39;.


== Stimuli ==

The stimulus lists are described in the R functions that were used to create the stimuli, as well as
in the &#39;list&#39; column in the stimulus files.


== Participant-specific parameters for lab-based sessions ==

Each participant was assigned certain parameters in advance, including the mini-language, the order 
of the resting-state parts, and the stimulus lists. The code that was used to create this assignment 
is available in the &#39;stimulus_preparation&#39; folder. 

Due to the pre-assignment of the parameters, there is a fixed set of participant IDs that can be 
used in OpenSesame. These identification numbers range between 1 and 144. If an ID outside of this 
range is used, the OpenSesame session does not run.


== General procedure for lab-based sessions ==

At the beginning of Sessions 2, 3, 4 and 6, the experimenter starts OpenSesame by opening the 
program directly (not by opening the session-specific file), and then opens the appropriate session 
within OpenSesame. This procedure helps prevent the opening of a standalone Python window, the 
closing of which would result in the closing of OpenSesame. Next, the experimenter opens BrainVision 
Recorder.

Next, the experimenter fits the participant with the EEG cap, which they will wear throughout the 
session. To prevent them from being pulled down, please attach the splitter box neatly to the 
towel on the participant&#39;s back. 

Next, the experimenter returns to OpenSesame and runs the session in full screen by clicking on 
the full green triangle at the top left. Next, the experimenter selects a folder to store the 
logfile. It is important to select the folder corresponding to each session to avoid overwriting 
existing logfiles. Any prompts to overwrite a logfile must always be refused.

In the first screen, the experimenter can disable some of the tasks. This option can be used if a 
session has ended abruptly, in which case the session can be resumed from a near checkpoint. In 
such a case, the experimenter must first note this incident in their logbook, and rename the log 
file that was produced on the first run, by appending &#39;_first_run&#39; to the name. This prevents 
overwriting the file on the second run. Next, they must open a new session, enter the same 
participant ID, and select the appropriate part from which to begin. This part must be the part 
immediately following the last part that was completed in full. For instance, if a session ended
abruptly during the experiment, the beginning selected on the second run would be the experiment. 
Once the session has finished completely, the first log file and the second log file must be 
safely merged into a single file, keeping only the fully completed tasks.

In the first instructional screen, participants are asked to refrain from asking any questions 
unless it is necessary, so that all participants can receive the same instructions.

At the beginning of the Resting-state part (present in Sessions 2 and 4) and at the beginning of 
the Experiment part, instructions are presented on the screen that ask participants to stay as 
still as possible during the following task. The screen contains an orange-coloured square with 
the letters &#39;i.s.r&#39;, that remind the experimenter to check the impedance and the signal, and 
finally to begin recording the EEG signal. If the impedance of any electrodes is poor, the 
experimenter may enter the booth to lower the impedance of the electrodes affected. Otherwise, 
after validating the signal and the impedance, the experimenter can begin the recording in 
BrainVision, and press the letter &#39;C&#39; twice in the stimulus computer. At that point, a green 
circle will appear, along with instructions for the participant. 

Similarly, at the end of the Resting-state part and at the end of the Experiment part, a screen
with a crossed-out R appears to remind the experimenter to stop recording the EEG. 

Notice that, at some important stages during the sessions, the letter &#39;C&#39; must be pressed twice 
by the experimenter to let the session continue. This protocol provides the experimenter with 
control when necessary. These moments are signalled by a &#39;wait a moment&#39; notice for the 
participant, and by two orange-coloured stripes on the screen. The experimenter should be aware 
of the use of the letter &#39;C&#39; at these points, as the requirement is not signalled on the screen 
to prevent participants from pressing the letter themselves. 

During the experiment, it is important to monitor the EEG signal. If it ever becomes very noisy, 
the experiment must be paused by pressing the ESC key, and the problem must be resolved. If the 
noise is due to movement by the participant, they should be asked again to please stay as still 
possible. If the noise is due to an increase in the impedance of some electrodes, the impedance 
of those electrodes should be revised.

The Experiment part in each session contains a break every 40 trials. During these breaks, the 
number of the current trial appears in grey on the bottom right corner of the screen.


== Definition of items in OpenSesame (only for programming purposes, not for in-session use) ==

  -- Each major part of the session is contained in a sequence item that is named in capital 
     letters (e.g., &#39;PRETRAINING&#39;, &#39;TRAINING&#39;, &#39;TEST&#39;, &#39;EXPERIMENT&#39;).

  -- &#39;continue_space&#39;: allows proceeding to the following screen after pressing the space bar, 
     which should be done by the participant. In most cases, two presses are required, as 
     detailed on the screen.

  -- &#39;continue_c&#39;: allows proceeding to the following screen after pressing the letter &#39;C&#39;, 
     which should be done by the experimenter. In most cases, two presses are required, as 
     detailed on the screen.


== Variables in the OpenSesame log files ==

In the log files produced by OpenSesame, each part of the session (e.g., Test, Experiment) is 
identified in the variable &#39;session_part&#39;. The names of the response variables are &#39;response&#39;,
&#39;response_time&#39; and &#39;correct&#39;. Item-specific response variables follow the formats of 
&#39;response_[item_name]&#39;, &#39;response_time_[item_name]&#39; and &#39;correct_[item_name]&#39; 
(see https://osdoc.cogsci.nl/3.3/manual/variables/#response-variables).

The output is verbose and requires preprocessing of the data. For instance, the last response 
in each loop may appear twice in the output, due to the processing of the response. These 
duplicates can--and must--be cleaned up by discarding the rows that have the same trial number
as the preceding row.


== EEG triggers ==

Triggers are sent to the EEG recorder throughout the experiment. The system for sending 
triggers is set up in OpenSesame script within the inline script &#39;EEG_trigger_setup&#39;.

The key to the triggers is provided below.

  0: reset trigger port in BrainVision Recorder. This trigger is integrated in the 
     trigger-sending function.

  -- Resting-state EEG part --

    10: beginning of eyes-open resting-state EEG

    11: end of eyes-open resting-state EEG

    12: beginning of eyes-closed resting-state EEG

    13: end of eyes-closed resting-state EEG

  -- Experiment part --

    5: fixation mark

    -- ID of each target sentence (only applicable to target trials) --

        110--253: triggers ranging between 110 and 253, time-locked to the onset of the 
          word of interest in each trial.
&lt;/textarea&gt;
&lt;div id=&#34;comments-in-code-scripts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comments in code scripts&lt;/h3&gt;
&lt;p&gt;It is helpful for our future selves, for our collaborators, and for any other stakeholders associated with a project—which includes any fellow researchers worldwide—to include comments in code scripts. These comments should introduce the purpose of the script at the top, and the purpose of various components of the code. Some excerpts are shown below as examples.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Ffrequentist_bayesian_plot.R%23L3-L35&amp;style=a11y-dark&amp;type=code&amp;showFullPath=on&amp;showCopy=on&amp;showLineNumbers=on&amp;showFileMeta=on&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Fdata%2Fsemanticpriming_data_preparation.R%23L29-L60&amp;style=a11y-dark&amp;type=code&amp;showFullPath=on&amp;showCopy=on&amp;showLineNumbers=on&amp;showFileMeta=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;open-source-software&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Open-source software&lt;/h2&gt;
&lt;p&gt;Where possible, open-source software should be used. Open-source software is free, and hence more accessible. Open-source software can be classified in various dimensions, such as the size of the user base. The more users, the greater the support, because the core developers have more resources, and the users will often help each other in public forums such as Stack Exchange. For instance, a programming language such as R boasts millions of users worldwide who count on support in public forums and in R-specific forums such as the &lt;a href=&#34;https://community.rstudio.com&#34;&gt;Posit Community&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Other software are not as large. For instance, open-source software for psychological research (e.g., &lt;a href=&#34;https://osdoc.cogsci.nl/&#34;&gt;OpenSesame&lt;/a&gt;, &lt;a href=&#34;https://www.psychopy.org/&#34;&gt;psychoPy&lt;/a&gt;) are far smaller than R in terms of community. Yet, these software too can count on substantial support. For the more basic uses, most of the way has already been paved, and the existing documentation suffices. For more advanced uses, the smaller size of the community can become more obvious, as one needs to spend more time researching on solutions for their needs.&lt;/p&gt;
&lt;p&gt;Regardless of the size of the community, all else being equal, open-source software is the right choice to ensure access to one’s work for all (potential) stakeholders in the future. The other option, proprietory software, entails dependence on the services of a private company.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidiness-and-parsimony-in-computer-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidiness and parsimony in computer code&lt;/h2&gt;
&lt;p&gt;Code scripts should be as tidy and parsimonious as possible. For instance, to prevent overly long scripts that would impair the comprehension of the materials, it is useful to break down large projects into nested scripts, and &lt;code&gt;source&lt;/code&gt; (i.e., run) the smaller scripts in the larger scripts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compose all stimuli for Sessions 2, 3, 4 and 6

# Create participant-specific parameters
source(&amp;#39;stimulus_preparation/participant_parameters.R&amp;#39;)

# Frame base images
source(&amp;#39;stimulus_preparation/base_images.R&amp;#39;)

# Session 2
source(&amp;#39;stimulus_preparation/Session 2/Session2_compile_all_stimuli.R&amp;#39;)

# Session 3
source(&amp;#39;stimulus_preparation/Session 3/Session3_compile_all_stimuli.R&amp;#39;)

# Session 4
source(&amp;#39;stimulus_preparation/Session 4/Session4_compile_all_stimuli.R&amp;#39;)

# Session 6
source(&amp;#39;stimulus_preparation/Session 6/Session6_compile_all_stimuli.R&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tidiness-and-parsimony-in-project-directories&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidiness and parsimony in project directories&lt;/h2&gt;
&lt;p&gt;A directory tree is useful to display all the folders in a project. The tree can be produced in the RStudio ‘Terminal’ console using the following one-line command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;find . -type d | sed -e &amp;quot;s/[^-][^\/]*\//  |/g&amp;quot; -e &amp;quot;s/|\([^ ]\)/| - \1/&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output will look like the following (excerpt from &lt;a href=&#34;https://osf.io/gt5uf/wiki&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf/wiki&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.
  | - bayesian_priors
  |  | - plots
  | - semanticpriming
  |  | - analysis_with_visualsimilarity
  |  |  | - model_diagnostics
  |  |  |  | - results
  |  |  |  | - plots
  |  |  | - results
  |  |  | - plots
  |  |  | - correlations
  |  |  |  | - plots
  |  | - frequentist_bayesian_plots
  |  |  | - plots
  |  | - frequentist_analysis
  |  |  | - model_diagnostics
  |  |  |  | - results
  |  |  |  | - plots
  |  |  | - lexical_covariates_selection
  |  |  |  | - results
  |  |  |  | - plots
  |  |  | - results
  |  |  | - plots&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      
            <category>research methods</category>
      
            <category>programming</category>
      
            <category>R</category>
      
            <category>s</category>
      
      
            <category>research methods</category>
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Preprocessing the Norwegian Web as Corpus (NoWaC) in R</title>
      <link>https://pablobernabeu.github.io/2023/preprocessing-the-norwegian-web-as-corpus-nowac-in-r/</link>
      <pubDate>Thu, 28 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2023/preprocessing-the-norwegian-web-as-corpus-nowac-in-r/</guid>
      <description>


&lt;div id=&#34;the-present-script-can-be-used-to-pre-process-data-from-a-frequency-list-of-the-norwegian-as-web-corpus-nowac-guevara-2010.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The present script can be used to pre-process data from a frequency list of the Norwegian as Web Corpus (NoWaC; Guevara, 2010).&lt;/h3&gt;
&lt;p&gt;Before using the script, the frequency list should be downloaded from &lt;a href=&#34;https://www.hf.uio.no/iln/english/about/organization/text-laboratory/projects/nowac/nowac-frequency.html&#34;&gt;this URL&lt;/a&gt;. The list is described as ‘frequency list sorted primary alphabetic and secondary by frequency within each character’, and &lt;a href=&#34;https://www.tekstlab.uio.no/nowac/download/nowac-1.1.lemma.frek.sort_alf_frek.txt.gz&#34;&gt;this is the direct URL&lt;/a&gt;. The download requires signing in to an institutional network. Last, the downloaded file should be unzipped.&lt;/p&gt;
&lt;p&gt;The script is shown below.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fpreprocessing-NoWaC-Corpus-in-R%2Fblob%2Fmain%2Fpreprocessing_NoWaC_Corpus.R%23L19-L74&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Guevara, E. R. (2010). NoWaC: A large web-based corpus for Norwegian. In &lt;em&gt;Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop&lt;/em&gt; (pp. 1-7). &lt;a href=&#34;https://aclanthology.org/W10-1501&#34; class=&#34;uri&#34;&gt;https://aclanthology.org/W10-1501&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rodina, Y., &amp;amp; Westergaard, M. (2015). Grammatical gender in Norwegian: Language acquisition and language change. &lt;em&gt;Journal of Germanic Linguistics, 27&lt;/em&gt;(2), 145–187. &lt;a href=&#34;https://doi.org/10.1017/S1470542714000245&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1017/S1470542714000245&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rodina, Y., &amp;amp; Westergaard, M. (2021). Grammatical gender and declension class in language change: A study of the loss of feminine gender in Norwegian. &lt;em&gt;Journal of Germanic Linguistics, 33&lt;/em&gt;(3), 235–263. &lt;a href=&#34;https://doi.org/10.1017/S1470542719000217&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1017/S1470542719000217&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>R</category>
      
            <category>corpora</category>
      
            <category>corpus</category>
      
            <category>Norwegian</category>
      
            <category>NoWaC</category>
      
            <category>linguistics</category>
      
            <category>language</category>
      
            <category>research methods</category>
      
            <category>s</category>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>ggplotting power curves from the &#39;simr&#39; package</title>
      <link>https://pablobernabeu.github.io/2023/ggplotting-power-curves-from-simr-package/</link>
      <pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2023/ggplotting-power-curves-from-simr-package/</guid>
      <description>


&lt;p&gt;The R package ‘simr’ has greatly facilitated power analysis for mixed-effects models using Monte Carlo simulation (which involves running hundreds or thousands of tests under slight variations of the data). The &lt;code&gt;powerCurve&lt;/code&gt; function is used to estimate the statistical power for various sample sizes in one go. Since the tests are run serially, they can take a VERY long time; approximately, the time it takes to run the model supplied once (say, a few hours) &lt;em&gt;times&lt;/em&gt; the number of simulations (&lt;code&gt;nsim&lt;/code&gt;, which should be higher than 200), and &lt;em&gt;times&lt;/em&gt; the number of different sample sizes examined. While there isn’t a built-in parallel method, the power curves for different sample sizes can be run separately, and the results can be progressively combined as each component finishes running (see &lt;a href=&#34;https://pablobernabeu.github.io/2021/parallelizing-simr-powercurve&#34;&gt;tutorial&lt;/a&gt;). The power curves produced by &lt;code&gt;simr&lt;/code&gt; are so good they deserve ‘ggplot2’ rendering. So, here’s a function for it.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2FpowercurvePlot%2Fblob%2Fmain%2FpowercurvePlot.R%23L3-L82&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;a-usage-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A usage example&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
library(simr)
library(ggplot2)

# Toy model with data from &amp;#39;simr&amp;#39; package
fit = lmer(y ~ x + (x | g), data = simdata)

# Extend sample size of `g`
fit_extended_g = extend(fit, along = &amp;#39;g&amp;#39;, n = 12)

fit_powercurve = 
  powerCurve(fit_extended_g, fixed(&amp;#39;x&amp;#39;), 
             along = &amp;#39;g&amp;#39;, breaks = c(4, 6, 8, 10, 12), 
             nsim = 50, seed = 123, progress = FALSE)

# Read in custom function to ggplot results from simr::powerCurve

source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/powercurvePlot/main/powercurvePlot.R&amp;#39;)

powercurvePlot(fit_powercurve, number_x_axis_levels = 6) +
  
  # Change some defaults
  
  xlab(&amp;quot;Number of levels in &amp;#39;g&amp;#39;&amp;quot;) +
  
  theme(plot.title = element_blank(),
        axis.title.x = element_text(size = 18), 
        axis.title.y = element_text(size = 18), 
        axis.text = element_text(size = 17))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2023/ggplotting-power-curves-from-simr-package/index.en_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>power analysis</category>
      
            <category>statistical power</category>
      
            <category>simr</category>
      
            <category>research methods</category>
      
            <category>visualisation</category>
      
            <category>s</category>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>How to break down colour variable in sjPlot::plot_model into equally-sized bins</title>
      <link>https://pablobernabeu.github.io/2023/how-to-break-down-colour-variable-in-sjplot-plot-model-into-equally-sized-bins/</link>
      <pubDate>Sat, 24 Jun 2023 16:54:46 +0200</pubDate>
      
      <guid>https://pablobernabeu.github.io/2023/how-to-break-down-colour-variable-in-sjplot-plot-model-into-equally-sized-bins/</guid>
      <description>


&lt;p&gt;Whereas the direction of main effects can be interpreted from the sign of the estimate, the interpretation of interaction effects often requires plots. This task is facilitated by the R package &lt;a href=&#34;https://strengejacke.github.io/sjPlot&#34;&gt;&lt;code&gt;sjPlot&lt;/code&gt;&lt;/a&gt;. For instance, using the &lt;code&gt;plot_model&lt;/code&gt; function, I plotted the interaction between two continuous variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
#&amp;gt; Loading required package: Matrix
library(sjPlot)
#&amp;gt; Learn more about sjPlot with &amp;#39;browseVignettes(&amp;quot;sjPlot&amp;quot;)&amp;#39;.
library(ggplot2)

theme_set(theme_sjplot())

# Create data partially based on code by Ben Bolker  
# from https://stackoverflow.com/a/38296264/7050882

set.seed(101)

spin = runif(800, 1, 24)

trait = rep(1:40, each = 20)

ID = rep(1:80, each = 10)

testdata &amp;lt;- data.frame(spin, trait, ID)

testdata$fatigue &amp;lt;- 
  testdata$spin * testdata$trait / 
  rnorm(800, mean = 6, sd = 2)

# Model
fit = lmer(fatigue ~ spin * trait + (1|ID),
           data = testdata, REML = TRUE)
#&amp;gt; boundary (singular) fit: see help(&amp;#39;isSingular&amp;#39;)

plot_model(fit, type = &amp;#39;pred&amp;#39;, terms = c(&amp;#39;spin&amp;#39;, &amp;#39;trait&amp;#39;))
#&amp;gt; Warning: Ignoring unknown parameters: linewidth&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/7VTcfLu.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;Created on 2023-06-24 with &lt;a href=&#34;https://reprex.tidyverse.org&#34;&gt;reprex v2.0.2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;However, I needed an extra feature, as sjPlot by default breaks down the colour (&lt;code&gt;fill&lt;/code&gt;) variable into few levels that do not include the minimum or the maximum values in my variable. What I would like to do is to stratify the colour variable into equally-sized levels that include the minimum and the maximum values.&lt;/p&gt;
&lt;p&gt;Furthermore, in the legend, I would also like to display the number of levels of a grouping variable (&lt;code&gt;ID&lt;/code&gt;) that are contained in each level of the colour variable.&lt;/p&gt;
&lt;p&gt;Below is a solution using &lt;a href=&#34;https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-ten-or-six-bins&#34;&gt;custom functions called &lt;code&gt;deciles_interaction_plot&lt;/code&gt; and &lt;code&gt;sextiles_interaction_plot&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
#&amp;gt; Loading required package: Matrix
library(sjPlot)
library(ggplot2)

theme_set(theme_sjplot())

# Create data partially based on code by Ben Bolker  
# from https://stackoverflow.com/a/38296264/7050882

set.seed(101)

spin = runif(800, 1, 24)

trait = rep(1:40, each = 20)

ID = rep(1:80, each = 10)

testdata &amp;lt;- data.frame(spin, trait, ID)

testdata$fatigue &amp;lt;- 
  testdata$spin * testdata$trait / 
  rnorm(800, mean = 6, sd = 2)

# Model
fit = lmer(fatigue ~ spin * trait + (1|ID),
           data = testdata, REML = TRUE)
#&amp;gt; boundary (singular) fit: see help(&amp;#39;isSingular&amp;#39;)

# plot_model(fit, type = &amp;#39;pred&amp;#39;, terms = c(&amp;#39;spin&amp;#39;, &amp;#39;trait&amp;#39;))

# Binning the colour variable into ten levels (deciles)

# Read in function from GitHub
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/main/R_functions/deciles_interaction_plot.R&amp;#39;)

deciles_interaction_plot(
  model = fit, 
  x = &amp;#39;spin&amp;#39;,
  fill = &amp;#39;trait&amp;#39;,
  fill_nesting_factor = &amp;#39;ID&amp;#39;
)
#&amp;gt; Loading required package: dplyr
#&amp;gt; 
#&amp;gt; Attaching package: &amp;#39;dplyr&amp;#39;
#&amp;gt; The following objects are masked from &amp;#39;package:stats&amp;#39;:
#&amp;gt; 
#&amp;gt;     filter, lag
#&amp;gt; The following objects are masked from &amp;#39;package:base&amp;#39;:
#&amp;gt; 
#&amp;gt;     intersect, setdiff, setequal, union
#&amp;gt; Loading required package: RColorBrewer
#&amp;gt; Loading required package: ggtext
#&amp;gt; Loading required package: Cairo
#&amp;gt; Warning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette Set1 is 9
#&amp;gt; Returning the palette you asked for with that many colors
#&amp;gt; Warning: Ignoring unknown parameters: linewidth
#&amp;gt; Scale for &amp;#39;y&amp;#39; is already present. Adding another scale for &amp;#39;y&amp;#39;, which will
#&amp;gt; replace the existing scale.
#&amp;gt; Scale for &amp;#39;colour&amp;#39; is already present. Adding another scale for &amp;#39;colour&amp;#39;,
#&amp;gt; which will replace the existing scale.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/niqWOzx.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# If you wanted or needed to make six levels (sextiles) instead 
# of ten, you could use the function sextiles_interaction_plot.

# Read in function from GitHub
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/main/R_functions/sextiles_interaction_plot.R&amp;#39;)

sextiles_interaction_plot(
  model = fit, 
  x = &amp;#39;spin&amp;#39;,
  fill = &amp;#39;trait&amp;#39;,
  fill_nesting_factor = &amp;#39;ID&amp;#39;
)
#&amp;gt; Warning: Ignoring unknown parameters: linewidth
#&amp;gt; Scale for &amp;#39;y&amp;#39; is already present. Adding another scale for &amp;#39;y&amp;#39;, which will
#&amp;gt; replace the existing scale.
#&amp;gt; Scale for &amp;#39;colour&amp;#39; is already present. Adding another scale for &amp;#39;colour&amp;#39;,
#&amp;gt; which will replace the existing scale.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/w8Ydo4F.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;Created on 2023-06-24 with &lt;a href=&#34;https://reprex.tidyverse.org&#34;&gt;reprex v2.0.2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
</description>
      
            <category>visualisation</category>
      
            <category>linear-mixed effects models</category>
      
            <category>interactions</category>
      
            <category>binning</category>
      
            <category>sjPlot</category>
      
            <category>s</category>
      
      
            <category>R</category>
      
            <category>statistics</category>
      
    </item>
    
    <item>
      <title>How to map more informative values onto fill argument of sjPlot::plot_model</title>
      <link>https://pablobernabeu.github.io/2023/how-to-map-more-informative-values-onto-fill-argument-of-sjplot-plot-model/</link>
      <pubDate>Sat, 24 Jun 2023 16:51:11 +0200</pubDate>
      
      <guid>https://pablobernabeu.github.io/2023/how-to-map-more-informative-values-onto-fill-argument-of-sjplot-plot-model/</guid>
      <description>


&lt;p&gt;Whereas the direction of main effects can be interpreted from the sign of the estimate, the interpretation of interaction effects often requires plots. This task is facilitated by the R package &lt;a href=&#34;https://strengejacke.github.io/sjPlot&#34;&gt;&lt;code&gt;sjPlot&lt;/code&gt;&lt;/a&gt;. For instance, using the &lt;code&gt;plot_model&lt;/code&gt; function, I plotted the interaction between a continuous variable and a categorical variable. The categorical variable was passed to the &lt;code&gt;fill&lt;/code&gt; argument of plot_model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
#&amp;gt; Loading required package: Matrix
library(sjPlot)
#&amp;gt; Install package &amp;quot;strengejacke&amp;quot; from GitHub (`devtools::install_github(&amp;quot;strengejacke/strengejacke&amp;quot;)`) to load all sj-packages at once!
library(ggplot2)

theme_set(theme_sjplot())

cake$recipe_recoded = ifelse(cake$recipe == &amp;#39;A&amp;#39;, -0.5,
                             ifelse(cake$recipe == &amp;#39;B&amp;#39;, 0,
                                    ifelse(cake$recipe == &amp;#39;C&amp;#39;, 0.5,
                                           NA)))

fit = lmer(angle ~ recipe_recoded * temp + 
             (1|recipe_recoded:replicate), 
           cake, REML= FALSE)

plot_model(fit, type = &amp;#39;pred&amp;#39;, terms = c(&amp;#39;temp&amp;#39;, &amp;#39;recipe_recoded&amp;#39;))
#&amp;gt; Warning: Ignoring unknown parameters: linewidth&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/SiigAMp.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;Created on 2023-06-24 with &lt;a href=&#34;https://reprex.tidyverse.org&#34;&gt;reprex v2.0.2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;However, I needed an extra feature, as the categorical variable was not quite informative because it was a sum-coded transformation. Thus, I wanted the legend of the plot to show the values of the original variable (i.e., A, B and C), instead of those of the sum-coded variable that had been used in the model (i.e., -0.5, 0 and 0.5).&lt;/p&gt;
&lt;p&gt;Below is a solution using a custom function called &lt;a href=&#34;https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-alias-variables&#34;&gt;&lt;code&gt;alias_interaction_plot&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
#&amp;gt; Loading required package: Matrix
library(sjPlot)
library(ggplot2)

theme_set(theme_sjplot())

cake$recipe_recoded = ifelse(cake$recipe == &amp;#39;A&amp;#39;, -0.5,
                             ifelse(cake$recipe == &amp;#39;B&amp;#39;, 0,
                                    ifelse(cake$recipe == &amp;#39;C&amp;#39;, 0.5,
                                           NA)))

fit = lmer(angle ~ recipe_recoded * temp + 
             (1|recipe_recoded:replicate), 
           cake, REML= FALSE)

# plot_model(fit, type = &amp;#39;pred&amp;#39;, terms = c(&amp;#39;temp&amp;#39;, &amp;#39;recipe_recoded&amp;#39;))

# Displaying the original variable instead

# Read in function from GitHub
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/main/R_functions/alias_interaction_plot.R&amp;#39;)

alias_interaction_plot(
  model = fit, 
  dataset = cake,
  x = &amp;#39;temp&amp;#39;,
  fill = &amp;#39;recipe_recoded&amp;#39;,
  fill_alias = &amp;#39;recipe&amp;#39;,
  fill_title = &amp;#39;recipe&amp;#39;
)
#&amp;gt; Loading required package: rlang
#&amp;gt; Loading required package: dplyr
#&amp;gt; 
#&amp;gt; Attaching package: &amp;#39;dplyr&amp;#39;
#&amp;gt; The following objects are masked from &amp;#39;package:stats&amp;#39;:
#&amp;gt; 
#&amp;gt;     filter, lag
#&amp;gt; The following objects are masked from &amp;#39;package:base&amp;#39;:
#&amp;gt; 
#&amp;gt;     intersect, setdiff, setequal, union
#&amp;gt; Loading required package: RColorBrewer
#&amp;gt; Loading required package: ggtext
#&amp;gt; Loading required package: Cairo
#&amp;gt; Warning: Ignoring unknown parameters: linewidth
#&amp;gt; Scale for &amp;#39;y&amp;#39; is already present. Adding another scale for &amp;#39;y&amp;#39;, which will
#&amp;gt; replace the existing scale.
#&amp;gt; Scale for &amp;#39;colour&amp;#39; is already present. Adding another scale for &amp;#39;colour&amp;#39;,
#&amp;gt; which will replace the existing scale.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/SS03gK6.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;Created on 2023-06-24 with &lt;a href=&#34;https://reprex.tidyverse.org&#34;&gt;reprex v2.0.2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
</description>
      
            <category>linear-mixed effects models</category>
      
            <category>interactions</category>
      
            <category>sjPlot</category>
      
            <category>alias</category>
      
            <category>recode</category>
      
            <category>visualisation</category>
      
            <category>s</category>
      
      
            <category>R</category>
      
            <category>statistics</category>
      
    </item>
    
    <item>
      <title>How to visually assess the convergence of a mixed-effects model by plotting various optimizers</title>
      <link>https://pablobernabeu.github.io/2023/how-to-visually-assess-the-convergence-of-a-mixed-effects-model-by-plotting-various-optimizers/</link>
      <pubDate>Sat, 24 Jun 2023 16:42:34 +0200</pubDate>
      
      <guid>https://pablobernabeu.github.io/2023/how-to-visually-assess-the-convergence-of-a-mixed-effects-model-by-plotting-various-optimizers/</guid>
      <description>


&lt;p&gt;To assess whether convergence warnings render the results invalid, or on the contrary, the results can be deemed valid in spite of the warnings, &lt;a href=&#34;https://cran.r-project.org/web/packages/lme4/lme4.pdf&#34;&gt;Bates et al. (2023)&lt;/a&gt; suggest refitting models affected by convergence warnings with a variety of optimizers. The authors argue that, if the different optimizers produce practically-equivalent results, the results are valid. The &lt;code&gt;allFit&lt;/code&gt; function from the ‘lme4’ package allows the refitting of models using a number of optimizers. To use the seven optimizers listed above, two extra packages must be installed: ‘dfoptim’ and ‘optimx’ (see lme4 manual). The output from &lt;code&gt;allFit&lt;/code&gt; contains several statistics on the fixed and the random effects fitted by each optimizer.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
#&amp;gt; Loading required package: Matrix
library(dfoptim)
library(optimx)

# Create data using code by Ben Bolker from 
# https://stackoverflow.com/a/38296264/7050882

set.seed(101)
spin = runif(600, 1, 24)
reg = runif(600, 1, 15)
ID = rep(c(&amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,&amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;7&amp;quot;, &amp;quot;8&amp;quot;, &amp;quot;9&amp;quot;, &amp;quot;10&amp;quot;))
day = rep(1:30, each = 10)
testdata &amp;lt;- data.frame(spin, reg, ID, day)
testdata$fatigue &amp;lt;- testdata$spin * testdata$reg/10 * rnorm(30, mean=3, sd=2)

# Model
fit = lmer(fatigue ~ spin * reg + (1|ID),
           data = testdata, REML = TRUE)

# Refit model using all available algorithms
multi_fit = allFit(fit)
#&amp;gt; bobyqa : [OK]
#&amp;gt; Nelder_Mead : [OK]
#&amp;gt; nlminbwrap : [OK]
#&amp;gt; nmkbw : [OK]
#&amp;gt; optimx.L-BFGS-B : [OK]
#&amp;gt; nloptwrap.NLOPT_LN_NELDERMEAD : [OK]
#&amp;gt; nloptwrap.NLOPT_LN_BOBYQA : [OK]

# Show results 
summary(multi_fit)$fixef
#&amp;gt;                               (Intercept)      spin       reg  spin:reg
#&amp;gt; bobyqa                          -2.975678 0.5926561 0.1437204 0.1834016
#&amp;gt; Nelder_Mead                     -2.975675 0.5926559 0.1437202 0.1834016
#&amp;gt; nlminbwrap                      -2.975677 0.5926560 0.1437203 0.1834016
#&amp;gt; nmkbw                           -2.975678 0.5926561 0.1437204 0.1834016
#&amp;gt; optimx.L-BFGS-B                 -2.975680 0.5926562 0.1437205 0.1834016
#&amp;gt; nloptwrap.NLOPT_LN_NELDERMEAD   -2.975666 0.5926552 0.1437196 0.1834017
#&amp;gt; nloptwrap.NLOPT_LN_BOBYQA       -2.975678 0.5926561 0.1437204 0.1834016

# Read in function from GitHub
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/plot.fixef.allFit/main/plot.fixef.allFit.R&amp;#39;)

plot.fixef.allFit(multi_fit, 
                  
                  select_predictors = c(&amp;#39;spin&amp;#39;, &amp;#39;reg&amp;#39;, &amp;#39;spin:reg&amp;#39;), 
                  
                  # Increase padding at top and bottom of Y axis
                  multiply_y_axis_limits = 1.3,
                  
                  y_title = &amp;#39;Fixed effect (*b*)&amp;#39;)
#&amp;gt; Loading required package: dplyr
#&amp;gt; 
#&amp;gt; Attaching package: &amp;#39;dplyr&amp;#39;
#&amp;gt; The following objects are masked from &amp;#39;package:stats&amp;#39;:
#&amp;gt; 
#&amp;gt;     filter, lag
#&amp;gt; The following objects are masked from &amp;#39;package:base&amp;#39;:
#&amp;gt; 
#&amp;gt;     intersect, setdiff, setequal, union
#&amp;gt; Loading required package: reshape2
#&amp;gt; Loading required package: stringr
#&amp;gt; Loading required package: scales
#&amp;gt; Loading required package: ggplot2
#&amp;gt; Loading required package: ggtext
#&amp;gt; Loading required package: patchwork&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/XYQDug2.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# Alternative using plot-specific Y axes and other modified settings

plot.fixef.allFit(multi_fit, 
                  
                  select_predictors = c(&amp;#39;spin&amp;#39;, &amp;#39;spin:reg&amp;#39;), 
                  
                  # Use plot-specific Y axis limits
                  shared_y_axis_limits = FALSE,
                  
                  decimal_places = 7, 
                  
                  # Move up Y axis title
                  y_title_hjust = 4.5,
                  
                  y_title = &amp;#39;Fixed effect (*b*)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/BYXJYxM.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;Created on 2023-06-26 with &lt;a href=&#34;https://reprex.tidyverse.org&#34;&gt;reprex v2.0.2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
</description>
      
            <category>convergence</category>
      
            <category>research methods</category>
      
            <category>statistics</category>
      
            <category>linear-mixed effects models</category>
      
            <category>visualisation</category>
      
            <category>s</category>
      
      
            <category>R</category>
      
            <category>statistics</category>
      
    </item>
    
    <item>
      <title>Table joins with conditional &#34;fuzzy&#34; string matching in R</title>
      <link>https://pablobernabeu.github.io/2023/table-joins-with-conditional-fuzzy-string-matching-in-r/</link>
      <pubDate>Sat, 24 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2023/table-joins-with-conditional-fuzzy-string-matching-in-r/</guid>
      <description>


&lt;p&gt;Here’s an example of fuzzy-matching strings in R that I shared on &lt;a href=&#34;https://stackoverflow.com/a/76368552/7050882&#34;&gt;StackOverflow&lt;/a&gt;. In &lt;code&gt;stringdist_join&lt;/code&gt;, the &lt;code&gt;max_dist&lt;/code&gt; argument is used to constrain the degree of fuzziness.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fuzzyjoin)
library(dplyr)
#&amp;gt; 
#&amp;gt; Attaching package: &amp;#39;dplyr&amp;#39;
#&amp;gt; The following objects are masked from &amp;#39;package:stats&amp;#39;:
#&amp;gt; 
#&amp;gt;     filter, lag
#&amp;gt; The following objects are masked from &amp;#39;package:base&amp;#39;:
#&amp;gt; 
#&amp;gt;     intersect, setdiff, setequal, union
library(knitr)


small_tab = data.frame(Food.Name = c(&amp;#39;Corn&amp;#39;, &amp;#39;Squash&amp;#39;, &amp;#39;Peppers&amp;#39;), 
                       Food.Code = c(NA, NA, NA))


large_tab = data.frame(Food.Name = c(&amp;#39;Sweet Corn&amp;#39;, &amp;#39;Red Corn&amp;#39;, &amp;#39;Baby Corns&amp;#39;, 
                                     &amp;#39;Squash&amp;#39;, &amp;#39;Long Squash&amp;#39;, &amp;#39;Red Pepper&amp;#39;, 
                                     &amp;#39;Green Pepper&amp;#39;, &amp;#39;Red Peppers&amp;#39;), 
                       Food.Code = c(532, 532, 944, 111, 123, 654, 655, 654))

joined_tab = stringdist_join(small_tab, large_tab, by = &amp;#39;Food.Name&amp;#39;,
                             ignore_case = TRUE, method = &amp;#39;cosine&amp;#39;, 
                             max_dist = 0.5, distance_col = &amp;#39;dist&amp;#39;) %&amp;gt;%
  
  # Tidy columns 
  select(Food.Name = Food.Name.x, -Food.Name.y, 
         Food.Code = Food.Code.y, -dist) %&amp;gt;%
  
  # Only keep most frequent food code per food name
  group_by(Food.Name) %&amp;gt;% count(Food.Name, Food.Code) %&amp;gt;% 
  slice(which.max(n)) %&amp;gt;% select(-n) %&amp;gt;%
  
  # Order food names as in the small table
  arrange(factor(Food.Name, levels = small_tab$Food.Name))

# Show table with columns renamed
joined_tab %&amp;gt;%
  rename(&amp;#39;Food Name&amp;#39; = Food.Name, 
         &amp;#39;Food Code&amp;#39; = Food.Code) %&amp;gt;%
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Food Name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Food Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Corn&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;532&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Squash&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;111&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Peppers&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;654&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;sup&gt;Created on 2023-05-31 with &lt;a href=&#34;https://reprex.tidyverse.org&#34;&gt;reprex v2.0.2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
</description>
      
            <category>string matching</category>
      
            <category>s</category>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>A new function to plot convergence diagnostics from lme4::allFit()</title>
      <link>https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/</link>
      <pubDate>Thu, 22 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/index_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/index_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/index_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;


&lt;p&gt;Linear mixed-effects models (LMM) offer a consistent way of performing regression and analysis of variance tests which allows accounting for non-independence in the data. Over the past decades, LMMs have subsumed most of the General Linear Model, with a steady increase in popularity (Meteyard &amp;amp; Davies, 2020). Since their conception, LMMs have presented the challenge of model &lt;em&gt;convergence&lt;/em&gt;. In essence, the issue of convergence boils down to the widespread tension between parsimony and completeness in data analysis. That is, on the one hand, a good model must allow an accurate, parsimonious analysis of each predictor, and thus, it must not be overfitted with too many parameters. Yet, on the other hand, the model must be complete enough to account for a sufficient amount of variation in the data. In LMMs, any predictors that entail non-independent observations (also known as repeated measures) will normally bring both fixed and random effects into the model. Where a few of these predictors coexist, models often struggle to find enough information in the data to account for every predictor—and especially, for every random effect. This difficulty translates into convergence warnings (Brauer &amp;amp; Curtin, 2018; Singmann &amp;amp; Kellen, 2019). In this article, I review the issue of convergence before presenting a new plotting function in R that facilitates the diagnosis of convergence by visualising the fixed effects fitted by different optimization algorithms (also dubbed optimizers).&lt;/p&gt;
&lt;div id=&#34;completeness-versus-parsimony&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Completeness versus parsimony&lt;/h2&gt;
&lt;p&gt;Both fixed and random effects comprise intercepts and slopes. The pressure exerted by each of those types of effects on the model is determined by the number of data points involved by each. First, slopes are more demanding than intercepts, as they involve a (far) larger number of data points. Second, random effects are more demanding than fixed effects, as random effects entail the number of estimates required for fixed effects &lt;em&gt;times&lt;/em&gt; the number of levels in the grouping factor. As a result, on the most lenient end of the scale lies the fixed intercept, and on the heaviest end lie the random slopes. Convergence warnings in LMMs are often due to the random slopes alone.&lt;/p&gt;
&lt;p&gt;Sounds easy, then! Not inviting the random slopes to the party should solve the problem. Indeed, since random slopes involve the highest number of estimates by far, removing them does often remove convergence warnings. This, however, leads to a different problem. Surrendering the information provided by random slopes can result in the violation of the assumption of independence of observations. For years, the removal of random slopes due to convergence warnings was standard practice. Currently, in contrast, proposals increasingly consider other options, such as removing random effects if they do not significantly improve the fit of the model (Matuschek et al., 2017), and keeping the random slopes in the model in spite of the convergence warnings to safeguard the assumption of independence (see Table 17 in Brauer &amp;amp; Curtin, 2018; Singmann &amp;amp; Kellen, 2019).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-multiple-optimizers-sanity-check-from-lme4allfit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The multiple-optimizers sanity check from &lt;code&gt;lme4::allFit()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Framed within the drive to maintain random slopes wherever possible, the developers of the ‘lme4’ package propose a sanity check that uses a part of the ‘lme4’ &lt;em&gt;engine&lt;/em&gt; called ‘optimizer’. Every model has a default optimizer, unless a specific one is chosen through &lt;code&gt;control = lmerControl(optimizer = &#39;...&#39;)&lt;/code&gt; (in lmer models) or &lt;code&gt;control = glmerControl(optimizer = &#39;...&#39;)&lt;/code&gt; (in glmer models). The seven widely-available optimizers are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bobyqa&lt;/li&gt;
&lt;li&gt;Nelder_Mead&lt;/li&gt;
&lt;li&gt;nlminbwrap&lt;/li&gt;
&lt;li&gt;nmkbw&lt;/li&gt;
&lt;li&gt;optimx.L-BFGS-B&lt;/li&gt;
&lt;li&gt;nloptwrap.NLOPT_LN_NELDERMEAD&lt;/li&gt;
&lt;li&gt;nloptwrap.NLOPT_LN_BOBYQA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To assess whether convergence warnings render the results invalid, or on the contrary, the results can be deemed valid in spite of the warnings, Bates et al. (2022) suggest refitting models affected by convergence warnings with a variety of optimizers. The authors argue that if the different optimizers produce practically-equivalent results, the results are valid. The &lt;code&gt;allFit&lt;/code&gt; function from the ‘lme4’ package allows the refitting of models using a number of optimizers. To use the seven optimizers listed above, two extra packages must be installed: ‘dfoptim’ and ‘optimx’ (see &lt;a href=&#34;https://cran.r-project.org/web/packages/lme4/lme4.pdf&#34;&gt;lme4 manual&lt;/a&gt;). The output from &lt;code&gt;allFit()&lt;/code&gt; contains several statistics on the fixed and the random effects fitted by each optimizer (see &lt;a href=&#34;https://github.com/lme4/lme4/issues/512#issue-425198940&#34;&gt;example&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-the-fixed-effects-from-allfit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plotting the fixed effects from allFit()&lt;/h2&gt;
&lt;p&gt;Several R users have ventured into &lt;a href=&#34;https://www.google.com/search?q=%22ggplot%22+%22allfit%22+optimizers&#34;&gt;plotting the allFit() output&lt;/a&gt; but there is not a function in ‘lme4’ yet at the time of writing. I (Bernabeu, 2022) developed a &lt;a href=&#34;https://github.com/pablobernabeu/plot.fixef.allFit/blob/main/plot.fixef.allFit.R&#34;&gt;function&lt;/a&gt; that takes the output from &lt;code&gt;allFit()&lt;/code&gt;, tidies it, selects the fixed effects and plots them using ‘ggplot2’. The function is shown below, and can be copied through the &lt;code&gt;Copy Code&lt;/code&gt; button at the top right corner. It can be renamed by changing &lt;code&gt;plot.fixef.allFit&lt;/code&gt; to another valid name.&lt;/p&gt;
&lt;div style=&#34;height: 800px; border: 0.5px dotted grey; padding: 10px; resize: both; overflow: auto;&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot the results from the fixed effects produced by different optimizers. This function 
# takes the output from lme4::allFit(), tidies it, selects fixed effects and plots them.

plot.fixef.allFit = function(allFit_output, 
                             # Set the same Y axis limits in every plot
                             shared_y_axis_limits = TRUE,
                             # Multiply Y axis limits by a factor (only 
                             # available if shared_y_axis_limits = TRUE)
                             multiply_y_axis_limits = 1, 
                             # Number of decimal places
                             decimal_places = NULL,
                             # Select predictors
                             select_predictors = NULL, 
                             # Number of rows
                             nrow = NULL, 
                             # Y axis title
                             y_title = &amp;#39;Fixed effect&amp;#39;,
                             # Alignment of the Y axis title
                             y_title_hjust = NULL,
                             # Add number to the names of optimizers
                             number_optimizers = TRUE,
                             # Replace colon in interactions with x
                             interaction_symbol_x = TRUE) {
  
  require(lme4)
  require(dfoptim)
  require(optimx)
  require(dplyr)
  require(reshape2)
  require(stringr)
  require(scales)
  require(ggplot2)
  require(ggtext)
  require(patchwork)
  library(Cairo)
  
  # Tidy allFit output
  
  # Extract fixed effects from the allFit() output
  allFit_fixef = summary(allFit_output)$fixef %&amp;gt;%  # Select fixed effects in the allFit results
    reshape2::melt() %&amp;gt;%  # Structure the output as a data frame
    rename(&amp;#39;Optimizer&amp;#39; = &amp;#39;Var1&amp;#39;, &amp;#39;fixed_effect&amp;#39; = &amp;#39;Var2&amp;#39;)  # set informative names
  
  # If number_optimizers = TRUE, assign number to each optimizer and place it before its name
  if(number_optimizers == TRUE) {
    allFit_fixef$Optimizer = paste0(as.numeric(allFit_fixef$Optimizer), &amp;#39;. &amp;#39;, allFit_fixef$Optimizer)
  }
  
  # If select_predictors was supplied, select them along with the intercept (the latter required)
  if(!is.null(select_predictors)) {
    allFit_fixef = allFit_fixef %&amp;gt;% dplyr::filter(fixed_effect %in% c(&amp;#39;(Intercept)&amp;#39;, select_predictors))
  }
  
  # Order variables
  allFit_fixef = allFit_fixef[, c(&amp;#39;Optimizer&amp;#39;, &amp;#39;fixed_effect&amp;#39;, &amp;#39;value&amp;#39;)]
  
  # PLOT. The overall plot is formed of a first row containing the intercept and the legend 
  # (intercept_plot), and a second row containing the predictors (predictors_plot), 
  # which may in turn occupy several rows.
  
  # If multiply_y_axis_limits was supplied but shared_y_axis_limits = FALSE,
  # warn that shared_y_axis_limits is required.
  if(!multiply_y_axis_limits == 1 &amp;amp; shared_y_axis_limits == FALSE) {
    message(&amp;#39;The argument `multiply_y_axis_limits` has not been used because \n it requires `shared_y_axis_limits` set to TRUE.&amp;#39;)
  }
  
  # If extreme values were entered in y_title_hjust, show warning
  if(!is.null(y_title_hjust)) {
    if(y_title_hjust &amp;lt; 0.5 | y_title_hjust &amp;gt; 6) {
      message(&amp;#39;NOTE: For y_title_hjust, a working range of values is between 0.6 and 6.&amp;#39;)
    }
  }
  
  # If decimal_places was supplied, convert number to the format used in &amp;#39;scales&amp;#39; package
  if(!is.null(decimal_places)) {
    decimal_places = 
      ifelse(decimal_places == 1, 0.1, 
             ifelse(decimal_places == 2, 0.01, 
                    ifelse(decimal_places == 3, 0.001, 
                           ifelse(decimal_places == 4, 0.0001, 
                                  ifelse(decimal_places == 5, 0.00001, 
                                         ifelse(decimal_places == 6, 0.000001, 
                                                ifelse(decimal_places == 7, 0.0000001, 
                                                       ifelse(decimal_places == 8, 0.00000001, 
                                                              ifelse(decimal_places == 9, 0.000000001, 
                                                                     ifelse(decimal_places == 10, 0.0000000001,
                                                                            ifelse(decimal_places == 11, 0.00000000001,
                                                                                   ifelse(decimal_places == 12, 0.000000000001,
                                                                                          ifelse(decimal_places == 13, 0.0000000000001,
                                                                                                 ifelse(decimal_places == 14, 0.00000000000001,
                                                                                                        ifelse(decimal_places &amp;gt;= 15, 0.000000000000001, 
                                                                                                               0.001
                                                                                                        )))))))))))))))
  }
  
  # First row: intercept_plot
  
  # Select intercept data only
  intercept = allFit_fixef %&amp;gt;% dplyr::filter(fixed_effect == &amp;#39;(Intercept)&amp;#39;)
  
  intercept_plot = intercept %&amp;gt;%
    ggplot(., aes(fixed_effect, value, colour = Optimizer)) +
    geom_point(position = position_dodge(1)) +
    facet_wrap(~fixed_effect, scale = &amp;#39;free&amp;#39;) +
    guides(colour = guide_legend(title.position = &amp;#39;left&amp;#39;)) +
    theme_bw() + 
    theme(axis.title = element_blank(), axis.ticks.x = element_blank(),
          axis.text.x = element_blank(), 
          strip.text = element_text(size = 10, margin = margin(t = 4, b = 6)),
          strip.background = element_rect(fill = &amp;#39;grey96&amp;#39;),
          legend.margin = margin(0.3, 0, 0.8, 1, &amp;#39;cm&amp;#39;), 
          legend.title = element_text(size = unit(15, &amp;#39;pt&amp;#39;), angle = 90, hjust = 0.5))
  
  # Second row: predictors_plot
  
  # Select all predictors except intercept
  predictors = allFit_fixef %&amp;gt;% dplyr::filter(!fixed_effect == &amp;#39;(Intercept)&amp;#39;)
  
  # If interaction_symbol_x = TRUE (default), replace colon with times symbol x between spaces
  if(interaction_symbol_x == TRUE) {
    # Replace colon in interactions with \u00D7, i.e., x; then set factor class
    predictors$fixed_effect = predictors$fixed_effect %&amp;gt;% 
      str_replace_all(&amp;#39;:&amp;#39;, &amp;#39; \u00D7 &amp;#39;) %&amp;gt;% factor()
  }
  
  # Order predictors as in the original output from lme4::allFit()
  predictors$fixed_effect = factor(predictors$fixed_effect, 
                                   levels = unique(predictors$fixed_effect))
  
  # Set number of rows for the predictors excluding the intercept.
  # First, if nrow argument supplied, use it
  if(!is.null(nrow)) {
    predictors_plot_nrow = nrow - 1  # Subtract 1 as intercept row not considered
    
    # Else, if nrow argument not supplied, calculate sensible number of rows: i.e., divide number of
    # predictors (exc. intercept) by 2 and round up the result. For instance, 7 predictors --&amp;gt; 3 rows
  } else predictors_plot_nrow = (length(unique(predictors$fixed_effect)) / 2) %&amp;gt;% ceiling()
  
  predictors_plot = ggplot(predictors, aes(fixed_effect, value, colour = Optimizer)) +
    geom_point(position = position_dodge(1)) +
    facet_wrap(~fixed_effect, scale = &amp;#39;free&amp;#39;,
               # Note that predictors_plot_nrow was defined a few lines above
               nrow = predictors_plot_nrow, 
               # Wrap names of predictors with more than 54 characters into new lines
               labeller = labeller(fixed_effect = label_wrap_gen(width = 55))) +
    labs(y = y_title) +
    theme_bw() + 
    theme(axis.title.x = element_blank(), axis.text.x = element_blank(),
          axis.ticks.x = element_blank(),
          axis.title.y = ggtext::element_markdown(size = 14, margin = margin(0, 15, 0, 0)),
          strip.text = element_text(size = 10, margin = margin(t = 4, b = 6)),
          strip.background = element_rect(fill = &amp;#39;grey96&amp;#39;), legend.position = &amp;#39;none&amp;#39;)
  
  # Below, the function scale_y_continuous is applied conditionally to avoid overriding settings. First, 
  # if shared_y_axis_limits = TRUE and decimal_places was supplied, set the same Y axis limits in 
  # every plot and set decimal_places. By default, also expand limits by a seventh of its original 
  # limit, and allow further multiplication of limits through multiply_y_axis_limits.
  if(shared_y_axis_limits == TRUE &amp;amp; !is.null(decimal_places)) {
    
    intercept_plot = intercept_plot +
      scale_y_continuous(limits = c(min(allFit_fixef$value) - allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits,
                                    max(allFit_fixef$value) + allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits), 
                         # Set number of decimal places
                         labels = scales::label_number(accuracy = decimal_places))
    
    predictors_plot = predictors_plot + 
      scale_y_continuous(limits = c(min(allFit_fixef$value) - allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits,
                                    max(allFit_fixef$value) + allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits), 
                         # Set number of decimal places
                         labels = scales::label_number(accuracy = decimal_places))
    
    # Else, if shared_y_axis_limits = TRUE but decimal_places were not supplied, do as above but without
    # setting decimal_places.
  } else if(shared_y_axis_limits == TRUE &amp;amp; is.null(decimal_places)) {
    
    intercept_plot = intercept_plot +
      scale_y_continuous(limits = c(min(allFit_fixef$value) - allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits,
                                    max(allFit_fixef$value) + allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits),
                         # Set number of decimal places
                         labels = scales::label_number(accuracy = decimal_places))
    
    predictors_plot = predictors_plot + 
      scale_y_continuous(limits = c(min(allFit_fixef$value) - allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits,
                                    max(allFit_fixef$value) + allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits),
                         # Set number of decimal places
                         labels = scales::label_number(accuracy = decimal_places))
    
    # Else, if shared_y_axis_limits = FALSE and decimal_places was supplied, set decimal_places. 
  } else if(shared_y_axis_limits == FALSE &amp;amp; !is.null(decimal_places)) {
    
    # Set number of decimal places in both plots
    intercept_plot = intercept_plot +
      scale_y_continuous(labels = scales::label_number(accuracy = decimal_places))
    
    predictors_plot = predictors_plot +
      scale_y_continuous(labels = scales::label_number(accuracy = decimal_places))
  }
  
  # Plot matrix: based on number of predictors_plot_nrow, adjust height of Y axis title
  # (unless supplied), and assign space to intercept_plot and predictors_plot
  if(predictors_plot_nrow == 1) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 3.6))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 11, l = 0)      # predictors row(s)
    )
    
  } else if(predictors_plot_nrow == 2) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 1.4))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 16, l = 0)      # predictors row(s)
    )
    
  } else if(predictors_plot_nrow == 3) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 0.92))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 21, l = 0)      # predictors row(s)
    )
    
  } else if(predictors_plot_nrow == 4) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 0.8))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 26, l = 0)      # predictors row(s)
    )
    
  } else if(predictors_plot_nrow == 5) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 0.73))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 31, l = 0)      # predictors row(s)
    )
    
  } else if(predictors_plot_nrow &amp;gt; 5) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 0.65))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 36, l = 0)      # predictors row(s)
    )
    
    # Also, advise user to consider distributing predictors into several plots
    message(&amp;#39;  Many rows! Consider distributing predictors into several plots \n  using argument `select_predictors`&amp;#39;)
  } 
  
  # Add margin
  predictors_plot = predictors_plot + theme(plot.margin = margin(15, 15, 15, 15))
  
  # Return matrix of plots
  wrap_plots(intercept_plot, predictors_plot, design = layout,
             # The 2 below corresponds to intercept_plot and predictors_plot
             nrow = 2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;optional-arguments&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Optional arguments&lt;/h3&gt;
&lt;p&gt;Below are the optional arguments allowed by the function, with their default values.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
# Set the same Y axis limits in every plot
shared_y_axis_limits = TRUE,

# Multiply Y axis limits by a factor (only 
# available if shared_y_axis_limits = TRUE)
multiply_y_axis_limits = 1, 

# Number of decimal places
decimal_places = NULL,

# Select predictors
select_predictors = NULL, 

# Number of rows
nrow = NULL, 

# Y axis title
y_title = &amp;#39;Fixed effect&amp;#39;,

# Alignment of the Y axis title
y_title_hjust = NULL,

# Add number to the names of optimizers
number_optimizers = TRUE,

# Replace colon in interactions with x
interaction_symbol_x = TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The argument &lt;code&gt;shared_y_axis_limits&lt;/code&gt; deserves a comment. It allows using the same Y axis limits (i.e., range) in all plots or, alternatively, using plot-specific limits. The parameter is &lt;code&gt;TRUE&lt;/code&gt; by default to prevent overinterpretations of small differences across optimizers (see the first figure below). In contrast, when &lt;code&gt;shared_y_axis_limits = FALSE&lt;/code&gt;, plot-specific limits are used, which results in a narrower range of values in the Y axis (see the second figure below). Since data points will span the entire Y axis in that case, any difference across optimizers—regardless of its relative importance—might be perceived as large, unless the specific range of values in each plot is noticed.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;use-case&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Use case&lt;/h2&gt;
&lt;p&gt;Let’s test the function with a minimal model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create data using code by Ben Bolker from 
# https://stackoverflow.com/a/38296264/7050882

set.seed(101)
spin = runif(600, 1, 24)
reg = runif(600, 1, 15)
ID = rep(c(&amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,&amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;7&amp;quot;, &amp;quot;8&amp;quot;, &amp;quot;9&amp;quot;, &amp;quot;10&amp;quot;))
day = rep(1:30, each = 10)
testdata &amp;lt;- data.frame(spin, reg, ID, day)
testdata$fatigue &amp;lt;- testdata$spin * testdata$reg/10 * rnorm(30, mean=3, sd=2)

# Model

library(lme4)

fit = lmer(fatigue ~ spin * reg + (1|ID),
           data = testdata, REML = TRUE)

# Refit model using all available algorithms
multi_fit = allFit(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## bobyqa : [OK]
## Nelder_Mead : [OK]
## nlminbwrap : [OK]
## nmkbw : [OK]
## optimx.L-BFGS-B : [OK]
## nloptwrap.NLOPT_LN_NELDERMEAD : [OK]
## nloptwrap.NLOPT_LN_BOBYQA : [OK]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(multi_fit)$fixef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                               (Intercept)      spin       reg  spin:reg
## bobyqa                          -2.975678 0.5926561 0.1437204 0.1834016
## Nelder_Mead                     -2.975675 0.5926559 0.1437202 0.1834016
## nlminbwrap                      -2.975677 0.5926560 0.1437203 0.1834016
## nmkbw                           -2.975678 0.5926561 0.1437204 0.1834016
## optimx.L-BFGS-B                 -2.975680 0.5926562 0.1437205 0.1834016
## nloptwrap.NLOPT_LN_NELDERMEAD   -2.975666 0.5926552 0.1437196 0.1834017
## nloptwrap.NLOPT_LN_BOBYQA       -2.975678 0.5926561 0.1437204 0.1834016&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The effects to be visualised are selected below using the argument &lt;code&gt;select_predictors&lt;/code&gt;. Notice that the intercept is plotted by default on the first row, along with the legend that lists all the optimizers used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Read in function from GitHub
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/plot.fixef.allFit/main/plot.fixef.allFit.R&amp;#39;)

plot.fixef.allFit(multi_fit, 
                  
                  select_predictors = c(&amp;#39;spin&amp;#39;, &amp;#39;reg&amp;#39;, &amp;#39;spin:reg&amp;#39;), 
                  
                  # Increase padding at top and bottom of Y axis
                  multiply_y_axis_limits = 1.3,
                  
                  y_title = &amp;#39;Fixed effect (*b*)&amp;#39;,
                  
                  # Align y title
                  y_title_hjust = .9)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/index_files/figure-html/demo-plot.fixef.allFit-function-1-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot produced by &lt;code&gt;plot.fixef.allFit()&lt;/code&gt; by default replaces the colons in interaction effects (e.g., &lt;code&gt;spin:reg&lt;/code&gt;) with ’ × ’ to facilitate the visibility (this can be overriden by setting &lt;code&gt;interaction_symbol_x = FALSE&lt;/code&gt;). Yet, it is important to note that any interactions passed to &lt;code&gt;select_predictors&lt;/code&gt; must have the colon, as that is the symbol present in the &lt;code&gt;lme4::allFit()&lt;/code&gt; output.&lt;/p&gt;
&lt;p&gt;The output of &lt;code&gt;plot.fixef.allFit()&lt;/code&gt; is a &lt;a href=&#34;https://ggplot2.tidyverse.org/&#34;&gt;ggplot2&lt;/a&gt; object that can be stored for further use, as in the example below, in which new parameters are used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

plot_fit_convergence = 
  
  plot.fixef.allFit(multi_fit, 
                    
                    select_predictors = c(&amp;#39;spin&amp;#39;, &amp;#39;spin:reg&amp;#39;), 
                    
                    # Use plot-specific Y axis limits
                    shared_y_axis_limits = FALSE,
                    
                    decimal_places = 7, 
                    
                    # Move up Y axis title
                    y_title_hjust = -20,
                    
                    y_title = &amp;#39;Fixed effect (*b*)&amp;#39;)

# Print
plot_fit_convergence&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/index_files/figure-html/demo-plot.fixef.allFit-function-2-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot can be saved to disk as pdf, png, etc. through `ggplot2::ggsave()`
# ggsave(&amp;#39;plot_fit_convergence.pdf&amp;#39;, plot_fit_convergence, 
#        device = cairo_pdf, width = 9, height = 9, dpi = 900)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Bates, D., Maechler, M., Bolker, B., Walker, S., Christensen, R. H. B., Singmann, H., Dai, B., Scheipl, F., Grothendieck, G., Green, P., Fox, J., Bauer, A., &amp;amp; Krivitsky, P. N. (2022). &lt;em&gt;Package ‘lme4’.&lt;/em&gt; CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/lme4/lme4.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/lme4/lme4.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P. (2022). Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://doi.org/10.1037/met0000159&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., &amp;amp; Bates, D. (2017). Balancing type 1 error and power in linear mixed models. &lt;em&gt;Journal of Memory and Language, 94&lt;/em&gt;, 305–315. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2017.01.001&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.jml.2017.01.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meteyard, L., &amp;amp; Davies, R. A. (2020). Best practice guidance for linear mixed-effects models in psychological science. &lt;em&gt;Journal of Memory and Language, 112&lt;/em&gt;, 104092. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2020.104092&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.jml.2020.104092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Singmann, H., &amp;amp; Kellen, D. (2019). An introduction to mixed models for experimental psychology. In D. H. Spieler &amp;amp; E. Schumacher (Eds.), &lt;em&gt;New methods in cognitive psychology&lt;/em&gt; (pp. 4–31). Psychology Press.&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>s</category>
      
            <category>R</category>
      
            <category>linear mixed-effects models</category>
      
            <category>visualisation</category>
      
            <category>statistics</category>
      
      
            <category>R</category>
      
            <category>linear mixed-effects models</category>
      
    </item>
    
    <item>
      <title>A table of results for Bayesian mixed-effects models: Grouping variables and specifying random slopes</title>
      <link>https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/</link>
      <pubDate>Thu, 29 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;Here I share the format applied to tables presenting the results of &lt;em&gt;Bayesian&lt;/em&gt; models in Bernabeu (2022; the &lt;a href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes&#34;&gt;table for frequentist models is covered in this other post&lt;/a&gt;). The sample table presents a Bayesian mixed-effects model that was fitted using the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34;&gt;&lt;code&gt;brms&lt;/code&gt;&lt;/a&gt; (Bürkner et al., 2022). The mixed effects were driven by the maximal principle (Brauer &amp;amp; Curtin, 2018). The format of the table resembles one of the examples published by the &lt;a href=&#34;https://apastyle.apa.org/style-grammar-guidelines/tables-figures/sample-tables&#34;&gt;American Psychological Association&lt;/a&gt;. However, there are also deviations from those examples. For instance, in the present table, the effects are grouped under informative labels to facilitate the readers’ comprehension, using the &lt;a href=&#34;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&#34;&gt;&lt;code&gt;kableExtra&lt;/code&gt;&lt;/a&gt; package (Zhu, 2022). Furthermore, the random slopes are specified using superscript letters and a footnote. The table can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;loading-packages-and-the-results-of-the-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading packages and the results of the models&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(knitr)
library(tibble)
library(stringr)
library(lmerTest)
library(kableExtra)

# Load Bayesian results summary

semanticpriming_summary_weaklyinformativepriors_exgaussian = 
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_summary_weaklyinformativepriors_exgaussian.rds?raw=true&amp;#39;)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adjusting-the-names-of-the-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adjusting the names of the effects&lt;/h3&gt;
&lt;p&gt;First, to facilitate the understanding of the results, the original names of the effects will be adjusted in the &lt;code&gt;brms&lt;/code&gt; summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Rename effects in plain language and specify the random slopes
# (if any) for each effect, in the footnote. For this purpose, 
# superscripts are added to the names of the appropriate effects.
# 
# In the interactions below, word-level variables are presented 
# first for the sake of consistency (the order does not affect 
# the results in any way). Also in the interactions, double 
# colons are used to inform the &amp;#39;bayesian_model_table&amp;#39; 
# function that the two terms in the interaction must be split 
# into two lines.

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_attentional_control&amp;#39;] = &amp;#39;Attentional control&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_attentional_control&amp;#39;] = &amp;#39;Attentional control&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_vocabulary_size&amp;#39;] = &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_recoded_participant_gender&amp;#39;] = &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_target_word_frequency&amp;#39;] = &amp;#39;Word frequency&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_target_number_syllables&amp;#39;] = &amp;#39;Number of syllables&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_word_concreteness_diff&amp;#39;] = &amp;#39;Word-concreteness difference&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_cosine_similarity&amp;#39;] = &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_visual_rating_diff&amp;#39;] = &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_recoded_interstimulus_interval&amp;#39;] = &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_word_concreteness_diff:z_vocabulary_size&amp;#39;] = 
  &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Word-concreteness difference : SOA&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_participant_gender&amp;#39;] = 
  &amp;#39;Word-concreteness difference : Gender&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_attentional_control:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Attentional control&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_attentional_control:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Attentional control&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_vocabulary_size:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_vocabulary_size:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_recoded_participant_gender:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : Gender&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_recoded_participant_gender:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : Gender&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian_model_table&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;bayesian_model_table()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;In Bernabeu (2022), the following custom function was used.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Fbayesian_model_table.R%23L3-L181&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;The above function was used to render a PDF output. In the current scenario, however, we have an HTML output. In the above function, the code used for the &lt;span class=&#34;math inline&#34;&gt;\(\widehat{R}\)&lt;/span&gt; tailored to the HTML output (&lt;code&gt;&amp;amp;Rcirc;&lt;/code&gt;) does not render properly.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Fbayesian_model_table.R%23L119-L121&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;Instead, the LaTeX code &lt;code&gt;$\\widehat{R}$&lt;/code&gt; must be used. Therefore, we’ll correct this error and load the function below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function used in the manuscript to present summaries from &amp;#39;brms&amp;#39; models 
# in APA-formatted tables. The only obligatory argument to be supplied is 
# a summary of a &amp;#39;brms&amp;#39; model.

bayesian_model_table = 
  
  function(model_summary, show_intercept = TRUE, select_effects = NULL, 
           order_effects = NULL, format = NULL, 
           
           # If interaction_symbol_x = TRUE, replace double colons with 
           # times symbols followed by line breaks and indentation. 
           # Then, replace single colons with times symbols.
           interaction_symbol_x = FALSE,
           
           caption = &amp;#39;Summary of the lmerTest model.&amp;#39;) {
    
    require(dplyr)
    require(knitr)
    require(tibble)
    require(stringr)
    require(lmerTest)
    require(kableExtra)
    
    # Create data frame
    model_summary = 
      data.frame(Effect = rownames(model_summary$fixed), 
                 Estimate = model_summary$fixed$Estimate, 
                 SE = model_summary$fixed$Est.Error, 
                 CrI_2.5 = model_summary$fixed$`l-95% CI`, 
                 CrI_97.5 = model_summary$fixed$`u-95% CI`, 
                 Rhat = model_summary$fixed$Rhat,
                 row.names = NULL)
    
    # Process credible intervals and present both inside square brackets
    
    model_summary$CrI_2.5 = model_summary$CrI_2.5 %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .) %&amp;gt;% 
      # Remove minus sign from pure zeros
      sub(&amp;#39;-0.00&amp;#39;, &amp;#39;0.00&amp;#39;, .)
    
    model_summary$CrI_97.5 = model_summary$CrI_97.5 %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .) %&amp;gt;% 
      # Remove minus sign from pure zeros
      sub(&amp;#39;-0.00&amp;#39;, &amp;#39;0.00&amp;#39;, .)
    
    model_summary$CrI_95 = paste0(&amp;#39;[&amp;#39;, model_summary$CrI_2.5, &amp;#39;, &amp;#39;, 
                                  model_summary$CrI_97.5, &amp;#39;]&amp;#39;)
    
    # If show_intercept = FALSE, remove it
    if(isFALSE(show_intercept)) {
      model_summary = model_summary %&amp;gt;% filter(!grepl(&amp;#39;Intercept&amp;#39;, Effect))
      
      # Put &amp;#39;Intercept&amp;#39; in parentheses
    } else if(!is.null(model_summary[model_summary$Effect == &amp;#39;Intercept&amp;#39;, &amp;#39;Effect&amp;#39;])) {
      model_summary[model_summary$Effect == &amp;#39;Intercept&amp;#39;, &amp;#39;Effect&amp;#39;] = &amp;#39;(Intercept)&amp;#39;
    }
    
    # If select_effects was supplied, apply it and order effects accordingly
    if(!is.null(select_effects)) {
      model_summary = model_summary %&amp;gt;% filter(Effect %in% select_effects) %&amp;gt;%
        arrange(factor(Effect, levels = select_effects))
    }
    
    # If order_effects was supplied, apply order
    if(!is.null(order_effects)) {
      model_summary = model_summary %&amp;gt;%
        arrange(factor(Effect, levels = order_effects))
    }
    
    # Round other values
    
    model_summary$Estimate = model_summary$Estimate %&amp;gt;% as.numeric %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .) %&amp;gt;% 
      # Remove minus sign from pure zeros
      sub(&amp;#39;-0.00&amp;#39;, &amp;#39;0.00&amp;#39;, .)
    
    model_summary$SE = model_summary$SE %&amp;gt;% as.numeric %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .)
    
    model_summary$Rhat = model_summary$Rhat %&amp;gt;% as.numeric %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .)
    
    # Order columns
    model_summary = model_summary %&amp;gt;% select(Effect, Estimate, SE, CrI_95, Rhat)
    
    # Right-align all columns after first one
    align = c(&amp;#39;l&amp;#39;, &amp;#39;r&amp;#39;, &amp;#39;r&amp;#39;, &amp;#39;r&amp;#39;, &amp;#39;r&amp;#39;)
    
    # Establish latex or HTML format: if no format supplied, 
    # try to obtain it from knitr, or apply HTML
    if(missing(format) || is.null(format)) {
      if(knitr::is_latex_output()) {
        format = &amp;#39;latex&amp;#39;
      } else format = &amp;#39;html&amp;#39;
    }
    
    # HTML format
    if(format == &amp;#39;html&amp;#39;) {
      
      # If interaction_symbol_x = TRUE, replace double colons with times 
      # symbols followed by line breaks and indentation. Then, replace 
      # single colons with times symbols.
      if(interaction_symbol_x) {
        model_summary$Effect = model_summary$Effect %&amp;gt;% 
          gsub(&amp;#39;::&amp;#39;, &amp;#39; &amp;amp;times; &amp;lt;br&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;#39;, .) %&amp;gt;%
          gsub(&amp;#39;:&amp;#39;, &amp;#39; &amp;amp;times; &amp;#39;, .)
      }
      
      # Output table
      model_summary %&amp;gt;% 
        
        # Remove header of first column and rename other headers
        rename(&amp;#39; &amp;#39; = &amp;#39;Effect&amp;#39;, &amp;#39;&amp;amp;beta;&amp;#39; = &amp;#39;Estimate&amp;#39;, &amp;#39;&amp;lt;i&amp;gt;SE&amp;lt;/i&amp;gt;&amp;#39; = &amp;#39;SE&amp;#39;, 
               &amp;#39;95% CrI&amp;#39; = &amp;#39;CrI_95&amp;#39;, &amp;#39;$\\widehat{R}$&amp;#39; = &amp;#39;Rhat&amp;#39;) %&amp;gt;%
        
        # Present table
        kbl(digits = 2, booktabs = TRUE, escape = FALSE, align = align,
            
            # Caption of the table (default unless specified)
            caption = caption, 
            
            # Disable occasional line gap (https://stackoverflow.com/a/49018919/7050882)
            linesep = &amp;#39;&amp;#39;) %&amp;gt;%
        
        # Apply nice kableExtra format
        kable_styling() %&amp;gt;%
        
        # Center-align header row
        row_spec(0, align = &amp;#39;c&amp;#39;)
      
      # LaTeX format
    } else {
      
      # If interaction_symbol_x = TRUE, replace double colons with times 
      # symbols followed by line breaks and indentation. Then, replace 
      # single colons with times symbols.
      if(interaction_symbol_x) {
        model_summary$Effect = model_summary$Effect %&amp;gt;% 
          gsub(&amp;#39;::&amp;#39;, &amp;#39; $\\\\times$ \n \\\\hspace{0.3cm}&amp;#39;, .) %&amp;gt;%
          gsub(&amp;#39;:&amp;#39;, &amp;#39; $\\\\times$ &amp;#39;, .)
      }
      
      model_summary$Effect = model_summary$Effect %&amp;gt;%
        
        # Escape underscores to avoid error in table
        str_replace_all(&amp;#39;_&amp;#39;, &amp;#39;\\\\_&amp;#39;) %&amp;gt;%
        
        # Allow line breaks in the names of the effects
        # (used in the interactions)
        kableExtra::linebreak(align = &amp;#39;l&amp;#39;)
      
      # Output table
      model_summary %&amp;gt;% 
        
        # Remove header of first column and rename other headers
        rename(&amp;#39; &amp;#39; = &amp;#39;Effect&amp;#39;, &amp;#39;$\\upbeta$&amp;#39; = &amp;#39;Estimate&amp;#39;, &amp;#39;$SE$&amp;#39; = &amp;#39;SE&amp;#39;, 
               &amp;#39;95\\% CrI&amp;#39; = &amp;#39;CrI_95&amp;#39;, &amp;#39;$\\widehat R$&amp;#39; = &amp;#39;Rhat&amp;#39;) %&amp;gt;%
        
        # Present table
        kbl(digits = 2, booktabs = TRUE, escape = FALSE, align = align,
            
            # Caption of the table (default unless specified)
            caption = caption, 
            
            # Disable occasional line gap (https://stackoverflow.com/a/49018919/7050882)
            linesep = &amp;#39;&amp;#39;) %&amp;gt;%
        
        # Apply nice kableExtra format
        kable_styling() %&amp;gt;%
        
        # Center-align header row
        row_spec(0, align = &amp;#39;c&amp;#39;)
    }
  }&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-function-in-use&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function in use&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create table (using custom function from the &amp;#39;R_functions&amp;#39; folder)
bayesian_model_table(
  semanticpriming_summary_weaklyinformativepriors_exgaussian,
  order_effects = c(&amp;#39;(Intercept)&amp;#39;,
                    &amp;#39;Attentional control&amp;#39;,
                    &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Word frequency&amp;#39;,
                    &amp;#39;Number of syllables&amp;#39;,
                    &amp;#39;Word-concreteness difference&amp;#39;,
                    &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;,
                    &amp;#39;Word-concreteness difference : SOA&amp;#39;,
                    &amp;#39;Word-concreteness difference : Gender&amp;#39;,
                    &amp;#39;Language-based similarity :: Attentional control&amp;#39;,
                    &amp;#39;Visual-strength difference :: Attentional control&amp;#39;,
                    &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;,
                    &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;,
                    &amp;#39;Language-based similarity : Gender&amp;#39;,
                    &amp;#39;Visual-strength difference : Gender&amp;#39;,
                    &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;),
  
  # Replace colons in the names of interactions with times symbols
  interaction_symbol_x = TRUE, 
  
  # No title
  caption = NULL) %&amp;gt;%
  
  # Group predictors under headings
  pack_rows(&amp;#39;Individual differences&amp;#39;, 2, 4) %&amp;gt;% 
  pack_rows(&amp;#39;Target-word lexical covariates&amp;#39;, 5, 6) %&amp;gt;% 
  pack_rows(&amp;#39;Prime--target relationship&amp;#39;, 7, 9) %&amp;gt;% 
  pack_rows(&amp;#39;Task condition&amp;#39;, 10, 10) %&amp;gt;% 
  pack_rows(&amp;#39;Interactions&amp;#39;, 11, 21) %&amp;gt;% 
  
  # Apply white background to override default shading in HTML output
  row_spec(1:21, background = &amp;#39;white&amp;#39;) %&amp;gt;%
  
  # Highlight covariates
  row_spec(c(2, 5:7, 11:15), background = &amp;#39;#FFFFF1&amp;#39;) %&amp;gt;%
  
  # Format
  kable_classic(full_width = FALSE, html_font = &amp;#39;Cambria&amp;#39;) %&amp;gt;%
  
  # Footnote describing abbreviations, random slopes, etc. 
  # LaTeX code used to format the text.
  footnote(escape = FALSE, threeparttable = TRUE, general_title = &amp;#39;&amp;lt;br&amp;gt;&amp;#39;, 
           general = paste(&amp;#39;*Note*. &amp;amp;beta; = Estimate based on $z$-scored predictors; *SE* = standard error;&amp;#39;,
                           &amp;#39;CrI = credible interval. Yellow rows contain covariates. Some interactions are &amp;#39;,
                           &amp;#39;split over two lines, with the second line indented. &amp;lt;br&amp;gt;&amp;#39;, 
                           &amp;#39;&amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt; By-word random slopes were included for this effect.&amp;#39;,
                           &amp;#39;&amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt; By-participant random slopes were included for this effect.&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table lightable-classic&#34; style=&#34;margin-left: auto; margin-right: auto; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;border-bottom: 0;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
β
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;i&gt;SE&lt;/i&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
95% CrI
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\widehat{R}\)&lt;/span&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;background-color: white !important;&#34;&gt;
(Intercept)
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Individual differences&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Vocabulary size &lt;sup&gt;a&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Gender &lt;sup&gt;a&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Target-word lexical covariates&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word frequency
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.12, -0.11]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Number of syllables
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.06, 0.07]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Prime–target relationship&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.07, -0.06]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.01, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;1&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Task condition&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Stimulus onset asynchrony (SOA) &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.02, 0.04]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;11&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Interactions&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × SOA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × &lt;br&gt;    Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × &lt;br&gt;    Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × SOA &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × SOA &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; &#34; colspan=&#34;100%&#34;&gt;
&lt;span style=&#34;font-style: italic;&#34;&gt;&lt;br&gt;&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; &#34; colspan=&#34;100%&#34;&gt;
&lt;sup&gt;&lt;/sup&gt; &lt;em&gt;Note&lt;/em&gt;. β = Estimate based on &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scored predictors; &lt;em&gt;SE&lt;/em&gt; = standard error; CrI = credible interval. Yellow rows contain covariates. Some interactions are split over two lines, with the second line indented. &lt;br&gt; &lt;sup&gt;a&lt;/sup&gt; By-word random slopes were included for this effect. &lt;sup&gt;b&lt;/sup&gt; By-participant random slopes were included for this effect.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
&lt;div id=&#34;shading-specific-rows&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Shading specific rows&lt;/h4&gt;
&lt;p&gt;Shading specific rows is done differently when the output is PDF, as shown below (see p. 170 in &lt;a href=&#34;https://eprints.lancs.ac.uk/id/eprint/177833/6/2022deJuanBernabeuPhD.pdf&#34;&gt;Bernabeu, 2022&lt;/a&gt;).&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fthesis%2Fappendix-E-Bayesian-analysis-results.Rmd%23L164-L165&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Session info&lt;/h3&gt;
&lt;p&gt;If you encounter any blockers while reproducing the table using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;, my current session info may be useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.3.2 (2023-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 11 x64 (build 22621)
## 
## Matrix products: default
## 
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.utf8 
## [2] LC_CTYPE=English_United Kingdom.utf8   
## [3] LC_MONETARY=English_United Kingdom.utf8
## [4] LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.utf8    
## 
## time zone: Europe/Oslo
## tzcode source: internal
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] kableExtra_1.3.4    lmerTest_3.1-3      lme4_1.1-35.1      
## [4] Matrix_1.6-4        stringr_1.5.1       tibble_3.2.1       
## [7] dplyr_1.1.4         knitr_1.45          xaringanExtra_0.7.0
## 
## loaded via a namespace (and not attached):
##  [1] sass_0.4.8          utf8_1.2.4          generics_0.1.3     
##  [4] xml2_1.3.6          blogdown_1.18       stringi_1.8.3      
##  [7] lattice_0.22-5      digest_0.6.33       magrittr_2.0.3     
## [10] evaluate_0.23       grid_4.3.2          bookdown_0.37      
## [13] fastmap_1.1.1       jsonlite_1.8.8      httr_1.4.7         
## [16] rvest_1.0.3         fansi_1.0.6         viridisLite_0.4.2  
## [19] scales_1.3.0        numDeriv_2016.8-1.1 jquerylib_0.1.4    
## [22] cli_3.6.2           rlang_1.1.2         munsell_0.5.0      
## [25] splines_4.3.2       withr_2.5.2         cachem_1.0.8       
## [28] yaml_2.3.8          tools_4.3.2         uuid_1.1-1         
## [31] nloptr_2.0.3        minqa_1.2.6         colorspace_2.1-0   
## [34] ggplot2_3.4.4       webshot_0.5.5       boot_1.3-28.1      
## [37] vctrs_0.6.5         R6_2.5.1            lifecycle_1.0.4    
## [40] MASS_7.3-60         pkgconfig_2.0.3     pillar_1.9.0       
## [43] bslib_0.6.1         gtable_0.3.4        glue_1.6.2         
## [46] Rcpp_1.0.11         systemfonts_1.0.5   xfun_0.41          
## [49] tidyselect_1.2.0    rstudioapi_0.15.0   htmltools_0.5.7    
## [52] nlme_3.1-164        svglite_2.1.3       rmarkdown_2.25     
## [55] compiler_4.3.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://doi.org/10.1037/met0000159&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bürkner, P.-C., Gabry, J., Weber, S., Johnson, A., Modrak, M., Badr, H. S., Weber, F., Ben-Shachar, M. S., &amp;amp; Rabel, H. (2022). &lt;em&gt;Package ’brms’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/brms/brms.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhu, H. (2022). &lt;em&gt;Package ’kableExtra’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>R</category>
      
            <category>statistics</category>
      
            <category>rstats</category>
      
            <category>brms</category>
      
            <category>credible intervals</category>
      
            <category>s</category>
      
      
            <category>R</category>
      
            <category>statistics</category>
      
    </item>
    
    <item>
      <title>A table of results for frequentist mixed-effects models: Grouping variables and specifying random slopes</title>
      <link>https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/</link>
      <pubDate>Thu, 29 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;Here I share the format applied to tables presenting the results of &lt;em&gt;frequentist&lt;/em&gt; models in Bernabeu (2022; the &lt;a href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes&#34;&gt;table for Bayesian models is covered in this other post&lt;/a&gt;). The sample table presents a mixed-effects model that was fitted using the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; (Kuznetsova et al., 2022). The mixed effects were driven by the maximal principle (Brauer &amp;amp; Curtin, 2018). The format of the table resembles one of the examples published by the &lt;a href=&#34;https://apastyle.apa.org/style-grammar-guidelines/tables-figures/sample-tables&#34;&gt;American Psychological Association&lt;/a&gt;. However, there are also deviations from those examples. For instance, in the present table, the effects are grouped under informative labels to facilitate the readers’ comprehension, using the &lt;a href=&#34;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&#34;&gt;&lt;code&gt;kableExtra&lt;/code&gt;&lt;/a&gt; package (Zhu, 2022). Furthermore, the random slopes are specified using superscript letters and a footnote. The table can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;loading-packages-and-the-results-of-the-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading packages and the results of the models&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(knitr)
library(tibble)
library(stringr)
library(lmerTest)
library(kableExtra)

# Load frequentist coefficients (estimates and confidence intervals)

KR_summary_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/KR_summary_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

confint_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/confint_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adjusting-the-names-of-the-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adjusting the names of the effects&lt;/h3&gt;
&lt;p&gt;First, to facilitate the understanding of the results, the original names of the effects will be adjusted in the &lt;code&gt;lmerTest&lt;/code&gt; summary and in the confidence intervals object.&lt;/p&gt;
&lt;p&gt;Incidentally, the confidence intervals were obtained using the &lt;code&gt;confint.merMod&lt;/code&gt; function from the &lt;code&gt;lme4&lt;/code&gt; package, as neither &lt;code&gt;lmerTest&lt;/code&gt; nor &lt;code&gt;lme4&lt;/code&gt; currently provide confidence intervals in their default results output. However, computing the confidence intervals is uncomplicated (&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/semanticpriming_lmerTest.R#L126-L130&#34;&gt;see code&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Rename effects in plain language and specify the random slopes
# (if any) for each effect, in the footnote. For this purpose, 
# superscripts are added to the names of the appropriate effects.
# 
# In the interactions below, word-level variables are presented 
# first for the sake of consistency (the order does not affect 
# the results in any way). Also in the interactions, double 
# colons are used to inform the &amp;#39;frequentist_model_table&amp;#39; 
# function that the two terms in the interaction must be split 
# into two lines.

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_attentional_control&amp;#39;] = &amp;#39;Attentional control&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_vocabulary_size&amp;#39;] = &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_participant_gender&amp;#39;] = &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_target_word_frequency&amp;#39;] = &amp;#39;Word frequency&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_target_number_syllables&amp;#39;] = &amp;#39;Number of syllables&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_word_concreteness_diff&amp;#39;] = &amp;#39;Word-concreteness difference&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_cosine_similarity&amp;#39;] = &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_visual_rating_diff&amp;#39;] = &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_interstimulus_interval&amp;#39;] = &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_word_concreteness_diff:z_vocabulary_size&amp;#39;] = 
  &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Word-concreteness difference : SOA&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_participant_gender&amp;#39;] = 
  &amp;#39;Word-concreteness difference : Gender&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_attentional_control:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Attentional control&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_attentional_control:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Attentional control&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_vocabulary_size:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_vocabulary_size:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_participant_gender:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : Gender&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_participant_gender:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : Gender&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;


# Next, change the names in the confidence intervals object

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_attentional_control&amp;#39;] = &amp;#39;Attentional control&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_vocabulary_size&amp;#39;] = &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_participant_gender&amp;#39;] = &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_target_word_frequency&amp;#39;] = &amp;#39;Word frequency&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_target_number_syllables&amp;#39;] = &amp;#39;Number of syllables&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_word_concreteness_diff&amp;#39;] = &amp;#39;Word-concreteness difference&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_cosine_similarity&amp;#39;] = &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_visual_rating_diff&amp;#39;] = &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_interstimulus_interval&amp;#39;] = &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_word_concreteness_diff:z_vocabulary_size&amp;#39;] = 
  &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Word-concreteness difference : SOA&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_participant_gender&amp;#39;] = 
  &amp;#39;Word-concreteness difference : Gender&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_attentional_control:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Attentional control&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_attentional_control:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Attentional control&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_vocabulary_size:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_vocabulary_size:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_participant_gender:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : Gender&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_participant_gender:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : Gender&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;frequentist_model_table&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;frequentist_model_table()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The following custom function was used.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Ffrequentist_model_table.R%23L3-L224&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;loading-the-function-from-github&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Loading the function from GitHub&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/raw/main/R_functions/frequentist_model_table.R&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-function-in-use&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function in use&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create table (using custom function from the &amp;#39;R_functions&amp;#39; folder)
frequentist_model_table(
  KR_summary_semanticpriming_lmerTest, 
  confint_semanticpriming_lmerTest,
  order_effects = c(&amp;#39;(Intercept)&amp;#39;,
                    &amp;#39;Attentional control&amp;#39;,
                    &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Word frequency&amp;#39;,
                    &amp;#39;Number of syllables&amp;#39;,
                    &amp;#39;Word-concreteness difference&amp;#39;,
                    &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;,
                    &amp;#39;Word-concreteness difference : SOA&amp;#39;,
                    &amp;#39;Word-concreteness difference : Gender&amp;#39;,
                    &amp;#39;Language-based similarity :: Attentional control&amp;#39;,
                    &amp;#39;Visual-strength difference :: Attentional control&amp;#39;,
                    &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;,
                    &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;,
                    &amp;#39;Language-based similarity : Gender&amp;#39;,
                    &amp;#39;Visual-strength difference : Gender&amp;#39;,
                    &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;),
  
  # Replace colons in the names of interactions with times symbols
  interaction_symbol_x = TRUE, 
  
  # No title
  caption = NULL) %&amp;gt;%
  
  # Group predictors under headings
  pack_rows(&amp;#39;Individual differences&amp;#39;, 2, 4) %&amp;gt;% 
  pack_rows(&amp;#39;Target-word lexical covariates&amp;#39;, 5, 6) %&amp;gt;% 
  pack_rows(&amp;#39;Prime--target relationship&amp;#39;, 7, 9) %&amp;gt;% 
  pack_rows(&amp;#39;Task condition&amp;#39;, 10, 10) %&amp;gt;% 
  pack_rows(&amp;#39;Interactions&amp;#39;, 11, 21) %&amp;gt;% 
  
  # Apply white background to override default shading in HTML output
  row_spec(1:21, background = &amp;#39;white&amp;#39;) %&amp;gt;%
  
  # Highlight covariates
  row_spec(c(2, 5:7, 11:15), background = &amp;#39;#FFFFF1&amp;#39;) %&amp;gt;%
  
  # Format
  kable_classic(full_width = FALSE, html_font = &amp;#39;Cambria&amp;#39;) %&amp;gt;%
  
  # Footnote describing abbreviations, random slopes, etc. 
  # LaTeX code used to format the text.
  footnote(escape = FALSE, threeparttable = TRUE, general_title = &amp;#39;&amp;lt;br&amp;gt;&amp;#39;, 
           general = paste(&amp;#39;*Note*. &amp;amp;beta; = Estimate based on $z$-scored predictors; *SE* = standard error;&amp;#39;,
                           &amp;#39;CI = confidence interval. Yellow rows contain covariates. Some interactions are &amp;#39;,
                           &amp;#39;split over two lines, with the second line indented. &amp;lt;br&amp;gt;&amp;#39;, 
                           &amp;#39;&amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt; By-word random slopes were included for this effect.&amp;#39;,
                           &amp;#39;&amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt; By-participant random slopes were included for this effect.&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table lightable-classic&#34; style=&#34;margin-left: auto; margin-right: auto; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;border-bottom: 0;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
β
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;i&gt;SE&lt;/i&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
95% CI
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;i&gt;t&lt;/i&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;i&gt;p&lt;/i&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;background-color: white !important;&#34;&gt;
(Intercept)
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.112
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Individual differences&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.56
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.577
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Vocabulary size &lt;sup&gt;a&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.02
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.987
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Gender &lt;sup&gt;a&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.979
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Target-word lexical covariates&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word frequency
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.16, -0.15]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-49.40
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Number of syllables
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.07, 0.08]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
22.81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Prime–target relationship&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.01, 0.02]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
3.48
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.08, -0.07]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-22.44
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.01, 0.02]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
4.18
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;1&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Task condition&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Stimulus onset asynchrony (SOA) &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.04, 0.07]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
7.47
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;11&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Interactions&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.189
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × SOA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
2.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.010
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.97
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.332
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × &lt;br&gt;    Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-2.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.014
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × &lt;br&gt;    Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.810
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-2.34
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.020
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-1.37
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.172
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.433
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.144
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × SOA &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
3.22
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × SOA &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-2.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.025
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; &#34; colspan=&#34;100%&#34;&gt;
&lt;span style=&#34;font-style: italic;&#34;&gt;&lt;br&gt;&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; &#34; colspan=&#34;100%&#34;&gt;
&lt;sup&gt;&lt;/sup&gt; &lt;em&gt;Note&lt;/em&gt;. β = Estimate based on &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scored predictors; &lt;em&gt;SE&lt;/em&gt; = standard error; CI = confidence interval. Yellow rows contain covariates. Some interactions are split over two lines, with the second line indented. &lt;br&gt; &lt;sup&gt;a&lt;/sup&gt; By-word random slopes were included for this effect. &lt;sup&gt;b&lt;/sup&gt; By-participant random slopes were included for this effect.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
&lt;div id=&#34;shading-specific-rows&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Shading specific rows&lt;/h4&gt;
&lt;p&gt;Shading specific rows is done differently when the output is PDF, as shown below (see p. 62 in &lt;a href=&#34;https://eprints.lancs.ac.uk/id/eprint/177833/6/2022deJuanBernabeuPhD.pdf&#34;&gt;Bernabeu, 2022&lt;/a&gt;).&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fthesis%2FChapter-3-Study-2.Rmd%23L690-L691&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Session info&lt;/h3&gt;
&lt;p&gt;If you encounter any blockers while reproducing the table using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;, my current session info may be useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.3.2 (2023-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 11 x64 (build 22621)
## 
## Matrix products: default
## 
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.utf8 
## [2] LC_CTYPE=English_United Kingdom.utf8   
## [3] LC_MONETARY=English_United Kingdom.utf8
## [4] LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.utf8    
## 
## time zone: Europe/Oslo
## tzcode source: internal
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] kableExtra_1.3.4    lmerTest_3.1-3      lme4_1.1-35.1      
## [4] Matrix_1.6-4        stringr_1.5.1       tibble_3.2.1       
## [7] dplyr_1.1.4         knitr_1.45          xaringanExtra_0.7.0
## 
## loaded via a namespace (and not attached):
##  [1] sass_0.4.8          utf8_1.2.4          generics_0.1.3     
##  [4] xml2_1.3.6          blogdown_1.18       stringi_1.8.3      
##  [7] lattice_0.22-5      digest_0.6.33       magrittr_2.0.3     
## [10] evaluate_0.23       grid_4.3.2          bookdown_0.37      
## [13] fastmap_1.1.1       jsonlite_1.8.8      httr_1.4.7         
## [16] rvest_1.0.3         fansi_1.0.6         viridisLite_0.4.2  
## [19] scales_1.3.0        numDeriv_2016.8-1.1 jquerylib_0.1.4    
## [22] cli_3.6.2           rlang_1.1.2         munsell_0.5.0      
## [25] splines_4.3.2       withr_2.5.2         cachem_1.0.8       
## [28] yaml_2.3.8          tools_4.3.2         uuid_1.1-1         
## [31] nloptr_2.0.3        minqa_1.2.6         colorspace_2.1-0   
## [34] ggplot2_3.4.4       webshot_0.5.5       boot_1.3-28.1      
## [37] vctrs_0.6.5         R6_2.5.1            lifecycle_1.0.4    
## [40] MASS_7.3-60         pkgconfig_2.0.3     pillar_1.9.0       
## [43] bslib_0.6.1         gtable_0.3.4        glue_1.6.2         
## [46] Rcpp_1.0.11         systemfonts_1.0.5   xfun_0.41          
## [49] tidyselect_1.2.0    rstudioapi_0.15.0   htmltools_0.5.7    
## [52] nlme_3.1-164        svglite_2.1.3       rmarkdown_2.25     
## [55] compiler_4.3.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://doi.org/10.1037/met0000159&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kuznetsova, A., Brockhoff, P. B., &amp;amp; Christensen, R. H. B. (2022). &lt;em&gt;Package ’lmerTest’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhu, H. (2022). &lt;em&gt;Package ’kableExtra’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>R</category>
      
            <category>statistics</category>
      
            <category>rstats</category>
      
            <category>lmerTest</category>
      
            <category>lme4</category>
      
            <category>confidence intervals</category>
      
            <category>s</category>
      
      
            <category>R</category>
      
            <category>statistics</category>
      
    </item>
    
    <item>
      <title>Plotting two-way interactions from mixed-effects models using alias variables</title>
      <link>https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-alias-variables/</link>
      <pubDate>Mon, 26 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-alias-variables/</guid>
      <description>


&lt;p&gt;Whereas the direction of main effects can be interpreted from the sign of the estimate, the interpretation of interaction effects often requires plots. This task is facilitated by the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34;&gt;&lt;code&gt;sjPlot&lt;/code&gt;&lt;/a&gt; (Lüdecke, 2022). In Bernabeu (2022), the sjPlot function called &lt;code&gt;plot_model&lt;/code&gt; served as the basis for the creation of some &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/tree/main/R_functions&#34;&gt;custom functions&lt;/a&gt;. One of these functions is &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/R_functions/alias_interaction_plot.R&#34;&gt;&lt;code&gt;alias_interaction_plot&lt;/code&gt;&lt;/a&gt;, which allows the plotting of interactions between a continuous variable and a categorical variable. Importantly, the categorical variable is replaced with an alias variable. This feature allows the back-transformation of the categorical variable to facilitate the communication of the results, for instance, when the categorical variable was sum-coded, which has been recommended for mixed-effects models (Brauer &amp;amp; Curtin, 2018).&lt;/p&gt;
&lt;p&gt;Below, we’ll use the function with a model fitted using &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; (Kuznetsova et al., 2022), although the function also works with several other models (see &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34;&gt;sjPlot manual&lt;/a&gt;). The plot can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;alias-interaction-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Alias interaction plot&lt;/h2&gt;
&lt;div id=&#34;the-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Falias_interaction_plot.R&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;the-function-in-use&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function in use&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Ffrequentist_analysis%2Fsemanticpriming-interactions-with-SOA.R&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/plots/semanticpriming-interactions-with-SOA.pdf&#34;&gt;&lt;img src=&#34;Screenshot%202022-12-27%20234345.png&#34; width=&#34;550&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://doi.org/10.1037/met0000159&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kuznetsova, A., Brockhoff, P. B., &amp;amp; Christensen, R. H. B. (2022). &lt;em&gt;Package ’lmerTest’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lüdecke, D. (2022). &lt;em&gt;Package ’sjPlot’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      
            <category>R</category>
      
            <category>visualisation</category>
      
            <category>plotting</category>
      
            <category>alias</category>
      
            <category>sjPlot</category>
      
            <category>linear mixed-effects models</category>
      
            <category>statistics</category>
      
            <category>s</category>
      
      
            <category>R</category>
      
            <category>data visualisation</category>
      
    </item>
    
    <item>
      <title>Plotting two-way interactions from mixed-effects models using ten or six bins</title>
      <link>https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-ten-or-six-bins/</link>
      <pubDate>Mon, 26 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-ten-or-six-bins/</guid>
      <description>


&lt;p&gt;Whereas the direction of main effects can be interpreted from the sign of the estimate, the interpretation of interaction effects often requires plots. This task is facilitated by the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34;&gt;&lt;code&gt;sjPlot&lt;/code&gt;&lt;/a&gt; (Lüdecke, 2022). In Bernabeu (2022), the sjPlot function called &lt;code&gt;plot_model&lt;/code&gt; served as the basis for the creation of some &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/tree/main/R_functions&#34;&gt;custom functions&lt;/a&gt;. Two of these functions are &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/R_functions/deciles_interaction_plot.R&#34;&gt;&lt;code&gt;deciles_interaction_plot&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/R_functions/sextiles_interaction_plot.R&#34;&gt;&lt;code&gt;sextiles_interaction_plot&lt;/code&gt;&lt;/a&gt;. These functions allow the plotting of interactions between two continuous variables. In the case of &lt;code&gt;deciles_interaction_plot&lt;/code&gt;, one of the variables is divided into ten bins, known as deciles, and the other variable is unchanged. In the case of &lt;code&gt;sextiles_interaction_plot&lt;/code&gt;, one of the variables is divided into six bins, or sextiles, and the other variable is unchanged.&lt;/p&gt;
&lt;p&gt;Below, we’ll use these functions with models fitted using &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; (Kuznetsova et al., 2022), although the functions also work with several other models (see &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34;&gt;sjPlot manual&lt;/a&gt;). The plots can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;deciles-interaction-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Deciles interaction plot&lt;/h2&gt;
&lt;div id=&#34;the-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Fdeciles_interaction_plot.R&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;the-function-in-use&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function in use&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Ffrequentist_analysis%2Fsemanticpriming-interactions-with-vocabulary-size.R&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/plots/semanticpriming-interactions-with-vocabulary-size.pdf&#34;&gt;&lt;img src=&#34;Screenshot%202022-12-27%20234321.png&#34; width=&#34;580&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sextiles-interaction-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sextiles interaction plot&lt;/h2&gt;
&lt;div id=&#34;the-function-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Fsextiles_interaction_plot.R&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;the-function-in-use-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function in use&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Flexicaldecision%2Ffrequentist_analysis%2Flexicaldecision-interactions-with-vocabulary-age.R&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/lexicaldecision/frequentist_analysis/plots/lexicaldecision-interactions-with-vocabulary-age.pdf&#34;&gt;&lt;img src=&#34;Screenshot%202022-12-27%20234252.png&#34; width=&#34;650&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kuznetsova, A., Brockhoff, P. B., &amp;amp; Christensen, R. H. B. (2022). &lt;em&gt;Package ’lmerTest’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lüdecke, D. (2022). &lt;em&gt;Package ’sjPlot’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      
            <category>R</category>
      
            <category>visualisation</category>
      
            <category>plotting</category>
      
            <category>deciles</category>
      
            <category>sextiles</category>
      
            <category>sjPlot</category>
      
            <category>linear mixed-effects models</category>
      
            <category>statistics</category>
      
            <category>s</category>
      
      
            <category>R</category>
      
            <category>data visualisation</category>
      
    </item>
    
    <item>
      <title>Why can&#39;t we be friends? Plotting frequentist (lmerTest) and Bayesian (brms) mixed-effects models</title>
      <link>https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/</link>
      <pubDate>Fri, 23 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/index.en_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;


&lt;p&gt;Frequentist and Bayesian statistics are sometimes regarded as fundamentally different philosophies. Indeed, can both methods qualify as philosophies, or is one of them just a pointless ritual? Is frequentist statistics about &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; values only? Are frequentist estimates diametrically opposed to Bayesian posterior distributions? Are confidence intervals and credible intervals irreconcilable? Will R crash if &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34;&gt;&lt;code&gt;brms&lt;/code&gt;&lt;/a&gt; are simultaneously loaded? If only we could fit frequentist and Bayesian models to the same data and plot the results together, we might get a glimpse into these puzzles.&lt;/p&gt;
&lt;p&gt;All the analyses shown below can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;. The combination of the frequentist and the Bayesian estimates in the same plot is achieved using the following custom function from &lt;a href=&#34;https://pablobernabeu.github.io/publication/pablo-bernabeu-2022-phd-thesis&#34;&gt;Bernabeu (2022)&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;visualising-frequentist-and-bayesian-estimates-in-one-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualising frequentist and Bayesian estimates in one plot&lt;/h3&gt;
&lt;p&gt;Both frequentist and Bayesian statistics offer the options of hypothesis testing and parameter estimation (Cumming, 2014; Kruschke &amp;amp; Liddell, 2018; Rouder et al., 2018; Schmalz et al., 2022; Tendeiro &amp;amp; Kiers, 2019, 2022; van Ravenzwaaij &amp;amp; Wagenmakers, 2022). In the statistical analyses conducted by Bernabeu (2022), hypothesis testing was performed within the frequentist framework, whereas parameter estimation was performed within both the frequentist and the Bayesian frameworks (for other examples of the &lt;em&gt;estimation&lt;/em&gt; approach, see Milek et al., 2018; Pregla et al., 2021; Rodríguez-Ferreiro et al., 2020).&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Ffrequentist_bayesian_plot.R%23L3-L179&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Let’s load the function from GitHub and put it to the test.&lt;/p&gt;
&lt;div style=&#34;height: 800px; border: 0.5px dotted grey; padding: 10px; resize: both; overflow: auto;&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Presenting the frequentist and the Bayesian estimates in the same plot. 
# For this purpose, the frequentist results are merged into a plot from 
# brms::mcmc_plot()

source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/main/R_functions/frequentist_bayesian_plot.R&amp;#39;)

# install.packages(&amp;#39;devtools&amp;#39;)
# library(devtools)
# install_version(&amp;#39;tidyverse&amp;#39;, &amp;#39;1.3.1&amp;#39;)  # Due to breaking changes, Version 1.3.1 is required.
# install_version(&amp;#39;ggplot2&amp;#39;, &amp;#39;3.3.5&amp;#39;)  # Due to breaking changes, Version 3.3.5 is required.
library(tidyverse)
library(ggplot2)
library(Cairo)

# Load frequentist coefficients (estimates and confidence intervals)

KR_summary_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/KR_summary_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

confint_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/confint_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

# Below are the default names of the effects
# rownames(KR_summary_semanticpriming_lmerTest$coefficients)
# rownames(confint_semanticpriming_lmerTest)

# Load Bayesian posterior distributions

semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian = 
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian.rds?raw=true&amp;#39;)))

# Below are the default names of the effects
# levels(semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter)


# Reorder the components of interactions in the frequentist results to match 
# with the order present in the Bayesian results.

rownames(KR_summary_semanticpriming_lmerTest$coefficients) =
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;, 
              replacement = &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;, 
              replacement = &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;)

rownames(confint_semanticpriming_lmerTest)  = 
  rownames(confint_semanticpriming_lmerTest) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;, 
              replacement = &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;, 
              replacement = &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;)


# Create a vector containing the names of the effects. This vector will be passed 
# to the plotting function.

new_labels = 
  
  semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter %&amp;gt;% 
  unique %&amp;gt;%
  
  # Remove the default &amp;#39;b_&amp;#39; from the beginning of each effect
  str_remove(&amp;#39;^b_&amp;#39;) %&amp;gt;%
  
  # Put Intercept in parentheses
  str_replace(pattern = &amp;#39;Intercept&amp;#39;, replacement = &amp;#39;(Intercept)&amp;#39;) %&amp;gt;%
  
  # First, adjust names of variables (both in main effects and in interactions)
  str_replace(pattern = &amp;#39;z_target_word_frequency&amp;#39;,
              replacement = &amp;#39;Target-word frequency&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_target_number_syllables&amp;#39;,
              replacement = &amp;#39;Number of target-word syllables&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_word_concreteness_diff&amp;#39;,
              replacement = &amp;#39;Word-concreteness difference&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_cosine_similarity&amp;#39;,
              replacement = &amp;#39;Language-based similarity&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_visual_rating_diff&amp;#39;,
              replacement = &amp;#39;Visual-strength difference&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_attentional_control&amp;#39;,
              replacement = &amp;#39;Attentional control&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_vocabulary_size&amp;#39;,
              replacement = &amp;#39;Vocabulary size&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_participant_gender&amp;#39;,
              replacement = &amp;#39;Gender&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval&amp;#39;,
              replacement = &amp;#39;SOA&amp;#39;) %&amp;gt;%
  # Show acronym in main effect of SOA
  str_replace(pattern = &amp;#39;^SOA$&amp;#39;,
              replacement = &amp;#39;Stimulus onset asynchrony (SOA)&amp;#39;) %&amp;gt;%
  
  # Second, adjust order of effects in interactions. In the output from the model, 
  # the word-level variables of interest (i.e., &amp;#39;z_cosine_similarity&amp;#39; and 
  # &amp;#39;z_visual_rating_diff&amp;#39;) sometimes appeared second in their interactions. For 
  # better consistency, the code below moves those word-level variables (with 
  # their new names) to the first position in their interactions. Note that the 
  # order does not affect the results in any way.
  sub(&amp;#39;(\\w+.*):(Language-based similarity|Visual-strength difference)&amp;#39;, 
      &amp;#39;\\2:\\1&amp;#39;, 
      .) %&amp;gt;%
  
  # Replace colons denoting interactions with times symbols
  str_replace(pattern = &amp;#39;:&amp;#39;, replacement = &amp;#39; &amp;amp;times; &amp;#39;)


# Create plot
plot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian =
  frequentist_bayesian_plot(KR_summary_semanticpriming_lmerTest,
                            confint_semanticpriming_lmerTest,
                            semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian,
                            labels = new_labels, interaction_symbol_x = TRUE,
                            vertical_line_at_x = 0, x_title = &amp;#39;Effect size (&amp;amp;beta;)&amp;#39;,
                            legend_ncol = 1) + 
  theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/index.en_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Frequentist and Bayesian estimates are not so polar opposites, are they? What is more, the larger differences between some estimates are the result of the priors that were set on the corresponding effects. With uninformative priors, the frequentist and the Bayesian estimates are virtually identical.&lt;/p&gt;
&lt;script src=&#39;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Fbayesian_analysis%2Fsemanticpriming_brms_weaklyinformativepriors_exgaussian.R%23L16-L35&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#39;&gt;&lt;/script&gt;
&lt;p&gt;Now it’s time to consider in earnest:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Is frequentist statistics about &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; values only? Are frequentist estimates diametrically opposed to Bayesian posterior distributions? Are confidence intervals and credible intervals irreconcilable? Will R crash if &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34;&gt;&lt;code&gt;brms&lt;/code&gt;&lt;/a&gt; are simultaneously loaded?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Session info&lt;/h3&gt;
&lt;p&gt;If you encounter any blockers while reproducing the above analyses using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;, my current session info may be useful. For instance, the legend of the plot may not show if the latest versions of the &lt;code&gt;ggplot2&lt;/code&gt; and the &lt;code&gt;tidyverse&lt;/code&gt; packages are used. Instead, &lt;code&gt;ggplot2 3.3.5&lt;/code&gt; and &lt;code&gt;tidyverse 1.3.1&lt;/code&gt; should be installed using &lt;code&gt;install_version(&#39;ggplot2&#39;, &#39;3.3.5&#39;)&lt;/code&gt; and &lt;code&gt;install_version(&#39;tidyverse&#39;, &#39;1.3.1&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 22621)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.utf8 
## [2] LC_CTYPE=English_United Kingdom.utf8   
## [3] LC_MONETARY=English_United Kingdom.utf8
## [4] LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] ggtext_0.1.2        Cairo_1.6-0         forcats_1.0.0      
##  [4] stringr_1.5.0       dplyr_1.1.1         purrr_1.0.1        
##  [7] readr_2.1.4         tidyr_1.3.0         tibble_3.2.1       
## [10] ggplot2_3.3.5       tidyverse_1.3.1     knitr_1.42         
## [13] xaringanExtra_0.7.0
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.10      lubridate_1.9.2  digest_0.6.31    utf8_1.2.3      
##  [5] plyr_1.8.8       R6_2.5.1         cellranger_1.1.0 ggridges_0.5.4  
##  [9] backports_1.4.1  reprex_2.0.2     evaluate_0.21    highr_0.10      
## [13] httr_1.4.6       blogdown_1.16    pillar_1.9.0     rlang_1.1.0     
## [17] uuid_1.1-0       readxl_1.4.2     rstudioapi_0.14  jquerylib_0.1.4 
## [21] rmarkdown_2.21   labeling_0.4.2   munsell_0.5.0    gridtext_0.1.5  
## [25] broom_1.0.4      compiler_4.2.3   modelr_0.1.11    xfun_0.38       
## [29] pkgconfig_2.0.3  htmltools_0.5.5  tidyselect_1.2.0 bookdown_0.33.3 
## [33] fansi_1.0.4      crayon_1.5.2     tzdb_0.4.0       dbplyr_2.3.2    
## [37] withr_2.5.0      commonmark_1.9.0 grid_4.2.3       jsonlite_1.8.4  
## [41] gtable_0.3.3     lifecycle_1.0.3  DBI_1.1.3        magrittr_2.0.3  
## [45] scales_1.2.1     cli_3.4.1        stringi_1.7.12   cachem_1.0.7    
## [49] farver_2.1.1     fs_1.6.1         xml2_1.3.3       bslib_0.4.2     
## [53] generics_0.1.3   vctrs_0.6.1      tools_4.2.3      glue_1.6.2      
## [57] markdown_1.5     hms_1.1.3        fastmap_1.1.1    yaml_2.3.7      
## [61] timechange_0.2.0 colorspace_2.1-0 rvest_1.0.3      haven_2.5.2     
## [65] sass_0.4.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cumming, G. (2014). The new statistics: Why and how. &lt;em&gt;Psychological Science, 25&lt;/em&gt;(1), 7–29. &lt;a href=&#34;https://doi.org/10.1177/0956797613504966&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/0956797613504966&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kruschke, J. K., &amp;amp; Liddell, T. M. (2018). The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 25&lt;/em&gt;(1), 178–206.&lt;/p&gt;
&lt;p&gt;Milek, A., Butler, E. A., Tackman, A. M., Kaplan, D. M., Raison, C. L., Sbarra, D. A., Vazire, S., &amp;amp; Mehl, M. R. (2018). “Eavesdropping on happiness” revisited: A pooled, multisample replication of the association between life satisfaction and observed daily conversation quantity and quality. &lt;em&gt;Psychological Science, 29&lt;/em&gt;(9), 1451–1462. &lt;a href=&#34;https://doi.org/10.1177/0956797618774252&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/0956797618774252&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pregla, D., Lissón, P., Vasishth, S., Burchert, F., &amp;amp; Stadie, N. (2021). Variability in sentence comprehension in aphasia in German. &lt;em&gt;Brain and Language, 222&lt;/em&gt;, 105008. &lt;a href=&#34;https://doi.org/10.1016/j.bandl.2021.105008&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.bandl.2021.105008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rodríguez-Ferreiro, J., Aguilera, M., &amp;amp; Davies, R. (2020). Semantic priming and schizotypal personality: Reassessing the link between thought disorder and enhanced spreading of semantic activation. &lt;em&gt;PeerJ, 8&lt;/em&gt;, e9511. &lt;a href=&#34;https://doi.org/10.7717/peerj.9511&#34; class=&#34;uri&#34;&gt;https://doi.org/10.7717/peerj.9511&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rouder, J. N., Haaf, J. M., &amp;amp; Vandekerckhove, J. (2018). Bayesian inference for psychology, part IV: Parameter estimation and Bayes factors. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 25&lt;/em&gt;(1), 102–113. &lt;a href=&#34;https://doi.org/10.3758/s13423-017-1420-7&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13423-017-1420-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Schmalz, X., Biurrun Manresa, J., &amp;amp; Zhang, L. (2021). What is a Bayes factor? &lt;em&gt;Psychological Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1037/met0000421&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000421&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tendeiro, J. N., &amp;amp; Kiers, H. A. L. (2019). A review of issues about null hypothesis Bayesian testing. &lt;em&gt;Psychological Methods, 24&lt;/em&gt;(6), 774–795. &lt;a href=&#34;https://doi.org/10.1037/met0000221&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000221&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tendeiro, J. N., &amp;amp; Kiers, H. A. L. (2022). On the white, the black, and the many shades of gray in between: Our reply to van Ravenzwaaij and Wagenmakers (2021). &lt;em&gt;Psychological Methods, 27&lt;/em&gt;(3), 466–475. &lt;a href=&#34;https://doi.org/10.1037/met0000505&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000505&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;van Ravenzwaaij, D., &amp;amp; Wagenmakers, E.-J. (2022). Advantages masquerading as “issues” in Bayesian hypothesis testing: A commentary on Tendeiro and Kiers (2019). &lt;em&gt;Psychological Methods, 27&lt;/em&gt;(3), 451–465. &lt;a href=&#34;https://doi.org/10.1037/met0000415&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000415&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>statistics</category>
      
            <category>Bayesian statistics</category>
      
            <category>frequentist statistics</category>
      
            <category>linear-mixed effects models</category>
      
            <category>lmerTest</category>
      
            <category>lme4</category>
      
            <category>brms</category>
      
            <category>plotting</category>
      
            <category>data visualisation</category>
      
            <category>s</category>
      
      
            <category>R</category>
      
            <category>statistics</category>
      
    </item>
    
    <item>
      <title>Bayesian workflow: Prior determination, predictive checks and sensitivity analyses</title>
      <link>https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/</link>
      <pubDate>Thu, 22 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/index.en_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
library(ggridges)
library(ggtext)
library(patchwork)
library(papaja)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This post presents a code-through of a Bayesian workflow in R, which can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;. The content is &lt;em&gt;closely&lt;/em&gt; based on &lt;span class=&#34;citation&#34;&gt;Bernabeu (&lt;a href=&#34;#ref-bernabeu2022a&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;, which was in turn based on lots of other references. In addition to those, you may wish to consider &lt;span class=&#34;citation&#34;&gt;Nicenboim et al. (&lt;a href=&#34;#ref-nicenboimInprep&#34;&gt;n.d.&lt;/a&gt;)&lt;/span&gt;, a book in preparation that is already available online (&lt;a href=&#34;https://vasishth.github.io/bayescogsci/book&#34; class=&#34;uri&#34;&gt;https://vasishth.github.io/bayescogsci/book&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In &lt;span class=&#34;citation&#34;&gt;Bernabeu (&lt;a href=&#34;#ref-bernabeu2022a&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;, a Bayesian analysis was performed to complement the estimates that had been obtained in the frequentist analysis. Whereas the goal of the frequentist analysis had been hypothesis testing, for which &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; values were used, the goal of the Bayesian analysis was parameter estimation. Accordingly, we estimated the posterior distribution of every effect, without calculating Bayes factors &lt;span class=&#34;citation&#34;&gt;(for other examples of the same &lt;em&gt;estimation approach&lt;/em&gt;, see &lt;a href=&#34;#ref-milekEavesdroppingHappinessRevisited2018&#34;&gt;Milek et al., 2018&lt;/a&gt;; &lt;a href=&#34;#ref-preglaVariabilitySentenceComprehension2021&#34;&gt;Pregla et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;; for comparisons between estimation and hypothesis testing, see &lt;a href=&#34;#ref-cummingNewStatisticsWhy2014&#34;&gt;Cumming, 2014&lt;/a&gt;; &lt;a href=&#34;#ref-kruschkeBayesianNewStatistics2018&#34;&gt;Kruschke &amp;amp; Liddell, 2018&lt;/a&gt;; &lt;a href=&#34;#ref-rouderBayesianInferencePsychology2018&#34;&gt;Rouder et al., 2018&lt;/a&gt;; &lt;a href=&#34;#ref-schmalzWhatBayesFactor2021&#34;&gt;Schmalz et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-tendeiroReviewIssuesNull2019&#34;&gt;Tendeiro &amp;amp; Kiers, 2019&lt;/a&gt;, &lt;a href=&#34;#ref-tendeiroOnTheWhite2022&#34;&gt;in press&lt;/a&gt;; &lt;a href=&#34;#ref-vanravenzwaaijAdvantagesMasqueradingIssues2021&#34;&gt;van Ravenzwaaij &amp;amp; Wagenmakers, 2021&lt;/a&gt;)&lt;/span&gt;. In the estimation approach, the estimates are interpreted by considering the position of their credible intervals in relation to the expected effect size. That is, the closer an interval is to an effect size of 0, the smaller the effect of that predictor. For instance, an interval that is symmetrically centred on 0 indicates a very small effect, whereas—in comparison—an interval that does not include 0 at all indicates a far larger effect.&lt;/p&gt;
&lt;p&gt;This analysis served two purposes: first, to ascertain the interpretation of the smaller effects—which were identified as unreliable in the power analyses—, and second, to complement the estimates obtained in the frequentist analysis. The latter purpose was pertinent because the frequentist models presented convergence warnings—even though it must be noted that a previous study found that frequentist and Bayesian estimates were similar despite convergence warnings appearing in the frequentist analysis &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;)&lt;/span&gt;. Furthermore, the complementary analysis was pertinent because the frequentist models presented residual errors that deviated from normality—even though mixed-effects models are fairly robust to such a deviation &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kniefViolatingNormalityAssumption2021&#34;&gt;Knief &amp;amp; Forstmeier, 2021&lt;/a&gt;; &lt;a href=&#34;#ref-schielzethRobustnessLinearMixed2020&#34;&gt;Schielzeth et al., 2020&lt;/a&gt;)&lt;/span&gt;. Owing to these precedents, we expected to find broadly similar estimates in the frequentist analyses and in the Bayesian ones. Across studies, each frequentist model has a Bayesian counterpart, with the exception of the secondary analysis performed in Study 2.1 (semantic priming) that included &lt;code&gt;vision-based similarity&lt;/code&gt; as a predictor. The R package ‘brms’, Version 2.17.0, was used for the Bayesian analysis &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34;&gt;Bürkner, 2018&lt;/a&gt;; &lt;a href=&#34;#ref-burknerPackageBrms2022&#34;&gt;Bürkner et al., 2022&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Priors&lt;/h2&gt;
&lt;p&gt;Priors are one of the hardest nuts to crack in Bayesian statistics. First, it can be useful to inspect what priors can be set in the model. Second, it is important to visualise a reasonable set of priors based on the available literature or any other available sources. Third, just before fitting the model, the adequacy of a range of priors should be assessed using prior predictive checks. Fourth, posterior predictive checks were performed to assess the consistency between the observed data and new data predicted by the posterior distributions. Fifth, the influence of the priors on the results should be assessed through a prior sensitivity analysis &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-leeBayesianCognitiveModeling2014&#34;&gt;Lee &amp;amp; Wagenmakers, 2014&lt;/a&gt;; &lt;a href=&#34;#ref-schootBayesianStatisticsModelling2021&#34;&gt;Van de Schoot et al., 2021&lt;/a&gt;; also see &lt;a href=&#34;#ref-bernabeu2022a&#34;&gt;Bernabeu, 2022&lt;/a&gt;; &lt;a href=&#34;#ref-preglaVariabilitySentenceComprehension2021&#34;&gt;Pregla et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34;&gt;Stone et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneInteractionGrammaticallyDistinct2021&#34;&gt;Stone et al., 2021&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-what-priors-can-be-set&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Checking what priors can be set&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;brms::get_prior&lt;/code&gt; function can be used to check what effects in the model can be assigned a prior. The output (see &lt;a href=&#34;http://paul-buerkner.github.io/brms/reference/get_prior.html&#34;&gt;example&lt;/a&gt;) will include the current (perhaps default) prior on each effect.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Fbayesian_analysis%2Fprior_predictive_checks%2Fsemanticpriming_priorpredictivecheck_informativepriors.R%23L28-L100&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;determining-the-priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Determining the priors&lt;/h2&gt;
&lt;p&gt;The priors were established by inspecting the effect sizes obtained in previous studies as well as the effect sizes obtained in our frequentist analyses of the present data (reported in &lt;a href=&#34;https://pablobernabeu.github.io/thesis/study-2.1-semantic-priming.html#study-2.1-semantic-priming&#34;&gt;Studies 2.1, 2.2 and 2.3&lt;/a&gt;). In the first regard, the previous studies that were considered were selected because the experimental paradigms, variables and analytical procedures they had used were similar to those used in our current studies. Specifically, regarding paradigms, we sought studies that implemented: (I) semantic priming with a lexical decision task—as in Study 2.1—, (II) semantic decision—as in Study 2.2—, or (III) lexical decision—as in Study 2.3. Regarding analytical procedures, we sought studies in which both the dependent and the independent variables were &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scored. We found two studies that broadly matched these criteria: &lt;span class=&#34;citation&#34;&gt;Lim et al. (&lt;a href=&#34;#ref-lim2020a&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; (see Table 5 therein) and &lt;span class=&#34;citation&#34;&gt;Pexman &amp;amp; Yap (&lt;a href=&#34;#ref-pexman2018a&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; (see Tables 6 and 7 therein). Out of these studies, &lt;span class=&#34;citation&#34;&gt;Pexman &amp;amp; Yap (&lt;a href=&#34;#ref-pexman2018a&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; contained the variables that were most similar to ours, which included vocabulary size (labelled ‘NAART’) and word frequency.&lt;/p&gt;
&lt;p&gt;Based on both these studies and on the &lt;a href=&#34;https://pablobernabeu.github.io/thesis/study-2.1-semantic-priming.html#study-2.1-semantic-priming&#34;&gt;frequentist analyses&lt;/a&gt;, a range of effect sizes was identified that spanned between β = -0.30 and β = 0.30. This range was centred around 0 as the variables were &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scored. The bounds of this range were determined by the largest effects, which appeared in &lt;span class=&#34;citation&#34;&gt;Pexman &amp;amp; Yap (&lt;a href=&#34;#ref-pexman2018a&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;. Pexman et al. conducted a semantic decision study, and split the data set into abstract and concrete words. The two largest effects they found were—first—a word concreteness effect in the concrete-words analysis of β = -0.41, and—second—a word concreteness effect in the abstract-words analysis of β = 0.20. Unlike Pexman et al., we did not split the data set into abstract and concrete words, but analysed these sets together. Therefore, we averaged between the aforementioned values, obtaining a range between β = -0.30 and β = 0.30.&lt;/p&gt;
&lt;p&gt;In the results of &lt;span class=&#34;citation&#34;&gt;Lim et al. (&lt;a href=&#34;#ref-lim2020a&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Pexman &amp;amp; Yap (&lt;a href=&#34;#ref-pexman2018a&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;, and in our frequentist results, some effects consistently presented a negative polarity (i.e., leading to shorter response times), whereas some other effects were consistently positive. We incorporated the direction of effects into the priors only in cases of large effects that had presented a consistent direction (either positive or negative) in previous studies and in our frequentist analyses in the present studies. These criteria were matched by the following variables: word frequency—with a negative direction, as higher word frequency leads to shorter RTs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-brysbaertImpactWordPrevalence2016&#34;&gt;Brysbaert et al., 2016&lt;/a&gt;; &lt;a href=&#34;#ref-brysbaertWordFrequencyEffect2018a&#34;&gt;Brysbaert et al., 2018&lt;/a&gt;; &lt;a href=&#34;#ref-lim2020a&#34;&gt;Lim et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-mendesPervasiveEffectWord2021&#34;&gt;Mendes &amp;amp; Undorf, 2021&lt;/a&gt;; &lt;a href=&#34;#ref-pexman2018a&#34;&gt;Pexman &amp;amp; Yap, 2018&lt;/a&gt;)&lt;/span&gt;—, number of letters and number of syllables—both with positive directions &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bartonWordlengthEffectReading2014&#34;&gt;Barton et al., 2014&lt;/a&gt;; &lt;a href=&#34;#ref-beyersmannEvidenceEmbeddedWord2020&#34;&gt;Beyersmann et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-pexman2018a&#34;&gt;Pexman &amp;amp; Yap, 2018&lt;/a&gt;)&lt;/span&gt;—, and orthographic Levenshtein distance—with a positive direction &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cerniMotorExpertiseTyping2016&#34;&gt;Cerni et al., 2016&lt;/a&gt;; &lt;a href=&#34;#ref-dijkstraMultilinkComputationalModel2019&#34;&gt;Dijkstra et al., 2019&lt;/a&gt;; &lt;a href=&#34;#ref-kimEffectsLexicalFeatures2018&#34;&gt;Kim et al., 2018&lt;/a&gt;; &lt;a href=&#34;#ref-yarkoniMovingColtheartNew2008&#34;&gt;Yarkoni et al., 2008&lt;/a&gt;)&lt;/span&gt;. We did not incorporate information about the direction of the word concreteness effect, as this effect can follow different directions in abstract and concrete words &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-brysbaert2014a&#34;&gt;Brysbaert et al., 2014&lt;/a&gt;; &lt;a href=&#34;#ref-pexman2018a&#34;&gt;Pexman &amp;amp; Yap, 2018&lt;/a&gt;)&lt;/span&gt;, and we analysed both sets of words together. In conclusion, the four predictors that had directional priors were covariates. All the other predictors had priors centred on 0. Last, as a methodological matter, it is noteworthy that most of the psycholinguistic studies applying Bayesian analysis have not incorporated any directional information in priors &lt;span class=&#34;citation&#34;&gt;(e.g., &lt;a href=&#34;#ref-preglaVariabilitySentenceComprehension2021&#34;&gt;Pregla et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34;&gt;Stone et al., 2020&lt;/a&gt;; cf. &lt;a href=&#34;#ref-stoneInteractionGrammaticallyDistinct2021&#34;&gt;Stone et al., 2021&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;prior-distributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Prior distributions&lt;/h3&gt;
&lt;p&gt;The choice of priors can influence the results in consequential ways. To assess the extent of this influence, &lt;em&gt;prior sensitivity analyses&lt;/em&gt; have been recommended. These analyses are performed by comparing the effect of more and less strict priors—or, in other words, priors varying in their degree of informativeness. The degree of variation is adjusted through the standard deviation, and the means are not varied &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-leeBayesianCognitiveModeling2014&#34;&gt;Lee &amp;amp; Wagenmakers, 2014&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34;&gt;Stone et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-schootBayesianStatisticsModelling2021&#34;&gt;Van de Schoot et al., 2021&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In this way, we compared the results obtained using ‘informative’ priors (&lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; = 0.1), ‘weakly-informative’ priors (&lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; = 0.2) and ‘diffuse’ priors (&lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; = 0.3). These standard deviations were chosen so that around 95% of values in the informative priors would fall within our initial range of effect sizes that spanned from -0.30 to 0.30. All priors are illustrated in the figure below. These priors resembled others from previous psycholinguistic studies &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-preglaVariabilitySentenceComprehension2021&#34;&gt;Pregla et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34;&gt;Stone et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneInteractionGrammaticallyDistinct2021&#34;&gt;Stone et al., 2021&lt;/a&gt;)&lt;/span&gt;. For instance, &lt;span class=&#34;citation&#34;&gt;Stone et al. (&lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; used the following priors: &lt;span class=&#34;math inline&#34;&gt;\(Normal\)&lt;/span&gt;(0, 0.1), &lt;span class=&#34;math inline&#34;&gt;\(Normal\)&lt;/span&gt;(0, 0.3) and &lt;span class=&#34;math inline&#34;&gt;\(Normal\)&lt;/span&gt;(0, 1). The range of standard deviations we used—i.e., 0.1, 0.2 and 0.3—was narrower than those of previous studies because our dependent variable and our predictors were &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scored, resulting in small estimates and small &lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt;s &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-lim2020a&#34;&gt;Lim et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-pexman2018a&#34;&gt;Pexman &amp;amp; Yap, 2018&lt;/a&gt;)&lt;/span&gt;. These priors were used on the fixed effects and on the standard deviation parameters of the fixed effects. For the correlations among the random effects, an &lt;span class=&#34;math inline&#34;&gt;\(LKJ\)&lt;/span&gt;(2) prior was used &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-lewandowskiGeneratingRandomCorrelation2009&#34;&gt;Lewandowski et al., 2009&lt;/a&gt;)&lt;/span&gt;. This is a ‘regularising’ prior, as it assumes that high correlations among random effects are rare &lt;span class=&#34;citation&#34;&gt;(also used in &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34;&gt;Stone et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneInteractionGrammaticallyDistinct2021&#34;&gt;Stone et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-vasishthBayesianDataAnalysis2018&#34;&gt;Vasishth et al., 2018&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set seed number to ensure exact reproducibility 
# of the random distributions
set.seed(123)

# The code below plots all our types of priors. Each distribution 
# contains 10,000 simulations, resulting in 90,000 rows.

# The green vertical rectangle shows the range of plausible effect 
# sizes based on previous studies that applied a similar analysis 
# (Lim et al., 2020, https://doi.org/10.1177/1747021820906566; 
# Pexman &amp;amp; Yap, 2018, https://doi.org/10.1037/xlm0000499) as 
# well as on the frequentist analyses of the current data.

priors = data.frame(
  
  informativeness = 
    as.factor(c(rep(&amp;#39;Informative priors (*SD* = 0.1)&amp;#39;, 30000),
                rep(&amp;#39;Weakly-informative priors (*SD* = 0.2)&amp;#39;, 30000),
                rep(&amp;#39;Diffuse priors (*SD* = 0.3)&amp;#39;, 30000))), 
  
  direction = as.factor(c(rep(&amp;#39;negative&amp;#39;, 10000), 
                          rep(&amp;#39;neutral&amp;#39;, 10000),
                          rep(&amp;#39;positive&amp;#39;, 10000),
                          rep(&amp;#39;negative&amp;#39;, 10000), 
                          rep(&amp;#39;neutral&amp;#39;, 10000),
                          rep(&amp;#39;positive&amp;#39;, 10000),
                          rep(&amp;#39;negative&amp;#39;, 10000), 
                          rep(&amp;#39;neutral&amp;#39;, 10000),
                          rep(&amp;#39;positive&amp;#39;, 10000))),
  
  direction_and_distribution = 
    as.factor(c(rep(&amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.1)&amp;#39;, 10000), 
                rep(&amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.1)&amp;#39;, 10000),
                rep(&amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.1)&amp;#39;, 10000),
                rep(&amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.2)&amp;#39;, 10000),
                rep(&amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.2)&amp;#39;, 10000),
                rep(&amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.2)&amp;#39;, 10000),
                rep(&amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.3)&amp;#39;, 10000), 
                rep(&amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.3)&amp;#39;, 10000),
                rep(&amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.3)&amp;#39;, 10000))),
  
  estimate = c(rnorm(10000, m = -0.1, sd = 0.1),
               rnorm(10000, m = 0, sd = 0.1),
               rnorm(10000, m = 0.1, sd = 0.1),
               rnorm(10000, m = -0.1, sd = 0.2),
               rnorm(10000, m = 0, sd = 0.2),
               rnorm(10000, m = 0.1, sd = 0.2),
               rnorm(10000, m = -0.1, sd = 0.3),
               rnorm(10000, m = 0, sd = 0.3),
               rnorm(10000, m = 0.1, sd = 0.3))
)

# Order factor levels

priors$informativeness = 
  ordered(priors$informativeness, 
          levels = c(&amp;#39;Informative priors (*SD* = 0.1)&amp;#39;, 
                     &amp;#39;Weakly-informative priors (*SD* = 0.2)&amp;#39;, 
                     &amp;#39;Diffuse priors (*SD* = 0.3)&amp;#39;))

priors$direction = 
  ordered(priors$direction, 
          levels = c(&amp;#39;negative&amp;#39;, &amp;#39;neutral&amp;#39;, &amp;#39;positive&amp;#39;))

priors$direction_and_distribution =
  ordered(priors$direction_and_distribution,
          levels = c(&amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.1)&amp;#39;, 
                     &amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.1)&amp;#39;,
                     &amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.1)&amp;#39;,
                     &amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.2)&amp;#39;, 
                     &amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.2)&amp;#39;,
                     &amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.2)&amp;#39;,
                     &amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.3)&amp;#39;, 
                     &amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.3)&amp;#39;,
                     &amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.3)&amp;#39;))


# PLOT zone

colours = c(&amp;#39;#7276A2&amp;#39;, &amp;#39;black&amp;#39;, &amp;#39;#A27272&amp;#39;)
fill_colours = c(&amp;#39;#CCCBE7&amp;#39;, &amp;#39;#D7D7D7&amp;#39;, &amp;#39;#E7CBCB&amp;#39;)

# Initialise plot (`aes` specified separately to allow 
# use of `geom_rect` at the end)
ggplot() +
  
  # Turn to the distributions
  stat_density_ridges(data = priors, 
                      aes(x = estimate, y = direction_and_distribution, 
                          color = direction, fill = direction),
                      geom = &amp;#39;density_ridges_gradient&amp;#39;, alpha = 0.7, 
                      jittered_points = TRUE, quantile_lines = TRUE, 
                      quantiles = c(0.025, 0.975), show.legend = F) +
  scale_color_manual(values = colours) + 
  scale_fill_manual(values = fill_colours) + 
  # Adjust X axis to the random distributions obtained
  scale_x_continuous(limits = c(min(priors$estimate), 
                                max(priors$estimate)), 
                     n.breaks = 6, expand = c(0.04, 0.04)) +
  scale_y_discrete(expand = expansion(add = c(0.18, 1.9))) +
  # Facets containing the three models varying in informativeness
  facet_wrap(vars(informativeness), scales = &amp;#39;free&amp;#39;, dir = &amp;#39;v&amp;#39;) +
  # Vertical line at x = 0
  geom_vline(xintercept = 0, linetype = &amp;#39;dashed&amp;#39;, color = &amp;#39;grey50&amp;#39;) +
  xlab(&amp;#39;Effect size (&amp;amp;beta;)&amp;#39;) + 
  ylab(&amp;#39;Direction of the prior and corresponding distribution&amp;#39;) +
  theme_minimal() +
  theme(axis.title.x = ggtext::element_markdown(size = 12, margin = margin(t = 9)),
        axis.text.x = ggtext::element_markdown(size = 11, margin = margin(t = 4)),
        axis.title.y = ggtext::element_markdown(size = 12, margin = margin(r = 9)),
        axis.text.y = ggtext::element_markdown(lineheight = 1.6, colour = colours),
        strip.background = element_rect(fill = &amp;#39;grey98&amp;#39;, colour = &amp;#39;grey90&amp;#39;,
                                        linetype = &amp;#39;solid&amp;#39;),
        strip.text = element_markdown(size = 11, margin = margin(t = 7, b = 7)),
        panel.spacing.y = unit(9, &amp;#39;pt&amp;#39;), panel.grid.minor = element_blank(), 
        plot.margin = margin(8, 8, 9, 8)
  ) +
  
  # Shaded rectangle containing range of previous effects
  geom_rect(data = data.frame(x = 1), xmin = -0.3, xmax = 0.3, 
            ymin = -Inf, ymax = Inf, fill = &amp;#39;darkgreen&amp;#39;, alpha = .3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/index.en_files/figure-html/bayesian-priors-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Priors used in the three studies. The green vertical rectangle shows the range of plausible effect sizes based on previous studies and on our frequentist analyses. In the informative priors, around 95% of the values fall within the range.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;prior-predictive-checks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Prior predictive checks&lt;/h2&gt;
&lt;p&gt;The adequacy of each of these priors was assessed by performing prior predictive checks, in which we compared the observed data to the predictions of the model &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-schootBayesianStatisticsModelling2021&#34;&gt;Van de Schoot et al., 2021&lt;/a&gt;)&lt;/span&gt;. Furthermore, in these checks we also tested the adequacy of two model-wide distributions: the traditional Gaussian distribution (default in most analyses) and an exponentially modified Gaussian—dubbed ‘ex-Gaussian’—distribution &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-matzkePsychologicalInterpretationExGaussian2009&#34;&gt;Matzke &amp;amp; Wagenmakers, 2009&lt;/a&gt;)&lt;/span&gt;. The ex-Gaussian distribution was considered because the residual errors of the frequentist models were not normally distributed &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-loTransformNotTransform2015&#34;&gt;Lo &amp;amp; Andrews, 2015&lt;/a&gt;)&lt;/span&gt;, and because this distribution was found to be more appropriate than the Gaussian one in a previous, related study &lt;span class=&#34;citation&#34;&gt;(see supplementary materials of &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;)&lt;/span&gt;. The ex-Gaussian distribution had an identity link function, which preserves the interpretability of the coefficients, as opposed to a transformation applied directly to the dependent variable &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-loTransformNotTransform2015&#34;&gt;Lo &amp;amp; Andrews, 2015&lt;/a&gt;)&lt;/span&gt;. The results of these prior predictive checks revealed that the priors were adequate, and that the ex-Gaussian distribution was more appropriate than the Gaussian one, converging with &lt;span class=&#34;citation&#34;&gt;Rodríguez-Ferreiro et al. (&lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. Therefore, the ex-Gaussian distribution was used in the final models.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Fbayesian_analysis%2Fprior_predictive_checks%2Fsemanticpriming_priorpredictivecheck_informativepriors.R%23L105-L235&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;models-with-a-gaussian-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Models with a Gaussian distribution&lt;/h3&gt;
&lt;p&gt;The figures below show the prior predictive checks for the Gaussian models. These plots show the maximum, mean and minimum values of the observed data (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) and those of the predicted distribution (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;, which stands for &lt;em&gt;rep&lt;/em&gt;lications of the outcome). The way of interpreting these plots is by comparing the observed data to the predicted distribution. The specifics of this comparison vary across the three plots. First, in the upper plot, which shows the maximum values, the ideal scenario would show the observed maximum value (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) overlapping with the maximum value of the predicted distribution (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;). Second, in the middle plot, showing the mean values, the ideal scenario would show the observed mean value (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) overlapping with the mean value of the predicted distribution (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;). Last, in the lower plot, which shows the minimum values, the ideal scenario would have the observed minimum value (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) overlapping with the minimum value of the predicted distribution (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;). While the overlap need not be absolute, the closer the observed and the predicted values are on the X axis, the better. As such, the three predictive checks below—corresponding to models that used the default Gaussian distribution—show that the priors fitted the data acceptably but not very well.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_informativepriors.pdf&#34;&gt;See plot on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the Gaussian, informative prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_weaklyinformativepriors.pdf&#34;&gt;See plot on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the Gaussian, weakly-informative prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_diffusepriors.pdf&#34;&gt;See plot on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the Gaussian, diffuse prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;models-with-an-exponentially-modified-gaussian-i.e.-ex-gaussian-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Models with an exponentially-modified Gaussian (i.e., ex-Gaussian) distribution&lt;/h3&gt;
&lt;p&gt;In contrast to the above results, the figures below demonstrate that, when an ex-Gaussian distribution was used, the priors fitted the data far better, which converged with the results of a similar comparison performed by Rodríguez-Ferreiro et al. (2020; see supplementary materials of the latter study).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_informativepriors_exgaussian.pdf&#34;&gt;See plot on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the ex-Gaussian, informative prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_weaklyinformativepriors_exgaussian.pdf&#34;&gt;See plot on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the ex-Gaussian, weakly-informative prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_diffusepriors_exgaussian.pdf&#34;&gt;See plot on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the ex-Gaussian, diffuse prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-predictive-checks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Posterior predictive checks&lt;/h2&gt;
&lt;p&gt;Based on the results from the prior predictive checks, the ex-Gaussian distribution was used in the final models. Next, posterior predictive checks were performed to assess the consistency between the observed data and new data predicted by the posterior distributions &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-schootBayesianStatisticsModelling2021&#34;&gt;Van de Schoot et al., 2021&lt;/a&gt;)&lt;/span&gt;. The figure below presents the posterior predictive checks for the latter models. The interpretation of these plots is simple: the distributions of the observed (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) and the predicted data (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;) should be as similar as possible. As such, the plots below suggest that the results are trustworthy.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Fbayesian_analysis%2Fposterior_predictive_checks%2Fsemanticpriming_posteriorpredictivechecks.R&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/posterior_predictive_checks/plots/semanticpriming_posteriorpredictivechecks_allpriors_exgaussian.pdf&#34;&gt;See plot on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Posterior predictive checks for the (ex-Gaussian) models from the semantic priming study. The observed data (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) and the predicted data (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;) almost entirely overlap with each other, demonstrating a very good fit.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prior-sensitivity-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5. Prior sensitivity analysis&lt;/h2&gt;
&lt;p&gt;In the main analysis, the informative, weakly-informative and diffuse priors were used in separate models. In other words, in each model, all priors had the same degree of informativeness &lt;span class=&#34;citation&#34;&gt;(as done in &lt;a href=&#34;#ref-preglaVariabilitySentenceComprehension2021&#34;&gt;Pregla et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34;&gt;Stone et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneInteractionGrammaticallyDistinct2021&#34;&gt;Stone et al., 2021&lt;/a&gt;)&lt;/span&gt;. In this way, a prior sensitivity analysis was performed to acknowledge the likely influence of the priors on the posterior distributions—that is, on the results &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-leeBayesianCognitiveModeling2014&#34;&gt;Lee &amp;amp; Wagenmakers, 2014&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34;&gt;Stone et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-schootBayesianStatisticsModelling2021&#34;&gt;Van de Schoot et al., 2021&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We’ll first load a &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/R_functions/frequentist_bayesian_plot.R&#34;&gt;custom function (&lt;code&gt;frequentist_bayesian_plot&lt;/code&gt;)&lt;/a&gt; from GitHub.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/main/R_functions/frequentist_bayesian_plot.R&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;height: 800px; border: 0.5px dotted grey; padding: 10px; resize: both; overflow: auto;&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Presenting the frequentist and the Bayesian estimates in the same plot. 
# For this purpose, the frequentist results are merged into a plot from 
# brms::mcmc_plot()

# install.packages(&amp;#39;devtools&amp;#39;)
# library(devtools)
# install_version(&amp;#39;tidyverse&amp;#39;, &amp;#39;1.3.1&amp;#39;)  # Due to breaking changes, Version 1.3.1 is required.
# install_version(&amp;#39;ggplot2&amp;#39;, &amp;#39;5.3.5&amp;#39;)  # Due to breaking changes, Version 5.3.5 is required.
library(tidyverse)
library(ggplot2)
library(Cairo)

# Load frequentist coefficients (estimates and confidence intervals)

KR_summary_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/KR_summary_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

confint_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/confint_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

# Below are the default names of the effects
# rownames(KR_summary_semanticpriming_lmerTest$coefficients)
# rownames(confint_semanticpriming_lmerTest)

# Load Bayesian posterior distributions

semanticpriming_posteriordistributions_informativepriors_exgaussian = 
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_posteriordistributions_informativepriors_exgaussian.rds?raw=true&amp;#39;)))

semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian.rds?raw=true&amp;#39;)))

semanticpriming_posteriordistributions_diffusepriors_exgaussian = 
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_posteriordistributions_diffusepriors_exgaussian.rds?raw=true&amp;#39;)))

# Below are the default names of the effects
# levels(semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter)


# Reorder the components of interactions in the frequentist results to match 
# with the order present in the Bayesian results.

rownames(KR_summary_semanticpriming_lmerTest$coefficients) =
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;, 
              replacement = &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;, 
              replacement = &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;)

rownames(confint_semanticpriming_lmerTest)  = 
  rownames(confint_semanticpriming_lmerTest) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;, 
              replacement = &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;, 
              replacement = &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;)


# Create a vector containing the names of the effects. This vector will be passed 
# to the plotting function.

new_labels = 
  
  semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter %&amp;gt;% 
  unique %&amp;gt;%
  
  # Remove the default &amp;#39;b_&amp;#39; from the beginning of each effect
  str_remove(&amp;#39;^b_&amp;#39;) %&amp;gt;%
  
  # Put Intercept in parentheses
  str_replace(pattern = &amp;#39;Intercept&amp;#39;, replacement = &amp;#39;(Intercept)&amp;#39;) %&amp;gt;%
  
  # First, adjust names of variables (both in main effects and in interactions)
  str_replace(pattern = &amp;#39;z_target_word_frequency&amp;#39;,
              replacement = &amp;#39;Target-word frequency&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_target_number_syllables&amp;#39;,
              replacement = &amp;#39;Number of target-word syllables&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_word_concreteness_diff&amp;#39;,
              replacement = &amp;#39;Word-concreteness difference&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_cosine_similarity&amp;#39;,
              replacement = &amp;#39;Language-based similarity&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_visual_rating_diff&amp;#39;, 
              replacement = &amp;#39;Visual-strength difference&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_attentional_control&amp;#39;,
              replacement = &amp;#39;Attentional control&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_vocabulary_size&amp;#39;,
              replacement = &amp;#39;Vocabulary size&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_participant_gender&amp;#39;,
              replacement = &amp;#39;Gender&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval&amp;#39;,
              replacement = &amp;#39;SOA&amp;#39;) %&amp;gt;%
  # Show acronym in main effect of SOA
  str_replace(pattern = &amp;#39;^SOA$&amp;#39;,
              replacement = &amp;#39;Stimulus onset asynchrony (SOA)&amp;#39;) %&amp;gt;%
  
  # Second, adjust order of effects in interactions. In the output from the model, 
  # the word-level variables of interest (i.e., &amp;#39;z_cosine_similarity&amp;#39; and 
  # &amp;#39;z_visual_rating_diff&amp;#39;) sometimes appeared second in their interactions. For 
  # better consistency, the code below moves those word-level variables (with 
  # their new names) to the first position in their interactions. Note that the 
  # order does not affect the results in any way.
  sub(&amp;#39;(\\w+.*):(Language-based similarity|Visual-strength difference)&amp;#39;, 
      &amp;#39;\\2:\\1&amp;#39;, 
      .) %&amp;gt;%
  
  # Replace colons denoting interactions with times symbols
  str_replace(pattern = &amp;#39;:&amp;#39;, replacement = &amp;#39; &amp;amp;times; &amp;#39;) 


# Create plots, beginning with the informative-prior model

plot_semanticpriming_frequentist_bayesian_plot_informativepriors_exgaussian =
  frequentist_bayesian_plot(KR_summary_semanticpriming_lmerTest,
                            confint_semanticpriming_lmerTest,
                            semanticpriming_posteriordistributions_informativepriors_exgaussian,
                            labels = new_labels, interaction_symbol_x = TRUE,
                            vertical_line_at_x = 0, x_title = &amp;#39;Effect size (&amp;amp;beta;)&amp;#39;, 
                            x_axis_labels = 3, note_frequentist_no_prior = TRUE) +
  ggtitle(&amp;#39;Prior *SD* = 0.1&amp;#39;)

#####

plot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian =
  frequentist_bayesian_plot(KR_summary_semanticpriming_lmerTest,
                            confint_semanticpriming_lmerTest,
                            semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian,
                            labels = new_labels, interaction_symbol_x = TRUE,
                            vertical_line_at_x = 0, x_title = &amp;#39;Effect size (&amp;amp;beta;)&amp;#39;,
                            x_axis_labels = 3, note_frequentist_no_prior = TRUE) +
  ggtitle(&amp;#39;Prior *SD* = 0.2&amp;#39;) +
  theme(axis.text.y = element_blank())

#####

plot_semanticpriming_frequentist_bayesian_plot_diffusepriors_exgaussian =
  frequentist_bayesian_plot(KR_summary_semanticpriming_lmerTest,
                            confint_semanticpriming_lmerTest,
                            semanticpriming_posteriordistributions_diffusepriors_exgaussian,
                            labels = new_labels, interaction_symbol_x = TRUE,
                            vertical_line_at_x = 0, x_title = &amp;#39;Effect size (&amp;amp;beta;)&amp;#39;, 
                            x_axis_labels = 3, note_frequentist_no_prior = TRUE) +
  ggtitle(&amp;#39;Prior *SD* = 0.3&amp;#39;) + 
  theme(axis.text.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The figure below presents the posterior distribution of each effect in each model. The frequentist estimates are also shown to facilitate the comparison.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_semanticpriming_frequentist_bayesian_plot_informativepriors_exgaussian +
    plot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian +
    plot_semanticpriming_frequentist_bayesian_plot_diffusepriors_exgaussian +
    
    plot_layout(ncol = 3, guides = &amp;#39;collect&amp;#39;) &amp;amp; theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/index.en_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Estimates from the frequentist analysis (in red) and from the Bayesian analysis (in blue) for the semantic priming study, in each model. The frequentist means (represented by points) are flanked by 95% confidence intervals. The Bayesian means (represented by vertical lines) are flanked by 95% credible intervals in light blue (in some cases, the interval is occluded by the bar of the mean)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;A blog post on the &lt;a href=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lme4-and-bayesian-brms-mixed-effects-models&#34;&gt;frequentist-Bayesian plots is also available&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Session info&lt;/h3&gt;
&lt;p&gt;If you encounter any blockers while reproduce the above analyses using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;, my current session info may be useful. For instance, the legend of the last plot may not show if the latest versions of the &lt;code&gt;ggplot2&lt;/code&gt; and the &lt;code&gt;tidyverse&lt;/code&gt; packages are used. Instead, &lt;code&gt;ggplot2 3.3.5&lt;/code&gt; and &lt;code&gt;tidyverse 1.3.1&lt;/code&gt; should be installed using &lt;code&gt;install_version(&#39;ggplot2&#39;, &#39;3.3.5&#39;)&lt;/code&gt; and &lt;code&gt;install_version(&#39;tidyverse&#39;, &#39;1.3.1&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 22621)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.utf8 
## [2] LC_CTYPE=English_United Kingdom.utf8   
## [3] LC_MONETARY=English_United Kingdom.utf8
## [4] LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] Cairo_1.6-0         forcats_1.0.0       stringr_1.5.0      
##  [4] purrr_1.0.1         readr_2.1.4         tidyr_1.3.0        
##  [7] tibble_3.2.1        tidyverse_1.3.1     papaja_0.1.1       
## [10] tinylabels_0.2.3    patchwork_1.1.2     ggtext_0.1.2       
## [13] ggridges_0.5.4      ggplot2_3.3.5       dplyr_1.1.1        
## [16] knitr_1.42          xaringanExtra_0.7.0
## 
## loaded via a namespace (and not attached):
##  [1] fs_1.6.1           lubridate_1.9.2    insight_0.19.2     httr_1.4.6        
##  [5] tools_4.2.3        backports_1.4.1    bslib_0.4.2        utf8_1.2.3        
##  [9] R6_2.5.1           DBI_1.1.3          colorspace_2.1-0   withr_2.5.0       
## [13] tidyselect_1.2.0   emmeans_1.8.7      compiler_4.2.3     cli_3.4.1         
## [17] rvest_1.0.3        xml2_1.3.3         sandwich_3.0-2     labeling_0.4.2    
## [21] bookdown_0.33.3    bayestestR_0.13.1  sass_0.4.6         scales_1.2.1      
## [25] mvtnorm_1.1-3      commonmark_1.9.0   digest_0.6.31      rmarkdown_2.21    
## [29] pkgconfig_2.0.3    htmltools_0.5.5    dbplyr_2.3.2       fastmap_1.1.1     
## [33] highr_0.10         rlang_1.1.0        readxl_1.4.2       rstudioapi_0.14   
## [37] jquerylib_0.1.4    farver_2.1.1       generics_0.1.3     zoo_1.8-11        
## [41] jsonlite_1.8.4     magrittr_2.0.3     parameters_0.21.1  Matrix_1.6-1      
## [45] Rcpp_1.0.10        munsell_0.5.0      fansi_1.0.4        lifecycle_1.0.3   
## [49] stringi_1.7.12     multcomp_1.4-23    yaml_2.3.7         MASS_7.3-60       
## [53] plyr_1.8.8         grid_4.2.3         crayon_1.5.2       lattice_0.21-8    
## [57] haven_2.5.2        splines_4.2.3      gridtext_0.1.5     hms_1.1.3         
## [61] pillar_1.9.0       uuid_1.1-0         markdown_1.5       estimability_1.4.1
## [65] effectsize_0.8.3   codetools_0.2-19   reprex_2.0.2       glue_1.6.2        
## [69] evaluate_0.21      blogdown_1.16      modelr_0.1.11      vctrs_0.6.1       
## [73] tzdb_0.4.0         cellranger_1.1.0   gtable_0.3.3       datawizard_0.8.0  
## [77] cachem_1.0.7       xfun_0.38          xtable_1.8-4       broom_1.0.4       
## [81] survival_3.5-7     timechange_0.2.0   TH.data_1.1-1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-bartonWordlengthEffectReading2014&#34; class=&#34;csl-entry&#34;&gt;
Barton, J. J. S., Hanif, H. M., Eklinder Björnström, L., &amp;amp; Hills, C. (2014). The word-length effect in reading: &lt;span&gt;A&lt;/span&gt; review. &lt;em&gt;Cognitive Neuropsychology&lt;/em&gt;, &lt;em&gt;31&lt;/em&gt;(5-6), 378–412. &lt;a href=&#34;https://doi.org/10.1080/02643294.2014.895314&#34;&gt;https://doi.org/10.1080/02643294.2014.895314&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bernabeu2022a&#34; class=&#34;csl-entry&#34;&gt;
Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: &lt;span&gt;Multilevel&lt;/span&gt; analysis and statistical power&lt;/em&gt;. &lt;span&gt;Lancaster University&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-beyersmannEvidenceEmbeddedWord2020&#34; class=&#34;csl-entry&#34;&gt;
Beyersmann, E., Grainger, J., &amp;amp; Taft, M. (2020). Evidence for embedded word length effects in complex nonwords. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;35&lt;/em&gt;(2), 235–245. &lt;a href=&#34;https://doi.org/10.1080/23273798.2019.1659989&#34;&gt;https://doi.org/10.1080/23273798.2019.1659989&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brysbaertWordFrequencyEffect2018a&#34; class=&#34;csl-entry&#34;&gt;
Brysbaert, M., Mandera, P., &amp;amp; Keuleers, E. (2018). The word frequency effect in word processing: &lt;span&gt;An&lt;/span&gt; updated review. &lt;em&gt;Current Directions in Psychological Science&lt;/em&gt;, &lt;em&gt;27&lt;/em&gt;(1), 45–50. &lt;a href=&#34;https://doi.org/10.1177/0963721417727521&#34;&gt;https://doi.org/10.1177/0963721417727521&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brysbaertImpactWordPrevalence2016&#34; class=&#34;csl-entry&#34;&gt;
Brysbaert, M., Stevens, M., Mandera, P., &amp;amp; Keuleers, E. (2016). The impact of word prevalence on lexical decision times: &lt;span&gt;Evidence&lt;/span&gt; from the &lt;span&gt;Dutch Lexicon Project&lt;/span&gt; 2. &lt;em&gt;Journal of Experimental Psychology: Human Perception and Performance&lt;/em&gt;, &lt;em&gt;42&lt;/em&gt;(3), 441–458. &lt;a href=&#34;https://doi.org/10.1037/xhp0000159&#34;&gt;https://doi.org/10.1037/xhp0000159&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brysbaert2014a&#34; class=&#34;csl-entry&#34;&gt;
Brysbaert, M., Warriner, A. B., &amp;amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known &lt;span&gt;English&lt;/span&gt; word lemmas. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;46&lt;/em&gt;, 904–911. &lt;a href=&#34;https://doi.org/10.3758/s13428-013-0403-5&#34;&gt;https://doi.org/10.3758/s13428-013-0403-5&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://journal.r-project.org/archive/2018/RJ-2018-017/index.html&#34;&gt;https://journal.r-project.org/archive/2018/RJ-2018-017/index.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerPackageBrms2022&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C., Gabry, J., Weber, S., Johnson, A., Modrak, M., Badr, H. S., Weber, F., Ben-Shachar, M. S., &amp;amp; Rabel, H. (2022). &lt;em&gt;Package ’&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;’&lt;/em&gt;. &lt;span&gt;CRAN&lt;/span&gt;. &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34;&gt;https://cran.r-project.org/web/packages/brms/brms.pdf&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cerniMotorExpertiseTyping2016&#34; class=&#34;csl-entry&#34;&gt;
Cerni, T., Velay, J.-L., Alario, F.-X., Vaugoyeau, M., &amp;amp; Longcamp, M. (2016). Motor expertise for typing impacts lexical decision performance. &lt;em&gt;Trends in Neuroscience and Education&lt;/em&gt;, &lt;em&gt;5&lt;/em&gt;(3), 130–138. &lt;a href=&#34;https://doi.org/10.1016/j.tine.2016.07.007&#34;&gt;https://doi.org/10.1016/j.tine.2016.07.007&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cummingNewStatisticsWhy2014&#34; class=&#34;csl-entry&#34;&gt;
Cumming, G. (2014). The new statistics: &lt;span&gt;Why&lt;/span&gt; and how. &lt;em&gt;Psychological Science&lt;/em&gt;, &lt;em&gt;25&lt;/em&gt;(1), 7–29. &lt;a href=&#34;https://doi.org/10.1177/0956797613504966&#34;&gt;https://doi.org/10.1177/0956797613504966&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-dijkstraMultilinkComputationalModel2019&#34; class=&#34;csl-entry&#34;&gt;
Dijkstra, T., Wahl, A., Buytenhuijs, F., Halem, N. V., Al-Jibouri, Z., Korte, M. D., &amp;amp; Rekké, S. (2019). Multilink: &lt;span&gt;A&lt;/span&gt; computational model for bilingual word recognition and word translation. &lt;em&gt;Bilingualism: Language and Cognition&lt;/em&gt;, &lt;em&gt;22&lt;/em&gt;(4), 657–679. &lt;a href=&#34;https://doi.org/10.1017/S1366728918000287&#34;&gt;https://doi.org/10.1017/S1366728918000287&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kimEffectsLexicalFeatures2018&#34; class=&#34;csl-entry&#34;&gt;
Kim, M., Crossley, S. A., &amp;amp; Skalicky, S. (2018). Effects of lexical features, textual properties, and individual differences on word processing times during second language reading comprehension. &lt;em&gt;Reading and Writing&lt;/em&gt;, &lt;em&gt;31&lt;/em&gt;(5), 1155–1180. &lt;a href=&#34;https://doi.org/10.1007/s11145-018-9833-x&#34;&gt;https://doi.org/10.1007/s11145-018-9833-x&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kniefViolatingNormalityAssumption2021&#34; class=&#34;csl-entry&#34;&gt;
Knief, U., &amp;amp; Forstmeier, W. (2021). Violating the normality assumption may be the lesser of two evils. &lt;em&gt;Behavior Research Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3758/s13428-021-01587-5&#34;&gt;https://doi.org/10.3758/s13428-021-01587-5&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeBayesianNewStatistics2018&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K., &amp;amp; Liddell, T. M. (2018). The &lt;span&gt;Bayesian New Statistics&lt;/span&gt;: &lt;span&gt;Hypothesis&lt;/span&gt; testing, estimation, meta-analysis, and power analysis from a &lt;span&gt;Bayesian&lt;/span&gt; perspective. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, &lt;em&gt;25&lt;/em&gt;(1), 178–206. &lt;a href=&#34;https://doi.org/10.3758/s13423-016-1221-4&#34;&gt;https://doi.org/10.3758/s13423-016-1221-4&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-leeBayesianCognitiveModeling2014&#34; class=&#34;csl-entry&#34;&gt;
Lee, M. D., &amp;amp; Wagenmakers, E.-J. (2014). &lt;em&gt;Bayesian cognitive modeling: &lt;span&gt;A&lt;/span&gt; practical course&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/CBO9781139087759&#34;&gt;https://doi.org/10.1017/CBO9781139087759&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lewandowskiGeneratingRandomCorrelation2009&#34; class=&#34;csl-entry&#34;&gt;
Lewandowski, D., Kurowicka, D., &amp;amp; Joe, H. (2009). Generating random correlation matrices based on vines and extended onion method. &lt;em&gt;Journal of Multivariate Analysis&lt;/em&gt;, &lt;em&gt;100&lt;/em&gt;(9), 1989–2001. &lt;a href=&#34;https://doi.org/10.1016/j.jmva.2009.04.008&#34;&gt;https://doi.org/10.1016/j.jmva.2009.04.008&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lim2020a&#34; class=&#34;csl-entry&#34;&gt;
Lim, R. Y., Yap, M. J., &amp;amp; Tse, C.-S. (2020). Individual differences in &lt;span&gt;Cantonese Chinese&lt;/span&gt; word recognition: &lt;span&gt;Insights&lt;/span&gt; from the &lt;span&gt;Chinese Lexicon Project&lt;/span&gt;. &lt;em&gt;Quarterly Journal of Experimental Psychology&lt;/em&gt;, &lt;em&gt;73&lt;/em&gt;(4), 504–518. &lt;a href=&#34;https://doi.org/10.1177/1747021820906566&#34;&gt;https://doi.org/10.1177/1747021820906566&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-loTransformNotTransform2015&#34; class=&#34;csl-entry&#34;&gt;
Lo, S., &amp;amp; Andrews, S. (2015). To transform or not to transform: Using generalized linear mixed models to analyse reaction time data. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;, 1171. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2015.01171&#34;&gt;https://doi.org/10.3389/fpsyg.2015.01171&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-matzkePsychologicalInterpretationExGaussian2009&#34; class=&#34;csl-entry&#34;&gt;
Matzke, D., &amp;amp; Wagenmakers, E.-J. (2009). Psychological interpretation of the ex-&lt;span&gt;Gaussian&lt;/span&gt; and shifted &lt;span&gt;Wald&lt;/span&gt; parameters: &lt;span&gt;A&lt;/span&gt; diffusion model analysis. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, &lt;em&gt;16&lt;/em&gt;(5), 798–817. &lt;a href=&#34;https://doi.org/10.3758/PBR.16.5.798&#34;&gt;https://doi.org/10.3758/PBR.16.5.798&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mendesPervasiveEffectWord2021&#34; class=&#34;csl-entry&#34;&gt;
Mendes, P. S., &amp;amp; Undorf, M. (2021). On the pervasive effect of word frequency in metamemory. &lt;em&gt;Quarterly Journal of Experimental Psychology&lt;/em&gt;, 17470218211053329. &lt;a href=&#34;https://doi.org/10.1177/17470218211053329&#34;&gt;https://doi.org/10.1177/17470218211053329&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-milekEavesdroppingHappinessRevisited2018&#34; class=&#34;csl-entry&#34;&gt;
Milek, A., Butler, E. A., Tackman, A. M., Kaplan, D. M., Raison, C. L., Sbarra, D. A., Vazire, S., &amp;amp; Mehl, M. R. (2018). &lt;span&gt;“&lt;span&gt;Eavesdropping&lt;/span&gt; on happiness”&lt;/span&gt; revisited: &lt;span&gt;A&lt;/span&gt; pooled, multisample replication of the association between life satisfaction and observed daily conversation quantity and quality. &lt;em&gt;Psychological Science&lt;/em&gt;, &lt;em&gt;29&lt;/em&gt;(9), 1451–1462. &lt;a href=&#34;https://doi.org/10.1177/0956797618774252&#34;&gt;https://doi.org/10.1177/0956797618774252&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-nicenboimInprep&#34; class=&#34;csl-entry&#34;&gt;
Nicenboim, B., Schad, D., &amp;amp; Vasishth, S. (n.d.). &lt;em&gt;An introduction to &lt;span&gt;Bayesian&lt;/span&gt; data analysis for cognitive science&lt;/em&gt;. &lt;span&gt;Chapman and Hall/CRC Statistics in the Social and Behavioral Sciences Series&lt;/span&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-pexman2018a&#34; class=&#34;csl-entry&#34;&gt;
Pexman, P. M., &amp;amp; Yap, M. J. (2018). Individual differences in semantic processing: &lt;span&gt;Insights&lt;/span&gt; from the &lt;span&gt;Calgary&lt;/span&gt; semantic decision project. &lt;em&gt;Journal of Experimental Psychology: Learning, Memory, and Cognition&lt;/em&gt;, &lt;em&gt;44&lt;/em&gt;(7), 1091–1112. &lt;a href=&#34;https://doi.org/10.1037/xlm0000499&#34;&gt;https://doi.org/10.1037/xlm0000499&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-preglaVariabilitySentenceComprehension2021&#34; class=&#34;csl-entry&#34;&gt;
Pregla, D., Lissón, P., Vasishth, S., Burchert, F., &amp;amp; Stadie, N. (2021). Variability in sentence comprehension in aphasia in &lt;span&gt;German&lt;/span&gt;. &lt;em&gt;Brain and Language&lt;/em&gt;, &lt;em&gt;222&lt;/em&gt;, 105008. &lt;a href=&#34;https://doi.org/10.1016/j.bandl.2021.105008&#34;&gt;https://doi.org/10.1016/j.bandl.2021.105008&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34; class=&#34;csl-entry&#34;&gt;
Rodríguez-Ferreiro, J., Aguilera, M., &amp;amp; Davies, R. (2020). Semantic priming and schizotypal personality: Reassessing the link between thought disorder and enhanced spreading of semantic activation. &lt;em&gt;PeerJ&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;, e9511. &lt;a href=&#34;https://doi.org/10.7717/peerj.9511&#34;&gt;https://doi.org/10.7717/peerj.9511&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rouderBayesianInferencePsychology2018&#34; class=&#34;csl-entry&#34;&gt;
Rouder, J. N., Haaf, J. M., &amp;amp; Vandekerckhove, J. (2018). Bayesian inference for psychology, part &lt;span&gt;IV&lt;/span&gt;: &lt;span&gt;Parameter&lt;/span&gt; estimation and &lt;span&gt;Bayes&lt;/span&gt; factors. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, &lt;em&gt;25&lt;/em&gt;(1), 102–113. &lt;a href=&#34;https://doi.org/10.3758/s13423-017-1420-7&#34;&gt;https://doi.org/10.3758/s13423-017-1420-7&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schielzethRobustnessLinearMixed2020&#34; class=&#34;csl-entry&#34;&gt;
Schielzeth, H., Dingemanse, N. J., Nakagawa, S., Westneat, D. F., Allegue, H., Teplitsky, C., Réale, D., Dochtermann, N. A., Garamszegi, L. Z., &amp;amp; Araya‐Ajoy, Y. G. (2020). Robustness of linear mixed‐effects models to violations of distributional assumptions. &lt;em&gt;Methods in Ecology and Evolution&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt;(9), 1141–1152. &lt;a href=&#34;https://doi.org/10.1111/2041-210X.13434&#34;&gt;https://doi.org/10.1111/2041-210X.13434&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schmalzWhatBayesFactor2021&#34; class=&#34;csl-entry&#34;&gt;
Schmalz, X., Biurrun Manresa, J., &amp;amp; Zhang, L. (2021). What is a &lt;span&gt;Bayes&lt;/span&gt; factor? &lt;em&gt;Psychological Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1037/met0000421&#34;&gt;https://doi.org/10.1037/met0000421&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-stoneEffectDecayLexical2020&#34; class=&#34;csl-entry&#34;&gt;
Stone, K., Malsburg, T. von der, &amp;amp; Vasishth, S. (2020). The effect of decay and lexical uncertainty on processing long-distance dependencies in reading. &lt;em&gt;PeerJ&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;, e10438. &lt;a href=&#34;https://doi.org/10.7717/peerj.10438&#34;&gt;https://doi.org/10.7717/peerj.10438&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-stoneInteractionGrammaticallyDistinct2021&#34; class=&#34;csl-entry&#34;&gt;
Stone, K., Veríssimo, J., Schad, D. J., Oltrogge, E., Vasishth, S., &amp;amp; Lago, S. (2021). The interaction of grammatically distinct agreement dependencies in predictive processing. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;36&lt;/em&gt;(9), 1159–1179. &lt;a href=&#34;https://doi.org/10.1080/23273798.2021.1921816&#34;&gt;https://doi.org/10.1080/23273798.2021.1921816&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-tendeiroReviewIssuesNull2019&#34; class=&#34;csl-entry&#34;&gt;
Tendeiro, J. N., &amp;amp; Kiers, H. A. L. (2019). A review of issues about null hypothesis &lt;span&gt;Bayesian&lt;/span&gt; testing. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;24&lt;/em&gt;(6), 774–795. &lt;a href=&#34;https://doi.org/10.1037/met0000221&#34;&gt;https://doi.org/10.1037/met0000221&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-tendeiroOnTheWhite2022&#34; class=&#34;csl-entry&#34;&gt;
Tendeiro, J. N., &amp;amp; Kiers, H. A. L. (in press). On the white, the black, and the many shades of gray in between: &lt;span&gt;Our&lt;/span&gt; reply to van &lt;span&gt;Ravenzwaaij&lt;/span&gt; and &lt;span&gt;Wagenmakers&lt;/span&gt; (2021). &lt;em&gt;Psychological Methods&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-schootBayesianStatisticsModelling2021&#34; class=&#34;csl-entry&#34;&gt;
Van de Schoot, R., Depaoli, S., Gelman, A., King, R., Kramer, B., Märtens, K., Tadesse, M. G., Vannucci, M., Willemsen, J., &amp;amp; Yau, C. (2021). Bayesian statistics and modelling. &lt;em&gt;Nature Reviews Methods Primers&lt;/em&gt;, &lt;em&gt;1&lt;/em&gt;, 3. &lt;a href=&#34;https://doi.org/10.1038/s43586-020-00003-0&#34;&gt;https://doi.org/10.1038/s43586-020-00003-0&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vanravenzwaaijAdvantagesMasqueradingIssues2021&#34; class=&#34;csl-entry&#34;&gt;
van Ravenzwaaij, D., &amp;amp; Wagenmakers, E.-J. (2021). Advantages masquerading as &lt;span&gt;“issues”&lt;/span&gt; in &lt;span&gt;Bayesian&lt;/span&gt; hypothesis testing: &lt;span&gt;A&lt;/span&gt; commentary on &lt;span&gt;Tendeiro&lt;/span&gt; and &lt;span&gt;Kiers&lt;/span&gt; (2019). &lt;em&gt;Psychological Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1037/met0000415&#34;&gt;https://doi.org/10.1037/met0000415&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vasishthBayesianDataAnalysis2018&#34; class=&#34;csl-entry&#34;&gt;
Vasishth, S., Nicenboim, B., Beckman, M. E., Li, F., &amp;amp; Kong, E. J. (2018). Bayesian data analysis in the phonetic sciences: &lt;span&gt;A&lt;/span&gt; tutorial introduction. &lt;em&gt;Journal of Phonetics&lt;/em&gt;, &lt;em&gt;71&lt;/em&gt;, 147–161. &lt;a href=&#34;https://doi.org/10.1016/j.wocn.2018.07.008&#34;&gt;https://doi.org/10.1016/j.wocn.2018.07.008&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-yarkoniMovingColtheartNew2008&#34; class=&#34;csl-entry&#34;&gt;
Yarkoni, T., Balota, D., &amp;amp; Yap, M. J. (2008). Moving beyond &lt;span&gt;Coltheart&lt;/span&gt;’s &lt;span&gt;N&lt;/span&gt;: &lt;span&gt;A&lt;/span&gt; new measure of orthographic similarity. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, &lt;em&gt;15&lt;/em&gt;(5), 971–979. &lt;a href=&#34;https://doi.org/10.3758/PBR.15.5.971&#34;&gt;https://doi.org/10.3758/PBR.15.5.971&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      
            <category>Bayesian statistics</category>
      
            <category>linear mixed-effects models</category>
      
            <category>priors</category>
      
            <category>predictive checks</category>
      
            <category>sensitivity analysis</category>
      
            <category>R</category>
      
            <category>visualisation</category>
      
            <category>brms</category>
      
            <category>s</category>
      
      
            <category>R</category>
      
            <category>statistics</category>
      
    </item>
    
    <item>
      <title>Avoiding (R) Markdown knitting errors using knit_deleting_service_files()</title>
      <link>https://pablobernabeu.github.io/2021/avoiding-knitting-errors-in-r-markdown-using-knit-deleting-service-files/</link>
      <pubDate>Fri, 17 Dec 2021 21:46:06 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2021/avoiding-knitting-errors-in-r-markdown-using-knit-deleting-service-files/</guid>
      <description>


&lt;p&gt;The function &lt;a href=&#34;https://github.com/pablobernabeu/knit_deleting_service_files/blob/main/knit_deleting_service_files.R&#34;&gt;&lt;code&gt;knit_deleting_service_files()&lt;/code&gt;&lt;/a&gt; helps avoid (R) Markdown &lt;a href=&#34;https://pablobernabeu.github.io/2021/tackling-knitting-errors-in-r-markdown/&#34;&gt;knitting errors&lt;/a&gt; caused by files and folders remaining from previous knittings (e.g., manuscript.tex, ZHJhZnQtYXBhLlJtZA==.Rmd, manuscript.synctex.gz). The only obligatory argument for this function is the name of a .Rmd or .md file. The optional argument is a path to a directory containing this file.&lt;/p&gt;
&lt;p&gt;The function first offers deleting potential service files and folders, for which the user’s approval is requested in the console (see screenshot below). Next, the document is knitted. Last, the function offers deleting potential service files and folders again.&lt;/p&gt;
&lt;p&gt;NOTE: The deletions, if accepted, are irreversible as they are made through &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/base/html/unlink.html&#34;&gt;&lt;code&gt;unlink()&lt;/code&gt;&lt;/a&gt;. Therefore, our familiar adage truly applies: this function comes with ABSOLUTELY NO WARRANTY. Please ensure you understand the &lt;a href=&#34;https://github.com/pablobernabeu/knit_deleting_service_files/blob/main/knit_deleting_service_files.R&#34;&gt;source code&lt;/a&gt; before using the function.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;images/Screenshot%202021-12-18%20at%2020.27.49.png&#39; alt=&#39;Screenshot of the function in use&#39; style=&#39;margin-top:2px; margin-bottom:-2px;&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size:90%; color: #899499;&#34;&gt;Screenshot of the function in use.&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;the-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The function&lt;/h2&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fknit_deleting_service_files%2Fblob%2Fmain%2Fknit_deleting_service_files.R&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
</description>
      
            <category>s</category>
      
            <category>R Markdown</category>
      
            <category>papaja</category>
      
            <category>render</category>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Walking the line between reproducibility and efficiency in R Markdown: Three methods</title>
      <link>https://pablobernabeu.github.io/2021/walking-the-line-between-reproducibility-and-efficiency-in-r-markdown-three-methods/</link>
      <pubDate>Sun, 14 Nov 2021 00:10:08 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2021/walking-the-line-between-reproducibility-and-efficiency-in-r-markdown-three-methods/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2021/walking-the-line-between-reproducibility-and-efficiency-in-r-markdown-three-methods/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;As technology and research methods advance, the data sets tend to be larger and the methods more exhaustive. Consequently, the analyses take longer to run. This poses a challenge when the results are to be presented using &lt;a href=&#34;https://rmarkdown.rstudio.com/index.html&#34;&gt;R Markdown&lt;/a&gt;. One has to balance reproducibility and efficiency. On the one hand, it is desirable to keep the R Markdown document as self-contained as possible, so that those who may later examine the document can easily test and edit the code. On the other hand, it would be inefficient to create a document that is very slow to run or very long. The context of the task will determine how how time-consuming and long the code in an Rmd file can be. For instance, one could decide that the knitting can take up to 15 minutes, and each code chunk can span up to 30 lines.&lt;/p&gt;
&lt;p&gt;Several methods can be used in each document to accommodate different types of code. Three methods are presented below, ordered from easier-to-reproduce to easier-to-knit.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;For &lt;strong&gt;fast- and concise-enough code:&lt;/strong&gt; Provide the original code in the Rmd file. The code is run as the document is knitted. Example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; nrow(myData)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For &lt;strong&gt;fast-enough but very long code:&lt;/strong&gt; Store the code in a separate script and &lt;code&gt;source&lt;/code&gt; it in the Rmd file. The code is run as the document is knitted. Example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; source(&amp;#39;analysis/model_diagnostics.R&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For &lt;strong&gt;very slow and/or long code:&lt;/strong&gt; Store the code in a separate script and run it prior to knitting the Rmd file, so that the output from the code (e.g., a model, a plot) is saved and can be read into the Rmd. Example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; model_1 = readRDS(&amp;#39;results/model_1.rds&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Importantly, even the third method allows the reproducibility of the code. It just requires a bit of additional documentation to ensure that the end user can also access the script in which the result was produced (e.g., ‘analysis/model_1.R’).&lt;/p&gt;
</description>
      
            <category>s</category>
      
            <category>R</category>
      
            <category>R Markdown</category>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Tackling knitting errors in R Markdown</title>
      <link>https://pablobernabeu.github.io/2021/tackling-knitting-errors-in-r-markdown/</link>
      <pubDate>Sat, 13 Nov 2021 18:11:22 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2021/tackling-knitting-errors-in-r-markdown/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2021/tackling-knitting-errors-in-r-markdown/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;When knitting an R Markdown document after the first time, errors may sometimes appear. Three tips are recommended below.&lt;/p&gt;
&lt;div id=&#34;close-pdf-reader-window&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Close PDF reader window&lt;/h2&gt;
&lt;p&gt;When the document is knitted through the ‘Knit’ button, a PDF reader window opens to present the result. Closing this window can help resolve errors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;delete-service-files&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Delete service files&lt;/h2&gt;
&lt;p&gt;Every time the Rmd is knitted, some service files are created. Some of these files have the ‘.tex’ extension (e.g., index.tex, Appendix-B.tex), whereas others do not (e.g., index.log, ZHJhZnQtYXBhLlJtZA==.Rmd, index.synctex.gz). Deleting these files can help resolve errors (see a &lt;a href=&#34;https://pablobernabeu.github.io/2021/avoiding-knitting-errors-in-r-markdown-using-knit-deleting-service-files/&#34;&gt;possible function&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;delete-code-chunks-related-to-the-appendices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Delete code chunks related to the appendices&lt;/h2&gt;
&lt;p&gt;When knitting &lt;a href=&#34;https://github.com/crsh/papaja&#34;&gt;papaja&lt;/a&gt; documents containing any appendices, some code chunks related to the appendices will automatically appear at the end of the primary Rmd file—e.g.:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```{r echo = FALSE, results = &amp;#39;asis&amp;#39;, cache = FALSE}
papaja::render_appendix(&amp;#39;Appendix-A.Rmd&amp;#39;)
```&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Normally, these code chunks are automatically removed as the knitting finishes. However, if the chunks remain in place, deleting them manually can help resolve errors.&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>s</category>
      
            <category>R</category>
      
            <category>R Markdown</category>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Parallelizing simr::powercurve() in R</title>
      <link>https://pablobernabeu.github.io/2021/parallelizing-simr-powercurve/</link>
      <pubDate>Fri, 23 Jul 2021 16:46:54 +0100</pubDate>
      
      <guid>https://pablobernabeu.github.io/2021/parallelizing-simr-powercurve/</guid>
      <description>


&lt;p&gt;The &lt;code&gt;powercurve&lt;/code&gt; function from the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/simr/simr.pdf&#34;&gt;‘simr’&lt;/a&gt; (Green &amp;amp; MacLeod, 2016) can incur very long running times when the method used for the calculation of &lt;em&gt;p&lt;/em&gt; values is Kenward-Roger or Satterthwaite (see Luke, 2017). Here I suggest three ways for cutting down this time.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Where possible, use a high-performance (or high-end) computing cluster. This removes the need to use personal computers for these long jobs.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In case you’re using the &lt;code&gt;fixed()&lt;/code&gt; parameter of the &lt;code&gt;powercurve&lt;/code&gt; function, and calculating the power for different effects, run these at the same time (‘in parallel’) on different machines, rather than one after another.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Parallelize&lt;/em&gt; the &lt;code&gt;breaks&lt;/code&gt; argument. The &lt;code&gt;breaks&lt;/code&gt; argument of the &lt;code&gt;powercurve&lt;/code&gt; function allows the calculation of power for different levels of the grouping factor passed to &lt;code&gt;along&lt;/code&gt;. Some grouping factors are &lt;em&gt;participant&lt;/em&gt;, &lt;em&gt;trial&lt;/em&gt; and &lt;em&gt;item&lt;/em&gt;. The &lt;code&gt;breaks&lt;/code&gt; argument sets the different sample sizes for which power will be calculated. Parallelizing &lt;code&gt;breaks&lt;/code&gt; is done by running each number of levels in a separate function. When each has been run and saved, they are &lt;code&gt;c&lt;/code&gt;ombined to allow the plotting. This procedure is demonstrated below.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;parallelizing-breaks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parallelizing &lt;code&gt;breaks&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Let’s do a minimal example using a toy &lt;code&gt;lmer&lt;/code&gt; model. A power curve will be created for the fixed effect of &lt;code&gt;x&lt;/code&gt; along different sample sizes of the grouping factor &lt;code&gt;g&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Notice that the six sections of the power curve below are serially arranged, one after another. In contrast, to enable parallel processing, each power curve would be placed in a single script, and they would all be run at the same time.&lt;/p&gt;
&lt;p&gt;Although the power curves below run in a few minutes, the settings that are often used (e.g., a larger model; &lt;code&gt;fixed(&#39;x&#39;, &#39;sa&#39;)&lt;/code&gt; instead of &lt;code&gt;fixed(&#39;x&#39;)&lt;/code&gt;; &lt;code&gt;nsim = 500&lt;/code&gt; instead of &lt;code&gt;nsim = 50&lt;/code&gt;) take far longer. That is where parallel processing becomes useful.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
library(simr)

# Toy model with data from &amp;#39;simr&amp;#39; package
fm = lmer(y ~ x + (x | g), data = simdata)

# Extend sample size of `g`
fm_extended_g = extend(fm, along = &amp;#39;g&amp;#39;, n = 12)

# Parallelize `breaks` by running each number of levels in a separate function.

# 4 levels of g
pwcurve_4g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 4, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 6 levels of g
pwcurve_6g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 6, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 8 levels of g
pwcurve_8g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 8, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 10 levels of g
pwcurve_10g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 10, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 12 levels of g
pwcurve_12g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 12, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Having saved each section of the power curve, we must now combine them to be able to plot them together (if you wish to automatise this procedure, consider &lt;a href=&#34;https://github.com/pablobernabeu/Language-and-vision-in-conceptual-processing-Multilevel-analysis-and-statistical-power/blob/main/R_functions/combine_powercurve_chunks.R&#34;&gt;this function&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a destination object using any of the power curves above.
all_pwcurve = pwcurve_4g

# Combine results
all_pwcurve$ps = c(pwcurve_4g$ps[1], pwcurve_6g$ps[1], pwcurve_8g$ps[1], 
                   pwcurve_10g$ps[1], pwcurve_12g$ps[1])

# Combine the different numbers of levels.
all_pwcurve$xval = c(pwcurve_4g$nlevels, pwcurve_6g$nlevels, pwcurve_8g$nlevels, 
                     pwcurve_10g$nlevels, pwcurve_12g$nlevels)

print(all_pwcurve)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Power for predictor &amp;#39;x&amp;#39;, (95% confidence interval),
## by number of levels in g:
##       4: 46.00% (31.81, 60.68) - 40 rows
##       6: 74.00% (59.66, 85.37) - 60 rows
##       8: 92.00% (80.77, 97.78) - 80 rows
##      10: 98.00% (89.35, 99.95) - 100 rows
##      12: 100.0% (92.89, 100.0) - 120 rows
## 
## Time elapsed: 0 h 0 m 7 s&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(all_pwcurve, xlab = &amp;#39;Levels of g&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2021/parallelizing-simr-powercurve/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For reproducibility purposes
sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 22621)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.utf8 
## [2] LC_CTYPE=English_United States.utf8   
## [3] LC_MONETARY=English_United States.utf8
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] simr_1.0.7          lme4_1.1-32         Matrix_1.5-3       
## [4] knitr_1.42          xaringanExtra_0.7.0
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.2.0 xfun_0.38        bslib_0.4.2      purrr_1.0.1     
##  [5] splines_4.2.3    lattice_0.20-45  carData_3.0-5    vctrs_0.6.1     
##  [9] generics_0.1.3   htmltools_0.5.5  yaml_2.3.7       mgcv_1.8-42     
## [13] utf8_1.2.3       rlang_1.1.0      jquerylib_0.1.4  nloptr_2.0.3    
## [17] pillar_1.9.0     glue_1.6.2       uuid_1.1-0       plyr_1.8.8      
## [21] binom_1.1-1.1    lifecycle_1.0.3  stringr_1.5.0    blogdown_1.16   
## [25] evaluate_0.21    fastmap_1.1.1    RLRsim_3.1-8     parallel_4.2.3  
## [29] pbkrtest_0.5.2   fansi_1.0.4      broom_1.0.4      Rcpp_1.0.10     
## [33] backports_1.4.1  plotrix_3.8-2    cachem_1.0.7     jsonlite_1.8.4  
## [37] abind_1.4-5      digest_0.6.31    stringi_1.7.12   bookdown_0.33.3 
## [41] dplyr_1.1.1      grid_4.2.3       cli_3.4.1        tools_4.2.3     
## [45] magrittr_2.0.3   sass_0.4.6       tibble_3.2.1     tidyr_1.3.0     
## [49] car_3.1-2        pkgconfig_2.0.3  MASS_7.3-58.3    minqa_1.2.5     
## [53] rmarkdown_2.21   rstudioapi_0.14  iterators_1.0.14 R6_2.5.1        
## [57] boot_1.3-28.1    nlme_3.1-162     compiler_4.2.3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;just-the-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Just the code&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;
library(lme4)
library(simr)

# Toy model
fm = lmer(y ~ x + (x | g), data = simdata)

# Extend sample size of `g`
fm_extended_g = extend(fm, along = &amp;#39;g&amp;#39;, n = 12)

# Parallelize `breaks` by running each number of levels in a separate function.

# 4 levels of g
pwcurve_4g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 4, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 6 levels of g
pwcurve_6g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 6, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 8 levels of g
pwcurve_8g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 8, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 10 levels of g
pwcurve_10g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 10, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 12 levels of g
pwcurve_12g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 12, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)


# Create a destination object using any of the power curves above.
all_pwcurve = pwcurve_4g

# Combine results
all_pwcurve$ps = c(pwcurve_4g$ps[1], pwcurve_6g$ps[1], pwcurve_8g$ps[1], 
                   pwcurve_10g$ps[1], pwcurve_12g$ps[1])

# Combine the different numbers of levels.
all_pwcurve$xval = c(pwcurve_4g$nlevels, pwcurve_6g$nlevels, pwcurve_8g$nlevels, 
                     pwcurve_10g$nlevels, pwcurve_12g$nlevels)


print(all_pwcurve)

plot(all_pwcurve, xlab = &amp;#39;Levels of g&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Brysbaert, M., &amp;amp; Stevens, M. (2018). Power analysis and effect size in mixed effects models: A tutorial. &lt;em&gt;Journal of Cognition, 1&lt;/em&gt;(1), 9. &lt;a href=&#34;http://doi.org/10.5334/joc.10&#34; class=&#34;uri&#34;&gt;http://doi.org/10.5334/joc.10&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Green, P., &amp;amp; MacLeod, C. J. (2016). SIMR: An R package for power analysis of generalized linear mixed models by simulation. &lt;em&gt;Methods in Ecology and Evolution 7&lt;/em&gt;(4), 493–498, &lt;a href=&#34;https://doi.org/10.1111/2041-210X.12504&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/2041-210X.12504&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kumle, L., Vo, M. L. H., &amp;amp; Draschkow, D. (2021). Estimating power in (generalized) linear mixed models: An open introduction and tutorial in R. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, 1–16. &lt;a href=&#34;https://doi.org/10.3758/s13428-021-01546-0&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13428-021-01546-0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Luke, S. G. (2017). Evaluating significance in linear mixed-effects models in R. &lt;em&gt;Behavior Research Methods, 49&lt;/em&gt;(4), 1494–1502. &lt;a href=&#34;https://doi.org/10.3758/s13428-016-0809-y&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13428-016-0809-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The number of simulations set by &lt;code&gt;nsim&lt;/code&gt; should be larger (Brysbaert &amp;amp; Stevens, 2018; Green &amp;amp; MacLeod, 2016). In addition, the effect size for &lt;code&gt;x&lt;/code&gt; should be adjusted to the value that best fits with the planned study (Kumle et al., 2021).&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
            <category>s</category>
      
            <category>power analysis</category>
      
      
            <category>R</category>
      
            <category>research methods</category>
      
    </item>
    
    <item>
      <title>WebVTT caption transcription app</title>
      <link>https://pablobernabeu.github.io/applications-and-dashboards/vtt-transcription-app/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/applications-and-dashboards/vtt-transcription-app/</guid>
      <description>


&lt;p&gt;This open-source, R-based web application allows the conversion of video captions (subtitles) from the &lt;a href=&#34;https://www.w3.org/TR/webvtt1/&#34;&gt;Web Video Text Tracks (WebVTT) Format&lt;/a&gt; into plain texts. For this purpose, users upload a WebVTT file with the extension &lt;code&gt;.vtt&lt;/code&gt; or &lt;code&gt;.txt&lt;/code&gt; (examples available &lt;a href=&#34;https://github.com/pablobernabeu/VTT-Transcription-App/blob/main/assets/Example_subtitles_1.vtt&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://github.com/pablobernabeu/VTT-Transcription-App/blob/main/assets/Example_subtitles_2.txt&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2FVTT-Transcription-App%2Fblob%2Fmain%2Fassets%2FExample_subtitles_1.vtt&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;a href=&#34;https://osf.io/download/ustmx&#34;&gt;&lt;strong&gt;Download VTT file&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When the caption file is uploaded to the web application, metadata such as timestamps are automatically removed, and the text is formatted into a paragraph. The result is displayed on the website, and can be downloaded as &lt;code&gt;.docx&lt;/code&gt; and &lt;code&gt;.txt&lt;/code&gt; documents. Overall, this application serves to improve the accessibility of video captions.&lt;/p&gt;
&lt;p&gt;The data is only available to the user, and is deleted when the website is closed.&lt;/p&gt;
&lt;div id=&#34;globe_with_meridians-web-application&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;🌐 Web application&lt;/h2&gt;
&lt;p&gt;The application can be used below, or it can be &lt;a href=&#34;https://pablo-bernabeu.shinyapps.io/VTT-Transcription-App&#34;&gt;opened separately here&lt;/a&gt;.&lt;/p&gt;
&lt;div style=&#34;margin-bottom: 25px;&#34;&gt;
&lt;div style=&#34;position: relative; padding-top: 56.25%; height: 1100px;&#34;&gt;
&lt;iframe src=&#34;https://pablo-bernabeu.shinyapps.io/VTT-Transcription-App&#34; frameborder=&#34;0&#34; allowfullscreen style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;programming-details&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Programming details&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/pablobernabeu/VTT-transcription&#34;&gt;source code is available on Github&lt;/a&gt;, where the app can be extended via pull requests. Questions and suggestions can be submitted as &lt;a href=&#34;https://github.com/pablobernabeu/VTT-transcription/issues&#34;&gt;issues&lt;/a&gt; or emailed to &lt;a href=&#34;mailto:pcbernabeu@gmail.com&#34; class=&#34;email&#34;&gt;pcbernabeu@gmail.com&lt;/a&gt;. The licence is &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/&#34;&gt;Creative Commons Attribution 4.0 International&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The core of the application is in the &lt;a href=&#34;https://github.com/pablobernabeu/VTT-Transcription-App/blob/main/index.Rmd&#34;&gt;index.Rmd&lt;/a&gt; script, which uses ‘&lt;a href=&#34;https://stringr.tidyverse.org/articles/regular-expressions.html&#34;&gt;regular expressions&lt;/a&gt;’ to process the VTT file.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2FVTT-Transcription-App%2Fblob%2Fmain%2Findex.Rmd%23L172-L188&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;In turn, the script above draws on &lt;a href=&#34;https://github.com/pablobernabeu/VTT-Transcription-App/blob/main/assets/VTT-Transcription-App_doc_renderer.Rmd&#34;&gt;the one below&lt;/a&gt; to enable the download of &lt;code&gt;.docx&lt;/code&gt; documents. Last, the latter script draws on &lt;a href=&#34;https://github.com/pablobernabeu/VTT-Transcription-App/blob/main/assets/VTT-Transcription-App-format-template.docx&#34;&gt;this Word template&lt;/a&gt;.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2FVTT-Transcription-App%2Fblob%2Fmain%2Fassets%2FVTT-Transcription-App_doc_renderer.Rmd%23L1-L13&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>subtitles</category>
      
            <category>accessibility</category>
      
            <category>transcription</category>
      
            <category>webvtt</category>
      
            <category>web video text tracks format</category>
      
            <category>web application</category>
      
            <category>data science</category>
      
            <category>regular expressions</category>
      
            <category>stringr</category>
      
            <category>R</category>
      
            <category>R Shiny</category>
      
            <category>Flexdashboard</category>
      
            <category>software</category>
      
      
            <category>research and teaching applications</category>
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>R Markdown amidst Madison parks</title>
      <link>https://pablobernabeu.github.io/2020/r-markdown-amidst-madison-parks/</link>
      <pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2020/r-markdown-amidst-madison-parks/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/rmarkdown-libs/plotly-binding/plotly.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/rmarkdown-libs/typedarray/typedarray.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/rmarkdown-libs/crosstalk/css/crosstalk.min.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/rmarkdown-libs/crosstalk/js/crosstalk.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/rmarkdown-libs/plotly-main/plotly-latest.min.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This document is part of teaching materials created for the workshop &lt;a href=&#34;https://github.com/pablobernabeu/CarpentryCon-2020-workshop-Open-Data-Reproducibility&#34; target=&#34;_top&#34;&gt;‘Open data and reproducibility v2.1: R Markdown, dashboards and Binder’&lt;/a&gt;, delivered at the &lt;a href=&#34;https://2020.carpentrycon.org/&#34;&gt;CarpentryCon 2020 conference&lt;/a&gt;. The purpose of this specific document is to practise R Markdown, including basic features such as Markdown markup and code chunks, along with more special features such as &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook/cross-ref.html&#34;&gt;cross-references for figures, tables, code chunks, etc&lt;/a&gt;. The code is &lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/blob/master/examples-documents-dashboards/R%20Markdown/R-Markdown-amidst-Madison-parks.Rmd&#34;&gt;on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Since this conference was originally going to take place in Madison, let’s look at some &lt;a href=&#34;https://data-cityofmadison.opendata.arcgis.com/datasets/parks?geometry=-89.997%2C43.007%2C-88.679%2C43.183&#34;&gt;open data from the City of Madison&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;park-types&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Park types&lt;/h2&gt;
&lt;p&gt;&lt;font style=&#39;color:grey&#39;&gt;[Placeholder text] Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&lt;/font&gt; Figure &lt;a href=&#34;#fig:park-types&#34;&gt;1&lt;/a&gt; shows the number of parks within each type.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Showing the code because echo = TRUE 
dat = read.csv(&amp;#39;https://opendata.arcgis.com/datasets/9e00ff81868e49b7ba65d4e628b9e14f_6.csv&amp;#39;)

dat = 
  dat %&amp;gt;%
  group_by(Type) %&amp;gt;%
  mutate(parks_number = n())

ggplotly(
  ggplot(dat, aes(x=reorder(Type, parks_number), y=parks_number,
                  text = paste(&amp;#39;Number of parks =&amp;#39;, parks_number))) + 
    theme(axis.title.y=element_blank()) + stat_identity(geom=&amp;#39;bar&amp;#39;) + 
    labs(x=&amp;#39;Type&amp;#39;, y=&amp;#39;Number of parks&amp;#39;) + coord_flip(), 
  tooltip = &amp;#39;text&amp;#39;
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:park-types&#34;&gt;&lt;/span&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;plotly html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;data&#34;:[{&#34;orientation&#34;:&#34;h&#34;,&#34;width&#34;:[0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.9,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.9,0.899999999999999,0.9,0.899999999999999,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.9,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.9,0.899999999999999,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.9,0.9,0.9,0.9,0.899999999999999,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.9,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.899999999999999,0.9,0.9,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999,0.899999999999999,0.9,0.9,0.9,0.9,0.899999999999999,0.899999999999999],&#34;base&#34;:[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],&#34;x&#34;:[100,100,22,78,78,21,31,100,78,100,78,31,100,78,100,78,78,100,21,31,100,78,100,2,100,100,100,100,78,100,100,100,78,100,100,21,2,78,78,100,21,100,31,100,22,1,78,100,78,78,78,100,100,13,78,100,78,100,22,100,78,100,31,78,100,100,78,31,100,100,100,78,100,100,78,21,100,21,100,78,100,21,21,100,100,100,1,31,100,78,100,78,78,13,21,78,22,100,22,100,31,31,31,78,100,31,31,100,22,31,78,100,78,100,100,100,78,100,100,31,21,22,78,78,78,22,13,100,31,100,78,31,78,21,100,21,31,78,100,78,31,100,78,78,22,78,21,100,100,100,31,78,100,78,100,22,22,78,100,78,78,13,78,100,31,78,100,78,31,100,13,100,22,13,13,78,78,100,78,78,78,100,100,22,78,21,13,21,22,78,21,13,22,100,78,31,31,13,78,78,100,100,100,78,100,100,100,21,100,31,100,31,22,100,78,22,100,100,100,78,31,100,78,31,78,100,78,78,78,100,22,78,100,100,100,78,100,100,100,78,78,21,31,78,78,13,22,78,100,100,100,78,21,100,13,21,22,31,31,31,100,78,100,13,22,21,22,100,78],&#34;y&#34;:[9,9,6,8,8,5,7,9,8,9,8,7,9,8,9,8,8,9,5,7,9,8,9,3,9,9,9,9,8,9,9,9,8,9,9,5,3,8,8,9,5,9,7,9,6,2,8,9,8,8,8,9,9,4,8,9,8,9,6,9,8,9,7,8,9,9,8,7,9,9,9,8,9,9,8,5,9,5,9,8,9,5,5,9,9,9,1,7,9,8,9,8,8,4,5,8,6,9,6,9,7,7,7,8,9,7,7,9,6,7,8,9,8,9,9,9,8,9,9,7,5,6,8,8,8,6,4,9,7,9,8,7,8,5,9,5,7,8,9,8,7,9,8,8,6,8,5,9,9,9,7,8,9,8,9,6,6,8,9,8,8,4,8,9,7,8,9,8,7,9,4,9,6,4,4,8,8,9,8,8,8,9,9,6,8,5,4,5,6,8,5,4,6,9,8,7,7,4,8,8,9,9,9,8,9,9,9,5,9,7,9,7,6,9,8,6,9,9,9,8,7,9,8,7,8,9,8,8,8,9,6,8,9,9,9,8,9,9,9,8,8,5,7,8,8,4,6,8,9,9,9,8,5,9,4,5,6,7,7,7,9,8,9,4,6,5,6,9,8],&#34;text&#34;:[&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 2&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 2&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 1&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 13&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 1&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 13&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 13&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 13&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 13&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 13&#34;,&#34;Number of parks = 13&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 13&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 13&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 13&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 13&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 13&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 31&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 13&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 21&#34;,&#34;Number of parks = 22&#34;,&#34;Number of parks = 100&#34;,&#34;Number of parks = 78&#34;],&#34;type&#34;:&#34;bar&#34;,&#34;textposition&#34;:&#34;none&#34;,&#34;marker&#34;:{&#34;autocolorscale&#34;:false,&#34;color&#34;:&#34;rgba(89,89,89,1)&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;transparent&#34;}},&#34;showlegend&#34;:false,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null}],&#34;layout&#34;:{&#34;margin&#34;:{&#34;t&#34;:26.2283105022831,&#34;r&#34;:7.30593607305936,&#34;b&#34;:40.1826484018265,&#34;l&#34;:92.7853881278539},&#34;plot_bgcolor&#34;:&#34;rgba(235,235,235,1)&#34;,&#34;paper_bgcolor&#34;:&#34;rgba(255,255,255,1)&#34;,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.6118721461187},&#34;xaxis&#34;:{&#34;domain&#34;:[0,1],&#34;automargin&#34;:true,&#34;type&#34;:&#34;linear&#34;,&#34;autorange&#34;:false,&#34;range&#34;:[-5,105],&#34;tickmode&#34;:&#34;array&#34;,&#34;ticktext&#34;:[&#34;0&#34;,&#34;25&#34;,&#34;50&#34;,&#34;75&#34;,&#34;100&#34;],&#34;tickvals&#34;:[0,25,50,75,100],&#34;categoryorder&#34;:&#34;array&#34;,&#34;categoryarray&#34;:[&#34;0&#34;,&#34;25&#34;,&#34;50&#34;,&#34;75&#34;,&#34;100&#34;],&#34;nticks&#34;:null,&#34;ticks&#34;:&#34;outside&#34;,&#34;tickcolor&#34;:&#34;rgba(51,51,51,1)&#34;,&#34;ticklen&#34;:3.65296803652968,&#34;tickwidth&#34;:0.66417600664176,&#34;showticklabels&#34;:true,&#34;tickfont&#34;:{&#34;color&#34;:&#34;rgba(77,77,77,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:11.689497716895},&#34;tickangle&#34;:-0,&#34;showline&#34;:false,&#34;linecolor&#34;:null,&#34;linewidth&#34;:0,&#34;showgrid&#34;:true,&#34;gridcolor&#34;:&#34;rgba(255,255,255,1)&#34;,&#34;gridwidth&#34;:0.66417600664176,&#34;zeroline&#34;:false,&#34;anchor&#34;:&#34;y&#34;,&#34;title&#34;:{&#34;text&#34;:&#34;Number of parks&#34;,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.6118721461187}},&#34;hoverformat&#34;:&#34;.2f&#34;},&#34;yaxis&#34;:{&#34;domain&#34;:[0,1],&#34;automargin&#34;:true,&#34;type&#34;:&#34;linear&#34;,&#34;autorange&#34;:false,&#34;range&#34;:[0.4,9.6],&#34;tickmode&#34;:&#34;array&#34;,&#34;ticktext&#34;:[&#34;OTHER&#34;,&#34;SPECIAL&#34;,&#34;SPORTS COMPLEX&#34;,&#34;TRAFFICWAY&#34;,&#34;CONSERVATION&#34;,&#34;OPEN SPACE&#34;,&#34;COMMUNITY&#34;,&#34;NEIGHBORHOOD&#34;,&#34;MINI&#34;],&#34;tickvals&#34;:[1,2,3,4,5,6,7,8,9],&#34;categoryorder&#34;:&#34;array&#34;,&#34;categoryarray&#34;:[&#34;OTHER&#34;,&#34;SPECIAL&#34;,&#34;SPORTS COMPLEX&#34;,&#34;TRAFFICWAY&#34;,&#34;CONSERVATION&#34;,&#34;OPEN SPACE&#34;,&#34;COMMUNITY&#34;,&#34;NEIGHBORHOOD&#34;,&#34;MINI&#34;],&#34;nticks&#34;:null,&#34;ticks&#34;:&#34;outside&#34;,&#34;tickcolor&#34;:&#34;rgba(51,51,51,1)&#34;,&#34;ticklen&#34;:3.65296803652968,&#34;tickwidth&#34;:0.66417600664176,&#34;showticklabels&#34;:true,&#34;tickfont&#34;:{&#34;color&#34;:&#34;rgba(77,77,77,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:11.689497716895},&#34;tickangle&#34;:-0,&#34;showline&#34;:false,&#34;linecolor&#34;:null,&#34;linewidth&#34;:0,&#34;showgrid&#34;:true,&#34;gridcolor&#34;:&#34;rgba(255,255,255,1)&#34;,&#34;gridwidth&#34;:0.66417600664176,&#34;zeroline&#34;:false,&#34;anchor&#34;:&#34;x&#34;,&#34;title&#34;:{&#34;text&#34;:&#34;&#34;,&#34;font&#34;:{&#34;color&#34;:null,&#34;family&#34;:null,&#34;size&#34;:0}},&#34;hoverformat&#34;:&#34;.2f&#34;},&#34;shapes&#34;:[{&#34;type&#34;:&#34;rect&#34;,&#34;fillcolor&#34;:null,&#34;line&#34;:{&#34;color&#34;:null,&#34;width&#34;:0,&#34;linetype&#34;:[]},&#34;yref&#34;:&#34;paper&#34;,&#34;xref&#34;:&#34;paper&#34;,&#34;x0&#34;:0,&#34;x1&#34;:1,&#34;y0&#34;:0,&#34;y1&#34;:1}],&#34;showlegend&#34;:false,&#34;legend&#34;:{&#34;bgcolor&#34;:&#34;rgba(255,255,255,1)&#34;,&#34;bordercolor&#34;:&#34;transparent&#34;,&#34;borderwidth&#34;:1.88976377952756,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:11.689497716895}},&#34;hovermode&#34;:&#34;closest&#34;,&#34;barmode&#34;:&#34;relative&#34;},&#34;config&#34;:{&#34;doubleClick&#34;:&#34;reset&#34;,&#34;modeBarButtonsToAdd&#34;:[&#34;hoverclosest&#34;,&#34;hovercompare&#34;],&#34;showSendToCloud&#34;:false},&#34;source&#34;:&#34;A&#34;,&#34;attrs&#34;:{&#34;e434646d209&#34;:{&#34;x&#34;:{},&#34;y&#34;:{},&#34;text&#34;:{},&#34;type&#34;:&#34;bar&#34;}},&#34;cur_data&#34;:&#34;e434646d209&#34;,&#34;visdat&#34;:{&#34;e434646d209&#34;:[&#34;function (y) &#34;,&#34;x&#34;]},&#34;highlight&#34;:{&#34;on&#34;:&#34;plotly_click&#34;,&#34;persistent&#34;:false,&#34;dynamic&#34;:false,&#34;selectize&#34;:false,&#34;opacityDim&#34;:0.2,&#34;selected&#34;:{&#34;opacity&#34;:1},&#34;debounce&#34;:0},&#34;shinyEvents&#34;:[&#34;plotly_hover&#34;,&#34;plotly_click&#34;,&#34;plotly_selected&#34;,&#34;plotly_relayout&#34;,&#34;plotly_brushed&#34;,&#34;plotly_brushing&#34;,&#34;plotly_clickannotation&#34;,&#34;plotly_doubleclick&#34;,&#34;plotly_deselect&#34;,&#34;plotly_afterplot&#34;,&#34;plotly_sunburstclick&#34;],&#34;base_url&#34;:&#34;https://plot.ly&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Number of parks within each type.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;twenty-largest-parks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Twenty largest parks&lt;/h2&gt;
&lt;p&gt;&lt;font style=&#39;color:grey&#39;&gt;[Placeholder text] Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&lt;/font&gt; Table &lt;a href=&#34;#tab:largest-parks&#34;&gt;1&lt;/a&gt; shows the twenty largest parks types, along with their type and acreage. The code doesn’t show below because &lt;code&gt;echo = FALSE&lt;/code&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:largest-parks&#34;&gt;Table 1: &lt;/span&gt;The twenty largest parks in Madison.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Name&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Type&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Acreage&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cherokee Marsh - North Unit&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CONSERVATION&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;946.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cherokee Marsh - South Unit (School Road Unit)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CONSERVATION&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;261.27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Elver Park&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;250.82&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Northeast Park&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;237.76&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Warner Park&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;213.49&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Door Creek Park&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;159.97&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cherokee Marsh - Mendota Unit&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CONSERVATION&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;122.08&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Owen Conservation Park&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CONSERVATION&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;96.79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Reindahl (Amund) Park&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;90.74&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Olbrich Park&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;90.01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Yahara Hills Park (West)&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;82.20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sycamore Park&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;71.42&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Turville Point Conservation Park&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CONSERVATION&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;64.28&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Edna Taylor Conservation Park&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CONSERVATION&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;60.27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Quann Park&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;55.43&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Demetral Park&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;49.18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Prairie Ridge Conservation Park&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CONSERVATION&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;48.76&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Olin Park&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;47.12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hiestand Park&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;46.27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Vilas (Henry) Park&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;45.67&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We could decide to display the code for the table at this point (or any other), which can be done using the parameters &lt;code&gt;ref.label = &#39;largest-parks&#39;, echo = TRUE, eval = FALSE&lt;/code&gt; in the chunk options (&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook/reuse-chunks.html&#34;&gt;Xie, Dervieux, &amp;amp; Riederer, 2020&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  summarize(Name = Park_Name, Type, Acreage) %&amp;gt;%
  arrange(-Acreage, Name, Type) %&amp;gt;%
  select(Name, Type, Acreage) %&amp;gt;%
  head(20) %&amp;gt;%
  kable(caption = &amp;#39;The twenty largest parks in Madison.&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>s</category>
      
            <category>R</category>
      
            <category>R Markdown</category>
      
            <category>CarpentryCon</category>
      
            <category>workshop</category>
      
            <category>RStudio</category>
      
            <category>bookdown</category>
      
            <category>cross-references</category>
      
      
            <category>programming</category>
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Web application for the simulation of experimental data</title>
      <link>https://pablobernabeu.github.io/applications-and-dashboards/experimental-data-simulation/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/applications-and-dashboards/experimental-data-simulation/</guid>
      <description>


&lt;div style=&#34;font-size: 25px; color: #614064; padding-top: 15px; padding-bottom: 10px;&#34;&gt;
&lt;i class=&#34;fas fa-chalkboard-teacher fa-xs&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt; &lt;i class=&#34;fas fa-university fa-xs&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;  Purposes
&lt;/div&gt;
&lt;p&gt;This open-source, R-based web application is suitable for educational and research purposes in experimental and quantitative sciences. It allows the &lt;strong&gt;creation of varied data sets with specified structures, such as between-group and within-participant variables, that can be categorical or continuous.&lt;/strong&gt; These parameters can be set throughout the various tabs (sections) from the top menu. In the last tab, the data set can be downloaded. The benefits of this application include time-saving and flexibility in the control of parameters.&lt;/p&gt;
&lt;div id=&#34;guidelines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guidelines&lt;/h3&gt;
&lt;p&gt;General guidelines include the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;In the names of variables&lt;/strong&gt;, it’s recommended only to use alphanumeric characters and underscore signs. The latter can be used to separate characters or words (e.g., &lt;em&gt;variable_name&lt;/em&gt;). Different names should be used for each variable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;In the levels of categorical variables&lt;/strong&gt;, alphanumeric, special characters and spaces are allowed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;In numeric fields&lt;/strong&gt; (e.g., ‘Mean’, ‘Standard deviation’, ‘Relative probability [0, 1]’), only numbers and decimal points are allowed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;As the data set increases&lt;/strong&gt;, so does the processing time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More specific guidelines are available in each section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;globe_with_meridians-the-web-application-can-be-launched-here.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;🌐  The web application can be &lt;a href=&#34;https://pablo-bernabeu.shinyapps.io/experimental-data-simulation/&#34;&gt;launched here&lt;/a&gt;.&lt;/h3&gt;
&lt;div style=&#34;padding-top:8px; padding-bottom:2px; margin-bottom:-20px; color:#665F5F;&#34;&gt;
Screenshot of the &lt;em&gt;Dependent&lt;/em&gt; tab (&lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/raw/master/Screenshot.png&#34;&gt;view larger&lt;/a&gt;)
&lt;/div&gt;
&lt;p&gt;&lt;img style=&#34;max-width: 800px; display: block; margin-left: auto; margin-right: auto; padding-bottom: 15px;&#34; src=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/raw/master/Screenshot.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;p&gt;Bernabeu, P., &amp;amp; Lynott, D. (2020). &lt;em&gt;Web application for the simulation of experimental data&lt;/em&gt; (Version 1.4). &lt;a href=&#34;https://github.com/pablobernabeu/Experiment-simulation-app/&#34;&gt;https://github.com/pablobernabeu/Experiment-simulation-app/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Code&lt;/h3&gt;
&lt;p&gt;This web application was developed in &lt;a href=&#34;https://www.r-project.org/about.html&#34;&gt;R&lt;/a&gt; (R Core Team, 2020). The code is &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/blob/master/index.Rmd&#34;&gt;available on Github&lt;/a&gt;, where contributions may be made. The initial code for this application was influenced by Section 5.7 (&lt;a href=&#34;https://crumplab.github.io/programmingforpsych/simulating-and-analyzing-data-in-r.html#simulating-data-for-multi-factor-designs&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Simulating data for multi-factor designs&lt;/em&gt;&lt;/a&gt;) in Crump (2017). The R packages used include ‘dplyr’ (Wickham, François, Henry, &amp;amp; Müller, 2018), ‘DT’ (Xie, 2020), ‘flexdashboard’ (Iannone, Allaire, &amp;amp; Borges, 2020), ‘shiny’ (Chang, Cheng, Allaire, Xie, &amp;amp; McPherson, 2020) and ‘stringr’ (Wickham, 2019).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;options-for-development-and-local-use-of-the-app&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Options for development and local use of the app&lt;/h3&gt;
&lt;div id=&#34;option-a-using-local-rrstudio-or-rstudio-cloud-project-or-binder-rstudio-environment&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Option A) Using local R/RStudio or &lt;a href=&#34;https://rstudio.cloud/project/1739958&#34;&gt;RStudio Cloud project&lt;/a&gt; or &lt;a href=&#34;https://mybinder.org/v2/gh/pablobernabeu/Experimental-data-simulation/master?urlpath=rstudio&#34;&gt;Binder RStudio environment&lt;/a&gt;&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;[Step only necessary in R/RStudio] Install the packages in the versions used in the &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/releases&#34;&gt;latest release of this application&lt;/a&gt;, by running:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;#39;devtools&amp;#39;)
library(devtools)
install_version(&amp;#39;dplyr&amp;#39;, &amp;#39;1.0.2&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;DT&amp;#39;, &amp;#39;0.15&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;flexdashboard&amp;#39;, &amp;#39;0.5.2&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;htmltools&amp;#39;, &amp;#39;0.5.0&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;knitr&amp;#39;, &amp;#39;1.30&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;ngram&amp;#39;, &amp;#39;3.0.4&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;purrr&amp;#39;, &amp;#39;0.3.4&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;shiny&amp;#39;, &amp;#39;1.5.0&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;stringr&amp;#39;, &amp;#39;1.4.0&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;tidyr&amp;#39;, &amp;#39;1.1.2&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open the &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/blob/master/index.Rmd&#34;&gt;index.Rmd&lt;/a&gt; script.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the application by clicking on &lt;kbd&gt;▶️ Run document&lt;/kbd&gt; at the top left, or by running &lt;code&gt;rmarkdown::run(&#39;index.Rmd&#39;)&lt;/code&gt; in the console.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Click on &lt;kbd&gt;Open in Browser&lt;/kbd&gt; at the top left.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;option-b-using-dockerfile-see-instructions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Option B) Using Dockerfile (&lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation#option-b-using-dockerfile-vsochs-pull-request&#34;&gt;see instructions&lt;/a&gt;)&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgements&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Acknowledgements&lt;/h3&gt;
&lt;p&gt;Thank you to RStudio for the free hosting server used by this application, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;shinyapps.io&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div style=&#34;text-indent:-1.5em; margin-left:1.5em;&#34;&gt;
&lt;p&gt;Chang, W., Cheng, J., Allaire, J., Xie, Y., &amp;amp; McPherson, J. (2020). shiny: Web Application Framework for R. R package version 1.4.0. Available at &lt;a href=&#34;http://CRAN.R-project.org/package=shiny&#34;&gt;http://CRAN.R-project.org/package=shiny&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Crump, M. J. C. (2017). Programming for Psychologists: Data Creation and Analysis (Version 1.1). &lt;a href=&#34;https://crumplab.github.io/programmingforpsych/&#34;&gt;https://crumplab.github.io/programmingforpsych/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Iannone, R., Allaire, J. J., &amp;amp; Borges, B. (2020). Flexdashboard: R Markdown Format for Flexible Dashboards. &lt;a href=&#34;http://rmarkdown.rstudio.com/flexdashboard&#34;&gt;http://rmarkdown.rstudio.com/flexdashboard&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;R Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Wickham, H. (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0. &lt;a href=&#34;https://CRAN.R-project.org/package=stringr&#34;&gt;https://CRAN.R-project.org/package=stringr&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Wickham, H., François, R., Henry, L., &amp;amp; Müller, K. (2018). dplyr: A Grammar of Data Manipulation. R package version 0.7.6. &lt;a href=&#34;https://CRAN.R-project.org/package=dplyr&#34;&gt;https://CRAN.R-project.org/package=dplyr&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Xie, Y. (2020). DT: A Wrapper of the JavaScript Library “DataTables”. R package version 0.14. Available at &lt;a href=&#34;https://CRAN.R-project.org/package=DT&#34;&gt;https://CRAN.R-project.org/package=DT&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;contact&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Contact&lt;/h3&gt;
&lt;p&gt;To submit any questions or feedback, please post &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/issues&#34;&gt;an issue&lt;/a&gt;, or email Pablo Bernabeu at &lt;a href=&#34;mailto:pcbernabeu@gmail.com&#34;&gt;pcbernabeu@gmail.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>web application</category>
      
            <category>data simulation</category>
      
            <category>randomisation</category>
      
            <category>research methods</category>
      
            <category>experiment</category>
      
            <category>statistics</category>
      
            <category>data science</category>
      
            <category>R</category>
      
            <category>Tidyverse</category>
      
            <category>R Shiny</category>
      
            <category>Flexdashboard</category>
      
            <category>HTML</category>
      
            <category>CSS</category>
      
            <category>Software Sustainability Institute Fellowship</category>
      
      
            <category>research and teaching applications</category>
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Data dashboard: Butterfly species richness in Los Angeles</title>
      <link>https://pablobernabeu.github.io/applications-and-dashboards/butterfly-species-richness-in-la/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/applications-and-dashboards/butterfly-species-richness-in-la/</guid>
      <description>&lt;a href=&#39;https://pablobernabeu.github.io/dashboards/Butterfly-species-richness-in-LA&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #196F27; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt;
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;i class=&#34;fas fa-mouse-pointer&#34;&gt;&lt;/i&gt;&lt;/span&gt;&amp;nbsp; Dashboard
      &lt;/h3&gt;&lt;/button&gt;
      &lt;/a&gt;
&lt;br&gt;
&lt;br&gt;
&lt;p&gt;This dashboard presents open data&lt;/a&gt; (&lt;a href=&#39;https://github.com/jcoliver/bioscan/blob/master/data/iNaturalist-clean-reduced.csv&#39; target=&#34;_blank&#34;&gt;iNaturalist&lt;/a&gt; and &lt;a href=&#39;https://github.com/jcoliver/bioscan/blob/master/data/BioScanDataComplete.csv&#39; target=&#34;_blank&#34;&gt;BioScan&lt;/a&gt;) from Prudic, K.L.; Oliver, J.C.; Brown, B.V.; Long, E.C. &lt;a href=&#39;https://doi.org/10.3390/insects9040186&#39; target=&#34;_blank&#34;&gt; &lt;strong&gt;Comparisons of Citizen Science Data-Gathering Approaches to Evaluate Urban Butterfly Diversity&lt;/strong&gt;. &lt;em&gt;Insects&lt;/em&gt; &lt;strong&gt;2018&lt;/strong&gt;, &lt;em&gt;9&lt;/em&gt;, 186&lt;/a&gt;. In their study, Prudic and colleagues compared citizen science with traditional methods in the measurement of butterfly populations.&lt;/p&gt;
&lt;p&gt;I developed this dashboard after reproducing the &lt;a href=&#34;https://github.com/jcoliver/bioscan&#34;&gt;analyses of the original study&lt;/a&gt; in a &lt;a href=&#34;https://github.com/reprohack/reprohack-hq/blob/master/README.md&#34;&gt;Reprohack session&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My coding tasks included transforming the data to a long format,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# There are pseudovariables, that is, observations entered as variables. Since most R processes 
# need the tidy format, convert below (see https://r4ds.had.co.nz/tidy-data.html).
# The specific numbers found through traps and crowdsourcing methods are preserved.

BioScan = BioScan %&amp;gt;% pivot_longer(
    cols = Anthocharis_sara:Vanessa_cardui, names_to = &amp;quot;Species&amp;quot;,
    values_to = &amp;quot;Number&amp;quot;, values_drop_na = TRUE
  )

# Compare
#str(BioScan)
#str(dat)
# 928 rows now; the result of 29 pseudo-variables being transposed into
# rows, interacting with 32 previous rows, i.e., 29 * 32 = 928.

&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;p&gt;merging three data sets,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# The iNaturalist data set presents a slightly different challenge from the pseudovariables found above.
# The number of animals of each species must be computed from repeated entries, per site.

iNaturalist = merge(iNaturalist, iNaturalist %&amp;gt;% count(species, site, name = &#39;Number&#39;))

&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;p&gt;and, as ever, wrangling with the format of the dashboard pages to preserve the format of a table.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Species details {style=&amp;quot;background-color: #FCFCFC;&amp;quot;}
=======================================================================

Column {style=&amp;quot;data-width:100%; position:static; height:1000px;&amp;quot;}
-----------------------------------------------------------------------

&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;div class = &#39;hanging-indent&#39;&gt;
&lt;p&gt;Bernabeu, P. (2020). Dashboard with data from Prudic, Oliver, Brown, &amp;amp; Long (2018), Comparisons of Citizen Science Data-Gathering Approaches to Evaluate Urban Butterfly Diversity, &lt;em&gt;Insects&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;, 186. &lt;a href=&#34;https://pablobernabeu.github.io/dashboards/Butterfly-species-richness-in-LA&#34;&gt;https://pablobernabeu.github.io/dashboards/Butterfly-species-richness-in-LA&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;br&gt;
</description>
      
            <category>data dashboard</category>
      
            <category>R</category>
      
            <category>citizen science</category>
      
            <category>butterflies</category>
      
            <category>nature</category>
      
            <category>open data</category>
      
            <category>BioScan</category>
      
            <category>iNaturalist</category>
      
            <category>HTML</category>
      
            <category>CSS</category>
      
            <category>tidy</category>
      
            <category>merge</category>
      
            <category>Software Sustainability Institute Fellowship</category>
      
      
            <category>data visualisation</category>
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Data is present: Workshops and datathons</title>
      <link>https://pablobernabeu.github.io/2020/data-is-present-workshops-and-datathons/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2020/data-is-present-workshops-and-datathons/</guid>
      <description>


&lt;div id=&#34;enhanced-data-presentation-using-reproducible-documents-and-dashboards&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Enhanced data presentation using reproducible documents and dashboards&lt;/h2&gt;
&lt;div id=&#34;calendar&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calendar&lt;/h3&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;9%&#34; /&gt;
&lt;col width=&#34;23%&#34; /&gt;
&lt;col width=&#34;39%&#34; /&gt;
&lt;col width=&#34;27%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Date&lt;/th&gt;
&lt;th&gt;Title&lt;/th&gt;
&lt;th&gt;Event and location&lt;/th&gt;
&lt;th&gt;Registration&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;26 Nov 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pablobernabeu.github.io/talk/2020-11-26-mixed-effects-models-in-r-and-a-new-tool-for-data-simulation&#34;&gt;Mixed-effects models in R, and a new tool for data simulation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;New Tricks Seminars, Dept. Psychology, Lancaster University [online]&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;8 Oct 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pablobernabeu.github.io/talk/2020-10-08-reproducibilidad-en-torno-a-una-aplicacion-web/&#34;&gt;Reproducibilidad en torno a una aplicación web&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Reprohack en español, LatinR Conference 2020 [online]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.eventbrite.com.ar/e/reprohack-en-espanol-latinr-2020-tickets-121741832097&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;13 Aug 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/pablobernabeu/CarpentryCon-2020-workshop-Open-Data-Reproducibility&#34;&gt;Open data and reproducibility: R Markdown, data dashboards and Binder v2.1&lt;/a&gt; (co-led with Florencia D’Andrea)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://2020.carpentrycon.org/schedule/&#34;&gt;CarpentryCon&lt;/a&gt;, &lt;a href=&#34;mailto:CarpentryCon@Home&#34; class=&#34;email&#34;&gt;CarpentryCon@Home&lt;/a&gt;, The Carpentries [online]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://2020.carpentrycon.org/schedule/&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;26 July 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/pablobernabeu/UKCLC2020-workshop-Open-data-and-reproducibility&#34;&gt;Open data and reproducibility: R Markdown, data dashboards and Binder&lt;/a&gt; (co-led with Eirini Zormpa)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.ukclc2020.com/pre-conference&#34;&gt;UK Cognitive Linguistics Conference&lt;/a&gt;, University of Birmingham [online]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.ukclc2020.com/registration&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;6 May 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pablobernabeu.github.io/PDF/LU-May-Markdown-workshop-programme.pdf&#34;&gt;R Markdown&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Lancaster University [online]&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Event cancelled&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://newcastle2020.satrdays.org/&#34;&gt;Open data and reproducibility 2.0&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://newcastle2020.satrdays.org/&#34;&gt;SatRday Newcastle upon Tyne&lt;/a&gt;, Newcastle University&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://newcastle2020.satrdays.org/&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Background&lt;/h3&gt;
&lt;p&gt;This project offers free activities to learn and practise reproducible data presentation. &lt;a href=&#34;https://www.software.ac.uk/about/fellows/pablo-bernabeu&#34;&gt;Pablo Bernabeu&lt;/a&gt; organises these events in the context of a &lt;a href=&#34;https://www.software.ac.uk/programmes-and-events/fellowship-programme&#34;&gt;Software Sustainability Institute Fellowship&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;open-source-software&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Open-source software&lt;/h4&gt;
&lt;p&gt;Programming languages such as &lt;a href=&#34;https://www.r-project.org/about.html&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt; offer free, powerful resources for data processing, visualisation and analysis. Experience in these programs is highly valued in data-intensive disciplines.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;open-data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Open data&lt;/h4&gt;
&lt;p&gt;Original data has become a &lt;a href=&#34;https://www.nature.com/articles/d41586-019-01506-x&#34;&gt;public good in many research fields&lt;/a&gt; thanks to cultural and technological advances. On the internet, we can find innumerable data sets from sources such as scientific journals and repositories (e.g., &lt;a href=&#34;https://osf.io&#34;&gt;OSF&lt;/a&gt;), local and national governments (e.g., &lt;a href=&#34;https://data.london.gov.uk/&#34;&gt;London&lt;/a&gt;, UK [&lt;a href=&#34;https://www.ukdataservice.ac.uk/&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://data.gov.uk/&#34;&gt;2&lt;/a&gt;]), non-governmental organisations (e.g., &lt;a href=&#34;https://data.world/datasets/ngo&#34;&gt;data.world&lt;/a&gt;), etc. Researchers inside and outside academia nowadays share a lot of their data under attribution licences (e.g., &lt;a href=&#34;https://creativecommons.org/&#34;&gt;Creative Commons&lt;/a&gt;, the UK &lt;a href=&#34;http://www.nationalarchives.gov.uk/doc/open-government-licence/version/1/&#34;&gt;Open Government Licence&lt;/a&gt;, etc.). This allows any external analysts to access these raw data, create (additional) visualisations and analyses, and share these. In society, making data more accessible can &lt;a href=&#34;https://digitalcommons.law.yale.edu/cgi/viewcontent.cgi?article=1140&amp;amp;context=yhrdlj&#34;&gt;demonstrably benefit citizens&lt;/a&gt; (despite &lt;a href=&#34;https://firstmonday.org/ojs/index.php/fm/article/view/3316/2764#author&#34;&gt;limitations&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;activities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Activities&lt;/h2&gt;
&lt;p&gt;Activities comprise free &lt;strong&gt;workshops&lt;/strong&gt; and &lt;a href=&#34;#datathons-creating-reproducible-documents-and-dashboards&#34;&gt;&lt;strong&gt;datathons&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;workshops&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Workshops&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.r-project.org&#34;&gt;R is a programming language&lt;/a&gt; greatly equipped for the creation of reproducible documents and dashboards. Four workshops are offered that cover a suite of interrelated tools—R, R Markdown, data dashboards and Binder environments—, all underlain by reproducible workflows and open-source software.&lt;/p&gt;
&lt;p&gt;Each workshop includes &lt;strong&gt;taught and practical sections&lt;/strong&gt;. The practice provides a chance for participants to experience and address common issues with the code. The level of taught sections is largely tailored to participants; similarly, practice sections are individually adaptable by means of easier and tougher tasks. The duration is also flexible, and some of the workshops can be combined into the same session.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://rstudio.com/&#34;&gt;RStudio&lt;/a&gt; interface is used in all workshops. Multi-levelled, real code examples are used. Throughout the workshops, and especially in the practice sections, individual questions will be encouraged.&lt;/p&gt;
&lt;div id=&#34;workshop-1-introduction-to-r&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 1: Introduction to R&lt;/h4&gt;
&lt;p&gt;This workshop can serve as an introduction to R or a revision. It demonstrates what can be done in R, and provides resources for individual training. Since the duration is limited, online courses are also recommended (&lt;a href=&#34;https://www.coursera.org/courses?query=r&#34;&gt;see examples&lt;/a&gt; and &lt;a href=&#34;https://learner.coursera.help/hc/en-us/articles/209819033-Apply-for-Financial-Aid-or-a-Scholarship&#34;&gt;fee waivers for full content&lt;/a&gt;).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://adv-r.had.co.nz/Data-structures.html&#34;&gt;Data structures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.sthda.com/english/wiki/installing-and-using-r-packages&#34;&gt;Packages&lt;/a&gt;: general-purpose examples (e.g., &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt;) and more specific ones (e.g., for &lt;a href=&#34;https://cran.r-project.org/web/packages/lsr/lsr.pdf&#34;&gt;statistics&lt;/a&gt; or &lt;a href=&#34;https://cran.r-project.org/web/packages/GEOmap/GEOmap.pdf&#34;&gt;geography&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/rio/vignettes/rio.html&#34;&gt;Loading and writing data, in native and foreign formats&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Wide&lt;/em&gt; format (also dubbed ‘untidy’) versus &lt;em&gt;tidy&lt;/em&gt; format (also dubbed ‘long’ or ‘narrow’). For most processes in R, &lt;a href=&#34;https://r4ds.had.co.nz/tidy-data.html&#34;&gt;data needs to be in a tidy format&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://doi.org/10.1371/journal.pbio.3000202.g001&#39;&gt;&lt;img width = &#39;25%&#39; src = &#39;https://journals.plos.org/plosbiology/article/file?id=10.1371/journal.pbio.3000202.g001&amp;type=large&#39; alt = &#39;Illustration of wide and tidy data formats, from Postma and Goedhart (2019)&#39; /&gt;&lt;/a&gt;
&lt;p align=&#34;center&#34; style=&#34;text-align:center; margin-top:-20px; margin-bottom:10px;&#34;&gt;
Image from Postma and Goedhart (2019; &lt;a href=&#34;https://doi.org/10.1371/journal.pbio.3000202.g001&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pbio.3000202.g001&lt;/a&gt;).
&lt;/p&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://psyteachr.github.io/msc-data-skills/joins.html#joins&#34;&gt;Combining data sets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cookbook-r.com/Manipulating_data/Summarizing_data/&#34;&gt;Data summaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://philmcaleer.github.io/ug2-practical/visualisation-through-ggplot2.html&#34;&gt;Plots with &lt;code&gt;ggplot2::ggplot()&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://plot.ly/ggplot2/&#34;&gt;Interactive plots with &lt;code&gt;plotly::ggplotly()&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learningstatisticswithr-bookdown.netlify.com/part-v-statistical-tools.html&#34;&gt;Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer_and_Curtin_LMEMs-2017-Psych_Methods.pdf&#34;&gt;Linear mixed-effects models&lt;/a&gt; (see also &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0749596X20300061?dgcid=coauthor#b0670&#34;&gt;a review of practices&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://swcarpentry.github.io/r-novice-inflammation/02-func-R/&#34;&gt;How functions work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Debugging&lt;/em&gt;. Code errors are known as bugs. They can tiresome, but also interesting sometimes! 😅 Some tips for the first many years of experience include: reading and investigating error messages, in both source and console windows; controlling letter case and typos; closing parentheses and inverted commas; ensuring to have the necessary packages installed and loaded; following the format required by each function. To debug, break up code into subcomponents and test each of those to find out the source of the error. Once we act on that, the best outcome is seeing the code work, but sometimes different errors overlap, in which case we may see one error disappearing before another one appears. Debugging soon leads to proficient information seeking. The search process often begins on an internet search engine and extends to user communities, package documentation, tutorials, blogs… (see &lt;a href=&#34;https://youtu.be/Nj9J5iCSMB0?t=2687&#34;&gt;video explanation&lt;/a&gt;). &lt;a href=&#34;https://adv-r.hadley.nz/debugging.html&#34;&gt;Advanced debugging tools are also available&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Vast availability of free resources on the internet, from &lt;a href=&#34;https://www.coursera.org/courses?query=r%20programming&#34;&gt;Coursera&lt;/a&gt; and other MOOC sites, &lt;a href=&#34;https://education.rstudio.com/&#34;&gt;RStudio&lt;/a&gt;, &lt;a href=&#34;https://psyteachr.github.io/&#34;&gt;University of Glasgow&lt;/a&gt;, &lt;a href=&#34;http://swcarpentry.github.io/r-novice-inflammation/&#34;&gt;Carpentries&lt;/a&gt;, etc.&lt;/li&gt;
&lt;li&gt;Community: &lt;a href=&#34;https://stackoverflow.com/&#34;&gt;StackOverflow&lt;/a&gt;, &lt;a href=&#34;https://community.rstudio.com/&#34;&gt;RStudio Community&lt;/a&gt;, &lt;a href=&#34;https://github.com&#34;&gt;Github issues&lt;/a&gt; (e.g., for R packages), etc. Using and contributing back.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rstudio.cloud/&#34;&gt;RStudio Cloud&lt;/a&gt;: a personal RStudio environment on the internet&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;workshop-2-r-markdown-documents&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 2: R Markdown documents&lt;/h4&gt;
&lt;p&gt;Set your input and output in stone using &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;R Markdown&lt;/a&gt;. The analysis reports may be enriched with website features (HTML/CSS) and published as HTML, PDF or Word documents. Moreover, with R packages such as &lt;code&gt;bookdown&lt;/code&gt;, &lt;code&gt;bookdownplus&lt;/code&gt;, &lt;code&gt;blogdown&lt;/code&gt; and &lt;code&gt;flexdashboard&lt;/code&gt;, documents can be formatted as &lt;a href=&#34;https://awesome-blogdown.com/&#34;&gt;websites&lt;/a&gt;, &lt;a href=&#34;https://bookdown.org/&#34;&gt;digital papers and books&lt;/a&gt; and &lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/&#34;&gt;data dashboards&lt;/a&gt;. Other useful packages include &lt;code&gt;rmarkdown&lt;/code&gt;, &lt;code&gt;knitr&lt;/code&gt;, &lt;code&gt;DT&lt;/code&gt;, &lt;code&gt;kableExtra&lt;/code&gt; and &lt;code&gt;ggplot2&lt;/code&gt;. Further background: &lt;a href=&#34;https://www.youtube.com/watch?v=Nj9J5iCSMB0&#34;&gt;presentation by Michael Frank&lt;/a&gt;, &lt;a href=&#34;https://www.eddjberry.com/talks/reproducible-writing-with-rmarkdown.html#1&#34;&gt;slides by Ed Berry&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As well as facilitating the reproducibility of analyses and results to third parties, R Markdown is helpful &lt;em&gt;during the creation&lt;/em&gt; of a report. In particular, it reduces the chances of errors and the number of repetitive tasks. For instance, any part of the data can be inputted in the text directly from the source, rather than manually copying it (e.g., &lt;code&gt;`r mean(dat[dat$location==&#39;Havana&#39;, &#39;measure&#39;])`&lt;/code&gt; (&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/r-code.html&#34;&gt;expand&lt;/a&gt;). Thus, if and when the analysis needs to be changed or updated, the report can be automatically updated at the click of a button. In another area, the captions for figures and tables can be automatised using &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook/cross-ref.html&#34;&gt;cross-reference labels&lt;/a&gt; (e.g., Table &lt;code&gt;\@ref(tab:mtcars)&lt;/code&gt;). This secures the match between the text and the captions of figures and tables, and it automatically updates the numbering whenever and wherever a new figure or table is introduced.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://bookdownplus.netlify.com/portfolio/&#39;&gt;&lt;img width = &#39;50%&#39; src = &#39;https://github.com/pablobernabeu/bookdownplus/blob/master/inst2/copernicus/showcase/copernicus2.png?raw=true&#39; alt = &#39;Example of paper created with bookdownplus (image retrieved from R bookdownplus package)&#39;/&gt;&lt;/a&gt;
&lt;p align=&#34;center&#34; style=&#34;text-align:center; margin-top:-30px; margin-bottom:30px;&#34;&gt;
Image from bookdownplus package (&lt;a href=&#34;https://bookdownplus.netlify.com/portfolio/&#34; class=&#34;uri&#34;&gt;https://bookdownplus.netlify.com/portfolio/&lt;/a&gt;).
&lt;/p&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;workshop-3-introduction-to-data-dashboards&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 3: Introduction to data dashboards&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01782/full&#34;&gt;Data dashboards are web applications used to visualise data&lt;/a&gt; in detail through tables and plots. They assist in explaining and accounting for our data processing and analysis. They don’t require any coding from the end user. While most dashboards and web applications present existing data, a few of them serve the purpose of creating or simulating new data (see &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation&#34;&gt;example&lt;/a&gt;).&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/&#39;&gt; &lt;img width = &#39;90%&#39; src = &#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/8.png&#39; alt = &#39;Illustration of the usage of dashboards alongside data repositories&#39; /&gt; &lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;These all-reproducible dashboards are published as websites, and thus, they can include hyperlinks and downloadable files. Some of the R packages used are &lt;code&gt;knitr&lt;/code&gt;, &lt;code&gt;DT&lt;/code&gt;, &lt;code&gt;kableExtra&lt;/code&gt;, &lt;code&gt;reactable&lt;/code&gt;, &lt;code&gt;ggplot2&lt;/code&gt;, &lt;code&gt;plotly&lt;/code&gt;, &lt;code&gt;rmarkdown&lt;/code&gt;, &lt;code&gt;flexdashboard&lt;/code&gt; and &lt;code&gt;shiny&lt;/code&gt;. The aim of this workshop is to practise creating different forms of dashboards—&lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/&#34;&gt;Flexdashboard&lt;/a&gt; and &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;Shiny&lt;/a&gt;—the latter of which offers &lt;a href=&#34;https://mastering-shiny.org/&#34;&gt;greater features&lt;/a&gt;, and to practise also with the hosting platforms fitting each type—such as personal websites, &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;, &lt;a href=&#34;https://mybinder.org/&#34;&gt;Binder&lt;/a&gt;, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;Shinyapps&lt;/a&gt; and &lt;a href=&#34;https://rstudio.com/products/shiny/shiny-server/&#34;&gt;custom servers&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A great thing about dashboards is that they may be made very simple, but they can also be taken to the next level using some HTML, CSS or Javascript code (on top of the back-end code present in the R packages used), which is addressed in the next workshop.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;workshop-4-binder-environments-and-improving-data-dashboards&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 4: Binder environments and improving data dashboards&lt;/h4&gt;
&lt;div id=&#34;binder&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Binder&lt;/h5&gt;
&lt;p&gt;&lt;a href=&#34;https://mybinder.org/&#34;&gt;Binder&lt;/a&gt; is a tool to facilitate public access to software environments—for instance, by publishing an RStudio environment on the internet. Binder can also host Shiny apps. It is generously free &lt;a href=&#34;https://discourse.jupyter.org/t/mybinder-org-cost-updates/2426&#34;&gt;for users&lt;/a&gt;. After looking at the &lt;a href=&#34;https://github.com/binder-examples/r&#34;&gt;nuts and bolts of a deployment&lt;/a&gt;, participants will be able to deploy their own Binder environments and check the result by the end of the workshop. For this purpose, it’s recommended to have data and R code ready, ideally in a GitHub repository.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;improving-data-dashboards&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Improving data dashboards&lt;/h5&gt;
&lt;p&gt;We will practise how to improve the functionality of dashboards using some HTML, CSS and Javascript code, which is &lt;a href=&#34;https://www.w3schools.com/whatis/&#34;&gt;the basis of websites&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!-- Javascript function to enable a hovering tooltip --&amp;gt;
&amp;lt;script&amp;gt;
$(document).ready(function(){
$(&amp;#39;[data-toggle=&amp;quot;tooltip1&amp;quot;]&amp;#39;).tooltip();
});
&amp;lt;/script&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://shiny.rstudio.com/gallery/&#39;&gt; &lt;img align = &#39;center&#39; width = &#39;60%&#39; src = &#39;https://raw.githubusercontent.com/pablobernabeu/data-is-present/master/dashboard%20gif.gif&#39; alt = &#39;Examples of data dashboards&#39; /&gt; &lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trade-offs-among-dashboards&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Trade-offs among dashboards&lt;/h5&gt;
&lt;p&gt;Next, we will practise with three dashboard types—&lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/using.html&#34;&gt;Flexdashboard&lt;/a&gt;, &lt;a href=&#34;https://shiny.rstudio.com/tutorial/&#34;&gt;Shiny&lt;/a&gt; and &lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/shiny.html&#34;&gt;Flexdashboard-Shiny&lt;/a&gt;—and with the suitable hosting platforms. Firstly, the strength of Flexdashboard (&lt;a href=&#34;http://rpubs.com/pcbernabeu/Dutch-modality-exclusivity-norms&#34;&gt;example&lt;/a&gt;) is its basis on R Markdown, yielding an unmatched user interface (&lt;em&gt;front-end&lt;/em&gt;). Secondly, the strength of Shiny (&lt;a href=&#34;https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment/&#34;&gt;example&lt;/a&gt;) is the input reactivity (&lt;em&gt;back-end&lt;/em&gt;) it offers, allowing users to download sections of data they select, in various formats. Last, Flexdashboard-Shiny (&lt;a href=&#34;https://pablobernabeu.shinyapps.io/dutch-modality-exclusivity-norms/&#34;&gt;example&lt;/a&gt;) combines the best of both worlds.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ &lt;b&gt; Flexdashboard &lt;/b&gt; ★
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ ★ &lt;b&gt; Shiny &lt;/b&gt; ★ ★
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ ★ ★ &lt;b&gt; Flexdashboard-Shiny &lt;/b&gt; ★ ★ ★
&lt;/p&gt;
&lt;p&gt;Flexdashboard types are rendered as an HTML document—simple websites—, and can therefore be easily published on personal sites or &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;. This is convenient because no special hosting is required. In contrast, Shiny and Flexdashboard-Shiny types offer greater features, but require Shiny servers. Fortunately, the shinyapps.io server is available for free, up to some &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;usage limit&lt;/a&gt;. This server can host any of the three dashboards mentioned here. Another good option is presented by Binder environments, which can host the Shiny-type dashboards with no (explicit) limit. Yet, the Flexdashboard-Shiny type cannot be hosted in this server (&lt;a href=&#34;https://github.com/jupyter/repo2docker/issues/799&#34;&gt;as of January 2020, at least&lt;/a&gt;). Consequently, greater functionality may come at a cost for dashboards that have any considerable traffic, whereas dashboards with low traffic may do well on shinyapps.io. Knowing these trade-offs can help navigate &lt;a href=&#34;https://support.rstudio.com/hc/en-us/articles/217592947-What-are-the-limits-of-the-shinyapps-io-Free-plan-&#34;&gt;usage limits&lt;/a&gt;, save on web hosting fees, and increase the availability of our dashboards online, as we can offer fall-back versions on different platforms, as in the example below:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;… &lt;em&gt;&lt;a href=&#34;https://pablobernabeu.shinyapps.io/dutch-modality-exclusivity-norms/&#34;&gt;preferred-dashboard&lt;/a&gt; (in case of downtime, please visit this &lt;a href=&#34;http://rpubs.com/pcbernabeu/Dutch-modality-exclusivity-norms&#34;&gt;alternative&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Transforming dashboards into the different versions can be as easy as enabling or disabling some features, especially input reactivity. For instance, if we want to downgrade a Flexdashboard-Shiny to a Flexdashboard, to publish it outside of a Shiny server (see &lt;a href=&#34;https://github.com/pablobernabeu/Modality-exclusivity-norms-Bernabeu-et-al/blob/master/Dutch-modality-exclusivity-norms-RPubs.Rmd&#34;&gt;example&lt;/a&gt;), we must delete &lt;code&gt;runtime:shiny&lt;/code&gt; from the header, and disable reactive features, as below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
```r
# Number of words selected on sidebar
# reactive(cat(paste0(&amp;#39;Words selected below: &amp;#39;, nrow(selected_props()))))
```&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;free-accounts-and-tips&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Free accounts and tips&lt;/h5&gt;
&lt;p&gt;Hosting sites have specific terms of use. For instance, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;shinyapps.io&lt;/a&gt; has a free starter license with limited use. Free apps can handle a large but limited amount of data, and up to five apps may be created. Beyond this, RStudio offers a wide range of subscriptions starting at $9/month.&lt;/p&gt;
&lt;p&gt;Memory and traffic limits of the free shinyapps.io account can sometimes present problems when heavy data data sets are used, or there are many visits to the app. The memory overload issue is often flagged as &lt;code&gt;Shiny cannot use on-disk bookmarking&lt;/code&gt;, whereas excessive traffic may see the app not loading. Fortunately, usage limits need not always require a paid subscription or a &lt;a href=&#34;https://www.r-bloggers.com/alternative-approaches-to-scaling-shiny-with-rstudio-shiny-server-shinyproxy-or-custom-architecture/&#34;&gt;custom server&lt;/a&gt;, thanks to the following workarounds:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;develop app locally as far as possible, and only deploy to shinyapps.io only at the last stage;&lt;/li&gt;
&lt;li&gt;prune data set, leaving only the necessary data;&lt;/li&gt;
&lt;li&gt;if necessary, unlink data by splitting it into different sets, reducing computational demands;&lt;/li&gt;
&lt;li&gt;if necessary, use various apps (five are allowed in each free shinyapps.io account);&lt;/li&gt;
&lt;li&gt;if necessary, link from the app to a PDF with visualisations requiring heavy, interlinked data. High-resolution plots can be rendered into a PDF document in a snap, using code such as below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;pdf(&amp;#39;List of plots per page&amp;#39;, width = 13, height = 5)
print(plot1)
print(plot2)
# ...
print(plot150)
dev.off()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conveniently, all text in a PDF—even in plots—is indexed, so it can be searched [ Ctrl+f / Cmd+f / 🔍 ] (see &lt;a href=&#34;https://osf.io/2tpxn/&#34;&gt;example&lt;/a&gt;). Furthermore, you may also &lt;a href=&#34;http://www.ilovepdf.com/&#34;&gt;merge the rendered PDF with any other documents&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;prerequisites-and-suggestions-for-participation-in-each-workshop&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Prerequisites and suggestions for participation in each workshop&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Necessary:&lt;/em&gt; laptop or computer with &lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;https://rstudio.com/products/rstudio/download/&#34;&gt;RStudio&lt;/a&gt; installed, or access to &lt;a href=&#34;https://rstudio.cloud/&#34;&gt;RStudio Cloud&lt;/a&gt;; familiarity with the content of the preceding workshops through the web links herein.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Suggested:&lt;/em&gt; having your own data and R code ready (on a Github repository if participating in Workshop 4); participation in some of the preceding workshops.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;datathons-creating-reproducible-documents-and-dashboards&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Datathons: creating reproducible documents and dashboards&lt;/h3&gt;
&lt;p&gt;In these coding meetups, participants collaborate to create reproducible documents or dashboards using the data and software they prefer (see &lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/tree/master/examples-documents-dashboards&#34;&gt;examples&lt;/a&gt;). Since the work can be split across different people and sections, some nice products may be achieved within a session. Any programming languages may be used.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Data used:&lt;/strong&gt; academic or non-academic data of your own or from open-access sources such as &lt;a href=&#34;https://osf.io&#34;&gt;OSF&lt;/a&gt;, scientific journals, governments, international institutions, NGOs, etc.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inspired by the great &lt;a href=&#34;https://reprohack.github.io/reprohack-hq/&#34;&gt;Reprohacks&lt;/a&gt;, content suggestions are encouraged. That is, if you’d like to have a reproducible document or dashboard created for a certain, open-access data set, please let us know, and some participants may take it on. Suggestions may be posted as &lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/issues&#34;&gt;issues&lt;/a&gt; or emailed to &lt;a href=&#34;mailto:p.bernabeu@lancaster.ac.uk&#34; class=&#34;email&#34;&gt;p.bernabeu@lancaster.ac.uk&lt;/a&gt;.
&lt;br&gt;&lt;/br&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Purposes&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;collaborating to visualise data in novel ways using reproducible documents or interactive dashboards. For this purpose, participants sometimes draw on additional data to look at a bigger picture;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;reflecting on the process by reviewing the techniques applied and challenges encountered.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt; A key aspect of datathons is the creation of output. Documents and dashboards are (co-)authored by the participants who work on them, who can then publish them on their websites, or on &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;, &lt;a href=&#34;https://mybinder.org/&#34;&gt;Binder&lt;/a&gt;, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;Shinyapps&lt;/a&gt; or &lt;a href=&#34;https://rstudio.com/products/shiny/shiny-server/&#34;&gt;custom servers&lt;/a&gt;. Time constraints notwithstanding, a lot of this output may be very enticing for further development by the same participants, or even by other people if the code is shared online. Just like with data, an attribution licence can be attached to the code.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;prerequisites-and-suggestions-for-participation-in-datathons&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Prerequisites and suggestions for participation in datathons&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Necessary:&lt;/em&gt; basic knowledge of reproducible documents or dashboards.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Suggested:&lt;/em&gt; familiarity with the development of reproducible documents or dashboards; an idea about the data you’d like to work with and the kind of document or dashboard you want to create.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;contact&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Contact&lt;/h2&gt;
&lt;p&gt;Please submit any queries or requests by &lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/issues&#34;&gt;posting an issue&lt;/a&gt; or emailing &lt;a href=&#34;mailto:p.bernabeu@lancaster.ac.uk&#34; class=&#34;email&#34;&gt;p.bernabeu@lancaster.ac.uk&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>s</category>
      
            <category>programming</category>
      
            <category>education</category>
      
            <category>workshop</category>
      
            <category>datathon</category>
      
            <category>data presentation</category>
      
            <category>data visualisation</category>
      
            <category>dashboard</category>
      
            <category>reproducibility</category>
      
            <category>open science</category>
      
            <category>open data</category>
      
            <category>statistics</category>
      
            <category>R</category>
      
            <category>R Shiny</category>
      
            <category>Flexdashboard</category>
      
            <category>HTML</category>
      
            <category>CSS</category>
      
            <category>Software Sustainability Institute Fellowship</category>
      
      
            <category>education</category>
      
            <category>programming</category>
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Naive principal component analysis in R</title>
      <link>https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Principal_component_analysis&#34;&gt;Principal Component Analysis (PCA)&lt;/a&gt; is a technique used to find the core components that underlie different variables. It comes in very useful whenever doubts arise about the true origin of three or more variables. There are two main methods for performing a PCA: naive or less naive. In the naive method, you first check some conditions in your data which will determine the essentials of the analysis. In the less-naive method, you set those yourself based on whatever prior information or purposes you had. The latter method is appropriate when you already have enough information about the intercorrelations, or when you are required to select a specific number of components. I will tackle the naive method, mainly by following the guidelines in &lt;a href=&#34;https://freethegeogbooks.files.wordpress.com/2016/08/book-for-r-language-stats.pdf&#34;&gt;Field, Miles, and Field (2012)&lt;/a&gt;, with updated code where necessary. A &lt;a href=&#34;https://freethegeogbooks.files.wordpress.com/2016/08/book-for-r-language-stats.pdf&#34;&gt;manual by Charles M. Friel&lt;/a&gt; (Sam Houston State University) was also useful.&lt;/p&gt;
&lt;p&gt;The ‘naive’ approach is characterized by a first stage that checks whether the PCA should actually be performed with your current variables, or if some should be removed. The variables that are accepted are taken to a second stage which identifies the number of principal components that seem to underlie your set of variables.&lt;/p&gt;
&lt;div id=&#34;stage-1.-determine-whether-pca-is-appropriate-at-all-considering-the-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;STAGE 1. Determine whether PCA is appropriate at all, considering the variables&lt;/h2&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;45%&#34; src=&#34;https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/1.jpg&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Variables should be &lt;strong&gt;inter-correlated enough but not too much.&lt;/strong&gt; Field et al. (2012) provide some thresholds, suggesting that no variable should have many correlations below .30, or &lt;em&gt;any&lt;/em&gt; correlation at all above .90. Thus, in the example here, variable Q06 should probably be excluded from the PCA.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Bartlett’s test&lt;/strong&gt;, on the nature of the intercorrelations, should be significant. Significance suggests that the variables are not an ‘identity matrix’ in which correlations are a sampling error.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;KMO&lt;/strong&gt; (Kaiser-Meyer-Olkin), a measure of sampling adequacy based on common variance (so similar purpose as Bartlett’s). As Field et al. review, ‘values between .5 and .7 are mediocre, values between .7 and .8 are good, values between .8 and .9 are great and values above .9 are superb’ (p. 761). There’s a general score as well as one per variable. The general one will often be good, whereas the individual scores may more likely fail. Any variable with a score below .5 should probably be removed, and the test should be run again.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Determinant:&lt;/strong&gt; A formula about multicollinearity. The result should preferably fall below .00001.
Note that some of these tests are run on the dataframe and others on a correlation matrix of the data, as distinguished below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Necessary libraries
library(ltm)
library(lattice)
library(psych)
library(car)
library(pastecs)
library(scales)
library(ggplot2)
library(arules)
library(plyr)
library(Rmisc)
library(GPArotation)
library(gdata)
library(MASS)
library(qpcR)
library(dplyr)
library(gtools)
library(Hmisc)

# Select variables of interest for the PCA
dataset = mydata[, c(&amp;#39;select_var1&amp;#39;, &amp;#39;select_var1&amp;#39;, 
  &amp;#39;select_var2&amp;#39;, &amp;#39;select_var3&amp;#39;, &amp;#39;select_var4&amp;#39;, 
  &amp;#39;select_var5&amp;#39;, &amp;#39;select_var6&amp;#39;, &amp;#39;select_var7&amp;#39;)]

# Create matrix: some tests will require it
data_matrix = cor(dataset, use = &amp;#39;complete.obs&amp;#39;)

# See intercorrelations
round(data_matrix, 2)

# Bartlett&amp;#39;s
cortest.bartlett(dataset)

# KMO (Kaiser-Meyer-Olkin)
KMO(data_matrix)

# Determinant
det(data_matrix)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-2.-identify-number-of-components-aka-factors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;STAGE 2. Identify number of components (aka factors)&lt;/h2&gt;
&lt;p&gt;In this stage, principal components (formally called ‘factors’ at this stage) are identified among the set of variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The identification is done through a basic, ‘unrotated’ PCA. The number of components set a priori must equal the number of variables that are being tested.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Start off with unrotated PCA

pc1 = psych::principal(dataset, nfactors = length(dataset), rotate=&amp;quot;none&amp;quot;)
pc1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is an example result:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  ## Principal Components Analysis
  ## Call: psych::principal(r = eng_prop, nfactors = 3, rotate = &amp;quot;none&amp;quot;)
  ## Standardized loadings (pattern matrix) based upon correlation matrix
  ##           PC1   PC2  PC3 h2       u2 com
  ## Aud_eng -0.89  0.13 0.44  1 -2.2e-16 1.5
  ## Hap_eng  0.64  0.75 0.15  1  1.1e-16 2.0
  ## Vis_eng  0.81 -0.46 0.36  1 -4.4e-16 2.0
  ## 
  ##                        PC1  PC2  PC3
  ## SS loadings           1.87 0.79 0.34
  ## Proportion Var        0.62 0.26 0.11
  ## Cumulative Var        0.62 0.89 1.00
  ## Proportion Explained  0.62 0.26 0.11
  ## Cumulative Proportion 0.62 0.89 1.00
  ## 
  ## Mean item complexity =  1.9
  ## Test of the hypothesis that 3 components are sufficient.
  ## 
  ## The root mean square of the residuals (RMSR) is  0 
  ##  with the empirical chi square  0  with prob &amp;lt;  NA 
  ## 
  ## Fit based upon off diagonal values = 1&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Among the columns, there are first the correlations between variables and components, followed by a column (h2) with the &lt;strong&gt;‘communalities’&lt;/strong&gt;. If less factors than variables had been selected, communality values would be below 1. Then there is the uniqueness column (u2): &lt;strong&gt;uniqueness&lt;/strong&gt; is equal to 1 minus the communality. Next is ‘com’, which reflects the &lt;strong&gt;complexity&lt;/strong&gt; with which a variable relates to the principal components. Those components are precisely found below. The first row contains the sums of squared loadings, or eigenvalues, namely, the total variance explained by each linear component. This value corresponds to the number of units explained out of all possible factors (which were three in the above example). The rows below all cut from the same cloth. &lt;em&gt;Proportion var&lt;/em&gt; = variance explained over a total of 1. This is the result of dividing the eigenvalue by the number of components. Multiply by 100 and you get the percentage of total variance explained, which becomes useful. In the example, 99% of the variance has been explained. Aside from the meddling maths, we should actually expect 100% there because the number of factors equaled the number of variables. &lt;em&gt;Cumulative var:&lt;/em&gt; variance added consecutively up to the last component. &lt;em&gt;Proportion explained:&lt;/em&gt; variance explained over what has actually been explained (only when variables = factors is this the same as Proportion var). &lt;em&gt;Cumulative proportion:&lt;/em&gt; the actually explained variance added consecutively up to the last component (Field et al., 2012).&lt;/p&gt;
&lt;p&gt;According to Field et al. (2012), two criteria can be used to determine the number of components that should be carried forward to the next stage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SS (sum of squares) loadings, with two possible cut-off points. On the one hand, following Kaiser’s threshold, we should select components with SS loadings &amp;gt; 1. In the example result shown above, only PC1 meets this criterion. A more lenient alternative is Kaiser’s threshold, whereby SS loadings &amp;gt; .7 are accepted. In the example result above, PC2 meet this criterion too.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Scree plot: retain as many components as the number of points after the point of inflection. To create a scree plot, call:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plot(pc1$values, type = &amp;#39;b&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;35%&#34; src=&#34;https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/2.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Imagine a straight line &lt;strong&gt;from the first point on the right.&lt;/strong&gt; Once this line bends considerably, count the points after the bend and up to the last point on the left. The number of points is the number of components to select. The example here is probably the most complicated (two components were finally chosen), but often it’s easier &lt;a href=&#34;https://www.google.nl/search?sca_esv=582945116&amp;amp;q=select+principal+components+scree+plot+point+inflection&amp;amp;tbm=isch&#34;&gt;see examples&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Based on either or both criteria, select the definitive number of components.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-3.-run-definitive-pca&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;STAGE 3. Run definitive PCA&lt;/h2&gt;
&lt;p&gt;Run a very similar command as you did before, but now with a more advanced method. The first PCA, a heuristic one, worked essentially on the inter-correlations. The definitive PCA, in contrast, will implement a prior shuffling known as ‘rotation’, to ensure that the result is robust enough (just like cards are shuffled). Explained variance is captured better this way. The go-to rotation method is the orthogonal, or ‘varimax’ (though others may be considered too).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Now with varimax rotation, Kaiser-normalized by default:
pc2 = psych::principal(dataset, nfactors=2, rotate = &amp;quot;varimax&amp;quot;, 
scores = TRUE)
pc2
pc2$loadings

# Sanity check
pc2$residual
pc2$fit
pc2$communality&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to Field et al. (2012), we would want:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Less than half of &lt;strong&gt;residuals&lt;/strong&gt; with absolute values &amp;gt; 0.05&lt;/li&gt;
&lt;li&gt;Model &lt;strong&gt;fit&lt;/strong&gt; &amp;gt; .9&lt;/li&gt;
&lt;li&gt;All &lt;strong&gt;communalities&lt;/strong&gt; &amp;gt; .7&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If any of this fails, consider changing the number of factors. Next, the rotated components that have been ‘extracted’ from the core of the set of variables can be added to the dataset. This would enable the use of these components as new variables that might prove powerful and useful (as in &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.1551-6709.2010.01157.x/full&#34;&gt;this research&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dataset = cbind(dataset, pc2$scores)
summary(dataset$RC1, dataset$RC2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-4.-determine-ascription-of-each-variable-to-components&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;STAGE 4. Determine ascription of each variable to components&lt;/h2&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;55%&#34; src=&#34;https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/3.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Check the main summary by just calling pc2, and see how each variable correlates with the rotated components. This is essential because it reveals how variables load on each component, or in other words, to which component a variable belongs. For instance, the table shown here belongs to a study about the meaning of words (Bernabeu, 2018). These results suggest that the visual and haptic modalities of words are quite related, whereas the auditory modality is relatively unique. When the analysis works out well, a cut-off point of &lt;em&gt;r&lt;/em&gt; = .8 may be applied for considering a variable as part of a component.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-5.-enjoy-the-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;STAGE 5. Enjoy the plot&lt;/h2&gt;
&lt;p&gt;The plot is perhaps the coolest part about PCA. It really makes an awesome illustration of the power of data analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ggplot(eng_props,
  aes(RC1, RC2, label = as.character(main_eng))) + stat_density2d (color = &amp;quot;gray87&amp;quot;) +
  geom_text(size = ifelse(eng_props$word_eng %in% w_set, 12, 7),
	fontface = ifelse(eng_props$word_eng %in% w_set, &amp;#39;bold&amp;#39;, &amp;#39;plain&amp;#39;)) +
  geom_point(data=eng_props[eng_props$word_eng %in% w_set,], pch=21, fill=NA, size=14, stroke=2, alpha=.6) +
  labs(subtitle=&amp;#39;(Data from Lynott &amp;amp; Connell, 2009)&amp;#39;, x = &amp;quot;Varimax-rotated Principal Component 1&amp;quot;, 
	y = &amp;quot;Varimax-rotated Principal Component 2&amp;quot;) +	theme_bw() +   
  theme( plot.background = element_blank(), panel.grid.major = element_blank(),
	panel.grid.minor = element_blank(), panel.border = element_blank(),
  	axis.line = element_line(color = &amp;#39;black&amp;#39;),
	axis.title.x = element_text(colour = &amp;#39;black&amp;#39;, size = 23, margin=margin(15,15,15,15)),
	axis.title.y = element_text(colour = &amp;#39;black&amp;#39;, size = 23, margin=margin(15,15,15,15)),
	axis.text.x = element_text(size=16), axis.text.y  = element_text(size=16),
	plot.title = element_text(hjust = 0.5, size = 32, face = &amp;quot;bold&amp;quot;, margin=margin(15,15,15,15)),
	plot.subtitle = element_text(hjust = 0.5, size = 20, margin=margin(2,15,15,15)) ) +
  geom_label_repel(data = eng_props[eng_props$word_eng %in% w_set,], aes(label = word_eng), size = 8, 
	alpha = 0.77, color = &amp;#39;black&amp;#39;, box.padding = 1.5 )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is an example combining PCA plots with code similar to the above. These plots illustrate something further with regard to the relationships among modalities. In property words, the different modalities spread out more clearly than they do in concept words. This makes sense because in language, properties define concepts (Bernabeu, 2018).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An example of these analyses is &lt;a href=&#34;https://pablobernabeu.shinyapps.io/Dutch-modality-exclusivity-norms&#34;&gt;available in available in this RStudio environment&lt;/a&gt;, in the &lt;code&gt;norms.R&lt;/code&gt; script.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Bernabeu, P. (2018). &lt;em&gt;Dutch modality exclusivity norms for 336 properties and 411 concepts&lt;/em&gt;. PsyArXiv. &lt;a href=&#34;https://doi.org/10.31234/osf.io/s2c5h&#34; class=&#34;uri&#34;&gt;https://doi.org/10.31234/osf.io/s2c5h&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Field, A. P., Miles, J., &amp;amp; Field, Z. (2012). &lt;em&gt;Discovering Statistics Using R&lt;/em&gt;. London, UK: Sage.&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>s</category>
      
            <category>principal component analysis</category>
      
            <category>statistics</category>
      
            <category>dimensionality reduction</category>
      
            <category>R</category>
      
      
            <category>statistics</category>
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Web application: Dutch modality exclusivity norms</title>
      <link>https://pablobernabeu.github.io/applications-and-dashboards/bernabeu-2018-modalitynorms/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/applications-and-dashboards/bernabeu-2018-modalitynorms/</guid>
      <description>&lt;a href=&#39;https://pablobernabeu.shinyapps.io/Dutch-modality-exclusivity-norms&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #196F27; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt; 
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;i class=&#34;fas fa-mouse-pointer&#34;&gt;&lt;/i&gt;&lt;/span&gt;&amp;nbsp; Complete web application &lt;font style=&#39;font-size:60%;&#39;&gt;&lt;i&gt;Flexdashboard-Shiny&lt;/i&gt;&lt;/font&gt; &lt;/h3&gt;&lt;/button&gt;&lt;/a&gt;
&lt;br&gt;
&lt;br&gt;
&lt;a href=&#39;https://pablobernabeu.github.io/dashboards/Dutch-modality-exclusivity-norms&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #4CAF50; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt; 
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;i class=&#34;fas fa-mouse-pointer&#34;&gt;&lt;/i&gt;&lt;/span&gt;&amp;nbsp; Reduced dashboard &lt;font style=&#39;font-size:60%;&#39;&gt;&lt;i&gt;Flexdashboard&lt;/i&gt;&lt;/font&gt; &lt;/h3&gt;&lt;/button&gt;&lt;/a&gt; &amp;nbsp; 
&lt;br&gt;
&lt;br&gt;
&lt;p&gt;This web application presents linguistic data over several tabs. The code combines the great front-end of Flexdashboard—based on R Markdown and yielding an unmatched user interface—, with the great back-end of Shiny—allowing users to download sections of data they select, in various formats.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A nice find was the &amp;lsquo;reactable&amp;rsquo; package, which implements Javascript under the hood to allow the use of colours, bar charts, etc.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  
Auditory = colDef(header = with_tooltip(&#39;Auditory Rating&#39;,
                                        &#39;Mean rating of each word on the auditory modality across participants.&#39;),
                  cell = function(value) {
                    width &amp;lt;- paste0(value / max(table_data$Auditory) * 100, &amp;quot;%&amp;quot;)
                    value = sprintf(&amp;quot;%.2f&amp;quot;, round(value,2))  # Round to two digits, keeping trailing zeros
                    bar_chart(value, width = width, fill = &#39;#ff3030&#39;)
                    },
                  align = &#39;left&#39;),
  
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One of the hardest nuts to crack was allowing the full functionality of tables—i.e, scaling to screen, frozen header, and vertical and horizontal scrolling—whilst having tweaked the vertical/horizontal orientation of the dashboard sections. Initial clashes were sorted by adjusting the section&#39;s CSS styles&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Table {#table style=&amp;quot;background-color:#FCFCFC;&amp;quot;}
=======================================================================
  
Inputs {.sidebar style=&#39;position:fixed; padding-top: 65px; padding-bottom:30px;&#39;}
-----------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and by also adjusting the reactable settings.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  
renderReactable({
  reactable(selected_words(),
            defaultSorted = list(cat = &#39;desc&#39;, word = &#39;asc&#39;),
            defaultColDef = colDef(footerStyle = list(fontWeight = &amp;quot;bold&amp;quot;)),
            height = 840, striped = TRUE, pagination = FALSE, highlight = TRUE,
  
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A nice feature, especially suited to Flexdashboard, was the use of different formats across tabs. Whereas the Info tab presents long text using HTML and CSS styling, along with rmarkdown code output, the other tabs rely more strongly on Javascript features, enabled by R packages such as ‘shiny’ and sweetalert (e.g., allowing modal dialogs—pop-ups), reactable and plotly (e.g., allowing information opened by hovering—tooltips).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```{r}
  
# reactive for the word bar
highlighted_properties = reactive(input$highlighted_properties)
  
renderPlotly({
 ggplotly(
  ggplot( selected_props(), aes(RC1, RC2, label = as.character(word), color = main, 
    # Html tags below used for format. Decimals rounded to two.
    text = paste0(&#39; &#39;, &#39;&amp;lt;span style=&amp;quot;padding-top:3px; padding-bottom:3px; font-size:2.2em; color:#EEEEEE&amp;quot;&amp;gt;&#39;, capitalize(word), &#39;&amp;lt;/span&amp;gt; &#39;, &#39;&amp;lt;br&amp;gt;&#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Dominant modality: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, main, &#39; &#39;,
     &#39; &#39;, &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Modality exclusivity: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Exclusivity, 2)), &#39;% &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Perceptual strength: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Perceptualstrength, 2)),
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Auditory rating: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Auditory, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Haptic rating: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Haptic, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Visual rating: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Visual, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Concreteness (Brysbaert et al., 2014): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, 
       sprintf(&amp;quot;%.2f&amp;quot;, round(concrete_Brysbaertetal2014, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Number of letters: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, letters, &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Number of phonemes (DutchPOND): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, 
     round(phonemes_DUTCHPOND, 2), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Contextual diversity (lg10CD SUBTLEX-NL): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;,
       sprintf(&amp;quot;%.2f&amp;quot;, round(freq_lg10CD_SUBTLEXNL, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Word frequency (lg10WF SUBTLEX-NL): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;,
       sprintf(&amp;quot;%.2f&amp;quot;, round(freq_lg10WF_SUBTLEXNL, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Lemma frequency (CELEX): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, 
       sprintf(&amp;quot;%.2f&amp;quot;, round(freq_CELEX_lem, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Phonological neighbourhood size (DutchPOND): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, 
     round(phon_neighbours_DUTCHPOND, 2), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Orthographic neighbourhood size (DutchPOND): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;,
     round(orth_neighbours_DUTCHPOND, 2), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Age of acquisition (Brysbaert et al., 2014): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;,
     sprintf(&amp;quot;%.2f&amp;quot;, round(AoA_Brysbaertetal2014, 2)), &#39; &#39;, &#39;&amp;lt;br&amp;gt; &#39;
     ) ) ) +
  geom_text(size = ifelse(selected_props()$word %in% highlighted_properties(), 7,
             ifelse(is.null(highlighted_properties()), 3, 2.8)),
      fontface = ifelse(selected_props()$word %in% highlighted_properties(), &#39;bold&#39;, &#39;plain&#39;)) +
geom_point(alpha = 0) +  # This geom_point helps to colour the tooltip according to the dominant modality
scale_colour_manual(values = colours, drop = FALSE) + theme_bw() + ggtitle(&#39;Property words&#39;) +
labs(x = &#39;Varimax-rotated Principal Component 1&#39;, y = &#39;Varimax-rotated Principal Component 2&#39;) +
guides(color = guide_legend(title = &#39;Main&amp;lt;br&amp;gt;modality&#39;)) +
theme( plot.background = element_blank(), panel.grid.major = element_blank(),
   panel.grid.minor = element_blank(), panel.border = element_blank(),
   axis.line = element_line(color = &#39;black&#39;), plot.title = element_text(size = 14, hjust = .5),
   axis.title.x = element_text(colour = &#39;black&#39;, size = 12, margin = margin(15,15,0,15)),
   axis.title.y = element_text(colour = &#39;black&#39;, size = 12, margin = margin(0,15,15,5)),
   axis.text.x = element_text(size = 8), axis.text.y  = element_text(size = 8),
   legend.background = element_rect(size = 2), legend.position = &#39;none&#39;,
 legend.title = element_blank(),
 legend.text = element_text(colour = colours, size = 13) ),
tooltip = &#39;text&#39;
)
})
  
# For download, save plot without the interactive &#39;plotly&#39; part
  
properties_png = reactive({ ggplot(selected_props(), aes(RC1, RC2, color = main, label = as.character(word))) +
geom_text(show.legend = FALSE, size = ifelse(selected_props()$word %in% highlighted_properties(), 7,
         ifelse(is.null(highlighted_properties()), 3, 2.8)),
      fontface = ifelse(selected_props()$word %in% highlighted_properties(), &#39;bold&#39;, &#39;plain&#39;)) +
geom_point(alpha = 0) + scale_colour_manual(values = colours, drop = FALSE) + theme_bw() +
guides(color = guide_legend(title = &#39;Main&amp;lt;br&amp;gt;modality&#39;, override.aes = list(size = 7, alpha = 1))) +
ggtitle( paste0(&#39;Properties&#39;, &#39; (showing &#39;, nrow(selected_props()), &#39; out of &#39;, nrow(props), &#39;)&#39;) ) + 
labs(x = &#39;Varimax-rotated Principal Component 1&#39;, y = &#39;Varimax-rotated Principal Component 2&#39;) +
theme( plot.background = element_blank(), panel.grid.major = element_blank(),
   panel.grid.minor = element_blank(), panel.border = element_blank(),
   axis.line = element_line(color = &#39;black&#39;), plot.title = element_text(size = 17, hjust = .5, margin = margin(3,3,7,3)),
   axis.title.x = element_text(colour = &#39;black&#39;, size = 12, margin = margin(10,10,2,10)),
   axis.title.y = element_text(colour = &#39;black&#39;, size = 12, margin = margin(10,10,10,5)),
   axis.text.x = element_text(size = 8), axis.text.y  = element_text(size = 8),
   legend.background = element_rect(size = 2), legend.position = &#39;right&#39;,
   legend.title = element_blank(), legend.text = element_text(size = 15))
})
  
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The only instance in which I drew on javascript code outside R packages was to enable tooltips beyond the packages’ limits—for instance, in the side bar. This javascript feature is created at the top of the script, in the head area.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!-- Javascript function to enable a hovering tooltip --&amp;gt;
&amp;lt;script&amp;gt;
$(document).ready(function(){
   $(&#39;[data-toggle=&amp;quot;tooltip1&amp;quot;]&#39;).tooltip();
});
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the side bar, I added a reactive mean for each variable, complementing the range selector.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;reactive(cat(paste0(&#39;Mean = &#39;, 
  sprintf(&amp;quot;%.2f&amp;quot;, round(mean(selected_words()$Exclusivity),2)))))
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2 id=&#34;static-version-published-on-rpubs&#34;&gt;Static version published on RPubs&lt;/h2&gt;
&lt;p&gt;A reduced, &lt;a href=&#34;https://rpubs.com/pcbernabeu/Dutch-modality-exclusivity-norms&#34;&gt;&lt;em&gt;static&lt;/em&gt; version&lt;/a&gt; was also created to increase the availability of the content. Removing some reactivity features allows the dashboard to be published as a standard website (i.e., on a personal website, on &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;, etc.), without the need for a back-end Shiny server. Note that this type of website is dubbed &amp;lsquo;static&amp;rsquo;, but it can retain multiple interactive features thanks to Javascript-based tools under the hood, allowed by R packages such as &lt;code&gt;leaflet&lt;/code&gt; for maps, &lt;code&gt;DT&lt;/code&gt; for tables, &lt;code&gt;plotly&lt;/code&gt; for plots, etc.&lt;/p&gt;
&lt;p&gt;To create the Flexdashboard-only version departing from the Flexdashboard-Shiny version, I deleted &lt;code&gt;runtime: shiny&lt;/code&gt; from the YAML header, and disabled Shiny reactive inputs and objects, as below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```{r}
# Number of words selected on sidebar
# reactive(cat(paste0(&#39;Words selected below: &#39;, nrow(selected_props()))))
```
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;div class = &#39;hanging-indent&#39;&gt;
&lt;p&gt;Bernabeu, P. (2018). Dutch modality exclusivity norms for 336 properties and 411 concepts [Web application]. Retrieved from &lt;a href=&#34;https://pablobernabeu.shinyapps.io/Dutch-Modality-Exclusivity-Norms&#34;&gt;https://pablobernabeu.shinyapps.io/Dutch-Modality-Exclusivity-Norms&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;br&gt;
</description>
      
            <category>web application</category>
      
            <category>data dashboard</category>
      
            <category>Flexdashboard</category>
      
            <category>R</category>
      
            <category>R Shiny</category>
      
            <category>statistics</category>
      
            <category>regression</category>
      
            <category>principal component analysis</category>
      
            <category>modality exclusivity norms</category>
      
            <category>Dutch</category>
      
            <category>linguistics</category>
      
            <category>HTML</category>
      
            <category>CSS</category>
      
      
            <category>linguistic materials</category>
      
            <category>research methods</category>
      
            <category>web-application</category>
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>The case for data dashboards: First steps in R Shiny</title>
      <link>https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/</guid>
      <description>


&lt;div style=&#34;font-size:110%;&#34;&gt;
&lt;b&gt; Dashboards for data visualisation, such as &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;R Shiny&lt;/a&gt; and &lt;a href=&#34;https://www.tableau.com/&#34;&gt;Tableau&lt;/a&gt;, allow the interactive exploration of data by means of drop-down lists and checkboxes, with no coding required from the final users. These web applications can be useful for both the data analyst and the public at large. &lt;/b&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;25%&#34; src=&#34;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/1.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Visualisation apps run on internet browsers. This allows for three options: private viewing (useful during analysis), selective sharing (used within work groups), or internet publication. Among the available platforms, &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;R Shiny&lt;/a&gt; and &lt;a href=&#34;https://www.tableau.com/&#34;&gt;Tableau&lt;/a&gt; stand out due to being relatively accessible to new users. Apps serve a broad variety of purposes (see &lt;a href=&#34;https://shiny.rstudio.com/gallery/&#34;&gt;this gallery&lt;/a&gt; and &lt;a href=&#34;https://www.tableau.com/products/desktop&#34;&gt;this one&lt;/a&gt;). In science and beyond, these apps allow us to go &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01782/full&#34;&gt;the extra mile in sharing data&lt;/a&gt;. Alongside files and code shared in repositories, we can present the data in a website, in the form of plots or tables. This facilitates the public exploration of each section of the data (groups, participants, trials…) &lt;a href=&#34;https://www.nature.com/articles/d41586-019-01506-x&#34;&gt;to anyone interested, and allows researchers to account for their proceeding&lt;/a&gt; in the analysis.&lt;/p&gt;
&lt;p&gt;&lt;img width = &#39;70%&#39; src=&#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/2.png&#39; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#39;60%&#39; src=&#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/3.png&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Publishers and journals highly encourage authors to make the most of their data by facilitating its easy exploration by the readership–even though they don’t normally offer the possibility of hosting any web applications yet.&lt;/p&gt;
&lt;p&gt;Apps can also prove valuable to those analysing the data. For instance, my app helped me to identify the extent of noise in a section of the data. Instead of running through a heavy score of code, the drop-down lists of the app let me seamlessly surf through the different sections.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At a certain point, I found a data section that was consistently noisier than the rest, and eventually I had to discard it from further statistical analyses. Yet, instead of removing that from the app, I maintained it with a note attached. This particular trait in the data was rather salient.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/5.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Beyond such a salient feature in the data, a visualisation app may also help to spot subtler patterns such as third variables or individual differences.&lt;/p&gt;
&lt;p&gt;There are several platforms for creating apps (e.g., Tableau, D3.js, and R Shiny). I focus on R Shiny here for three reasons: it is affordable to use, fairly accessible to new users, and well suited for science as it is based on the R language (see for instance &lt;a href=&#34;https://doi.org/10.1080/10691898.2018.1436999&#34;&gt;this article&lt;/a&gt;).&lt;/p&gt;
&lt;div id=&#34;how-to-shiny&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;How to Shiny&lt;/h4&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#39;30%&#39; src=&#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/6.png&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Shiny apps draw on any standard R code that you may already have. This is most commonly plots or tables, but other stuff such as images or Markdown texts are valid too. This is a nice thing to keep in mind when having to create a new app. Part of the job may already be done! The app is distributed among five different areas.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data file(s): These are any data files you’re using (e.g., with csv or rds extensions).&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;a.-server.r-script&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;1a. &lt;code&gt;server.R&lt;/code&gt; script&lt;/h5&gt;
&lt;p&gt;The &lt;code&gt;server&lt;/code&gt; script contains the central processes: plots, tables, etc. Code that existed independently of the app app may be brought into this script by slightly adapting it. At the top, call the &lt;code&gt;shiny&lt;/code&gt; library and any others used (e.g., ‘ggplot2’), and also read in the data. The snippet below shows the beginning of an example &lt;code&gt;server.R&lt;/code&gt; script.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
# server

library(shiny)
library(ggplot2)

EEG.ParticipantAndElectrode = readRDS(&amp;#39;EEG.ParticipantAndElectrode.rds&amp;#39;)
EEG.ParticipantAndBrainArea = readRDS(&amp;#39;EEG.ParticipantAndBrainArea.rds&amp;#39;)
EEG.GroupAndElectrode = readRDS(&amp;#39;EEG.GroupAndElectrode.rds&amp;#39;)
EEG.OLDGroupAndElectrode = readRDS(&amp;#39;EEG.OLDGroupAndElectrode.rds&amp;#39;)


server =

shinyServer(

  function(input, output) {

# plot_GroupAndElectrode:
    output$plot_GroupAndElectrode &amp;lt;- renderPlot({

dfelectrode &amp;lt;- aggregate(microvolts ~ electrode*time*condition, 
EEG.GroupAndElectrode[EEG.GroupAndElectrode$RT.based_Groups==input$var.Group,], mean)

df2 &amp;lt;- subset(dfelectrode, electrode == input$var.Electrodes.1)

df2$condition= as.factor(df2$condition)
df2$condition &amp;lt;- gsub(&amp;#39;visual2visual&amp;#39;, &amp;#39; Visual / Visual&amp;#39;, df2$condition)
df2$condition &amp;lt;- gsub(&amp;#39;haptic2visual&amp;#39;, &amp;#39; Haptic / Visual&amp;#39;, df2$condition)
df2$condition &amp;lt;- gsub(&amp;#39;auditory2visual&amp;#39;, &amp;#39; Auditory / Visual&amp;#39;, df2$condition)

df2$time &amp;lt;- as.integer(as.character(df2$time))
colours &amp;lt;- c(&amp;#39;firebrick1&amp;#39;, &amp;#39;dodgerblue&amp;#39;, &amp;#39;forestgreen&amp;#39;)
# green:visual2visual, blue:haptic2visual, red:auditory2visual

spec_title = paste0(&amp;#39;ERP waveforms for &amp;#39;, input$var.Group, &amp;#39; Group, Electrode &amp;#39;, input$var.Electrodes.1, &amp;#39; (negative values upward; time windows displayed)&amp;#39;)

plot_GroupAndElectrode = ggplot(df2, aes(x=time, y=-microvolts, color=condition)) +
  geom_rect(xmin=160, xmax=216, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=270, xmax=370, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=350, xmax=550, ymin=8, ymax=-7.5, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=500, xmax=750, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_line(size=1, alpha = 1) + scale_linetype_manual(values=colours) +
  scale_y_continuous(limits=c(-8.38, 8.3), breaks=seq(-8,8,by=1), expand = c(0,0.1)) +
  scale_x_continuous(limits=c(-208,808),breaks=seq(-200,800,by=100), expand = c(0.005,0), labels= c(&amp;#39;-200&amp;#39;,&amp;#39;-100 ms&amp;#39;,&amp;#39;0&amp;#39;,&amp;#39;100 ms&amp;#39;,&amp;#39;200&amp;#39;,&amp;#39;300 ms&amp;#39;,&amp;#39;400&amp;#39;,&amp;#39;500 ms&amp;#39;,&amp;#39;600&amp;#39;,&amp;#39;700 ms&amp;#39;,&amp;#39;800&amp;#39;)) +
  ggtitle(spec_title) + theme_bw() + geom_vline(xintercept=0) +
  annotate(geom=&amp;#39;segment&amp;#39;, y=seq(-8,8,1), yend=seq(-8,8,1), x=-4, xend=8, color=&amp;#39;black&amp;#39;) +
  annotate(geom=&amp;#39;segment&amp;#39;, y=-8.2, yend=-8.38, x=seq(-200,800,100), xend=seq(-200,800,100), color=&amp;#39;black&amp;#39;) +
  geom_segment(x = -200, y = 0, xend = 800, yend = 0, size=0.5, color=&amp;#39;black&amp;#39;) +
  theme(legend.position = c(0.100, 0.150), legend.background = element_rect(fill=&amp;#39;#EEEEEE&amp;#39;, size=0),
	axis.title=element_blank(), legend.key.width = unit(1.2,&amp;#39;cm&amp;#39;), legend.text=element_text(size=17),
	legend.title = element_text(size=17, face=&amp;#39;bold&amp;#39;), plot.title= element_text(size=20, hjust = 0.5, vjust=2),
	axis.text.y = element_blank(), axis.text.x = element_text(size = 14, vjust= 2.12, face=&amp;#39;bold&amp;#39;, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;),
	axis.ticks=element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), 
	panel.grid.minor = element_blank(), plot.margin = unit(c(0.1,0.1,0,0), &amp;#39;cm&amp;#39;)) +
  annotate(&amp;#39;segment&amp;#39;, x=160, xend=216, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=270, xend=370, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=350, xend=550, y=-7.5, yend=-7.5, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=500, xend=750, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  scale_fill_manual(name = &amp;#39;Context / Target trial&amp;#39;, values=colours) +
  scale_color_manual(name = &amp;#39;Context / Target trial&amp;#39;, values=colours) +
  guides(linetype=guide_legend(override.aes = list(size=1.2))) +
   guides(color=guide_legend(override.aes = list(size=2.5))) +
# Print y axis labels within plot area:
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;\u2013&amp;#39; * &amp;#39;3 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = 3, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;) +
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;+3 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = -3, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;) +
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;\u2013&amp;#39; * &amp;#39;6 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = 6, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;)

print(plot_GroupAndElectrode)

output$downloadPlot.1 &amp;lt;- downloadHandler(
	filename &amp;lt;- function(file){
	paste0(input$var.Group, &amp;#39; group, electrode &amp;#39;, input$var.Electrodes.1, &amp;#39;, &amp;#39;, Sys.Date(), &amp;#39;.png&amp;#39;)},
   	content &amp;lt;- function(file){
      		png(file, units=&amp;#39;in&amp;#39;, width=13, height=5, res=900)
      		print(plot_GroupAndElectrode)
      		dev.off()},
	contentType = &amp;#39;image/png&amp;#39;)
  } )

# ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://osf.io/uj8z4/&#34;&gt;— Whole script&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b.-ui.r-script&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;1b. &lt;code&gt;ui.R&lt;/code&gt; script&lt;/h5&gt;
&lt;p&gt;The &lt;code&gt;ui&lt;/code&gt; script defines the user interface. For instance, a factor column in the data that has multiple categories may be neatly displayed with a drop-down list on the side bar of the website. The interface may present a central plot before by a legend key below. The snippet below shows the beginning of an example &lt;code&gt;ui.R&lt;/code&gt; script.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
# UI

library(shiny)
library(ggplot2)

EEG.GroupAndElectrode = readRDS(&amp;#39;EEG.GroupAndElectrode.rds&amp;#39;)
EEG.ParticipantAndBrainArea = readRDS(&amp;#39;EEG.ParticipantAndBrainArea.rds&amp;#39;)
EEG.ParticipantAndElectrode = readRDS(&amp;#39;EEG.ParticipantAndElectrode.rds&amp;#39;)
EEG.OLDGroupAndElectrode = readRDS(&amp;#39;EEG.OLDGroupAndElectrode.rds&amp;#39;)


ui =

shinyUI(

   fluidPage(

    tags$head(tags$link(rel=&amp;#39;shortcut icon&amp;#39;, href=&amp;#39;https://image.ibb.co/fXUwzb/favic.png&amp;#39;)),  # web favicon
    tags$meta(charset=&amp;#39;UTF-8&amp;#39;),
    tags$meta(name=&amp;#39;description&amp;#39;, content=&amp;#39;This R Shiny visualisation dashboard presents data from a psycholinguistic ERP experiment (Bernabeu et al., 2017).&amp;#39;),
    tags$meta(name=&amp;#39;keywords&amp;#39;, content=&amp;#39;R, Shiny, ggplot2, visualisation, data, psycholinguistics, conceptual processing, modality switch, embodied cognition&amp;#39;),
    tags$meta(name=&amp;#39;viewport&amp;#39;, content=&amp;#39;width=device-width, initial-scale=1.0&amp;#39;),

    titlePanel(h3(strong(&amp;#39;Waveforms in detail from an ERP experiment on the Conceptual Modality Switch&amp;#39;), a(&amp;#39;(Bernabeu et al., 2017)&amp;#39;,
    href=&amp;#39;https://figshare.com/articles/EEG_study_on_conceptual_modality-switching_Bernabeu_et_al_in_prep_/4210863&amp;#39;, target=&amp;#39;_blank&amp;#39;,
    style = &amp;#39;color:#3E454E; text-decoration:underline; font-weight:normal&amp;#39;), 	align = &amp;#39;center&amp;#39;, style = &amp;#39;color:black&amp;#39;),

    windowTitle = &amp;#39;Visualization of ERP waveforms from experiment on Conceptual Modality Switch (Bernabeu et al., 2017)&amp;#39;),


    sidebarLayout(
	sidebarPanel(width = 2,


# Condition 1 for reactivity between tabs and sidebars

   conditionalPanel(
	condition = &amp;#39;input.tabvals == 1&amp;#39;,

	h5(a(strong(&amp;#39;See paper, statistics, all data.&amp;#39;), &amp;#39;Plots by group and brain area shown in paper.&amp;#39;,
	href=&amp;#39;https://figshare.com/articles/EEG_study_on_conceptual_modality-switching_Bernabeu_et_al_in_prep_/4210863&amp;#39;,
	target=&amp;#39;_blank&amp;#39;), align = &amp;#39;center&amp;#39;),
br(),

	selectInput(&amp;#39;var.Group&amp;#39;, label = &amp;#39;Group&amp;#39;, choices = list(&amp;#39;Quick&amp;#39;,&amp;#39;Slow&amp;#39;), selected = &amp;#39;Quick&amp;#39;),
	h6(&amp;#39;Quick G.: 23 participants&amp;#39;),
	h6(&amp;#39;Slow G.: 23 participants&amp;#39;),
br(),

	selectInput(&amp;#39;var.Electrodes.1&amp;#39;, label = h5(strong(&amp;#39;Electrode&amp;#39;), br(), &amp;#39;(see montage below)&amp;#39;),
                  choices = list(&amp;#39;1&amp;#39;,&amp;#39;2&amp;#39;,&amp;#39;3&amp;#39;,&amp;#39;4&amp;#39;,&amp;#39;5&amp;#39;,&amp;#39;6&amp;#39;,&amp;#39;7&amp;#39;,&amp;#39;8&amp;#39;,&amp;#39;9&amp;#39;,&amp;#39;10&amp;#39;,
			&amp;#39;11&amp;#39;,&amp;#39;12&amp;#39;,&amp;#39;13&amp;#39;,&amp;#39;14&amp;#39;,&amp;#39;15&amp;#39;,&amp;#39;16&amp;#39;,&amp;#39;17&amp;#39;,&amp;#39;18&amp;#39;,&amp;#39;19&amp;#39;,&amp;#39;20&amp;#39;,&amp;#39;21&amp;#39;,
			&amp;#39;22&amp;#39;,&amp;#39;23&amp;#39;,&amp;#39;24&amp;#39;,&amp;#39;25&amp;#39;,&amp;#39;26&amp;#39;,&amp;#39;27&amp;#39;,&amp;#39;28&amp;#39;,&amp;#39;29&amp;#39;,&amp;#39;30&amp;#39;,&amp;#39;31&amp;#39;,&amp;#39;33&amp;#39;,
			&amp;#39;34&amp;#39;,&amp;#39;35&amp;#39;,&amp;#39;36&amp;#39;,&amp;#39;37&amp;#39;,&amp;#39;38&amp;#39;,&amp;#39;39&amp;#39;,&amp;#39;40&amp;#39;,&amp;#39;41&amp;#39;,&amp;#39;42&amp;#39;,&amp;#39;43&amp;#39;,&amp;#39;44&amp;#39;,
			&amp;#39;45&amp;#39;,&amp;#39;46&amp;#39;,&amp;#39;47&amp;#39;,&amp;#39;48&amp;#39;,&amp;#39;49&amp;#39;,&amp;#39;50&amp;#39;,&amp;#39;51&amp;#39;,&amp;#39;52&amp;#39;,&amp;#39;53&amp;#39;,&amp;#39;54&amp;#39;,&amp;#39;55&amp;#39;,
			&amp;#39;56&amp;#39;,&amp;#39;57&amp;#39;,&amp;#39;58&amp;#39;,&amp;#39;59&amp;#39;,&amp;#39;60&amp;#39;), selected = &amp;#39;30&amp;#39; ),
br(), br(),

	h6(&amp;#39;Source code:&amp;#39;),
	h6(strong(&amp;#39;-  &amp;#39;), a(&amp;#39;server.R&amp;#39;, href=&amp;#39;https://osf.io/uj8z4/&amp;#39;, target=&amp;#39;_blank&amp;#39;, style = &amp;#39;text-decoration: underline;&amp;#39;)),
	h6(strong(&amp;#39;-  &amp;#39;), a(&amp;#39;ui.R&amp;#39;, href=&amp;#39;https://osf.io/8bwcx/&amp;#39;, target=&amp;#39;_blank&amp;#39;, style = &amp;#39;text-decoration: underline;&amp;#39;)),
br(),
	h6(a(&amp;#39;CC-By 4.0 License&amp;#39;, href=&amp;#39;https://osf.io/97unm/&amp;#39;, target=&amp;#39;_blank&amp;#39;), align = &amp;#39;center&amp;#39;, style = &amp;#39;text-decoration: underline;&amp;#39;),

br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(),
br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(),

	h5(a(strong(&amp;#39;See paper, statistics, all data.&amp;#39;),
	href=&amp;#39;https://figshare.com/articles/EEG_study_on_conceptual_modality-switching_Bernabeu_et_al_in_prep_/4210863&amp;#39;,
	target=&amp;#39;_blank&amp;#39;), align = &amp;#39;center&amp;#39;),
br(), br(), br(), br(), br(), br(), br(), br()
),

# ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://osf.io/8bwcx&#34;&gt;— Whole script&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;deployment-and-logs&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2. Deployment and logs&lt;/h5&gt;
&lt;p&gt;This script contains the commands for deploying the app on- or off-line, and for checking the session logs in case of any errors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;automatically-created-folder&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3. Automatically created folder&lt;/h5&gt;
&lt;p&gt;When the app is first deployed on the internet, a subfolder is automatically created with the name ‘rsconnect’. This folder contains a text file which can be used to modify the URL and the title of the webpage.&lt;/p&gt;
&lt;p&gt;Steps to create a Shiny app from scratch:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://shiny.rstudio.com/articles/shinyapps.html&#34;&gt;&lt;strong&gt;1. Tutorials (link).&lt;/strong&gt; Being open-source software, excellent directions are available through a Google search.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/7.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://shiny.rstudio.com/articles/shinyapps.html&#34;&gt;The core ideas are:&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As mentioned above, create a &lt;code&gt;ui.R&lt;/code&gt; script for the code containing the user interface, and create a &lt;code&gt;server.R&lt;/code&gt; script for the code containing the main content (your plots / tables, etc).&lt;/p&gt;
&lt;p&gt;At the top of both ui.R and server.R scripts, enter the command &lt;code&gt;library(shiny)&lt;/code&gt; and also load any other libraries you’re using (e.g., &lt;code&gt;ggplot2&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Test your app by deploying it locally, before launching online. For this purpose, first save the &lt;code&gt;ui&lt;/code&gt; and &lt;code&gt;server&lt;/code&gt; parts independently, as in:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
ui =

 shinyUI(

   fluidPage(

# ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then deploy locally by running:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;shinyApp(ui, server)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Managing to run the app locally is a great first step before launching online (which may sometimes prove a bit trickier).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://shiny.rstudio.com/articles/shinyapps.html&#34;&gt;&lt;strong&gt;2. User token (link).&lt;/strong&gt; Sign up and read in your private key—just to be done once in a computer.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Go for it.&lt;/strong&gt; After locally testing and saving the two main scripts (&lt;code&gt;ui.R&lt;/code&gt; and &lt;code&gt;server.R&lt;/code&gt;), run &lt;code&gt;deployApp()&lt;/code&gt; to launch the app online.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Bugs and session logs.&lt;/strong&gt; Most often they won’t be bugs actually, but fancies, as it were. For instance, some special characters have to get even more special (technically, UTF-8 encoding). For a character such as ‘μ’, Shiny prefers ‘Âμ’, or better, the Unicode &lt;code&gt;expression(&#34;\u03bc&#34;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Cling to your logs by calling the line below, which you may keep at hand in your ‘Shiny deployer.R’ script.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;showLogs(appPath = getwd(), appFile = NULL, appName = NULL, account = NULL,
entries = 50, streaming = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At best, the log output will mention any typos and unaccepted characters, pointing to specific lines in your code.&lt;/p&gt;
&lt;p&gt;It may take a couple of intense days to get a first Shiny app running. Although the usual rabbit holes do exist, years of Shiny have already yielded a sizeable body of free resources online (tutorials, blogs, vlogs). Moreover, there’s also &lt;a href=&#34;https://community.rstudio.com/&#34;&gt;the RStudio Community&lt;/a&gt;, and then StackOverflow etc., where you can post any needs/despair. Post your code, log, and explanation, and you’ll be rescued out in a couple of days. Long live those contributors.&lt;/p&gt;
&lt;p&gt;It’s sometimes enough to upload a bare app, but you might then think it can look better.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5&lt;/strong&gt; (optional). &lt;strong&gt;Advance.&lt;/strong&gt; Use tabs to combine multiple apps on one webpage, use different widgets, include a download option, etc. Tutorials like &lt;a href=&#34;https://www.youtube.com/watch?v=Q9sRKkaNveI&#34;&gt;this one on Youtube&lt;/a&gt; can take you there, especially those that provide the code, as in the description of that video. Use those scripts as templates. For example, I made use of tabs on the top of the dashboard in order to keep the side bar from having too many widgets. The appearance of these tabs can be adjusted. More importantly, the inputs in the sidebar can be modified depending on the active tab, by means of ‘reactivity’ conditions.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
mainPanel(

	tags$style(HTML(&amp;#39;
	    .tabbable &amp;gt; .nav &amp;gt; li &amp;gt; a                  		{background-color:white; color:#3E454E}
	    .tabbable &amp;gt; .nav &amp;gt; li &amp;gt; a:hover            		{background-color:#002555; color:white}
	    .tabbable &amp;gt; .nav &amp;gt; li[class=active] &amp;gt; a 		{background-color:#ECF4FF; color:black}
	    .tabbable &amp;gt; .nav &amp;gt; li[class=active] &amp;gt; a:hover	{background-color:#E7F1FF; color:black}
	&amp;#39;)),

	tabsetPanel(id=&amp;#39;tabvals&amp;#39;,

            tabPanel(value=1, h4(strong(&amp;#39;Group &amp;amp; Electrode&amp;#39;)), br(), plotOutput(&amp;#39;plot_GroupAndElectrode&amp;#39;),
			h5(a(strong(&amp;#39;See plots with 95% Confidence Intervals&amp;#39;), href=&amp;#39;https://osf.io/2tpxn/&amp;#39;,
			target=&amp;#39;_blank&amp;#39;), style=&amp;#39;text-decoration: underline;&amp;#39;), 
			downloadButton(&amp;#39;downloadPlot.1&amp;#39;, &amp;#39;Download HD plot&amp;#39;), br(), br(),
			# EEG montage
			img(src=&amp;#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&amp;#39;, height=500, width=1000)),

            tabPanel(value=2, h4(strong(&amp;#39;Participant &amp;amp; Area&amp;#39;)), br(), plotOutput(&amp;#39;plot_ParticipantAndLocation&amp;#39;),
			h5(a(strong(&amp;#39;See plots with 95% Confidence Intervals&amp;#39;), href=&amp;#39;https://osf.io/86ch9/&amp;#39;,
			target=&amp;#39;_blank&amp;#39;), style=&amp;#39;text-decoration: underline;&amp;#39;), 
			downloadButton(&amp;#39;downloadPlot.2&amp;#39;, &amp;#39;Download HD plot&amp;#39;), br(), br(),
			# EEG montage
			img(src=&amp;#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&amp;#39;, height=500, width=1000)),

            tabPanel(value=3, h4(strong(&amp;#39;Participant &amp;amp; Electrode&amp;#39;)), br(), plotOutput(&amp;#39;plot_ParticipantAndElectrode&amp;#39;),
			br(), downloadButton(&amp;#39;downloadPlot.3&amp;#39;, &amp;#39;Download HD plot&amp;#39;), br(), br(),
			# EEG montage
			img(src=&amp;#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&amp;#39;, height=500, width=1000)),

            tabPanel(value=4, h4(strong(&amp;#39;OLD Group &amp;amp; Electrode&amp;#39;)), br(), plotOutput(&amp;#39;plot_OLDGroupAndElectrode&amp;#39;),
			h5(a(strong(&amp;#39;See plots with 95% Confidence Intervals&amp;#39;), href=&amp;#39;https://osf.io/dvs2z/&amp;#39;,
			target=&amp;#39;_blank&amp;#39;), style=&amp;#39;text-decoration: underline;&amp;#39;), 
			downloadButton(&amp;#39;downloadPlot.4&amp;#39;, &amp;#39;Download HD plot&amp;#39;), br(), br(),
			# EEG montage
			img(src=&amp;#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&amp;#39;, height=500, width=1000))
	),
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://shiny.rstudio.com/gallery/&#34;&gt;official Shiny gallery&lt;/a&gt; offers a great array of apps including their code (e.g., &lt;a href=&#34;https://shiny.rstudio.com/gallery/kmeans-example.html&#34;&gt;basic example&lt;/a&gt;). Another feature you may add is the option to download your plots, tables, data…&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
# In ui.R script

downloadButton(&amp;#39;downloadPlot.1&amp;#39;, &amp;#39;Download HD plot&amp;#39;)

#___________________________________________________


# In server.R script

spec_title = paste0(&amp;#39;ERP waveforms for &amp;#39;, input$var.Group, &amp;#39; Group, Electrode &amp;#39;, input$var.Electrodes.1, &amp;#39; (negative values upward; time windows displayed)&amp;#39;)

plot_GroupAndElectrode = ggplot(df2, aes(x=time, y=-microvolts, color=condition)) +
  geom_rect(xmin=160, xmax=216, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=270, xmax=370, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=350, xmax=550, ymin=8, ymax=-7.5, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=500, xmax=750, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_line(size=1, alpha = 1) + scale_linetype_manual(values=colours) +
  scale_y_continuous(limits=c(-8.38, 8.3), breaks=seq(-8,8,by=1), expand = c(0,0.1)) +
  scale_x_continuous(limits=c(-208,808),breaks=seq(-200,800,by=100), expand = c(0.005,0), labels= c(&amp;#39;-200&amp;#39;,&amp;#39;-100 ms&amp;#39;,&amp;#39;0&amp;#39;,&amp;#39;100 ms&amp;#39;,&amp;#39;200&amp;#39;,&amp;#39;300 ms&amp;#39;,&amp;#39;400&amp;#39;,&amp;#39;500 ms&amp;#39;,&amp;#39;600&amp;#39;,&amp;#39;700 ms&amp;#39;,&amp;#39;800&amp;#39;)) +
  ggtitle(spec_title) + theme_bw() + geom_vline(xintercept=0) +
  annotate(geom=&amp;#39;segment&amp;#39;, y=seq(-8,8,1), yend=seq(-8,8,1), x=-4, xend=8, color=&amp;#39;black&amp;#39;) +
  annotate(geom=&amp;#39;segment&amp;#39;, y=-8.2, yend=-8.38, x=seq(-200,800,100), xend=seq(-200,800,100), color=&amp;#39;black&amp;#39;) +
  geom_segment(x = -200, y = 0, xend = 800, yend = 0, size=0.5, color=&amp;#39;black&amp;#39;) +
  theme(legend.position = c(0.100, 0.150), legend.background = element_rect(fill=&amp;#39;#EEEEEE&amp;#39;, size=0),
	axis.title=element_blank(), legend.key.width = unit(1.2,&amp;#39;cm&amp;#39;), legend.text=element_text(size=17),
	legend.title = element_text(size=17, face=&amp;#39;bold&amp;#39;), plot.title= element_text(size=20, hjust = 0.5, vjust=2),
	axis.text.y = element_blank(), axis.text.x = element_text(size = 14, vjust= 2.12, face=&amp;#39;bold&amp;#39;, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;),
	axis.ticks=element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), 
	panel.grid.minor = element_blank(), plot.margin = unit(c(0.1,0.1,0,0), &amp;#39;cm&amp;#39;)) +
  annotate(&amp;#39;segment&amp;#39;, x=160, xend=216, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=270, xend=370, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=350, xend=550, y=-7.5, yend=-7.5, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=500, xend=750, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  scale_fill_manual(name = &amp;#39;Context / Target trial&amp;#39;, values=colours) +
  scale_color_manual(name = &amp;#39;Context / Target trial&amp;#39;, values=colours) +
  guides(linetype=guide_legend(override.aes = list(size=1.2))) +
   guides(color=guide_legend(override.aes = list(size=2.5))) +
# Print y axis labels within plot area:
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;\u2013&amp;#39; * &amp;#39;3 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = 3, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;) +
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;+3 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = -3, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;) +
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;\u2013&amp;#39; * &amp;#39;6 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = 6, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;)

print(plot_GroupAndElectrode)

output$downloadPlot.1 &amp;lt;- downloadHandler(
	filename &amp;lt;- function(file){
	paste0(input$var.Group, &amp;#39; group, electrode &amp;#39;, input$var.Electrodes.1, &amp;#39;, &amp;#39;, Sys.Date(), &amp;#39;.png&amp;#39;)},
   	content &amp;lt;- function(file){
      		png(file, units=&amp;#39;in&amp;#39;, width=13, height=5, res=900)
      		print(plot_GroupAndElectrode)
      		dev.off()},
	contentType = &amp;#39;image/png&amp;#39;)
  } )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Apps can include any text, such as explanations of any length and web links. For instance, we can link back to the data repository, where the code for the app can be found.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/8.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An example of a &lt;a href=&#34;https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment/&#34;&gt;Shiny app is available&lt;/a&gt;, which may also be &lt;a href=&#34;https://mybinder.org/v2/gh/pablobernabeu/Modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing/master?urlpath=rstudio&#34;&gt;edited and run in this RStudio environment&lt;/a&gt;, inside the ‘Shiny-app’ folder.&lt;/p&gt;
&lt;p&gt;The Shiny server (shinyapps.io) allows publishing dashboards built with various frameworks besides Shiny proper. &lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/&#34;&gt;Flexdashboard&lt;/a&gt; and &lt;a href=&#34;https://rstudio.github.io/shinydashboard/&#34;&gt;Shinydashboard&lt;/a&gt; are two of these frameworks, which have visible advantages over basic Shiny, in terms of layout. An &lt;a href=&#34;https://pablobernabeu.shinyapps.io/Dutch-modality-exclusivity-norms/&#34;&gt;example with Flexdashboard is available&lt;/a&gt;.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ &lt;b&gt; Flexdashboard &lt;/b&gt; ★
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ ★ &lt;b&gt; Shiny &lt;/b&gt; ★ ★
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ ★ ★ &lt;b&gt; Flexdashboard-Shiny &lt;/b&gt; ★ ★ ★
&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pablobernabeu/data-is-present/master/dashboard%20gif.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;logistics&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Logistics&lt;/h4&gt;
&lt;p&gt;Memory capacity can become an issue as you go on, which will be flagged in the error logs as: ‘Shiny cannot use on-disk bookmarking’. This doesn’t necessarily lead you to a paid subscription or to &lt;a href=&#34;https://www.r-bloggers.com/alternative-approaches-to-scaling-shiny-with-rstudio-shiny-server-shinyproxy-or-custom-architecture/&#34;&gt;host the website on a custom server&lt;/a&gt;. Try pruning the data file, outsourcing data sections across the five available apps.&lt;/p&gt;
&lt;p&gt;App providers have specific terms of use. To begin, Shiny has a free starter license with limited use, where free apps can handle a certain amount of data, and up to five apps may be created. Beyond that, RStudio offers a wide range of &lt;a href=&#34;http://www.shinyapps.io/#_pricing&#34;&gt;subscriptions&lt;/a&gt; starting at $9/month. For its part, Tableau in principle deals only with &lt;a href=&#34;https://www.tableau.com/pricing&#34;&gt;subscriptions&lt;/a&gt; from $35/month on. While they offer 1-year licenses to students and instructors for free, these don’t include web hosting, unlike Shiny’s free plan. &lt;a href=&#34;https://www.linkedin.com/pulse/r-shiny-v-tableau-dawn-graphics-anand-gupta?lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3BCDbB2MVuQA6l%2BRNxwqWzQg%3D%3D&#34;&gt;Further comparisons&lt;/a&gt; of these platforms are available online. Last, I’ll just mention a third language, &lt;a href=&#34;https://d3js.org/&#34;&gt;D3&lt;/a&gt;, which is powerful, and may also be used &lt;a href=&#34;https://rstudio.github.io/r2d3/&#34;&gt;through R&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the case of very heavy data or frequent public use, if you don’t want to host your Shiny app externally, you might consider rendering a PDF with your visualisations instead.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
pdf(&amp;quot;List of plots per page&amp;quot;, width=13, height=5)
print(plot1)
print(plot2)
# ...
print(plot150)
dev.off()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;High-resolution plots can be rendered into a PDF document in a snap. Conveniently, all text is indexed, so it can be searched (Ctrl+f / Cmd+f / 🔍) (&lt;a href=&#34;https://osf.io/2tpxn/&#34;&gt;see example&lt;/a&gt;). Furthermore, you may also &lt;a href=&#34;http://www.ilovepdf.com/&#34;&gt;merge the rendered PDF&lt;/a&gt; with any other documents.&lt;/p&gt;
&lt;style&gt;.embed-responsive{position:relative;height:100%;}.embed-responsive iframe{position:absolute;height:100%;}&lt;/style&gt;
&lt;script&gt;window.jQuery || document.write(&#39;&lt;script src=&#34;//code.jquery.com/jquery-1.11.2.min.js&#34;&gt;\x3C/script&gt;&#39;) &lt;/script&gt;
&lt;link href=&#34;https://mfr.osf.io/static/css/mfr.css&#34; media=&#34;all&#34; rel=&#34;stylesheet&#34;&gt;
&lt;div id=&#34;mfrIframe&#34; class=&#34;mfr mfr-file&#34;&gt;

&lt;/div&gt;
&lt;script src=&#34;https://mfr.osf.io/static/js/mfr.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;var mfrRender = new mfr.Render(&#34;mfrIframe&#34;, &#34;https://mfr.osf.io/render?url=https://osf.io/2tpxn/?direct%26mode=render%26action=download%26mode=render&#34;);&lt;/script&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-in-slides&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Summary in &lt;a href=&#34;https://www.slideshare.net/PabloBernabeu/presenting-data-interactively-online-using-r-shiny-126064157&#34;&gt;slides&lt;/a&gt;&lt;/h3&gt;
&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/mDJ6IF1RGTiAR8&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;div style=&#34;margin-bottom:5px&#34;&gt;
&lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/PabloBernabeu/presenting-data-interactively-online-using-r-shiny-126064157&#34; title=&#34;Presenting data interactively online using R Shiny&#34; target=&#34;_blank&#34;&gt;Presenting data interactively online using R Shiny&lt;/a&gt; &lt;/strong&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      
            <category>s</category>
      
            <category>data presentation</category>
      
            <category>dashboard</category>
      
            <category>reproducibility</category>
      
            <category>open science</category>
      
            <category>open data</category>
      
            <category>R</category>
      
            <category>R Shiny</category>
      
            <category>Flexdashboard</category>
      
      
            <category>R</category>
      
            <category>data dashboards</category>
      
            <category>open data</category>
      
    </item>
    
    <item>
      <title>Web application: Modality switch effects emerge early and increase throughout conceptual processing</title>
      <link>https://pablobernabeu.github.io/applications-and-dashboards/bernabeu-etal-2017-modalityswitch/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pablobernabeu.github.io/applications-and-dashboards/bernabeu-etal-2017-modalityswitch/</guid>
      <description>&lt;a href=&#39;https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment/&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #4CAF50; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt; 
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;i class=&#34;fas fa-mouse-pointer&#34;&gt;&lt;/i&gt;&lt;/span&gt;&amp;nbsp; Web application &lt;/h3&gt;&lt;/button&gt;&lt;/a&gt; &amp;nbsp;
&lt;br&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;Content&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The data is from a psychology experiment on the comprehension of words, in which electroencephalographic (EEG) responses were measured. The data are presented in plots spanning 800 milliseconds (the duration of word processing). The aim of this Shiny app is to facilitate the exploration of the data by researchers and the public. Users can delve into the different sections of the data. In a hierarchical order, these sections are groups of participants, individual participants, brain areas, and electrodes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shiny apps in science&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;By creating this app, I tried to reach beyond the scope of current &lt;em&gt;open science&lt;/em&gt;, which is often confined to files shared on data repositories. I believe that Shiny apps will become general practice in science within a few years (&lt;a href=&#34;http://www.research.lancs.ac.uk/portal/en/activities/presenting-data-interactively-online-using-r-shiny(c9ce06ac-987e-4141-9121-016f6ee6d16b).html&#34;&gt;see blog post or slides for more information&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Technical details&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I used tabs on the upper area of the application page to avoid having cramming the side bar with widgets. I adjusted the appearance of these tabs, and by means of &amp;lsquo;reactivity&amp;rsquo; conditions, modified the inputs in the side bar depending on the active tab.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mainPanel(

	tags$style(HTML(&#39;
	    .tabbable &amp;gt; .nav &amp;gt; li &amp;gt; a                  		{background-color:white; color:#3E454E}
	    .tabbable &amp;gt; .nav &amp;gt; li &amp;gt; a:hover            		{background-color:#002555; color:white}
	    .tabbable &amp;gt; .nav &amp;gt; li[class=active] &amp;gt; a 		{background-color:#ECF4FF; color:black}
	    .tabbable &amp;gt; .nav &amp;gt; li[class=active] &amp;gt; a:hover	{background-color:#E7F1FF; color:black}
	&#39;)),

	tabsetPanel(id=&#39;tabvals&#39;,

            tabPanel(value=1, h4(strong(&#39;Group &amp;amp; Electrode&#39;)), br(), plotOutput(&#39;plot_GroupAndElectrode&#39;),
			h5(a(strong(&#39;See plots with 95% Confidence Intervals&#39;), href=&#39;https://osf.io/2tpxn/&#39;,
			target=&#39;_blank&#39;), style=&#39;text-decoration: underline;&#39;), 
			downloadButton(&#39;downloadPlot.1&#39;, &#39;Download HD plot&#39;), br(), br(),
			# EEG montage
			img(src=&#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&#39;, height=500, width=1000)),

            tabPanel(value=2, h4(strong(&#39;Participant &amp;amp; Area&#39;)), br(), plotOutput(&#39;plot_ParticipantAndLocation&#39;),
			h5(a(strong(&#39;See plots with 95% Confidence Intervals&#39;), href=&#39;https://osf.io/86ch9/&#39;,
			target=&#39;_blank&#39;), style=&#39;text-decoration: underline;&#39;), 
			downloadButton(&#39;downloadPlot.2&#39;, &#39;Download HD plot&#39;), br(), br(),
			# EEG montage
			img(src=&#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&#39;, height=500, width=1000)),

            tabPanel(value=3, h4(strong(&#39;Participant &amp;amp; Electrode&#39;)), br(), plotOutput(&#39;plot_ParticipantAndElectrode&#39;),
			br(), downloadButton(&#39;downloadPlot.3&#39;, &#39;Download HD plot&#39;), br(), br(),
			# EEG montage
			img(src=&#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&#39;, height=500, width=1000)),

            tabPanel(value=4, h4(strong(&#39;OLD Group &amp;amp; Electrode&#39;)), br(), plotOutput(&#39;plot_OLDGroupAndElectrode&#39;),
			h5(a(strong(&#39;See plots with 95% Confidence Intervals&#39;), href=&#39;https://osf.io/dvs2z/&#39;,
			target=&#39;_blank&#39;), style=&#39;text-decoration: underline;&#39;), 
			downloadButton(&#39;downloadPlot.4&#39;, &#39;Download HD plot&#39;), br(), br(),
			# EEG montage
			img(src=&#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&#39;, height=500, width=1000))
	),
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data set was fairly large, considering the fact that it&#39;s hosted with the free plan. In order to lighten the processing, I split the data into various files, reducing the total size. Furthermore, I outsourced a particularly heavy set of plots (those with Confidence Intervals) to PDF files, to which I linked in the app.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;h5(a(strong(&#39;See plots with 95% Confidence Intervals&#39;), href=&#39;https://osf.io/dvs2z/&#39;,
			target=&#39;_blank&#39;), style=&#39;text-decoration: underline;&#39;),
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also used web links to the published paper and raw data, as well as to the &lt;em&gt;server&lt;/em&gt; and &lt;em&gt;ui&lt;/em&gt; scripts. These files, along with the data, are publicly available &lt;a href=&#34;https://osf.io/97unm/&#34;&gt;in this repository&lt;/a&gt;; they may be accessed within the &amp;ldquo;Files&amp;rdquo; section, by opening the folders &amp;ldquo;ERPs&amp;rdquo; -&amp;gt; &amp;ldquo;Analyses of ERPs averaged across trials&amp;rdquo; -&amp;gt; &amp;ldquo;Shiny app&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Another feature I added was the download button.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# From server.R script

spec_title = paste0(&#39;ERP waveforms for &#39;, input$var.Group, &#39; Group, Electrode &#39;, input$var.Electrodes.1, &#39; (negative values upward; time windows displayed)&#39;)

plot_GroupAndElectrode = ggplot(df2, aes(x=time, y=-microvolts, color=condition)) +
  geom_rect(xmin=160, xmax=216, ymin=7.5, ymax=-8, color = &#39;grey75&#39;, fill=&#39;black&#39;, alpha=0, linetype=&#39;longdash&#39;) +
  geom_rect(xmin=270, xmax=370, ymin=7.5, ymax=-8, color = &#39;grey75&#39;, fill=&#39;black&#39;, alpha=0, linetype=&#39;longdash&#39;) +
  geom_rect(xmin=350, xmax=550, ymin=8, ymax=-7.5, color = &#39;grey75&#39;, fill=&#39;black&#39;, alpha=0, linetype=&#39;longdash&#39;) +
  geom_rect(xmin=500, xmax=750, ymin=7.5, ymax=-8, color = &#39;grey75&#39;, fill=&#39;black&#39;, alpha=0, linetype=&#39;longdash&#39;) +
  geom_line(size=1, alpha = 1) + scale_linetype_manual(values=colours) +
  scale_y_continuous(limits=c(-8.38, 8.3), breaks=seq(-8,8,by=1), expand = c(0,0.1)) +
  scale_x_continuous(limits=c(-208,808),breaks=seq(-200,800,by=100), expand = c(0.005,0), labels= c(&#39;-200&#39;,&#39;-100 ms&#39;,&#39;0&#39;,&#39;100 ms&#39;,&#39;200&#39;,&#39;300 ms&#39;,&#39;400&#39;,&#39;500 ms&#39;,&#39;600&#39;,&#39;700 ms&#39;,&#39;800&#39;)) +
  ggtitle(spec_title) + theme_bw() + geom_vline(xintercept=0) +
  annotate(geom=&#39;segment&#39;, y=seq(-8,8,1), yend=seq(-8,8,1), x=-4, xend=8, color=&#39;black&#39;) +
  annotate(geom=&#39;segment&#39;, y=-8.2, yend=-8.38, x=seq(-200,800,100), xend=seq(-200,800,100), color=&#39;black&#39;) +
  geom_segment(x = -200, y = 0, xend = 800, yend = 0, size=0.5, color=&#39;black&#39;) +
  theme(legend.position = c(0.100, 0.150), legend.background = element_rect(fill=&#39;#EEEEEE&#39;, size=0),
	axis.title=element_blank(), legend.key.width = unit(1.2,&#39;cm&#39;), legend.text=element_text(size=17),
	legend.title = element_text(size=17, face=&#39;bold&#39;), plot.title= element_text(size=20, hjust = 0.5, vjust=2),
	axis.text.y = element_blank(), axis.text.x = element_text(size = 14, vjust= 2.12, face=&#39;bold&#39;, color = &#39;grey32&#39;, family=&#39;sans&#39;),
	axis.ticks=element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), 
	panel.grid.minor = element_blank(), plot.margin = unit(c(0.1,0.1,0,0), &#39;cm&#39;)) +
  annotate(&#39;segment&#39;, x=160, xend=216, y=-8, yend=-8, colour = &#39;grey75&#39;, size = 1.5) +
  annotate(&#39;segment&#39;, x=270, xend=370, y=-8, yend=-8, colour = &#39;grey75&#39;, size = 1.5) +
  annotate(&#39;segment&#39;, x=350, xend=550, y=-7.5, yend=-7.5, colour = &#39;grey75&#39;, size = 1.5) +
  annotate(&#39;segment&#39;, x=500, xend=750, y=-8, yend=-8, colour = &#39;grey75&#39;, size = 1.5) +
  scale_fill_manual(name = &#39;Context / Target trial&#39;, values=colours) +
  scale_color_manual(name = &#39;Context / Target trial&#39;, values=colours) +
  guides(linetype=guide_legend(override.aes = list(size=1.2))) +
   guides(color=guide_legend(override.aes = list(size=2.5))) +
# Print y axis labels within plot area:
  annotate(&#39;text&#39;, label = expression(bold(&#39;\u2013&#39; * &#39;3 &#39; * &#39;\u03bc&#39; * &#39;V&#39;)), x = -29, y = 3, size = 4.5, color = &#39;grey32&#39;, family=&#39;sans&#39;) +
  annotate(&#39;text&#39;, label = expression(bold(&#39;+3 &#39; * &#39;\u03bc&#39; * &#39;V&#39;)), x = -29, y = -3, size = 4.5, color = &#39;grey32&#39;, family=&#39;sans&#39;) +
  annotate(&#39;text&#39;, label = expression(bold(&#39;\u2013&#39; * &#39;6 &#39; * &#39;\u03bc&#39; * &#39;V&#39;)), x = -29, y = 6, size = 4.5, color = &#39;grey32&#39;, family=&#39;sans&#39;)

print(plot_GroupAndElectrode)

output$downloadPlot.1 &amp;lt;- downloadHandler(
	filename &amp;lt;- function(file){
	paste0(input$var.Group, &#39; group, electrode &#39;, input$var.Electrodes.1, &#39;, &#39;, Sys.Date(), &#39;.png&#39;)},
   	content &amp;lt;- function(file){
      		png(file, units=&#39;in&#39;, width=13, height=5, res=900)
      		print(plot_GroupAndElectrode)
      		dev.off()},
	contentType = &#39;image/png&#39;)
  } )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# From ui.R script

downloadButton(&#39;downloadPlot.1&#39;, &#39;Download HD plot&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Rising to the challenge&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My experience with R Shiny has been so good I&#39;ve been &lt;a href=&#34;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/&#34;&gt;sharing it&lt;/a&gt;. Yet, on my first crawling days, I spent an eternity stuck with this elephant in my room: &amp;ldquo;μ&amp;rdquo;. This &lt;em&gt;μ&lt;/em&gt; letter (micro-souvenir from hell, as I later knew it), was part of the labels of my plots. All I knew was that I could not deploy the app online, even while I could perfectly launch it locally in my laptop. So, I wondered what use was to deploy locally if I couldn&#39;t publish the app?! Eventually, I read about UTF-8 encoding in one forum. Bless them forums. All I had to do was use &amp;ldquo;Âμ&amp;rdquo; instead of the single &amp;ldquo;μ&amp;rdquo;. A better option I found later was: &lt;code&gt;expression(&amp;quot;\u03bc&amp;quot;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Beyond encoding issues, I had a tough time embedding images. You know, the &amp;lsquo;www&amp;rsquo; folder&amp;hellip; To be honest, I still haven&#39;t handled the &amp;lsquo;www&amp;rsquo; way&amp;ndash;but where there&#39;s a will, there&#39;s a way. I managed to include my images by uploading them to a website and then entering their URL in &amp;ldquo;img(src&amp;rdquo;, avoiding the use of folder paths.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;img(src=&amp;quot;https://preview.ibb.co/n7qiYR/EEG_montage.png 1&amp;quot;, height=500, width=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Long after I had built the app, I added another image&amp;ndash;the &lt;em&gt;favicon&lt;/em&gt; (the little icon on the browser tab).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tags$head(tags$link(rel=&amp;quot;shortcut icon&amp;quot;, href=&amp;quot;https://image.ibb.co/fXUwzb/favic.png&amp;quot;)),  # web favicon
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;div class = &#39;hanging-indent&#39;&gt;
&lt;p&gt;Bernabeu, P., Willems, R. M., &amp;amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs [Web application]. Retrieved from &lt;a href=&#34;https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment&#34;&gt;https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>web application</category>
      
            <category>data dashboard</category>
      
            <category>R</category>
      
            <category>R Shiny</category>
      
            <category>conceptual modality switch</category>
      
            <category>conceptual processing</category>
      
            <category>reading</category>
      
            <category>event-related potentials</category>
      
            <category>cognition</category>
      
            <category>psycholinguistics</category>
      
            <category>HTML</category>
      
            <category>CSS</category>
      
      
            <category>conceptual processing</category>
      
            <category>R</category>
      
    </item>
    
  </channel>
</rss>
