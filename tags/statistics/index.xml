<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>statistics | Pablo Bernabeu</title>
    <link>https://pablobernabeu.github.io/tags/statistics/</link>
      <atom:link href="https://pablobernabeu.github.io/tags/statistics/index.xml" rel="self" type="application/rss+xml" />
    <description>statistics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-uk</language><copyright>Pablo Bernabeu, 2015—2023. Licence: [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/). Email: pcbernabeu@gmail.com. No cookies operated by this website. Cookies only used by third-party systems such as [Disqus](https://help.disqus.com/en/articles/1717155-use-of-cookies).</copyright><lastBuildDate>Sat, 24 Jun 2023 16:42:34 +0200</lastBuildDate>
    <image>
      <url>https://pablobernabeu.github.io/img/default_preview_image.png</url>
      <title>statistics</title>
      <link>https://pablobernabeu.github.io/tags/statistics/</link>
    </image>
    
    <item>
      <title>How to visually assess the convergence of a mixed-effects model by plotting various optimizers</title>
      <link>https://pablobernabeu.github.io/2023/how-to-visually-assess-the-convergence-of-a-mixed-effects-model-by-plotting-various-optimizers/</link>
      <pubDate>Sat, 24 Jun 2023 16:42:34 +0200</pubDate>
      <guid>https://pablobernabeu.github.io/2023/how-to-visually-assess-the-convergence-of-a-mixed-effects-model-by-plotting-various-optimizers/</guid>
      <description>


&lt;p&gt;To assess whether convergence warnings render the results invalid, or on the contrary, the results can be deemed valid in spite of the warnings, &lt;a href=&#34;https://cran.r-project.org/web/packages/lme4/lme4.pdf&#34;&gt;Bates et al. (2023)&lt;/a&gt; suggest refitting models affected by convergence warnings with a variety of optimizers. The authors argue that, if the different optimizers produce practically-equivalent results, the results are valid. The &lt;code&gt;allFit&lt;/code&gt; function from the ‘lme4’ package allows the refitting of models using a number of optimizers. To use the seven optimizers listed above, two extra packages must be installed: ‘dfoptim’ and ‘optimx’ (see lme4 manual). The output from &lt;code&gt;allFit&lt;/code&gt; contains several statistics on the fixed and the random effects fitted by each optimizer.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
#&amp;gt; Loading required package: Matrix
library(dfoptim)
library(optimx)

# Create data using code by Ben Bolker from 
# https://stackoverflow.com/a/38296264/7050882

set.seed(101)
spin = runif(600, 1, 24)
reg = runif(600, 1, 15)
ID = rep(c(&amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,&amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;7&amp;quot;, &amp;quot;8&amp;quot;, &amp;quot;9&amp;quot;, &amp;quot;10&amp;quot;))
day = rep(1:30, each = 10)
testdata &amp;lt;- data.frame(spin, reg, ID, day)
testdata$fatigue &amp;lt;- testdata$spin * testdata$reg/10 * rnorm(30, mean=3, sd=2)

# Model
fit = lmer(fatigue ~ spin * reg + (1|ID),
           data = testdata, REML = TRUE)

# Refit model using all available algorithms
multi_fit = allFit(fit)
#&amp;gt; bobyqa : [OK]
#&amp;gt; Nelder_Mead : [OK]
#&amp;gt; nlminbwrap : [OK]
#&amp;gt; nmkbw : [OK]
#&amp;gt; optimx.L-BFGS-B : [OK]
#&amp;gt; nloptwrap.NLOPT_LN_NELDERMEAD : [OK]
#&amp;gt; nloptwrap.NLOPT_LN_BOBYQA : [OK]

# Show results 
summary(multi_fit)$fixef
#&amp;gt;                               (Intercept)      spin       reg  spin:reg
#&amp;gt; bobyqa                          -2.975678 0.5926561 0.1437204 0.1834016
#&amp;gt; Nelder_Mead                     -2.975675 0.5926559 0.1437202 0.1834016
#&amp;gt; nlminbwrap                      -2.975677 0.5926560 0.1437203 0.1834016
#&amp;gt; nmkbw                           -2.975678 0.5926561 0.1437204 0.1834016
#&amp;gt; optimx.L-BFGS-B                 -2.975680 0.5926562 0.1437205 0.1834016
#&amp;gt; nloptwrap.NLOPT_LN_NELDERMEAD   -2.975666 0.5926552 0.1437196 0.1834017
#&amp;gt; nloptwrap.NLOPT_LN_BOBYQA       -2.975678 0.5926561 0.1437204 0.1834016

# Read in function from GitHub
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/plot.fixef.allFit/main/plot.fixef.allFit.R&amp;#39;)

plot.fixef.allFit(multi_fit, 
                  
                  select_predictors = c(&amp;#39;spin&amp;#39;, &amp;#39;reg&amp;#39;, &amp;#39;spin:reg&amp;#39;), 
                  
                  # Increase padding at top and bottom of Y axis
                  multiply_y_axis_limits = 1.3,
                  
                  y_title = &amp;#39;Fixed effect (*b*)&amp;#39;)
#&amp;gt; Loading required package: dplyr
#&amp;gt; 
#&amp;gt; Attaching package: &amp;#39;dplyr&amp;#39;
#&amp;gt; The following objects are masked from &amp;#39;package:stats&amp;#39;:
#&amp;gt; 
#&amp;gt;     filter, lag
#&amp;gt; The following objects are masked from &amp;#39;package:base&amp;#39;:
#&amp;gt; 
#&amp;gt;     intersect, setdiff, setequal, union
#&amp;gt; Loading required package: reshape2
#&amp;gt; Loading required package: stringr
#&amp;gt; Loading required package: scales
#&amp;gt; Loading required package: ggplot2
#&amp;gt; Loading required package: ggtext
#&amp;gt; Loading required package: patchwork&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/XYQDug2.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# Alternative using plot-specific Y axes and other modified settings

plot.fixef.allFit(multi_fit, 
                  
                  select_predictors = c(&amp;#39;spin&amp;#39;, &amp;#39;spin:reg&amp;#39;), 
                  
                  # Use plot-specific Y axis limits
                  shared_y_axis_limits = FALSE,
                  
                  decimal_places = 7, 
                  
                  # Move up Y axis title
                  y_title_hjust = 4.5,
                  
                  y_title = &amp;#39;Fixed effect (*b*)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/BYXJYxM.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;Created on 2023-06-26 with &lt;a href=&#34;https://reprex.tidyverse.org&#34;&gt;reprex v2.0.2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A new function to plot convergence diagnostics from lme4::allFit()</title>
      <link>https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/</link>
      <pubDate>Thu, 22 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/index_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/index_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/index_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;


&lt;p&gt;Linear mixed-effects models (LMM) offer a consistent way of performing regression and analysis of variance tests which allows accounting for non-independence in the data. Over the past decades, LMMs have subsumed most of the General Linear Model, with a steady increase in popularity (Meteyard &amp;amp; Davies, 2020). Since their conception, LMMs have presented the challenge of model &lt;em&gt;convergence&lt;/em&gt;. In essence, the issue of convergence boils down to the widespread tension between parsimony and completeness in data analysis. That is, on the one hand, a good model must allow an accurate, parsimonious analysis of each predictor, and thus, it must not be overfitted with too many parameters. Yet, on the other hand, the model must be complete enough to account for a sufficient amount of variation in the data. In LMMs, any predictors that entail non-independent observations (also known as repeated measures) will normally bring both fixed and random effects into the model. Where a few of these predictors coexist, models often struggle to find enough information in the data to account for every predictor—and especially, for every random effect. This difficulty translates into convergence warnings (Brauer &amp;amp; Curtin, 2018; Singmann &amp;amp; Kellen, 2019). In this article, I review the issue of convergence before presenting a new plotting function in R that facilitates the diagnosis of convergence by visualising the fixed effects fitted by different optimization algorithms (also dubbed optimizers).&lt;/p&gt;
&lt;div id=&#34;completeness-versus-parsimony&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Completeness versus parsimony&lt;/h2&gt;
&lt;p&gt;Both fixed and random effects comprise intercepts and slopes. The pressure exerted by each of those types of effects on the model is determined by the number of data points involved by each. First, slopes are more demanding than intercepts, as they involve a (far) larger number of data points. Second, random effects are more demanding than fixed effects, as random effects entail the number of estimates required for fixed effects &lt;em&gt;times&lt;/em&gt; the number of levels in the grouping factor. As a result, on the most lenient end of the scale lies the fixed intercept, and on the heaviest end lie the random slopes. Convergence warnings in LMMs are often due to the random slopes alone.&lt;/p&gt;
&lt;p&gt;Sounds easy, then! Not inviting the random slopes to the party should solve the problem. Indeed, since random slopes involve the highest number of estimates by far, removing them does often remove convergence warnings. This, however, leads to a different problem. Surrendering the information provided by random slopes can result in the violation of the assumption of independence of observations. For years, the removal of random slopes due to convergence warnings was standard practice. Currently, in contrast, proposals increasingly consider other options, such as removing random effects if they do not significantly improve the fit of the model (Matuschek et al., 2017), and keeping the random slopes in the model in spite of the convergence warnings to safeguard the assumption of independence (see Table 17 in Brauer &amp;amp; Curtin, 2018; Singmann &amp;amp; Kellen, 2019).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-multiple-optimizers-sanity-check-from-lme4allfit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The multiple-optimizers sanity check from &lt;code&gt;lme4::allFit()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Framed within the drive to maintain random slopes wherever possible, the developers of the ‘lme4’ package propose a sanity check that uses a part of the ‘lme4’ &lt;em&gt;engine&lt;/em&gt; called ‘optimizer’. Every model has a default optimizer, unless a specific one is chosen through &lt;code&gt;control = lmerControl(optimizer = &#39;...&#39;)&lt;/code&gt; (in lmer models) or &lt;code&gt;control = glmerControl(optimizer = &#39;...&#39;)&lt;/code&gt; (in glmer models). The seven widely-available optimizers are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bobyqa&lt;/li&gt;
&lt;li&gt;Nelder_Mead&lt;/li&gt;
&lt;li&gt;nlminbwrap&lt;/li&gt;
&lt;li&gt;nmkbw&lt;/li&gt;
&lt;li&gt;optimx.L-BFGS-B&lt;/li&gt;
&lt;li&gt;nloptwrap.NLOPT_LN_NELDERMEAD&lt;/li&gt;
&lt;li&gt;nloptwrap.NLOPT_LN_BOBYQA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To assess whether convergence warnings render the results invalid, or on the contrary, the results can be deemed valid in spite of the warnings, Bates et al. (2022) suggest refitting models affected by convergence warnings with a variety of optimizers. The authors argue that if the different optimizers produce practically-equivalent results, the results are valid. The &lt;code&gt;allFit&lt;/code&gt; function from the ‘lme4’ package allows the refitting of models using a number of optimizers. To use the seven optimizers listed above, two extra packages must be installed: ‘dfoptim’ and ‘optimx’ (see &lt;a href=&#34;https://cran.r-project.org/web/packages/lme4/lme4.pdf&#34;&gt;lme4 manual&lt;/a&gt;). The output from &lt;code&gt;allFit()&lt;/code&gt; contains several statistics on the fixed and the random effects fitted by each optimizer (see &lt;a href=&#34;https://github.com/lme4/lme4/issues/512#issue-425198940&#34;&gt;example&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-the-fixed-effects-from-allfit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plotting the fixed effects from allFit()&lt;/h2&gt;
&lt;p&gt;Several R users have ventured into &lt;a href=&#34;https://www.google.com/search?q=%22ggplot%22+%22allfit%22+optimizers&#34;&gt;plotting the allFit() output&lt;/a&gt; but there is not a function in ‘lme4’ yet at the time of writing. I (Bernabeu, 2022) developed a &lt;a href=&#34;https://github.com/pablobernabeu/plot.fixef.allFit/blob/main/plot.fixef.allFit.R&#34;&gt;function&lt;/a&gt; that takes the output from &lt;code&gt;allFit()&lt;/code&gt;, tidies it, selects the fixed effects and plots them using ‘ggplot2’. The function is shown below, and can be copied through the &lt;code&gt;Copy Code&lt;/code&gt; button at the top right corner. It can be renamed by changing &lt;code&gt;plot.fixef.allFit&lt;/code&gt; to another valid name.&lt;/p&gt;
&lt;div style=&#34;height: 800px; border: 0.5px dotted grey; padding: 10px; resize: both; overflow: auto;&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot the results from the fixed effects produced by different optimizers. This function 
# takes the output from lme4::allFit(), tidies it, selects fixed effects and plots them.

plot.fixef.allFit = function(allFit_output, 
                             # Set the same Y axis limits in every plot
                             shared_y_axis_limits = TRUE,
                             # Multiply Y axis limits by a factor (only 
                             # available if shared_y_axis_limits = TRUE)
                             multiply_y_axis_limits = 1, 
                             # Number of decimal places
                             decimal_places = NULL,
                             # Select predictors
                             select_predictors = NULL, 
                             # Number of rows
                             nrow = NULL, 
                             # Y axis title
                             y_title = &amp;#39;Fixed effect&amp;#39;,
                             # Alignment of the Y axis title
                             y_title_hjust = NULL,
                             # Add number to the names of optimizers
                             number_optimizers = TRUE,
                             # Replace colon in interactions with x
                             interaction_symbol_x = TRUE) {
  
  require(lme4)
  require(dfoptim)
  require(optimx)
  require(dplyr)
  require(reshape2)
  require(stringr)
  require(scales)
  require(ggplot2)
  require(ggtext)
  require(patchwork)
  library(Cairo)
  
  # Tidy allFit output
  
  # Extract fixed effects from the allFit() output
  allFit_fixef = summary(allFit_output)$fixef %&amp;gt;%  # Select fixed effects in the allFit results
    reshape2::melt() %&amp;gt;%  # Structure the output as a data frame
    rename(&amp;#39;Optimizer&amp;#39; = &amp;#39;Var1&amp;#39;, &amp;#39;fixed_effect&amp;#39; = &amp;#39;Var2&amp;#39;)  # set informative names
  
  # If number_optimizers = TRUE, assign number to each optimizer and place it before its name
  if(number_optimizers == TRUE) {
    allFit_fixef$Optimizer = paste0(as.numeric(allFit_fixef$Optimizer), &amp;#39;. &amp;#39;, allFit_fixef$Optimizer)
  }
  
  # If select_predictors was supplied, select them along with the intercept (the latter required)
  if(!is.null(select_predictors)) {
    allFit_fixef = allFit_fixef %&amp;gt;% dplyr::filter(fixed_effect %in% c(&amp;#39;(Intercept)&amp;#39;, select_predictors))
  }
  
  # Order variables
  allFit_fixef = allFit_fixef[, c(&amp;#39;Optimizer&amp;#39;, &amp;#39;fixed_effect&amp;#39;, &amp;#39;value&amp;#39;)]
  
  # PLOT. The overall plot is formed of a first row containing the intercept and the legend 
  # (intercept_plot), and a second row containing the predictors (predictors_plot), 
  # which may in turn occupy several rows.
  
  # If multiply_y_axis_limits was supplied but shared_y_axis_limits = FALSE,
  # warn that shared_y_axis_limits is required.
  if(!multiply_y_axis_limits == 1 &amp;amp; shared_y_axis_limits == FALSE) {
    message(&amp;#39;The argument `multiply_y_axis_limits` has not been used because \n it requires `shared_y_axis_limits` set to TRUE.&amp;#39;)
  }
  
  # If extreme values were entered in y_title_hjust, show warning
  if(!is.null(y_title_hjust)) {
    if(y_title_hjust &amp;lt; 0.5 | y_title_hjust &amp;gt; 6) {
      message(&amp;#39;NOTE: For y_title_hjust, a working range of values is between 0.6 and 6.&amp;#39;)
    }
  }
  
  # If decimal_places was supplied, convert number to the format used in &amp;#39;scales&amp;#39; package
  if(!is.null(decimal_places)) {
    decimal_places = 
      ifelse(decimal_places == 1, 0.1, 
             ifelse(decimal_places == 2, 0.01, 
                    ifelse(decimal_places == 3, 0.001, 
                           ifelse(decimal_places == 4, 0.0001, 
                                  ifelse(decimal_places == 5, 0.00001, 
                                         ifelse(decimal_places == 6, 0.000001, 
                                                ifelse(decimal_places == 7, 0.0000001, 
                                                       ifelse(decimal_places == 8, 0.00000001, 
                                                              ifelse(decimal_places == 9, 0.000000001, 
                                                                     ifelse(decimal_places == 10, 0.0000000001,
                                                                            ifelse(decimal_places == 11, 0.00000000001,
                                                                                   ifelse(decimal_places == 12, 0.000000000001,
                                                                                          ifelse(decimal_places == 13, 0.0000000000001,
                                                                                                 ifelse(decimal_places == 14, 0.00000000000001,
                                                                                                        ifelse(decimal_places &amp;gt;= 15, 0.000000000000001, 
                                                                                                               0.001
                                                                                                        )))))))))))))))
  }
  
  # First row: intercept_plot
  
  # Select intercept data only
  intercept = allFit_fixef %&amp;gt;% dplyr::filter(fixed_effect == &amp;#39;(Intercept)&amp;#39;)
  
  intercept_plot = intercept %&amp;gt;%
    ggplot(., aes(fixed_effect, value, colour = Optimizer)) +
    geom_point(position = position_dodge(1)) +
    facet_wrap(~fixed_effect, scale = &amp;#39;free&amp;#39;) +
    guides(colour = guide_legend(title.position = &amp;#39;left&amp;#39;)) +
    theme_bw() + 
    theme(axis.title = element_blank(), axis.ticks.x = element_blank(),
          axis.text.x = element_blank(), 
          strip.text = element_text(size = 10, margin = margin(t = 4, b = 6)),
          strip.background = element_rect(fill = &amp;#39;grey96&amp;#39;),
          legend.margin = margin(0.3, 0, 0.8, 1, &amp;#39;cm&amp;#39;), 
          legend.title = element_text(size = unit(15, &amp;#39;pt&amp;#39;), angle = 90, hjust = 0.5))
  
  # Second row: predictors_plot
  
  # Select all predictors except intercept
  predictors = allFit_fixef %&amp;gt;% dplyr::filter(!fixed_effect == &amp;#39;(Intercept)&amp;#39;)
  
  # If interaction_symbol_x = TRUE (default), replace colon with times symbol x between spaces
  if(interaction_symbol_x == TRUE) {
    # Replace colon in interactions with \u00D7, i.e., x; then set factor class
    predictors$fixed_effect = predictors$fixed_effect %&amp;gt;% 
      str_replace_all(&amp;#39;:&amp;#39;, &amp;#39; \u00D7 &amp;#39;) %&amp;gt;% factor()
  }
  
  # Order predictors as in the original output from lme4::allFit()
  predictors$fixed_effect = factor(predictors$fixed_effect, 
                                   levels = unique(predictors$fixed_effect))
  
  # Set number of rows for the predictors excluding the intercept.
  # First, if nrow argument supplied, use it
  if(!is.null(nrow)) {
    predictors_plot_nrow = nrow - 1  # Subtract 1 as intercept row not considered
    
    # Else, if nrow argument not supplied, calculate sensible number of rows: i.e., divide number of
    # predictors (exc. intercept) by 2 and round up the result. For instance, 7 predictors --&amp;gt; 3 rows
  } else predictors_plot_nrow = (length(unique(predictors$fixed_effect)) / 2) %&amp;gt;% ceiling()
  
  predictors_plot = ggplot(predictors, aes(fixed_effect, value, colour = Optimizer)) +
    geom_point(position = position_dodge(1)) +
    facet_wrap(~fixed_effect, scale = &amp;#39;free&amp;#39;,
               # Note that predictors_plot_nrow was defined a few lines above
               nrow = predictors_plot_nrow, 
               # Wrap names of predictors with more than 54 characters into new lines
               labeller = labeller(fixed_effect = label_wrap_gen(width = 55))) +
    labs(y = y_title) +
    theme_bw() + 
    theme(axis.title.x = element_blank(), axis.text.x = element_blank(),
          axis.ticks.x = element_blank(),
          axis.title.y = ggtext::element_markdown(size = 14, margin = margin(0, 15, 0, 0)),
          strip.text = element_text(size = 10, margin = margin(t = 4, b = 6)),
          strip.background = element_rect(fill = &amp;#39;grey96&amp;#39;), legend.position = &amp;#39;none&amp;#39;)
  
  # Below, the function scale_y_continuous is applied conditionally to avoid overriding settings. First, 
  # if shared_y_axis_limits = TRUE and decimal_places was supplied, set the same Y axis limits in 
  # every plot and set decimal_places. By default, also expand limits by a seventh of its original 
  # limit, and allow further multiplication of limits through multiply_y_axis_limits.
  if(shared_y_axis_limits == TRUE &amp;amp; !is.null(decimal_places)) {
    
    intercept_plot = intercept_plot +
      scale_y_continuous(limits = c(min(allFit_fixef$value) - allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits,
                                    max(allFit_fixef$value) + allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits), 
                         # Set number of decimal places
                         labels = scales::label_number(accuracy = decimal_places))
    
    predictors_plot = predictors_plot + 
      scale_y_continuous(limits = c(min(allFit_fixef$value) - allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits,
                                    max(allFit_fixef$value) + allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits), 
                         # Set number of decimal places
                         labels = scales::label_number(accuracy = decimal_places))
    
    # Else, if shared_y_axis_limits = TRUE but decimal_places were not supplied, do as above but without
    # setting decimal_places.
  } else if(shared_y_axis_limits == TRUE &amp;amp; is.null(decimal_places)) {
    
    intercept_plot = intercept_plot +
      scale_y_continuous(limits = c(min(allFit_fixef$value) - allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits,
                                    max(allFit_fixef$value) + allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits),
                         # Set number of decimal places
                         labels = scales::label_number(accuracy = decimal_places))
    
    predictors_plot = predictors_plot + 
      scale_y_continuous(limits = c(min(allFit_fixef$value) - allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits,
                                    max(allFit_fixef$value) + allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits),
                         # Set number of decimal places
                         labels = scales::label_number(accuracy = decimal_places))
    
    # Else, if shared_y_axis_limits = FALSE and decimal_places was supplied, set decimal_places. 
  } else if(shared_y_axis_limits == FALSE &amp;amp; !is.null(decimal_places)) {
    
    # Set number of decimal places in both plots
    intercept_plot = intercept_plot +
      scale_y_continuous(labels = scales::label_number(accuracy = decimal_places))
    
    predictors_plot = predictors_plot +
      scale_y_continuous(labels = scales::label_number(accuracy = decimal_places))
  }
  
  # Plot matrix: based on number of predictors_plot_nrow, adjust height of Y axis title
  # (unless supplied), and assign space to intercept_plot and predictors_plot
  if(predictors_plot_nrow == 1) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 3.6))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 11, l = 0)      # predictors row(s)
    )
    
  } else if(predictors_plot_nrow == 2) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 1.4))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 16, l = 0)      # predictors row(s)
    )
    
  } else if(predictors_plot_nrow == 3) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 0.92))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 21, l = 0)      # predictors row(s)
    )
    
  } else if(predictors_plot_nrow == 4) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 0.8))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 26, l = 0)      # predictors row(s)
    )
    
  } else if(predictors_plot_nrow == 5) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 0.73))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 31, l = 0)      # predictors row(s)
    )
    
  } else if(predictors_plot_nrow &amp;gt; 5) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 0.65))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 36, l = 0)      # predictors row(s)
    )
    
    # Also, advise user to consider distributing predictors into several plots
    message(&amp;#39;  Many rows! Consider distributing predictors into several plots \n  using argument `select_predictors`&amp;#39;)
  } 
  
  # Add margin
  predictors_plot = predictors_plot + theme(plot.margin = margin(15, 15, 15, 15))
  
  # Return matrix of plots
  wrap_plots(intercept_plot, predictors_plot, design = layout,
             # The 2 below corresponds to intercept_plot and predictors_plot
             nrow = 2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;optional-arguments&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Optional arguments&lt;/h3&gt;
&lt;p&gt;Below are the optional arguments allowed by the function, with their default values.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
# Set the same Y axis limits in every plot
shared_y_axis_limits = TRUE,

# Multiply Y axis limits by a factor (only 
# available if shared_y_axis_limits = TRUE)
multiply_y_axis_limits = 1, 

# Number of decimal places
decimal_places = NULL,

# Select predictors
select_predictors = NULL, 

# Number of rows
nrow = NULL, 

# Y axis title
y_title = &amp;#39;Fixed effect&amp;#39;,

# Alignment of the Y axis title
y_title_hjust = NULL,

# Add number to the names of optimizers
number_optimizers = TRUE,

# Replace colon in interactions with x
interaction_symbol_x = TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The argument &lt;code&gt;shared_y_axis_limits&lt;/code&gt; deserves a comment. It allows using the same Y axis limits (i.e., range) in all plots or, alternatively, using plot-specific limits. The parameter is &lt;code&gt;TRUE&lt;/code&gt; by default to prevent overinterpretations of small differences across optimizers (see the first figure below). In contrast, when &lt;code&gt;shared_y_axis_limits = FALSE&lt;/code&gt;, plot-specific limits are used, which results in a narrower range of values in the Y axis (see the second figure below). Since data points will span the entire Y axis in that case, any difference across optimizers—regardless of its relative importance—might be perceived as large, unless the specific range of values in each plot is noticed.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;use-case&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Use case&lt;/h2&gt;
&lt;p&gt;Let’s test the function with a minimal model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create data using code by Ben Bolker from 
# https://stackoverflow.com/a/38296264/7050882

set.seed(101)
spin = runif(600, 1, 24)
reg = runif(600, 1, 15)
ID = rep(c(&amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,&amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;7&amp;quot;, &amp;quot;8&amp;quot;, &amp;quot;9&amp;quot;, &amp;quot;10&amp;quot;))
day = rep(1:30, each = 10)
testdata &amp;lt;- data.frame(spin, reg, ID, day)
testdata$fatigue &amp;lt;- testdata$spin * testdata$reg/10 * rnorm(30, mean=3, sd=2)

# Model

library(lme4)

fit = lmer(fatigue ~ spin * reg + (1|ID),
           data = testdata, REML = TRUE)

# Refit model using all available algorithms
multi_fit = allFit(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## bobyqa : [OK]
## Nelder_Mead : [OK]
## nlminbwrap : [OK]
## nmkbw : [OK]
## optimx.L-BFGS-B : [OK]
## nloptwrap.NLOPT_LN_NELDERMEAD : [OK]
## nloptwrap.NLOPT_LN_BOBYQA : [OK]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(multi_fit)$fixef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                               (Intercept)      spin       reg  spin:reg
## bobyqa                          -2.975678 0.5926561 0.1437204 0.1834016
## Nelder_Mead                     -2.975675 0.5926559 0.1437202 0.1834016
## nlminbwrap                      -2.975677 0.5926560 0.1437203 0.1834016
## nmkbw                           -2.975678 0.5926561 0.1437204 0.1834016
## optimx.L-BFGS-B                 -2.975680 0.5926562 0.1437205 0.1834016
## nloptwrap.NLOPT_LN_NELDERMEAD   -2.975666 0.5926552 0.1437196 0.1834017
## nloptwrap.NLOPT_LN_BOBYQA       -2.975678 0.5926561 0.1437204 0.1834016&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The effects to be visualised are selected below using the argument &lt;code&gt;select_predictors&lt;/code&gt;. Notice that the intercept is plotted by default on the first row, along with the legend that lists all the optimizers used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Read in function from GitHub
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/plot.fixef.allFit/main/plot.fixef.allFit.R&amp;#39;)

plot.fixef.allFit(multi_fit, 
                  
                  select_predictors = c(&amp;#39;spin&amp;#39;, &amp;#39;reg&amp;#39;, &amp;#39;spin:reg&amp;#39;), 
                  
                  # Increase padding at top and bottom of Y axis
                  multiply_y_axis_limits = 1.3,
                  
                  y_title = &amp;#39;Fixed effect (*b*)&amp;#39;,
                  
                  # Align y title
                  y_title_hjust = .9)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/index_files/figure-html/demo-plot.fixef.allFit-function-1-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot produced by &lt;code&gt;plot.fixef.allFit()&lt;/code&gt; by default replaces the colons in interaction effects (e.g., &lt;code&gt;spin:reg&lt;/code&gt;) with ’ × ’ to facilitate the visibility (this can be overriden by setting &lt;code&gt;interaction_symbol_x = FALSE&lt;/code&gt;). Yet, it is important to note that any interactions passed to &lt;code&gt;select_predictors&lt;/code&gt; must have the colon, as that is the symbol present in the &lt;code&gt;lme4::allFit()&lt;/code&gt; output.&lt;/p&gt;
&lt;p&gt;The output of &lt;code&gt;plot.fixef.allFit()&lt;/code&gt; is a &lt;a href=&#34;https://ggplot2.tidyverse.org/&#34;&gt;ggplot2&lt;/a&gt; object that can be stored for further use, as in the example below, in which new parameters are used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

plot_fit_convergence = 
  
  plot.fixef.allFit(multi_fit, 
                    
                    select_predictors = c(&amp;#39;spin&amp;#39;, &amp;#39;spin:reg&amp;#39;), 
                    
                    # Use plot-specific Y axis limits
                    shared_y_axis_limits = FALSE,
                    
                    decimal_places = 7, 
                    
                    # Move up Y axis title
                    y_title_hjust = -20,
                    
                    y_title = &amp;#39;Fixed effect (*b*)&amp;#39;)

# Print
plot_fit_convergence&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/index_files/figure-html/demo-plot.fixef.allFit-function-2-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot can be saved to disk as pdf, png, etc. through `ggplot2::ggsave()`
# ggsave(&amp;#39;plot_fit_convergence.pdf&amp;#39;, plot_fit_convergence, 
#        device = cairo_pdf, width = 9, height = 9, dpi = 900)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bates, D., Maechler, M., Bolker, B., Walker, S., Christensen, R. H. B., Singmann, H., Dai, B., Scheipl, F., Grothendieck, G., Green, P., Fox, J., Bauer, A., &amp;amp; Krivitsky, P. N. (2022). &lt;em&gt;Package ‘lme4’.&lt;/em&gt; CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/lme4/lme4.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/lme4/lme4.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P. (2022). Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://doi.org/10.1037/met0000159&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., &amp;amp; Bates, D. (2017). Balancing type 1 error and power in linear mixed models. &lt;em&gt;Journal of Memory and Language, 94&lt;/em&gt;, 305–315. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2017.01.001&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.jml.2017.01.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meteyard, L., &amp;amp; Davies, R. A. (2020). Best practice guidance for linear mixed-effects models in psychological science. &lt;em&gt;Journal of Memory and Language, 112&lt;/em&gt;, 104092. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2020.104092&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.jml.2020.104092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Singmann, H., &amp;amp; Kellen, D. (2019). An introduction to mixed models for experimental psychology. In D. H. Spieler &amp;amp; E. Schumacher (Eds.), &lt;em&gt;New methods in cognitive psychology&lt;/em&gt; (pp. 4–31). Psychology Press.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Covariates are necessary to validate the variables of interest and to prevent bogus theories</title>
      <link>https://pablobernabeu.github.io/2023/covariates-are-necessary-to-validate-the-variables-of-interest-and-to-prevent-bogus-theories/</link>
      <pubDate>Mon, 15 May 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/covariates-are-necessary-to-validate-the-variables-of-interest-and-to-prevent-bogus-theories/</guid>
      <description>


&lt;p&gt;The need for covariates—or &lt;em&gt;nuisance variables&lt;/em&gt;—in statistical analyses is twofold. The first reason is purely statistical and the second reason is academic.&lt;/p&gt;
&lt;p&gt;First, the use of covariates is often necessary when the variable(s) of interest in a study may be connected to, and affected by, some satellite variables (Bottini et al., 2022; Elze et al., 2017; Sassenhagen &amp;amp; Alday, 2016). This complex scenario is the most common one due to the multivariate, dynamic, interactive nature of the real world.&lt;/p&gt;
&lt;p&gt;Second, the use of covariates is often necessary to prevent the development of bogus, redundant theories. Academics are strongly rewarded for developing theories. As we know, wherever there are strong rewards, there are serious risks. An academic could—consciously or not—produce a theory that is too closely related to an existing theory. So closely related are these theories that the second version might not warrant a name of its own. In such a scenario, covariates are useful and indeed necessary to vet the unique nature of the second version. That is, the first and the second version must be tested in the same model, and the variables corresponding to the first version can be construed as &lt;em&gt;covariates&lt;/em&gt;. This allows both the developers of the theories and the readers to compare the effects corresponding to each version of the theory, and to assess the degree of separation between them.&lt;/p&gt;
&lt;p&gt;The perverted use of covariates (Stefan &amp;amp; Schönbrodt, 2023)—however frequent and harmful—stands completely orthogonal to the correct usage of covariates, in the same way that a stethoscope can be used for good or for bad purposes. It would be poorly informed and misleading to conflate the correct and the incorrect uses, or to reject the use of covariates altogether due to the incorrect uses.&lt;/p&gt;
&lt;p&gt;In conclusion, the effects of interest in correlational/observational studies can be subject to mediation and moderation by satellite variables. These variables cannot be manipulated in correlational/observational studies, but they can—and often should—be included as covariates in the statistical models, to ward off spurious results and to vet similar theories.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bottini, R., Morucci, P., D’Urso, A., Collignon, O., &amp;amp; Crepaldi, D. (2022). The concreteness advantage in lexical decision does not depend on perceptual simulations. &lt;em&gt;Journal of Experimental Psychology: General, 151&lt;/em&gt;(3), 731–738. &lt;a href=&#34;https://doi.org/10.1037/xge0001090&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/xge0001090&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Elze, M. C., Gregson, J., Baber, U., Williamson, E., Sartori, S., Mehran, R., Nichols, M., Stone, G. W., &amp;amp; Pocock, S. J. (2017). Comparison of propensity score methods and covariate adjustment: Evaluation in 4 cardiovascular studies. &lt;em&gt;Journal of the American College of Cardiology, 69&lt;/em&gt;(3), 345-357.&lt;/p&gt;
&lt;p&gt;Sassenhagen, J., &amp;amp; Alday, P. M. (2016). A common misapplication of statistical inference: Nuisance control with null-hypothesis significance tests. &lt;em&gt;Brain and Language, 162&lt;/em&gt;, 42-45. &lt;a href=&#34;https://doi.org/10.1016/j.bandl.2016.08.001&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.bandl.2016.08.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Stefan, A. M., &amp;amp; Schönbrodt, F. D. (2023). Big little lies: A compendium and simulation of p-hacking strategies. &lt;em&gt;Royal Society Open Science, 10&lt;/em&gt;(2), 220346. &lt;a href=&#34;https://doi.org/10.1098/rsos.220346&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1098/rsos.220346&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Cannot open plots created with brms::mcmc_plot due to lack of `discrete_range` function</title>
      <link>https://pablobernabeu.github.io/2023/cannot-open-plots-created-with-brms-mcmc-plot-due-to-lack-of-discrete-range-function/</link>
      <pubDate>Tue, 11 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/cannot-open-plots-created-with-brms-mcmc-plot-due-to-lack-of-discrete-range-function/</guid>
      <description>


</description>
    </item>
    
    <item>
      <title>A table of results for Bayesian mixed-effects models: Grouping variables and specifying random slopes</title>
      <link>https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/</link>
      <pubDate>Thu, 29 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;Here I share the format applied to tables presenting the results of &lt;em&gt;Bayesian&lt;/em&gt; models in Bernabeu (2022; the &lt;a href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes&#34;&gt;table for frequentist models is covered in this other post&lt;/a&gt;). The sample table presents a Bayesian mixed-effects model that was fitted using the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34;&gt;&lt;code&gt;brms&lt;/code&gt;&lt;/a&gt; (Bürkner et al., 2022). The mixed effects were driven by the maximal principle (Brauer &amp;amp; Curtin, 2018). The format of the table resembles one of the examples published by the &lt;a href=&#34;https://apastyle.apa.org/style-grammar-guidelines/tables-figures/sample-tables&#34;&gt;American Psychological Association&lt;/a&gt;. However, there are also deviations from those examples. For instance, in the present table, the effects are grouped under informative labels to facilitate the readers’ comprehension, using the &lt;a href=&#34;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&#34;&gt;&lt;code&gt;kableExtra&lt;/code&gt;&lt;/a&gt; package (Zhu, 2022). Furthermore, the random slopes are specified using superscript letters and a footnote. The table can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;loading-packages-and-the-results-of-the-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading packages and the results of the models&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(knitr)
library(tibble)
library(stringr)
library(lmerTest)
library(kableExtra)

# Load Bayesian results summary

semanticpriming_summary_weaklyinformativepriors_exgaussian = 
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_summary_weaklyinformativepriors_exgaussian.rds?raw=true&amp;#39;)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adjusting-the-names-of-the-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adjusting the names of the effects&lt;/h3&gt;
&lt;p&gt;First, to facilitate the understanding of the results, the original names of the effects will be adjusted in the &lt;code&gt;brms&lt;/code&gt; summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Rename effects in plain language and specify the random slopes
# (if any) for each effect, in the footnote. For this purpose, 
# superscripts are added to the names of the appropriate effects.
# 
# In the interactions below, word-level variables are presented 
# first for the sake of consistency (the order does not affect 
# the results in any way). Also in the interactions, double 
# colons are used to inform the &amp;#39;bayesian_model_table&amp;#39; 
# function that the two terms in the interaction must be split 
# into two lines.

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_attentional_control&amp;#39;] = &amp;#39;Attentional control&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_attentional_control&amp;#39;] = &amp;#39;Attentional control&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_vocabulary_size&amp;#39;] = &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_recoded_participant_gender&amp;#39;] = &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_target_word_frequency&amp;#39;] = &amp;#39;Word frequency&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_target_number_syllables&amp;#39;] = &amp;#39;Number of syllables&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_word_concreteness_diff&amp;#39;] = &amp;#39;Word-concreteness difference&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_cosine_similarity&amp;#39;] = &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_visual_rating_diff&amp;#39;] = &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_recoded_interstimulus_interval&amp;#39;] = &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_word_concreteness_diff:z_vocabulary_size&amp;#39;] = 
  &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Word-concreteness difference : SOA&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_participant_gender&amp;#39;] = 
  &amp;#39;Word-concreteness difference : Gender&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_attentional_control:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Attentional control&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_attentional_control:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Attentional control&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_vocabulary_size:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_vocabulary_size:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_recoded_participant_gender:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : Gender&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_recoded_participant_gender:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : Gender&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian_model_table&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;bayesian_model_table()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;This &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/R_functions/bayesian_model_table.R&#34;&gt;custom function&lt;/a&gt; was used in Bernabeu (2022), with a PDF output. In the current scenario, however, we have an HTML output. In the above function, the code used for the &lt;span class=&#34;math inline&#34;&gt;\(\widehat{R}\)&lt;/span&gt; tailored to the HTML output (&lt;code&gt;&amp;amp;Rcirc;&lt;/code&gt;) does not render properly.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Fbayesian_model_table.R%23L119-L121&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;Instead, the LaTeX code &lt;code&gt;$\\widehat{R}$&lt;/code&gt; must be used. Therefore, we’ll correct this error and load the function below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function used in the manuscript to present summaries from &amp;#39;brms&amp;#39; models 
# in APA-formatted tables. The only obligatory argument to be supplied is 
# a summary of a &amp;#39;brms&amp;#39; model.

bayesian_model_table = 
  
  function(model_summary, show_intercept = TRUE, select_effects = NULL, 
           order_effects = NULL, format = NULL, 
           
           # If interaction_symbol_x = TRUE, replace double colons with 
           # times symbols followed by line breaks and indentation. 
           # Then, replace single colons with times symbols.
           interaction_symbol_x = FALSE,
           
           caption = &amp;#39;Summary of the lmerTest model.&amp;#39;) {
    
    require(dplyr)
    require(knitr)
    require(tibble)
    require(stringr)
    require(lmerTest)
    require(kableExtra)
    
    # Create data frame
    model_summary = 
      data.frame(Effect = rownames(model_summary$fixed), 
                 Estimate = model_summary$fixed$Estimate, 
                 SE = model_summary$fixed$Est.Error, 
                 CrI_2.5 = model_summary$fixed$`l-95% CI`, 
                 CrI_97.5 = model_summary$fixed$`u-95% CI`, 
                 Rhat = model_summary$fixed$Rhat,
                 row.names = NULL)
    
    # Process credible intervals and present both inside square brackets
    
    model_summary$CrI_2.5 = model_summary$CrI_2.5 %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .) %&amp;gt;% 
      # Remove minus sign from pure zeros
      sub(&amp;#39;-0.00&amp;#39;, &amp;#39;0.00&amp;#39;, .)
    
    model_summary$CrI_97.5 = model_summary$CrI_97.5 %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .) %&amp;gt;% 
      # Remove minus sign from pure zeros
      sub(&amp;#39;-0.00&amp;#39;, &amp;#39;0.00&amp;#39;, .)
    
    model_summary$CrI_95 = paste0(&amp;#39;[&amp;#39;, model_summary$CrI_2.5, &amp;#39;, &amp;#39;, 
                                  model_summary$CrI_97.5, &amp;#39;]&amp;#39;)
    
    # If show_intercept = FALSE, remove it
    if(isFALSE(show_intercept)) {
      model_summary = model_summary %&amp;gt;% filter(!grepl(&amp;#39;Intercept&amp;#39;, Effect))
      
      # Put &amp;#39;Intercept&amp;#39; in parentheses
    } else if(!is.null(model_summary[model_summary$Effect == &amp;#39;Intercept&amp;#39;, &amp;#39;Effect&amp;#39;])) {
      model_summary[model_summary$Effect == &amp;#39;Intercept&amp;#39;, &amp;#39;Effect&amp;#39;] = &amp;#39;(Intercept)&amp;#39;
    }
    
    # If select_effects was supplied, apply it and order effects accordingly
    if(!is.null(select_effects)) {
      model_summary = model_summary %&amp;gt;% filter(Effect %in% select_effects) %&amp;gt;%
        arrange(factor(Effect, levels = select_effects))
    }
    
    # If order_effects was supplied, apply order
    if(!is.null(order_effects)) {
      model_summary = model_summary %&amp;gt;%
        arrange(factor(Effect, levels = order_effects))
    }
    
    # Round other values
    
    model_summary$Estimate = model_summary$Estimate %&amp;gt;% as.numeric %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .) %&amp;gt;% 
      # Remove minus sign from pure zeros
      sub(&amp;#39;-0.00&amp;#39;, &amp;#39;0.00&amp;#39;, .)
    
    model_summary$SE = model_summary$SE %&amp;gt;% as.numeric %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .)
    
    model_summary$Rhat = model_summary$Rhat %&amp;gt;% as.numeric %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .)
    
    # Order columns
    model_summary = model_summary %&amp;gt;% select(Effect, Estimate, SE, CrI_95, Rhat)
    
    # Right-align all columns after first one
    align = c(&amp;#39;l&amp;#39;, &amp;#39;r&amp;#39;, &amp;#39;r&amp;#39;, &amp;#39;r&amp;#39;, &amp;#39;r&amp;#39;)
    
    # Establish latex or HTML format: if no format supplied, 
    # try to obtain it from knitr, or apply HTML
    if(missing(format) || is.null(format)) {
      if(knitr::is_latex_output()) {
        format = &amp;#39;latex&amp;#39;
      } else format = &amp;#39;html&amp;#39;
    }
    
    # HTML format
    if(format == &amp;#39;html&amp;#39;) {
      
      # If interaction_symbol_x = TRUE, replace double colons with times 
      # symbols followed by line breaks and indentation. Then, replace 
      # single colons with times symbols.
      if(interaction_symbol_x) {
        model_summary$Effect = model_summary$Effect %&amp;gt;% 
          gsub(&amp;#39;::&amp;#39;, &amp;#39; &amp;amp;times; &amp;lt;br&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;#39;, .) %&amp;gt;%
          gsub(&amp;#39;:&amp;#39;, &amp;#39; &amp;amp;times; &amp;#39;, .)
      }
      
      # Output table
      model_summary %&amp;gt;% 
        
        # Remove header of first column and rename other headers
        rename(&amp;#39; &amp;#39; = &amp;#39;Effect&amp;#39;, &amp;#39;&amp;amp;beta;&amp;#39; = &amp;#39;Estimate&amp;#39;, &amp;#39;&amp;lt;i&amp;gt;SE&amp;lt;/i&amp;gt;&amp;#39; = &amp;#39;SE&amp;#39;, 
               &amp;#39;95% CrI&amp;#39; = &amp;#39;CrI_95&amp;#39;, &amp;#39;$\\widehat{R}$&amp;#39; = &amp;#39;Rhat&amp;#39;) %&amp;gt;%
        
        # Present table
        kbl(digits = 2, booktabs = TRUE, escape = FALSE, align = align,
            
            # Caption of the table (default unless specified)
            caption = caption, 
            
            # Disable occasional line gap (https://stackoverflow.com/a/49018919/7050882)
            linesep = &amp;#39;&amp;#39;) %&amp;gt;%
        
        # Apply nice kableExtra format
        kable_styling() %&amp;gt;%
        
        # Center-align header row
        row_spec(0, align = &amp;#39;c&amp;#39;)
      
      # LaTeX format
    } else {
      
      # If interaction_symbol_x = TRUE, replace double colons with times 
      # symbols followed by line breaks and indentation. Then, replace 
      # single colons with times symbols.
      if(interaction_symbol_x) {
        model_summary$Effect = model_summary$Effect %&amp;gt;% 
          gsub(&amp;#39;::&amp;#39;, &amp;#39; $\\\\times$ \n \\\\hspace{0.3cm}&amp;#39;, .) %&amp;gt;%
          gsub(&amp;#39;:&amp;#39;, &amp;#39; $\\\\times$ &amp;#39;, .)
      }
      
      model_summary$Effect = model_summary$Effect %&amp;gt;%
        
        # Escape underscores to avoid error in table
        str_replace_all(&amp;#39;_&amp;#39;, &amp;#39;\\\\_&amp;#39;) %&amp;gt;%
        
        # Allow line breaks in the names of the effects
        # (used in the interactions)
        kableExtra::linebreak(align = &amp;#39;l&amp;#39;)
      
      # Output table
      model_summary %&amp;gt;% 
        
        # Remove header of first column and rename other headers
        rename(&amp;#39; &amp;#39; = &amp;#39;Effect&amp;#39;, &amp;#39;$\\upbeta$&amp;#39; = &amp;#39;Estimate&amp;#39;, &amp;#39;$SE$&amp;#39; = &amp;#39;SE&amp;#39;, 
               &amp;#39;95\\% CrI&amp;#39; = &amp;#39;CrI_95&amp;#39;, &amp;#39;$\\widehat R$&amp;#39; = &amp;#39;Rhat&amp;#39;) %&amp;gt;%
        
        # Present table
        kbl(digits = 2, booktabs = TRUE, escape = FALSE, align = align,
            
            # Caption of the table (default unless specified)
            caption = caption, 
            
            # Disable occasional line gap (https://stackoverflow.com/a/49018919/7050882)
            linesep = &amp;#39;&amp;#39;) %&amp;gt;%
        
        # Apply nice kableExtra format
        kable_styling() %&amp;gt;%
        
        # Center-align header row
        row_spec(0, align = &amp;#39;c&amp;#39;)
    }
  }&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-function-in-use&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function in use&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create table (using custom function from the &amp;#39;R_functions&amp;#39; folder)
bayesian_model_table(
  semanticpriming_summary_weaklyinformativepriors_exgaussian,
  order_effects = c(&amp;#39;(Intercept)&amp;#39;,
                    &amp;#39;Attentional control&amp;#39;,
                    &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Word frequency&amp;#39;,
                    &amp;#39;Number of syllables&amp;#39;,
                    &amp;#39;Word-concreteness difference&amp;#39;,
                    &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;,
                    &amp;#39;Word-concreteness difference : SOA&amp;#39;,
                    &amp;#39;Word-concreteness difference : Gender&amp;#39;,
                    &amp;#39;Language-based similarity :: Attentional control&amp;#39;,
                    &amp;#39;Visual-strength difference :: Attentional control&amp;#39;,
                    &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;,
                    &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;,
                    &amp;#39;Language-based similarity : Gender&amp;#39;,
                    &amp;#39;Visual-strength difference : Gender&amp;#39;,
                    &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;),
  
  # Replace colons in the names of interactions with times symbols
  interaction_symbol_x = TRUE, 
  
  # No title
  caption = NULL) %&amp;gt;%
  
  # Group predictors under headings
  pack_rows(&amp;#39;Individual differences&amp;#39;, 2, 4) %&amp;gt;% 
  pack_rows(&amp;#39;Target-word lexical covariates&amp;#39;, 5, 6) %&amp;gt;% 
  pack_rows(&amp;#39;Prime--target relationship&amp;#39;, 7, 9) %&amp;gt;% 
  pack_rows(&amp;#39;Task condition&amp;#39;, 10, 10) %&amp;gt;% 
  pack_rows(&amp;#39;Interactions&amp;#39;, 11, 21) %&amp;gt;% 
  
  # Apply white background to override default shading in HTML output
  row_spec(1:21, background = &amp;#39;white&amp;#39;) %&amp;gt;%
  
  # Highlight covariates
  row_spec(c(2, 5:7, 11:15), background = &amp;#39;#FFFFF1&amp;#39;) %&amp;gt;%
  
  # Format
  kable_classic(full_width = FALSE, html_font = &amp;#39;Cambria&amp;#39;) %&amp;gt;%
  
  # Footnote describing abbreviations, random slopes, etc. 
  # LaTeX code used to format the text.
  footnote(escape = FALSE, threeparttable = TRUE, general_title = &amp;#39;&amp;lt;br&amp;gt;&amp;#39;, 
           general = paste(&amp;#39;*Note*. &amp;amp;beta; = Estimate based on $z$-scored predictors; *SE* = standard error;&amp;#39;,
                           &amp;#39;CrI = credible interval. Yellow rows contain covariates. Some interactions are &amp;#39;,
                           &amp;#39;split over two lines, with the second line indented. &amp;lt;br&amp;gt;&amp;#39;, 
                           &amp;#39;&amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt; By-word random slopes were included for this effect.&amp;#39;,
                           &amp;#39;&amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt; By-participant random slopes were included for this effect.&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table lightable-classic&#34; style=&#34;margin-left: auto; margin-right: auto; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;border-bottom: 0;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
β
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;i&gt;SE&lt;/i&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
95% CrI
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\widehat{R}\)&lt;/span&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;background-color: white !important;&#34;&gt;
(Intercept)
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Individual differences&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Vocabulary size &lt;sup&gt;a&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Gender &lt;sup&gt;a&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Target-word lexical covariates&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word frequency
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.12, -0.11]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Number of syllables
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.06, 0.07]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Prime–target relationship&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.07, -0.06]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.01, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;1&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Task condition&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Stimulus onset asynchrony (SOA) &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.02, 0.04]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;11&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Interactions&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × SOA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × &lt;br&gt;    Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × &lt;br&gt;    Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × SOA &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × SOA &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; &#34; colspan=&#34;100%&#34;&gt;
&lt;span style=&#34;font-style: italic;&#34;&gt;&lt;br&gt;&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; &#34; colspan=&#34;100%&#34;&gt;
&lt;sup&gt;&lt;/sup&gt; &lt;em&gt;Note&lt;/em&gt;. β = Estimate based on &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scored predictors; &lt;em&gt;SE&lt;/em&gt; = standard error; CrI = credible interval. Yellow rows contain covariates. Some interactions are split over two lines, with the second line indented. &lt;br&gt; &lt;sup&gt;a&lt;/sup&gt; By-word random slopes were included for this effect. &lt;sup&gt;b&lt;/sup&gt; By-participant random slopes were included for this effect.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
&lt;div id=&#34;shading-specific-rows&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Shading specific rows&lt;/h4&gt;
&lt;p&gt;Shading specific rows is done differently when the output is PDF, as shown below (see p. 170 in &lt;a href=&#34;https://eprints.lancs.ac.uk/id/eprint/177833/6/2022deJuanBernabeuPhD.pdf&#34;&gt;Bernabeu, 2022&lt;/a&gt;).&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fthesis%2Fappendix-E-Bayesian-analysis-results.Rmd%23L164-L165&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Session info&lt;/h3&gt;
&lt;p&gt;If you encounter any blockers while reproducing the table using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;, my current session info may be useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.2 (2022-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19045)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.utf8 
## [2] LC_CTYPE=English_United States.utf8   
## [3] LC_MONETARY=English_United States.utf8
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] kableExtra_1.3.4    lmerTest_3.1-3      lme4_1.1-31        
## [4] Matrix_1.5-1        stringr_1.5.0       tibble_3.1.8       
## [7] dplyr_1.0.10        knitr_1.41          xaringanExtra_0.7.0
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.2.0    xfun_0.36           bslib_0.4.2        
##  [4] splines_4.2.2       lattice_0.20-45     colorspace_2.0-3   
##  [7] vctrs_0.5.1         generics_0.1.3      viridisLite_0.4.1  
## [10] htmltools_0.5.4     yaml_2.3.6          utf8_1.2.2         
## [13] rlang_1.0.6         jquerylib_0.1.4     pillar_1.8.1       
## [16] nloptr_2.0.3        withr_2.5.0         glue_1.6.2         
## [19] DBI_1.1.3           uuid_1.1-0          lifecycle_1.0.3    
## [22] munsell_0.5.0       blogdown_1.16       gtable_0.3.1       
## [25] rvest_1.0.3         evaluate_0.19       fastmap_1.1.0      
## [28] fansi_1.0.3         Rcpp_1.0.9          scales_1.2.1       
## [31] cachem_1.0.6        webshot_0.5.4       jsonlite_1.8.4     
## [34] systemfonts_1.0.4   ggplot2_3.3.5       digest_0.6.31      
## [37] stringi_1.7.8       bookdown_0.31       numDeriv_2016.8-1.1
## [40] grid_4.2.2          cli_3.4.1           tools_4.2.2        
## [43] magrittr_2.0.3      sass_0.4.4          pkgconfig_2.0.3    
## [46] MASS_7.3-58.1       xml2_1.3.3          svglite_2.1.0      
## [49] httr_1.4.4          assertthat_0.2.1    minqa_1.2.5        
## [52] rmarkdown_2.19      rstudioapi_0.14     R6_2.5.1           
## [55] boot_1.3-28         nlme_3.1-160        compiler_4.2.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://doi.org/10.1037/met0000159&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bürkner, P.-C., Gabry, J., Weber, S., Johnson, A., Modrak, M., Badr, H. S., Weber, F., Ben-Shachar, M. S., &amp;amp; Rabel, H. (2022). &lt;em&gt;Package ’brms’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/brms/brms.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhu, H. (2022). &lt;em&gt;Package ’kableExtra’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A table of results for frequentist mixed-effects models: Grouping variables and specifying random slopes</title>
      <link>https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/</link>
      <pubDate>Thu, 29 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;Here I share the format applied to tables presenting the results of &lt;em&gt;frequentist&lt;/em&gt; models in Bernabeu (2022; the &lt;a href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes&#34;&gt;table for Bayesian models is covered in this other post&lt;/a&gt;). The sample table presents a mixed-effects model that was fitted using the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; (Kuznetsova et al., 2022). The mixed effects were driven by the maximal principle (Brauer &amp;amp; Curtin, 2018). The format of the table resembles one of the examples published by the &lt;a href=&#34;https://apastyle.apa.org/style-grammar-guidelines/tables-figures/sample-tables&#34;&gt;American Psychological Association&lt;/a&gt;. However, there are also deviations from those examples. For instance, in the present table, the effects are grouped under informative labels to facilitate the readers’ comprehension, using the &lt;a href=&#34;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&#34;&gt;&lt;code&gt;kableExtra&lt;/code&gt;&lt;/a&gt; package (Zhu, 2022). Furthermore, the random slopes are specified using superscript letters and a footnote. The table can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;loading-packages-and-the-results-of-the-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading packages and the results of the models&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(knitr)
library(tibble)
library(stringr)
library(lmerTest)
library(kableExtra)

# Load frequentist coefficients (estimates and confidence intervals)

KR_summary_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/KR_summary_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

confint_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/confint_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adjusting-the-names-of-the-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adjusting the names of the effects&lt;/h3&gt;
&lt;p&gt;First, to facilitate the understanding of the results, the original names of the effects will be adjusted in the &lt;code&gt;lmerTest&lt;/code&gt; summary and in the confidence intervals object.&lt;/p&gt;
&lt;p&gt;Incidentally, the confidence intervals were obtained using the &lt;code&gt;confint.merMod&lt;/code&gt; function from the &lt;code&gt;lme4&lt;/code&gt; package, as neither &lt;code&gt;lmerTest&lt;/code&gt; nor &lt;code&gt;lme4&lt;/code&gt; currently provide confidence intervals in their default results output. However, computing the confidence intervals is uncomplicated (&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/semanticpriming_lmerTest.R#L126-L130&#34;&gt;see code&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Rename effects in plain language and specify the random slopes
# (if any) for each effect, in the footnote. For this purpose, 
# superscripts are added to the names of the appropriate effects.
# 
# In the interactions below, word-level variables are presented 
# first for the sake of consistency (the order does not affect 
# the results in any way). Also in the interactions, double 
# colons are used to inform the &amp;#39;frequentist_model_table&amp;#39; 
# function that the two terms in the interaction must be split 
# into two lines.

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_attentional_control&amp;#39;] = &amp;#39;Attentional control&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_vocabulary_size&amp;#39;] = &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_participant_gender&amp;#39;] = &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_target_word_frequency&amp;#39;] = &amp;#39;Word frequency&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_target_number_syllables&amp;#39;] = &amp;#39;Number of syllables&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_word_concreteness_diff&amp;#39;] = &amp;#39;Word-concreteness difference&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_cosine_similarity&amp;#39;] = &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_visual_rating_diff&amp;#39;] = &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_interstimulus_interval&amp;#39;] = &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_word_concreteness_diff:z_vocabulary_size&amp;#39;] = 
  &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Word-concreteness difference : SOA&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_participant_gender&amp;#39;] = 
  &amp;#39;Word-concreteness difference : Gender&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_attentional_control:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Attentional control&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_attentional_control:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Attentional control&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_vocabulary_size:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_vocabulary_size:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_participant_gender:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : Gender&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_participant_gender:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : Gender&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;


# Next, change the names in the confidence intervals object

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_attentional_control&amp;#39;] = &amp;#39;Attentional control&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_vocabulary_size&amp;#39;] = &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_participant_gender&amp;#39;] = &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_target_word_frequency&amp;#39;] = &amp;#39;Word frequency&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_target_number_syllables&amp;#39;] = &amp;#39;Number of syllables&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_word_concreteness_diff&amp;#39;] = &amp;#39;Word-concreteness difference&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_cosine_similarity&amp;#39;] = &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_visual_rating_diff&amp;#39;] = &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_interstimulus_interval&amp;#39;] = &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_word_concreteness_diff:z_vocabulary_size&amp;#39;] = 
  &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Word-concreteness difference : SOA&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_participant_gender&amp;#39;] = 
  &amp;#39;Word-concreteness difference : Gender&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_attentional_control:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Attentional control&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_attentional_control:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Attentional control&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_vocabulary_size:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_vocabulary_size:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_participant_gender:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : Gender&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_participant_gender:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : Gender&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;frequentist_model_table&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;frequentist_model_table()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The following custom function was used.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Ffrequentist_model_table.R&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;loading-the-function-from-github&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Loading the function from GitHub&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/raw/main/R_functions/frequentist_model_table.R&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-function-in-use&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function in use&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create table (using custom function from the &amp;#39;R_functions&amp;#39; folder)
frequentist_model_table(
  KR_summary_semanticpriming_lmerTest, 
  confint_semanticpriming_lmerTest,
  order_effects = c(&amp;#39;(Intercept)&amp;#39;,
                    &amp;#39;Attentional control&amp;#39;,
                    &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Word frequency&amp;#39;,
                    &amp;#39;Number of syllables&amp;#39;,
                    &amp;#39;Word-concreteness difference&amp;#39;,
                    &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;,
                    &amp;#39;Word-concreteness difference : SOA&amp;#39;,
                    &amp;#39;Word-concreteness difference : Gender&amp;#39;,
                    &amp;#39;Language-based similarity :: Attentional control&amp;#39;,
                    &amp;#39;Visual-strength difference :: Attentional control&amp;#39;,
                    &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;,
                    &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;,
                    &amp;#39;Language-based similarity : Gender&amp;#39;,
                    &amp;#39;Visual-strength difference : Gender&amp;#39;,
                    &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;),
  
  # Replace colons in the names of interactions with times symbols
  interaction_symbol_x = TRUE, 
  
  # No title
  caption = NULL) %&amp;gt;%
  
  # Group predictors under headings
  pack_rows(&amp;#39;Individual differences&amp;#39;, 2, 4) %&amp;gt;% 
  pack_rows(&amp;#39;Target-word lexical covariates&amp;#39;, 5, 6) %&amp;gt;% 
  pack_rows(&amp;#39;Prime--target relationship&amp;#39;, 7, 9) %&amp;gt;% 
  pack_rows(&amp;#39;Task condition&amp;#39;, 10, 10) %&amp;gt;% 
  pack_rows(&amp;#39;Interactions&amp;#39;, 11, 21) %&amp;gt;% 
  
  # Apply white background to override default shading in HTML output
  row_spec(1:21, background = &amp;#39;white&amp;#39;) %&amp;gt;%
  
  # Highlight covariates
  row_spec(c(2, 5:7, 11:15), background = &amp;#39;#FFFFF1&amp;#39;) %&amp;gt;%
  
  # Format
  kable_classic(full_width = FALSE, html_font = &amp;#39;Cambria&amp;#39;) %&amp;gt;%
  
  # Footnote describing abbreviations, random slopes, etc. 
  # LaTeX code used to format the text.
  footnote(escape = FALSE, threeparttable = TRUE, general_title = &amp;#39;&amp;lt;br&amp;gt;&amp;#39;, 
           general = paste(&amp;#39;*Note*. &amp;amp;beta; = Estimate based on $z$-scored predictors; *SE* = standard error;&amp;#39;,
                           &amp;#39;CI = confidence interval. Yellow rows contain covariates. Some interactions are &amp;#39;,
                           &amp;#39;split over two lines, with the second line indented. &amp;lt;br&amp;gt;&amp;#39;, 
                           &amp;#39;&amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt; By-word random slopes were included for this effect.&amp;#39;,
                           &amp;#39;&amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt; By-participant random slopes were included for this effect.&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table lightable-classic&#34; style=&#34;margin-left: auto; margin-right: auto; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;border-bottom: 0;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
β
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;i&gt;SE&lt;/i&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
95% CI
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;i&gt;t&lt;/i&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;i&gt;p&lt;/i&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;background-color: white !important;&#34;&gt;
(Intercept)
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.112
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Individual differences&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.56
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.577
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Vocabulary size &lt;sup&gt;a&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.02
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.987
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Gender &lt;sup&gt;a&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.979
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Target-word lexical covariates&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word frequency
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.16, -0.15]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-49.40
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Number of syllables
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.07, 0.08]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
22.81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Prime–target relationship&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.01, 0.02]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
3.48
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.08, -0.07]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-22.44
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.01, 0.02]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
4.18
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;1&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Task condition&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Stimulus onset asynchrony (SOA) &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.04, 0.07]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
7.47
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;11&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Interactions&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.189
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × SOA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
2.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.010
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.97
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.332
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × &lt;br&gt;    Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-2.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.014
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × &lt;br&gt;    Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.810
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-2.34
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.020
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-1.37
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.172
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.433
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.144
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × SOA &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
3.22
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × SOA &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-2.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.025
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; &#34; colspan=&#34;100%&#34;&gt;
&lt;span style=&#34;font-style: italic;&#34;&gt;&lt;br&gt;&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; &#34; colspan=&#34;100%&#34;&gt;
&lt;sup&gt;&lt;/sup&gt; &lt;em&gt;Note&lt;/em&gt;. β = Estimate based on &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scored predictors; &lt;em&gt;SE&lt;/em&gt; = standard error; CI = confidence interval. Yellow rows contain covariates. Some interactions are split over two lines, with the second line indented. &lt;br&gt; &lt;sup&gt;a&lt;/sup&gt; By-word random slopes were included for this effect. &lt;sup&gt;b&lt;/sup&gt; By-participant random slopes were included for this effect.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
&lt;div id=&#34;shading-specific-rows&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Shading specific rows&lt;/h4&gt;
&lt;p&gt;Shading specific rows is done differently when the output is PDF, as shown below (see p. 62 in &lt;a href=&#34;https://eprints.lancs.ac.uk/id/eprint/177833/6/2022deJuanBernabeuPhD.pdf&#34;&gt;Bernabeu, 2022&lt;/a&gt;).&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fthesis%2FChapter-3-Study-2.Rmd%23L690-L691&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Session info&lt;/h3&gt;
&lt;p&gt;If you encounter any blockers while reproducing the table using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;, my current session info may be useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.2 (2022-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19045)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.utf8 
## [2] LC_CTYPE=English_United States.utf8   
## [3] LC_MONETARY=English_United States.utf8
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] kableExtra_1.3.4    lmerTest_3.1-3      lme4_1.1-31        
## [4] Matrix_1.5-1        stringr_1.5.0       tibble_3.1.8       
## [7] dplyr_1.0.10        knitr_1.41          xaringanExtra_0.7.0
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.2.0    xfun_0.36           bslib_0.4.2        
##  [4] splines_4.2.2       lattice_0.20-45     colorspace_2.0-3   
##  [7] vctrs_0.5.1         generics_0.1.3      viridisLite_0.4.1  
## [10] htmltools_0.5.4     yaml_2.3.6          utf8_1.2.2         
## [13] rlang_1.0.6         jquerylib_0.1.4     pillar_1.8.1       
## [16] nloptr_2.0.3        withr_2.5.0         glue_1.6.2         
## [19] DBI_1.1.3           uuid_1.1-0          lifecycle_1.0.3    
## [22] munsell_0.5.0       blogdown_1.16       gtable_0.3.1       
## [25] rvest_1.0.3         evaluate_0.19       fastmap_1.1.0      
## [28] fansi_1.0.3         Rcpp_1.0.9          scales_1.2.1       
## [31] cachem_1.0.6        webshot_0.5.4       jsonlite_1.8.4     
## [34] systemfonts_1.0.4   ggplot2_3.3.5       digest_0.6.31      
## [37] stringi_1.7.8       bookdown_0.31       numDeriv_2016.8-1.1
## [40] grid_4.2.2          cli_3.4.1           tools_4.2.2        
## [43] magrittr_2.0.3      sass_0.4.4          pkgconfig_2.0.3    
## [46] MASS_7.3-58.1       xml2_1.3.3          svglite_2.1.0      
## [49] httr_1.4.4          assertthat_0.2.1    minqa_1.2.5        
## [52] rmarkdown_2.19      rstudioapi_0.14     R6_2.5.1           
## [55] boot_1.3-28         nlme_3.1-160        compiler_4.2.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://doi.org/10.1037/met0000159&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kuznetsova, A., Brockhoff, P. B., &amp;amp; Christensen, R. H. B. (2022). &lt;em&gt;Package ’lmerTest’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhu, H. (2022). &lt;em&gt;Package ’kableExtra’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Plotting two-way interactions from mixed-effects models using alias variables</title>
      <link>https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-alias-variables/</link>
      <pubDate>Mon, 26 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-alias-variables/</guid>
      <description>


&lt;p&gt;Whereas the direction of main effects can be interpreted from the sign of the estimate, the interpretation of interaction effects often requires plots. This task is facilitated by the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34;&gt;&lt;code&gt;sjPlot&lt;/code&gt;&lt;/a&gt; (Lüdecke, 2022). In Bernabeu (2022), the sjPlot function called &lt;code&gt;plot_model&lt;/code&gt; served as the basis for the creation of some &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/tree/main/R_functions&#34;&gt;custom functions&lt;/a&gt;. One of these functions is &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/R_functions/alias_interaction_plot.R&#34;&gt;&lt;code&gt;alias_interaction_plot&lt;/code&gt;&lt;/a&gt;, which allows the plotting of interactions between a continuous variable and a categorical variable. Importantly, the categorical variable is replaced with an alias variable. This feature allows the back-transformation of the categorical variable to facilitate the communication of the results, for instance, when the categorical variable was sum-coded, which has been recommended for mixed-effects models (Brauer &amp;amp; Curtin, 2018).&lt;/p&gt;
&lt;p&gt;Below, we’ll use the function with a model fitted using &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; (Kuznetsova et al., 2022), although the function also works with several other models (see &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34;&gt;sjPlot manual&lt;/a&gt;). The plot can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;alias-interaction-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Alias interaction plot&lt;/h2&gt;
&lt;div id=&#34;the-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Falias_interaction_plot.R&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;the-function-in-use&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function in use&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Ffrequentist_analysis%2Fsemanticpriming-interactions-with-SOA.R&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/plots/semanticpriming-interactions-with-SOA.pdf&#34;&gt;&lt;img src=&#34;Screenshot%202022-12-27%20234345.png&#34; width=&#34;550&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://doi.org/10.1037/met0000159&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kuznetsova, A., Brockhoff, P. B., &amp;amp; Christensen, R. H. B. (2022). &lt;em&gt;Package ’lmerTest’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lüdecke, D. (2022). &lt;em&gt;Package ’sjPlot’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Plotting two-way interactions from mixed-effects models using ten or six bins</title>
      <link>https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-ten-or-six-bins/</link>
      <pubDate>Mon, 26 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-ten-or-six-bins/</guid>
      <description>


&lt;p&gt;Whereas the direction of main effects can be interpreted from the sign of the estimate, the interpretation of interaction effects often requires plots. This task is facilitated by the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34;&gt;&lt;code&gt;sjPlot&lt;/code&gt;&lt;/a&gt; (Lüdecke, 2022). In Bernabeu (2022), the sjPlot function called &lt;code&gt;plot_model&lt;/code&gt; served as the basis for the creation of some &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/tree/main/R_functions&#34;&gt;custom functions&lt;/a&gt;. Two of these functions are &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/R_functions/deciles_interaction_plot.R&#34;&gt;&lt;code&gt;deciles_interaction_plot&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/R_functions/sextiles_interaction_plot.R&#34;&gt;&lt;code&gt;sextiles_interaction_plot&lt;/code&gt;&lt;/a&gt;. These functions allow the plotting of interactions between two continuous variables. In the case of &lt;code&gt;deciles_interaction_plot&lt;/code&gt;, one of the variables is divided into ten bins, known as deciles, and the other variable is unchanged. In the case of &lt;code&gt;sextiles_interaction_plot&lt;/code&gt;, one of the variables is divided into six bins, or sextiles, and the other variable is unchanged.&lt;/p&gt;
&lt;p&gt;Below, we’ll use these functions with models fitted using &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; (Kuznetsova et al., 2022), although the functions also work with several other models (see &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34;&gt;sjPlot manual&lt;/a&gt;). The plots can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;deciles-interaction-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Deciles interaction plot&lt;/h2&gt;
&lt;div id=&#34;the-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Fdeciles_interaction_plot.R&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;the-function-in-use&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function in use&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Ffrequentist_analysis%2Fsemanticpriming-interactions-with-vocabulary-size.R&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/plots/semanticpriming-interactions-with-vocabulary-size.pdf&#34;&gt;&lt;img src=&#34;Screenshot%202022-12-27%20234321.png&#34; width=&#34;580&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sextiles-interaction-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sextiles interaction plot&lt;/h2&gt;
&lt;div id=&#34;the-function-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Fsextiles_interaction_plot.R&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;the-function-in-use-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function in use&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Flexicaldecision%2Ffrequentist_analysis%2Flexicaldecision-interactions-with-vocabulary-age.R&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/lexicaldecision/frequentist_analysis/plots/lexicaldecision-interactions-with-vocabulary-age.pdf&#34;&gt;&lt;img src=&#34;Screenshot%202022-12-27%20234252.png&#34; width=&#34;650&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kuznetsova, A., Brockhoff, P. B., &amp;amp; Christensen, R. H. B. (2022). &lt;em&gt;Package ’lmerTest’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lüdecke, D. (2022). &lt;em&gt;Package ’sjPlot’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why can&#39;t we be friends? Plotting frequentist (lmerTest) and Bayesian (brms) mixed-effects models</title>
      <link>https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/</link>
      <pubDate>Fri, 23 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/index.en_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;


&lt;p&gt;Frequentist and Bayesian statistics are sometimes regarded as fundamentally different philosophies. Indeed, can both methods qualify as philosophies, or is one of them just a pointless ritual? Is frequentist statistics about &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; values only? Are frequentist estimates diametrically opposed to Bayesian posterior distributions? Are confidence intervals and credible intervals irreconcilable? Will R crash if &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34;&gt;&lt;code&gt;brms&lt;/code&gt;&lt;/a&gt; are simultaneously loaded? If only we could fit frequentist and Bayesian models to the same data and plot the results together, we might get a glimpse into these puzzles.&lt;/p&gt;
&lt;p&gt;All the analyses shown below can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;. The combination of the frequentist and the Bayesian estimates in the same plot is achieved using the following custom function from &lt;a href=&#34;https://pablobernabeu.github.io/publication/pablo-bernabeu-2022-phd-thesis&#34;&gt;Bernabeu (2022)&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;visualising-frequentist-and-bayesian-estimates-in-one-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualising frequentist and Bayesian estimates in one plot&lt;/h3&gt;
&lt;p&gt;Both frequentist and Bayesian statistics offer the options of hypothesis testing and parameter estimation (Cumming, 2014; Kruschke &amp;amp; Liddell, 2018; Rouder et al., 2018; Schmalz et al., 2022; Tendeiro &amp;amp; Kiers, 2019, 2022; van Ravenzwaaij &amp;amp; Wagenmakers, 2022). In the statistical analyses conducted by Bernabeu (2022), hypothesis testing was performed within the frequentist framework, whereas parameter estimation was performed within both the frequentist and the Bayesian frameworks (for other examples of the &lt;em&gt;estimation&lt;/em&gt; approach, see Milek et al., 2018; Pregla et al., 2021; Rodríguez-Ferreiro et al., 2020).&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Ffrequentist_bayesian_plot.R&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Let’s load the function from GitHub and put it to the test.&lt;/p&gt;
&lt;div style=&#34;height: 800px; border: 0.5px dotted grey; padding: 10px; resize: both; overflow: auto;&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Presenting the frequentist and the Bayesian estimates in the same plot. 
# For this purpose, the frequentist results are merged into a plot from 
# brms::mcmc_plot()

source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/main/R_functions/frequentist_bayesian_plot.R&amp;#39;)

# install.packages(&amp;#39;devtools&amp;#39;)
# library(devtools)
# install_version(&amp;#39;tidyverse&amp;#39;, &amp;#39;1.3.1&amp;#39;)  # Due to breaking changes, Version 1.3.1 is required.
# install_version(&amp;#39;ggplot2&amp;#39;, &amp;#39;5.3.5&amp;#39;)  # Due to breaking changes, Version 5.3.5 is required.
library(tidyverse)
library(ggplot2)
library(Cairo)

# Load frequentist coefficients (estimates and confidence intervals)

KR_summary_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/KR_summary_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

confint_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/confint_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

# Below are the default names of the effects
# rownames(KR_summary_semanticpriming_lmerTest$coefficients)
# rownames(confint_semanticpriming_lmerTest)

# Load Bayesian posterior distributions

semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian = 
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian.rds?raw=true&amp;#39;)))

# Below are the default names of the effects
# levels(semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter)


# Reorder the components of interactions in the frequentist results to match 
# with the order present in the Bayesian results.

rownames(KR_summary_semanticpriming_lmerTest$coefficients) =
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;, 
              replacement = &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;, 
              replacement = &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;)

rownames(confint_semanticpriming_lmerTest)  = 
  rownames(confint_semanticpriming_lmerTest) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;, 
              replacement = &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;, 
              replacement = &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;)


# Create a vector containing the names of the effects. This vector will be passed 
# to the plotting function.

new_labels = 
  
  semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter %&amp;gt;% 
  unique %&amp;gt;%
  
  # Remove the default &amp;#39;b_&amp;#39; from the beginning of each effect
  str_remove(&amp;#39;^b_&amp;#39;) %&amp;gt;%
  
  # Put Intercept in parentheses
  str_replace(pattern = &amp;#39;Intercept&amp;#39;, replacement = &amp;#39;(Intercept)&amp;#39;) %&amp;gt;%
  
  # First, adjust names of variables (both in main effects and in interactions)
  str_replace(pattern = &amp;#39;z_target_word_frequency&amp;#39;,
              replacement = &amp;#39;Target-word frequency&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_target_number_syllables&amp;#39;,
              replacement = &amp;#39;Number of target-word syllables&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_word_concreteness_diff&amp;#39;,
              replacement = &amp;#39;Word-concreteness difference&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_cosine_similarity&amp;#39;,
              replacement = &amp;#39;Language-based similarity&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_visual_rating_diff&amp;#39;,
              replacement = &amp;#39;Visual-strength difference&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_attentional_control&amp;#39;,
              replacement = &amp;#39;Attentional control&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_vocabulary_size&amp;#39;,
              replacement = &amp;#39;Vocabulary size&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_participant_gender&amp;#39;,
              replacement = &amp;#39;Gender&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval&amp;#39;,
              replacement = &amp;#39;SOA&amp;#39;) %&amp;gt;%
  # Show acronym in main effect of SOA
  str_replace(pattern = &amp;#39;^SOA$&amp;#39;,
              replacement = &amp;#39;Stimulus onset asynchrony (SOA)&amp;#39;) %&amp;gt;%
  
  # Second, adjust order of effects in interactions. In the output from the model, 
  # the word-level variables of interest (i.e., &amp;#39;z_cosine_similarity&amp;#39; and 
  # &amp;#39;z_visual_rating_diff&amp;#39;) sometimes appeared second in their interactions. For 
  # better consistency, the code below moves those word-level variables (with 
  # their new names) to the first position in their interactions. Note that the 
  # order does not affect the results in any way.
  sub(&amp;#39;(\\w+.*):(Language-based similarity|Visual-strength difference)&amp;#39;, 
      &amp;#39;\\2:\\1&amp;#39;, 
      .) %&amp;gt;%
  
  # Replace colons denoting interactions with times symbols
  str_replace(pattern = &amp;#39;:&amp;#39;, replacement = &amp;#39; &amp;amp;times; &amp;#39;)


# Create plot
plot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian =
  frequentist_bayesian_plot(KR_summary_semanticpriming_lmerTest,
                            confint_semanticpriming_lmerTest,
                            semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian,
                            labels = new_labels, interaction_symbol_x = TRUE,
                            vertical_line_at_x = 0, x_title = &amp;#39;Effect size (&amp;amp;beta;)&amp;#39;,
                            legend_ncol = 1) + 
  theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/index.en_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Frequentist and Bayesian estimates are not so polar opposites, are they? What is more, the larger differences between some estimates are the result of the priors that were set on the corresponding effects. With uninformative priors, the frequentist and the Bayesian estimates are virtually identical.&lt;/p&gt;
&lt;script src=&#39;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Fbayesian_analysis%2Fsemanticpriming_brms_weaklyinformativepriors_exgaussian.R%23L16-L35&amp;style=default&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#39;&gt;&lt;/script&gt;
&lt;p&gt;Now it’s time to consider in earnest:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Is frequentist statistics about &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; values only? Are frequentist estimates diametrically opposed to Bayesian posterior distributions? Are confidence intervals and credible intervals irreconcilable? Will R crash if &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34;&gt;&lt;code&gt;brms&lt;/code&gt;&lt;/a&gt; are simultaneously loaded?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Session info&lt;/h3&gt;
&lt;p&gt;If you encounter any blockers while reproducing the above analyses using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;, my current session info may be useful. For instance, the legend of the plot may not show if the latest versions of the &lt;code&gt;ggplot2&lt;/code&gt; and the &lt;code&gt;tidyverse&lt;/code&gt; packages are used. Instead, &lt;code&gt;ggplot2 3.3.5&lt;/code&gt; and &lt;code&gt;tidyverse 1.3.1&lt;/code&gt; should be installed using &lt;code&gt;install_version(&#39;ggplot2&#39;, &#39;3.3.5&#39;)&lt;/code&gt; and &lt;code&gt;install_version(&#39;tidyverse&#39;, &#39;1.3.1&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 22621)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.utf8 
## [2] LC_CTYPE=English_United States.utf8   
## [3] LC_MONETARY=English_United States.utf8
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] ggtext_0.1.2        Cairo_1.6-0         forcats_1.0.0      
##  [4] stringr_1.5.0       dplyr_1.1.1         purrr_1.0.1        
##  [7] readr_2.1.4         tidyr_1.3.0         tibble_3.2.1       
## [10] ggplot2_3.3.5       tidyverse_1.3.1     knitr_1.42         
## [13] xaringanExtra_0.7.0
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.10      lubridate_1.9.2  digest_0.6.31    utf8_1.2.3      
##  [5] plyr_1.8.8       R6_2.5.1         cellranger_1.1.0 ggridges_0.5.4  
##  [9] backports_1.4.1  reprex_2.0.2     evaluate_0.21    highr_0.10      
## [13] httr_1.4.6       blogdown_1.16    pillar_1.9.0     rlang_1.1.0     
## [17] uuid_1.1-0       readxl_1.4.2     rstudioapi_0.14  jquerylib_0.1.4 
## [21] rmarkdown_2.21   labeling_0.4.2   munsell_0.5.0    gridtext_0.1.5  
## [25] broom_1.0.4      compiler_4.2.3   modelr_0.1.11    xfun_0.38       
## [29] pkgconfig_2.0.3  htmltools_0.5.5  tidyselect_1.2.0 bookdown_0.33.3 
## [33] fansi_1.0.4      crayon_1.5.2     tzdb_0.4.0       dbplyr_2.3.2    
## [37] withr_2.5.0      commonmark_1.9.0 grid_4.2.3       jsonlite_1.8.4  
## [41] gtable_0.3.3     lifecycle_1.0.3  DBI_1.1.3        magrittr_2.0.3  
## [45] scales_1.2.1     cli_3.4.1        stringi_1.7.12   cachem_1.0.7    
## [49] farver_2.1.1     fs_1.6.1         xml2_1.3.3       bslib_0.4.2     
## [53] generics_0.1.3   vctrs_0.6.1      tools_4.2.3      glue_1.6.2      
## [57] markdown_1.5     hms_1.1.3        fastmap_1.1.1    yaml_2.3.7      
## [61] timechange_0.2.0 colorspace_2.1-0 rvest_1.0.3      haven_2.5.2     
## [65] sass_0.4.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cumming, G. (2014). The new statistics: Why and how. &lt;em&gt;Psychological Science, 25&lt;/em&gt;(1), 7–29. &lt;a href=&#34;https://doi.org/10.1177/0956797613504966&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/0956797613504966&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kruschke, J. K., &amp;amp; Liddell, T. M. (2018). The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 25&lt;/em&gt;(1), 178–206.&lt;/p&gt;
&lt;p&gt;Milek, A., Butler, E. A., Tackman, A. M., Kaplan, D. M., Raison, C. L., Sbarra, D. A., Vazire, S., &amp;amp; Mehl, M. R. (2018). “Eavesdropping on happiness” revisited: A pooled, multisample replication of the association between life satisfaction and observed daily conversation quantity and quality. &lt;em&gt;Psychological Science, 29&lt;/em&gt;(9), 1451–1462. &lt;a href=&#34;https://doi.org/10.1177/0956797618774252&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/0956797618774252&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pregla, D., Lissón, P., Vasishth, S., Burchert, F., &amp;amp; Stadie, N. (2021). Variability in sentence comprehension in aphasia in German. &lt;em&gt;Brain and Language, 222&lt;/em&gt;, 105008. &lt;a href=&#34;https://doi.org/10.1016/j.bandl.2021.105008&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.bandl.2021.105008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rodríguez-Ferreiro, J., Aguilera, M., &amp;amp; Davies, R. (2020). Semantic priming and schizotypal personality: Reassessing the link between thought disorder and enhanced spreading of semantic activation. &lt;em&gt;PeerJ, 8&lt;/em&gt;, e9511. &lt;a href=&#34;https://doi.org/10.7717/peerj.9511&#34; class=&#34;uri&#34;&gt;https://doi.org/10.7717/peerj.9511&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rouder, J. N., Haaf, J. M., &amp;amp; Vandekerckhove, J. (2018). Bayesian inference for psychology, part IV: Parameter estimation and Bayes factors. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 25&lt;/em&gt;(1), 102–113. &lt;a href=&#34;https://doi.org/10.3758/s13423-017-1420-7&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13423-017-1420-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Schmalz, X., Biurrun Manresa, J., &amp;amp; Zhang, L. (2021). What is a Bayes factor? &lt;em&gt;Psychological Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1037/met0000421&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000421&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tendeiro, J. N., &amp;amp; Kiers, H. A. L. (2019). A review of issues about null hypothesis Bayesian testing. &lt;em&gt;Psychological Methods, 24&lt;/em&gt;(6), 774–795. &lt;a href=&#34;https://doi.org/10.1037/met0000221&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000221&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tendeiro, J. N., &amp;amp; Kiers, H. A. L. (2022). On the white, the black, and the many shades of gray in between: Our reply to van Ravenzwaaij and Wagenmakers (2021). &lt;em&gt;Psychological Methods, 27&lt;/em&gt;(3), 466–475. &lt;a href=&#34;https://doi.org/10.1037/met0000505&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000505&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;van Ravenzwaaij, D., &amp;amp; Wagenmakers, E.-J. (2022). Advantages masquerading as “issues” in Bayesian hypothesis testing: A commentary on Tendeiro and Kiers (2019). &lt;em&gt;Psychological Methods, 27&lt;/em&gt;(3), 451–465. &lt;a href=&#34;https://doi.org/10.1037/met0000415&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000415&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linguistic and embodied systems in conceptual processing: Variation across individuals and items</title>
      <link>https://pablobernabeu.github.io/talk/linguistic-and-embodied-systems-in-conceptual-processing-variation-across-individuals-and-items/</link>
      <pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/talk/linguistic-and-embodied-systems-in-conceptual-processing-variation-across-individuals-and-items/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/talk/linguistic-and-embodied-systems-in-conceptual-processing-variation-across-individuals-and-items/index_files/fitvids/fitvids.min.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;       &lt;a href=&#34;https://www.youtube.com/watch?v=y2bopgYWYvE&amp;amp;ab_channel=LancasterPsychology&#34;&gt;&lt;strong&gt;Video&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;       &lt;a href=&#34;https://pablobernabeu.github.io/talk/linguistic-and-embodied-systems-in-conceptual-processing-variation-across-individuals-and-items/slides&#34;&gt;&lt;strong&gt;Slides&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class=&#34;shareagain&#34; style=&#34;min-width:300px;margin:1em auto;&#34; data-exeternal=&#34;1&#34;&gt;
&lt;iframe src=&#34;https://pablobernabeu.github.io/talk/linguistic-and-embodied-systems-in-conceptual-processing-variation-across-individuals-and-items/slides&#34; width=&#34;400&#34; height=&#34;300&#34; style=&#34;border:2px solid currentColor;&#34; loading=&#34;lazy&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;script&gt;fitvids(&#39;.shareagain&#39;, {players: &#39;iframe&#39;});&lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Research in conceptual processing has suggested that the comprehension of words draws on complementary cognitive systems. In the milliseconds during which a word is processed, the linguistic system is activated first. Reading the word &lt;em&gt;entity&lt;/em&gt;, for instance, may activate words such as &lt;em&gt;being&lt;/em&gt;, &lt;em&gt;thing&lt;/em&gt; and &lt;em&gt;object&lt;/em&gt; (&lt;a href=&#34;https://smallworldofwords.org/en/project/explore&#34; class=&#34;uri&#34;&gt;https://smallworldofwords.org/en/project/explore&lt;/a&gt;; De Deyne et al., 2019). Thereupon, the embodied system is activated, incorporating sensorimotor, emotional and social dimensions (Borghi et al., 2019). For instance, &lt;em&gt;entity&lt;/em&gt; activates visual, auditory and head-specific meanings (&lt;a href=&#34;https://embodiedcognitionlab.shinyapps.io/sensorimotor_norms&#34; class=&#34;uri&#34;&gt;https://embodiedcognitionlab.shinyapps.io/sensorimotor_norms&lt;/a&gt;; Lynott et al., 2020). Research has also suggested that the linguistic system is more important for relatively abstract words—e.g., &lt;em&gt;attempt&lt;/em&gt;—, whereas the embodied system is more important for more concrete words—e.g., &lt;em&gt;building&lt;/em&gt; (Bolognesi &amp;amp; Steen, 2018). The role of individual differences has also been investigated (Dils &amp;amp; Boroditsky, 2010), although to a lesser extent. An individual’s linguistic experience (e.g., larger vocabulary) facilitates word processing and task-relevant attention (Pexman &amp;amp; Yap, 2018; Yap et al., 2017), while greater sensorimotor experience enables more detailed meaning activation within specific conceptual areas (e.g., space: Vukovic &amp;amp; Williams, 2015). The variation across individuals and items, within both the linguistic and embodied systems, is seldom considered simultaneously. We are undertaking this in two studies. The first study (Bernabeu et al., 2021) will merge existing datasets (Lynott et al., 2020; Pexman et al., 2017; Pexman &amp;amp; Yap, 2018; Wingfield &amp;amp; Connell, 2019). The second study will collect novel data to investigate questions such as the unique roles of vocabulary size, sensorimotor experience and attentional control. To determine the sample size for the latter study, two pilot studies with larger-than-average samples were conducted, using the aforementioned datasets and that of Hutchison et al. (2013). Simulation-based, prospective power curves were performed. These pilots revealed important roles for linguistic and embodied information, vocabulary size, and attentional control, as well as statistical power considerations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P., Lynott, D., &amp;amp; Connell, L. (2021). &lt;em&gt;Preregistration: The interplay between linguistic and embodied systems in conceptual processing&lt;/em&gt;. OSF. &lt;a href=&#34;https://osf.io/ftydw/&#34; class=&#34;uri&#34;&gt;https://osf.io/ftydw/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bolognesi, M., &amp;amp; Steen, G. (2018). Abstract concepts: Structure, processing, and modeling. &lt;em&gt;Topics in Cognitive Science, 10&lt;/em&gt;(3), 490–500. &lt;a href=&#34;https://doi.org/10.1111/tops.12354&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/tops.12354&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Borghi, A., M., Barca, L., Binkofski, F., Castelfranchi, C., Pezzulo, G., &amp;amp; Tummolini, L. (2019). Words as social tools: Language, sociality and inner grounding in abstract concepts. &lt;em&gt;Physics of Life Reviews, 29&lt;/em&gt;, 120–53. &lt;a href=&#34;https://doi.org/10.1016/j.plrev.2018.12.001&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.plrev.2018.12.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;De Deyne, S., Navarro, D. J., Perfors, A., Brysbaert, M., &amp;amp; Storms, G. (2019). The “Small World of Words” English word association norms for over 12,000 cue words. &lt;em&gt;Behavior Research Methods, 51&lt;/em&gt;, 987–1006. &lt;a href=&#34;http://dx.doi.org/10.3758/s13428-018-1115-7&#34; class=&#34;uri&#34;&gt;http://dx.doi.org/10.3758/s13428-018-1115-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dils, A. T., &amp;amp; Boroditsky, L. (2010). Visual motion aftereffect from understanding motion language. &lt;em&gt;Proceedings of the National Academy of Sciences, 107&lt;/em&gt;(37), 16396-16400. &lt;a href=&#34;https://doi.org/10.1073/pnas.1009438107&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1073/pnas.1009438107&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hutchison, K. A., Balota, D. A., Neely, J. H., Cortese, M. J., Cohen-Shikora, E. R., Tse, C.-S., Yap, M. J., Bengson, J. J., Niemeyer, D., &amp;amp; Buchanan, E. (2013). The semantic priming project. &lt;em&gt;Behavior Research Methods, 45&lt;/em&gt;, 1099–1114. &lt;a href=&#34;https://doi.org/10.3758/s13428-012-0304-z&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13428-012-0304-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lynott, D., Connell, L., Brysbaert, M., Brand, J., &amp;amp; Carney, J. (2020). The Lancaster Sensorimotor Norms: Multidimensional measures of perceptual and action strength for 40,000 English words. &lt;em&gt;Behavior Research Methods, 52&lt;/em&gt;, 1-21. &lt;a href=&#34;https://doi.org/10.3758/s13428-019-01316-z&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13428-019-01316-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pexman, P. M., Heard, A., Lloyd, E., &amp;amp; Yap, M. J. (2017). The Calgary semantic decision project: Concrete/abstract decision data for 10,000 English words. &lt;em&gt;Behavior Research Methods, 49&lt;/em&gt;(2), 407–417. &lt;a href=&#34;https://doi.org/10.3758/s13428-016-0720-6&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13428-016-0720-6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pexman, P. M., &amp;amp; Yap, M. J. (2018). Individual differences in semantic processing: Insights from the Calgary semantic decision project. &lt;em&gt;Journal of Experimental Psychology: Learning, Memory, and Cognition, 44&lt;/em&gt;(7), 1091–1112.
&lt;a href=&#34;https://doi.org/10.1037/xlm0000499&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/xlm0000499&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Vukovic, N., &amp;amp; Williams, J. N. (2015). Individual differences in spatial cognition influence mental simulation of language. &lt;em&gt;Cognition, 142&lt;/em&gt;, 110–122.
&lt;a href=&#34;https://doi.org/10.1016/j.cognition.2015.05.017&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.cognition.2015.05.017&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wingfield, C., &amp;amp; Connell, L. (2019). &lt;em&gt;Understanding the role of linguistic distributional knowledge in cognition&lt;/em&gt;. PsyArXiv. &lt;a href=&#34;https://doi.org/10.31234/osf.io/hpm4z&#34; class=&#34;uri&#34;&gt;https://doi.org/10.31234/osf.io/hpm4z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yap, M. J., Hutchison, K. A., &amp;amp; Tan, L. C. (2017). Individual differences in semantic priming performance: Insights from the semantic priming project. In M. N. Jones (Ed.), &lt;em&gt;Frontiers of cognitive psychology. Big data in cognitive science&lt;/em&gt; (p. 203–226). Routledge/Taylor &amp;amp; Francis Group.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Brief Clarifications, Open Questions: Commentary on Liu et al. (2018)</title>
      <link>https://pablobernabeu.github.io/2021/brief-clarifications-open-questions-commentary-on-liu-et-al-2018/</link>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2021/brief-clarifications-open-questions-commentary-on-liu-et-al-2018/</guid>
      <description>&lt;p&gt;Liu et al. (2018) present a study that implements the conceptual modality switch (CMS) paradigm, which has been used to investigate the modality-specific nature of conceptual representations (Pecher et al., 2003). Liu et al.&amp;lsquo;s experiment uses event-related potentials (ERPs; similarly, see Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013). In the design of the switch conditions, the experiment implements a corpus analysis to distinguish between purely-embodied modality switches and switches that are more liable to linguistic bootstrapping (also see Bernabeu et al., 2017; Louwerse &amp;amp; Connell, 2011). The procedure for stimulus selection was novel as well as novel; thus, it could prove useful in future studies, too. In addition, the application of bayesian statistics is an interesting and promising novelty in the present research area.&lt;/p&gt;
&lt;p&gt;In reviewing the literature, Liu (2018) and Liu et al. (2018) contend that previous studies may be strongly biased due to methodological decisions in the analysis of ERPs. These decisions particularly regard the latency&amp;mdash;i.e., time windows&amp;mdash;and the topographic regions of interest&amp;mdash;i.e., subsets of electrodes. Thus, Liu et al. identify a &amp;lsquo;highly inconsistent&amp;rsquo; (p. 6) landscape in the ERP components that have been ascribed to the CMS effect in previous studies. Similarly, Liu (p. 47) writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Several studies have looked for the ERP manifestations of modality switching costs (Bernabeu, Willems, &amp;amp; Louwerse, 2017; Collins, Pecher, Zeelenberg, &amp;amp; Coulson, 2011; Hald, Hocking, Vernon, Marshall, &amp;amp; Garnham, 2013; Hald, Marshall, Janssen, &amp;amp; Garnham, 2011). However, what they found was not a clear picture. Not only was a significant effect found in the time window for the N400 component, but also a so-called early N400-like effect around 300ms (Bernabeu et al., 2017; Hald et al., 2011), the N1-P2 complex around 200ms (Bernabeu et al., 2017; Hald et al., 2013, 2011), as well as the late positivity component (LPC) after 600ms (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;drastic-conclusions&#34;&gt;Drastic conclusions&lt;/h3&gt;
&lt;p&gt;Liu et al. (2018) conclude that a confirmatory research approach is not warranted, and adopt a semi-exploratory approach, creating time windows of 50 ms each, rather than windows linked to known ERP components (Swaab et al., 2012), and using bayesian statistics. Such a drastic conclusion appears to stem from the assumption that, if the CMS were robust enough, it would present in the same guise across studies. Such an assumption, however, may merit further examination, considering the multiplicity of known and unknown variables that may differ across experiments (Barsalou, 2019). This variability influences the &lt;em&gt;replication praxis&lt;/em&gt;, as it were. Indeed, Liu et al. themselves allude to one such variable (p. 7).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;These previous studies did not only examine the effect of modality switching but also other linguistic factors such as negated sentences, which could easily distort observed waveforms (Luck, 2005).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu et al.&amp;lsquo;s (2018) conclusions about the &amp;lsquo;inconsistent&amp;rsquo; results do not seem to duly weigh the differences across studies. None of the existing studies is a direct replication of another. The example quoted from Liu et al. above, regarding the presence of negated sentences, is one of the less important differences because one of the corresponding studies implemented the negation as a controlled, experimental condition, contrasting with affirmative sentences (Hald et al., 2013). Yet, other differences exist in virtually every aspect, from the types of modality switch used to the time-locking of ERPs. For instance, the studies vary in the implementation of the modality switches. Whereas Hald et al. (2011) distinguish between a switch and a non-switch condition, Bernabeu et al. (2017) analysed each type of switch separately&amp;mdash;i.e., auditory-to-visual, haptic-to-visual, visual-to-visual. Another difference across the studies is the onset point for ERPs. For instance, Bernabeu et al. time-locked ERPs to the point at which the modality switch is actually elicited in the CMS paradigm&amp;mdash;namely, the first word in target trials. In contrast, the other studies time-locked ERPs to the second word. In addition, these studies vary in their timelines&amp;mdash;i.e., presentation of words and inter-stimulus intervals&amp;mdash;, as well as in the words that were used as stimuli, in the preprocessing of ERPs, in the statistical analysis, and even in the language of testing in one case (all studies using English except Bernabeu et al., 2017, which used Dutch). Moreover, the studies differ in the sample size, ranging from ten finally-analysed participants (Hald et al, 2011) to 46 finally-analysed participants (Bernabeu et al., 2017). Last, the studies differ in the number of items per modality switch condition, ranging from 17 (in one of Liu et al.&amp;lsquo;s, 2018 conditions) to 40 per condition (Hald et al., 2011, 2013). Undoubtedly, seeing larger sample sizes and more stimulus items used in ERP studies is something to promote and celebrate. Last, it may be noted that Liu et al.&amp;lsquo;s stance on the inconsistency of previous results starkly contrasts with their use of a single study&amp;mdash;Hald et al. (2011)&amp;mdash;, with &lt;em&gt;N&lt;/em&gt; = 10, as the motivation for their sample size (Albers &amp;amp; Lakens, 2018).&lt;/p&gt;
&lt;h3 id=&#34;clarifications&#34;&gt;Clarifications&lt;/h3&gt;
&lt;p&gt;Liu et al. (2018) apply bayesian statistics reportedly to help reduce the bias that may exist in the present literature. However, the reviews by Liu (2018) and Liu et al. appear to gloss over some important aspects regarding previous studies. For instance, Liu writes (p. 53):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The inflation of the probability of Type I error leads to an over-confidence in the interpretation of the results. For example, in the studies on modality switching costs, different time windows were chosen to test the early effect of modality switching costs. While Bernabeu et al. (2017), Hald et al. (2011) and Hald et al. (2013) examined the segment of ERP waveform between 190ms and 300ms or 160ms and 215ms based on visual inspection and found significant effects, Collins et al. (2011) chose a prescribed time window between 100ms and 200ms before the analysis and did not find the effect.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu also refers to the issue of multiple tests related to the multiple time windows and electrodes we find in ERP studies (p. 59):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It is typical for ERP studies to conduct multiple comparisons (e.g., running the same ANOVA repeatedly on different subsets of data like different time windows and different groups of electrodes). This would massively increase Type I error if no post hoc correction is conducted. However, if Bonferroni or other correction is conducted, it will render the study over-conservative, thus increasing the chance of Type II error. In the present thesis, 90 electrodes will be analysed individually, with 20 time slices in each trial. That results in 1800 NHSTs for each critical variable. A correction of multiple comparison will require a critical level of 2.78 x 10ˆ-5 for each test for a family-wise critical level of .05 (and an uncorrected test will almost definitely lead to false positive results). This stringent criterion could conceivably render it meaningless any p-values we can obtain from a statistical package.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Arguably, the scenario presented by Liu (2018), in which a researcher could conduct a purely data-driven analysis of ERP data, is extreme. The field of psycholinguistics, in general, does not have a tradition of purely data-driven analysis. Instead, it blends a humanistic background with a scientific methodology. As a result, the hypotheses and methods tend to be largely driven by the available literature. For instance, Bernabeu et al. (2017, quoted below from p. 1632) based their time windows and regions of interest on the most relevant of the preceding studies (also see Bernabeu, 2017).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Electrodes were divided into an anterior and a posterior area (also done in Hald et al., 2011). Albeit a superficial division, we found it sufficient for the research question. Time windows were selected as in Hald et al., except for the last window, which was extended up to 750 ms post word onset, instead of 700 ms, because the characteristic component of that latency tends to extend until then, as we confirmed by visual inspection of these results.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The literature-based approach follows the advice from Luck and Gaspelin (2017, p. 149), who wrote: &amp;lsquo;a researcher who wants to avoid significant but bogus effects would be advised to focus on testing a priori predictions without using the observed data to guide the selection of time windows or electrode sites&amp;rsquo; (also see Armstrong, 2014; for a more conservative stance, see Luck &amp;amp; Gaspelin, 2017). In addition, notice that the extension of the last window by 50 ms was informed by Swaab et al. (2012), who report results by which the P600 component (the main component occurring after the N400 in word reading) extended up to 800 ms.&lt;/p&gt;
&lt;p&gt;Next, Liu et al. (2018, pp. 6&amp;ndash;7) write:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;However, the findings of these components have been highly inconsistent. The N400 effect alone was found in the posterior region in some cases (Bernabeu et al., 2017; Hald et al., 2013), while in anterior region in others (Collins et al., 2011, Hald et al. (2011)).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yet, Bernabeu et al. (2017, p. 1632) had written:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In certain parts over the time course, the effect appeared in both anterior and posterior areas&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu et al. (2018) continue (p. 7):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In some cases, it was found in the typical window around 400ms (Collins et al., 2011), while in others an earlier window from 270ms to 370ms (Bernabeu et al., 2017; Hald et al., 2011).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, Bernabeu et al. (2017, p. 1632) had written:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The ERP results revealed a CMS effect from Time Window 1 on, larger after 350 ms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Regarding the time-locking of ERPs, Liu (2018, p. 43) writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Because the properties were usually salient for the concepts, the switching costs might have already been incurred when participants were processing the concept word. Bernabeu et al. (2017), in their recent replication of previous ERP studies, reversed the order of concept and property and did not find an immediate effect from the property onset. In future studies, it is recommended to adopt the reverse order, control the concept words so that they do not automatically activate the properties before the words are shown, or analyse epochs after both the concept and property words.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above excerpt seems to reveal a misunderstanding of Bernabeu et al.&amp;lsquo;s (2017) method and results (we may assume that, by &amp;lsquo;immediate&amp;rsquo;, Liu (2018) is referring to the 200 ms point or afterwards, since that is about as immediate as it gets; see Amsel et al., 2014; Swaab et al., 2012; Van Dam et al., 2014). Bernabeu et al.&amp;lsquo;s abstract mentioned (p. 1629):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Event-Related Potentials (ERPs) were time-locked to the onset of the first word (property) in the target trials so as to measure the effect online and to avoid a within-trial confound. A switch effect was found, characterized by more negative ERP amplitudes for modality switches than no-switches. It proved significant in four typical time windows from 160 to 750 milliseconds post word onset, with greater strength in posterior brain regions, and after 350 milliseconds.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Indeed, time-locking ERPs to the beginning of the trial was one of the principal features of Bernabeu et al.&amp;lsquo;s (2017) experiment. Complementing that, the property words were placed in the first trial because they are more perceptually loaded than concepts (Lynott &amp;amp; Connell, 2013), thus better suiting the main basis of the CMS paradigm. We know that the semantic processing of a word often commences within the first 200 ms (Amsel et al., 2014; Van Dam et al., 2014). Considering the importance of the time course in the grounding of conceptual representations (Hauk, 2016), it seems important to measure the CMS from the moment that it is elicited&amp;mdash;namely, in all experiments, from the first word of the target trial (Bernabeu, 2017; Bernabeu et al., 2017), rather than letting several hundreds of milliseconds elapse. Nonetheless, from a methodological perspective, it would be interesting to compare the two approaches in a dedicated study. This would precisely reveal the speed at which modality-specific meaning becomes activated during conceptual processing.&lt;/p&gt;
&lt;h3 id=&#34;outstanding-issues-random-effects-and-correction-for-multiple-tests&#34;&gt;Outstanding issues: Random effects and correction for multiple tests&lt;/h3&gt;
&lt;p&gt;A methodological issue affecting the statistical analysis of all the studies hereby considered (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013; Liu et al., 2018) is the absence of some applicable random effects. Some of the studies did not apply any random effects (Collins et al., 2011; Hald et al., 2011, 2013). Even in psycholinguistics, use of linear mixed-effects models is still increasing (Meteyard &amp;amp; Davies, 2020; Yarkoni, 2020). Yet, in those studies that did apply random effects, the corresponding  structure was not as exhaustive as it should have been, as they lacked random slopes. In Bernabeu et al. (2017), a model selection approach was applied (Matuschek et al., 2017), whereby each random effect was tested and only kept in the model if it significantly improved the fit. In Liu et al. (2018), random slopes were deemed unfeasible due to computational constraints (for background, see Brauer &amp;amp; Curtin, 2017). Applying a complete random effects structure is important for a robust statistical analysis (Barr et al., 2013; Yarkoni, 2020).&lt;/p&gt;
&lt;p&gt;Another issue is that of multiple tests. Where a small number of levels is used (e.g., time windows, topographic regions of interest) and these are informed by the literature, the advice has often been ambiguous as to whether a correction should be applied (e.g., Armstrong, 2014; for a more conservative stance, see Luck &amp;amp; Gaspelin, 2017). Indeed, no correction was applied in any of the four studies that used frequentist statistics (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013).&lt;/p&gt;
&lt;h4 id=&#34;reanalysis-of-bernabeu-et-al-2017&#34;&gt;Reanalysis of Bernabeu et al. (2017)&lt;/h4&gt;
&lt;p&gt;The results of Bernabeu et al. (2017) were reanalysed after publication using a more complete random effects structure that incorporated by-participant random slopes for the modality-switch factor&amp;mdash;i.e., &lt;code&gt;(condition | participant)&lt;/code&gt;. The results were also corrected for multiple tests using the Holm-Bonferroni correction (Holm, 1979). For this purpose, the lowest p-value in the four time windows was multiplied by 4, the next p-value by 3, the next by 2, and the highest p-value was left as it unmodified. In this stepwise correction, if a nonsignificant p-value was reached, all the subsequent p-values became nonsignificant (see &lt;a href=&#34;https://osf.io/unvfs/&#34;&gt;analysis script&lt;/a&gt;). The &lt;a href=&#34;https://osf.io/qhe5s/&#34;&gt;results&lt;/a&gt; differed from the original, slopes-free models (available &lt;a href=&#34;https://osf.io/sx3nw/&#34;&gt;script&lt;/a&gt; and &lt;a href=&#34;https://osf.io/4v38d/&#34;&gt;results&lt;/a&gt;) in that the main effect of the modality switch factor became nonsignificant in the second time window (160&amp;ndash;216 ms) and in the fourth one (500&amp;ndash;750 ms), while remaining significant in the third time window (350&amp;ndash;550 ms). Incidentally, note that main effects may not be directly interpretable as the same same variables are present interactions (Kam &amp;amp; Franzese, 2007). Yet, the interactions of modality switch with participant group (quick/slow) and scalp location (anterior/posterior) retained the same significance.&lt;/p&gt;
&lt;h3 id=&#34;open-questions&#34;&gt;Open questions&lt;/h3&gt;
&lt;p&gt;Liu (2018) and Liu et al. (2018) raise interesting and important questions. Firstly, future research may be conducted to investigate what determines the variability of ERP results&amp;mdash;in terms of ERP components, time windows and topographic regions of interest. This research could include a comparison with other measurements, such as response times, to test whether ERPs are less reliable&amp;mdash;i.e., more variable across studies&amp;mdash;than response times. Similarly, future research may investigate whether the ERP literature is more biased than literature employing other measures, such as response times. In addition, future research could investigate whether moving to exploratory, bayesian research designs is a necessary or sufficient condition to reduce bias in research and improve the precision of experimental measurements. Current alternatives to such an approach include direct (or conceptual) replications designed to achieve a higher power than previous studies (e.g., Chen et al., 2019). Arguably, policies determining funding decisions would need to change if we are to fully acknowledge the importance of &lt;em&gt;direct&lt;/em&gt; replication (Howe &amp;amp; Perfors, 2018; Kunert, 2016; Simons, 2014; Zwaan et al., 2018). Last, future research may investigate whether &amp;lsquo;clear picture&amp;rsquo; results are realistic, desirable or necessary, and whether unclear-picture results should be eschewed; or whether, on the contrary, clear-picture results may largely be the product of publication bias&amp;mdash;that is, the pressure to hide or misreport those aspects of a study that could challenge its acceptance by peer-reviewers or any other academics.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;div style = &#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Albers, C., &amp;amp; Lakens, D. (2018). When power analyses based on pilot data are biased: Inaccurate effect size estimators and follow-up bias. &lt;em&gt;Journal of Experimental Social Psychology, 74&lt;/em&gt;, 187–195. &lt;a href=&#34;https://doi.org/10.1016/j.jesp.2017.09.004&#34;&gt;https://doi.org/10.1016/j.jesp.2017.09.004&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Amsel, B. D., Urbach, T. P., &amp;amp; Kutas, M. (2014). Empirically grounding grounded cognition: the case of color. &lt;em&gt;Neuroimage, 99&lt;/em&gt;, 149-157. &lt;a href=&#34;https://doi.org/10.1016/j.neuroimage.2014.05.025&#34;&gt;https://doi.org/10.1016/j.neuroimage.2014.05.025&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Armstrong, R. A. (2014). When to use the Bonferroni correction. &lt;em&gt;Ophthalmic and Physiological Optics, 34&lt;/em&gt;(5), 502-508. &lt;a href=&#34;https://doi.org/10.1111/opo.12131&#34;&gt;https://doi.org/10.1111/opo.12131&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barr, D. J., Levy, R., Scheepers, C., &amp;amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. &lt;em&gt;Journal of Memory and Language, 68&lt;/em&gt;, 255–278. &lt;a href=&#34;http://dx.doi.org/10.1016/j.jml.2012.11.001&#34;&gt;http://dx.doi.org/10.1016/j.jml.2012.11.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barsalou, L. W. (2019). Establishing generalizable mechanisms. &lt;em&gt;Psychological Inquiry, 30&lt;/em&gt;(4), 220-230. &lt;a href=&#34;https://doi.org/10.1080/1047840X.2019.1693857&#34;&gt;https://doi.org/10.1080/1047840X.2019.1693857&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P. (2017). &lt;em&gt;Modality switches occur early and extend late in conceptual processing: evidence from ERPs&lt;/em&gt; [Master&#39;s thesis]. School of Humanities, Tilburg University. &lt;a href=&#34;https://psyarxiv.com/5gjvk&#34;&gt;https://psyarxiv.com/5gjvk&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P., Willems, R. M., &amp;amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, &amp;amp; E. J. Davelaar (Eds.), &lt;em&gt;Proceedings of the 39th Annual Conference of the Cognitive Science Society&lt;/em&gt; (pp. 1629-1634). Austin, TX: Cognitive Science Society. &lt;a href=&#34;https://doi.org/10.31234/osf.io/a5pcz&#34;&gt;https://doi.org/10.31234/osf.io/a5pcz&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chen, S., Szabelska, A., Chartier, C. R., Kekecs, Z., Lynott, D., Bernabeu, P., … Schmidt, K. (2018). &lt;em&gt;Investigating object orientation effects across 14 languages&lt;/em&gt;. PsyArXiv. &lt;a href=&#34;https://doi.org/10.31234/osf.io/t2pjv/&#34;&gt;https://doi.org/10.31234/osf.io/t2pjv/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Collins, J., Pecher, D., Zeelenberg, R., &amp;amp; Coulson, S. (2011). Modality switching in a property verification task: an ERP study of what happens when candles flicker after high heels click. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2011.00010&#34;&gt;https://doi.org/10.3389/fpsyg.2011.00010&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hald, L. A., Hocking, I., Vernon, D., Marshall, J.-A., &amp;amp; Garnham, A. (2013). Exploring modality switching effects in negated sentences: further evidence for grounded representations. &lt;em&gt;Frontiers in Psychology, 4&lt;/em&gt;, 93. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2013.00093&#34;&gt;https://doi.org/10.3389/fpsyg.2013.00093&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hald, L. A., Marshall, J.-A., Janssen, D. P., &amp;amp; Garnham, A. (2011). Switching modalities in a sentence verification task: ERP evidence for embodied language processing. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2011.00045&#34;&gt;https://doi.org/10.3389/fpsyg.2011.00045&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hauk, O. (2016). Only time will tell–why temporal information is essential for our neuroscientific understanding of semantics. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;(4), 1072-1079. &lt;a href=&#34;https://doi.org/10.3758/s13423-015-0873-9&#34;&gt;https://doi.org/10.3758/s13423-015-0873-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Holm, S. (1979). A simple sequentially rejective multiple test procedure. &lt;em&gt;Scandinavian Journal of Statistics, 6&lt;/em&gt;, 65-70. &lt;a href=&#34;http://www.jstor.org/stable/4615733&#34;&gt;http://www.jstor.org/stable/4615733&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Howe, P. D., &amp;amp; Perfors, A. (2018). An argument for how (and why) to incentivise replication. &lt;em&gt;Behavioral and Brain Sciences, 41&lt;/em&gt;, e135-e135. &lt;a href=&#34;http://dx.doi.org/10.1017/S0140525X18000705&#34;&gt;http://dx.doi.org/10.1017/S0140525X18000705&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kam, C. D., &amp;amp; Franzese, R. J. (2007). &lt;em&gt;Modeling and interpreting interactive hypotheses in regression analysis&lt;/em&gt;. Ann Arbor, MI: University of Michigan Press.&lt;/p&gt;
&lt;p&gt;Kunert, R. (2016). Internal conceptual replications do not increase independent replication success. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;(5), 1631-1638. &lt;a href=&#34;https://doi.org/10.3758/s13423-016-1030-9&#34;&gt;https://doi.org/10.3758/s13423-016-1030-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, P. (2018). &lt;em&gt;Embodied-linguistic conceptual representations during metaphor processing&lt;/em&gt;. Doctoral thesis, Lancaster University, UK. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/489&#34;&gt;https://doi.org/10.17635/lancaster/thesis/489&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, P., Lynott, D., &amp;amp; Connell, L. (2018). Continuous neural activations of simulation-linguistic representations in modality switching costs. In P. Liu, &lt;em&gt;Embodied-linguistic conceptual representations during metaphor processing&lt;/em&gt;. Doctoral thesis, Lancaster University, UK. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/489&#34;&gt;https://doi.org/10.17635/lancaster/thesis/489&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Luck, S. J. (2005). Ten simple rules for designing ERP experiments. In T. C. Handy (Ed.), &lt;em&gt;Event-related potentials: A methods handbook&lt;/em&gt;.
MIT Press.&lt;/p&gt;
&lt;p&gt;Luck, S. J., &amp;amp; Gaspelin, N. (2017). How to get statistically significant effects in any ERP experiment (and why you shouldn&#39;t). &lt;em&gt;Psychophysiology, 54&lt;/em&gt;(1), 146-157. &lt;a href=&#34;https://doi.org/10.1111/psyp.12639&#34;&gt;https://doi.org/10.1111/psyp.12639&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., &amp;amp; Bates, D. (2017). Balancing type 1 error and power in linear mixed models. &lt;em&gt;Journal of Memory and Language, 94&lt;/em&gt;, 305–315. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2017.01.001&#34;&gt;https://doi.org/10.1016/j.jml.2017.01.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meteyard, L., &amp;amp; Davies, R. A. (2020). Best practice guidance for linear mixed-effects models in psychological science. &lt;em&gt;Journal of Memory and Language, 112&lt;/em&gt;, 104092. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2020.104092&#34;&gt;https://doi.org/10.1016/j.jml.2020.104092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pecher, D., Zeelenberg, R., &amp;amp; Barsalou, L. W. (2003). Verifying different-modality properties for concepts produces switching costs. &lt;em&gt;Psychological Science, 14&lt;/em&gt;, 2, 119-24. &lt;a href=&#34;https://doi.org/10.1111/1467-9280.t01-1-01429&#34;&gt;https://doi.org/10.1111/1467-9280.t01-1-01429&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Simons, D. J. (2014). The value of direct replication. &lt;em&gt;Perspectives on Psychological Science, 9&lt;/em&gt;(1), 76–80. &lt;a href=&#34;https://doi.org/10.1177/1745691613514755&#34;&gt;https://doi.org/10.1177/1745691613514755&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Swaab, T. Y., Ledoux, K., Camblin, C. C., &amp;amp; Boudewyn, M. A. (2012). Language-related ERP components. In S. J. Luck &amp;amp; E. S. Kappenman (Eds.), &lt;em&gt;Oxford handbook of event-related potential components&lt;/em&gt; (pp. 397–440). Oxford University Press. &lt;a href=&#34;https://doi.org/10.1093/oxfordhb/9780195374148.013.0197&#34;&gt;https://doi.org/10.1093/oxfordhb/9780195374148.013.0197&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Van Dam, W. O., Brazil, I. A., Bekkering, H., &amp;amp; Rueschemeyer, S.-A. (2014). Flexibility in embodied language processing: context effects in lexical access. &lt;em&gt;Topics in Cognitive Science, 6&lt;/em&gt;(3), 407–424. &lt;a href=&#34;https://doi.org/10.1111/tops.12100&#34;&gt;https://doi.org/10.1111/tops.12100&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yarkoni, T. (2020). The generalizability crisis. &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt;, 1-37. &lt;a href=&#34;https://doi.org/10.1017/S0140525X20001685&#34;&gt;https://doi.org/10.1017/S0140525X20001685&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zwaan, R., Etz, A., Lucas, R., &amp;amp; Donnellan, M. (2018). Making replication mainstream. &lt;em&gt;Behavioral and Brain Sciences, 41&lt;/em&gt;, E120. &lt;a href=&#34;https://doi.org/10.1017/S0140525X17001972&#34;&gt;https://doi.org/10.1017/S0140525X17001972&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Preregistration: The interplay between linguistic and embodied systems in conceptual processing</title>
      <link>https://pablobernabeu.github.io/publication/the-interplay-between-linguistic-and-embodied-systems-in-conceptual-processing/</link>
      <pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/publication/the-interplay-between-linguistic-and-embodied-systems-in-conceptual-processing/</guid>
      <description>


&lt;div id=&#34;reference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P., Lynott, D., &amp;amp; Connell, L. (2021). &lt;em&gt;Preregistration: The interplay between linguistic and embodied systems in conceptual processing&lt;/em&gt;. OSF. &lt;a href=&#34;https://osf.io/ftydw&#34; class=&#34;uri&#34;&gt;https://osf.io/ftydw&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Mixed-effects models in R, and a new tool for data simulation</title>
      <link>https://pablobernabeu.github.io/talk/2020-11-26-mixed-effects-models-in-r-and-a-new-tool-for-data-simulation/</link>
      <pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/talk/2020-11-26-mixed-effects-models-in-r-and-a-new-tool-for-data-simulation/</guid>
      <description>


&lt;div id=&#34;slides&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Slides   &lt;a href=&#34;https://hackmd.io/@pablobernabeu/SkRyLbaqw&#34;&gt;&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;500&#34; src=&#34;https://hackmd.io/@pablobernabeu/SkRyLbaqw&#34; frameborder=&#34;0&#34; style=&#34;padding-top:5px&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Linear mixed-effects models (LMEMs) are used to account for variation within factors with multiple observations, such as participants, trials, items, channels, etc (for an earlier approach, see Clark, 1973). This variation is modelled in terms of random intercepts (e.g., overall variation per participant) as well as random slopes for the fixed effects (e.g., treatment effect per participant). These measures help reduce false positives and false negatives (Barr et al., 2013), and the resulting models tend to be robust to violations of assumptions (Schielzeth et al., 2020). The use of LMEMs has grown over the past decade, under various implementation forms (Meteyard &amp;amp; Davies, 2020). In this talk, I will look over the rationale for LMEMs, and demonstrate how to fit them in R (Brauer &amp;amp; Curtin, 2018; Luke, 2017). Challenges will also be covered. For instance, when using the widely-accepted ‘maximal’ approach, based on fitting all possible random effects for each fixed effect, models sometimes fail to find a solution, or ‘convergence’. Advice for the problem of nonconvergence will be demonstrated, based on the progressive lightening of the random effects structure (Singman &amp;amp; Kellen, 2017; for an alternative approach, especially with small samples, see Matuschek et al., 2017). At the end, on a different note, I will present a web application that facilitates data simulation for research and teaching (Bernabeu &amp;amp; Lynott, 2020).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Barr, D. J., Levy, R., Scheepers, C., &amp;amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. &lt;em&gt;Journal of Memory and Language, 68&lt;/em&gt;, 255–278. &lt;a href=&#34;http://dx.doi.org/10.1016/j.jml.2012.11.001&#34; class=&#34;uri&#34;&gt;http://dx.doi.org/10.1016/j.jml.2012.11.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P., &amp;amp; Lynott, D. (2020). &lt;em&gt;Web application for the simulation of experimental data&lt;/em&gt; (Version 1.2). &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/&#34; class=&#34;uri&#34;&gt;https://github.com/pablobernabeu/Experimental-data-simulation/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34; class=&#34;uri&#34;&gt;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Clark, H. H. (1973). The language-as-fixed-effect fallacy: A critique of language statistics in psychological research. &lt;em&gt;Journal of Verbal Learning and Verbal Behavior, 12&lt;/em&gt;(4), 335-359. &lt;a href=&#34;https://doi.org/10.1016/S0022-5371(73)80014-3&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/S0022-5371(73)80014-3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Luke, S. G. (2017). Evaluating significance in linear mixed-effects models in R. &lt;em&gt;Behavior Research Methods, 49&lt;/em&gt;(4), 1494–1502. &lt;a href=&#34;https://doi.org/10.3758/s13428-016-0809-y&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13428-016-0809-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., &amp;amp; Bates, D. (2017). Balancing type 1 error and power in linear mixed models. &lt;em&gt;Journal of Memory and Language, 94&lt;/em&gt;, 305–315. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2017.01.001&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.jml.2017.01.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meteyard, L., &amp;amp; Davies, R. A. (2020). Best practice guidance for linear mixed-effects models in psychological science. &lt;em&gt;Journal of Memory and Language, 112&lt;/em&gt;, 104092. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2020.104092&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.jml.2020.104092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Schielzeth, H., Dingemanse, N. J., Nakagawa, S., Westneat, D. F., Allegue, H, Teplitsky, C., Reale, D., Dochtermann, N. A., Garamszegi, L. Z., &amp;amp; Araya-Ajoy, Y. G. (2020). Robustness of linear mixed-effects models to violations of distributional assumptions. &lt;em&gt;Methods in Ecology and Evolution, 00&lt;/em&gt;, 1– 12. &lt;a href=&#34;https://doi.org/10.1111/2041-210X.13434&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/2041-210X.13434&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Singmann, H., &amp;amp; Kellen, D. (2019). An Introduction to Mixed Models for Experimental Psychology. In D. H. Spieler &amp;amp; E. Schumacher (Eds.), &lt;em&gt;New Methods in Cognitive Psychology&lt;/em&gt; (pp. 4–31). Hove, UK: Psychology Press. &lt;a href=&#34;http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf&#34; class=&#34;uri&#34;&gt;http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reproducibilidad en torno a una aplicación web</title>
      <link>https://pablobernabeu.github.io/talk/2020-10-08-reproducibilidad-en-torno-a-una-aplicacion-web/</link>
      <pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/talk/2020-10-08-reproducibilidad-en-torno-a-una-aplicacion-web/</guid>
      <description>


&lt;div id=&#34;resumen&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Resumen&lt;/h3&gt;
&lt;p&gt;Las aplicaciones web nos ayudan a facilitar el uso de nuestro trabajo, ya que no requieren programación para utilizarlas (&lt;a href=&#34;https://shiny.rstudio.com/gallery/&#34;&gt;ver ejemplos&lt;/a&gt;). Crear estas aplicaciones en R, mediante paquetes como “shiny” o “flexdashboard”, ofrece múltiples ventajas. Entre ellas destaca la reproducibilidad, tal como veremos en torno a una &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation&#34;&gt;aplicación para la simulación de datos&lt;/a&gt;. Por un lado, los usuarios pueden exportar un registro de su actividad. Por otro lado, el código utilizado para crear estas aplicaciones se puede compartir, investigar y editar con la facilidad que ofrece un lenguaje de código abierto como R. Esto facilita el uso gratuito, el desarrollo colaborativo y una documentación accesible sobre cualquiera de los paquetes utilizados. Por último, la reproducibilidad se puede maximizar si se facilita a los usuarios que lo deseen la exportación de un código de R ajustado a sus requerimientos (más allá del código de la aplicación en general), lo cual añadiría a la aplicación las ventajas de un paquete de R. Esta última opción (no disponible actualmente en la aplicación de simulación, ni en la mayoría de las aplicaciones) se puede habilitar adaptando el código de la aplicación a funciones básicas de R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vídeo-y-filminas&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Vídeo y filminas&lt;/h3&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; style=&#34;margin-top:20px; margin-bottom:5px;&#34; src=&#34;https://www.youtube-nocookie.com/embed/1njLOAWqLPM&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;iframe src=&#34;https://www.slideshare.net/slideshow/embed_code/key/AlE6wv2USddNP6&#34; width=&#34;560&#34; height=&#34;355&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;margin-top:20px; border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;div style=&#34;font-size:85%; margin-bottom:5px&#34;&gt;
&lt;a href=&#34;https://www.slideshare.net/PabloBernabeu/resumido-reprohack-es-prestt&#34; title=&#34;Reproducibilidad entorno a una aplicación web&#34; target=&#34;_blank&#34;&gt;Slideshare&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Web application for the simulation of experimental data</title>
      <link>https://pablobernabeu.github.io/applications-and-dashboards/experimental-data-simulation/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/applications-and-dashboards/experimental-data-simulation/</guid>
      <description>


&lt;div style=&#34;font-size: 25px; color: #614064; padding-top: 15px; padding-bottom: 10px;&#34;&gt;
&lt;i class=&#34;fas fa-chalkboard-teacher fa-xs&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt; &lt;i class=&#34;fas fa-university fa-xs&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;  Purposes
&lt;/div&gt;
&lt;p&gt;This open-source, R-based web application is suitable for educational and research purposes in experimental and quantitative sciences. It allows the &lt;strong&gt;creation of varied data sets with specified structures, such as between-group and within-participant variables, that can be categorical or continuous.&lt;/strong&gt; These parameters can be set throughout the various tabs (sections) from the top menu. In the last tab, the data set can be downloaded. The benefits of this application include time-saving and flexibility in the control of parameters.&lt;/p&gt;
&lt;div id=&#34;guidelines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guidelines&lt;/h3&gt;
&lt;p&gt;General guidelines include the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;In the names of variables&lt;/strong&gt;, it’s recommended only to use alphanumeric characters and underscore signs. The latter can be used to separate characters or words (e.g., &lt;em&gt;variable_name&lt;/em&gt;). Different names should be used for each variable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;In the levels of categorical variables&lt;/strong&gt;, alphanumeric, special characters and spaces are allowed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;In numeric fields&lt;/strong&gt; (e.g., ‘Mean’, ‘Standard deviation’, ‘Relative probability [0, 1]’), only numbers and decimal points are allowed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;As the data set increases&lt;/strong&gt;, so does the processing time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More specific guidelines are available in each section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;globe_with_meridians-the-web-application-can-be-launched-here.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;🌐  The web application can be &lt;a href=&#34;https://pablo-bernabeu.shinyapps.io/experimental-data-simulation/&#34;&gt;launched here&lt;/a&gt;.&lt;/h3&gt;
&lt;div style=&#34;padding-top:8px; padding-bottom:2px; margin-bottom:-20px; color:#665F5F;&#34;&gt;
Screenshot of the &lt;em&gt;Dependent&lt;/em&gt; tab (&lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/raw/master/Screenshot.png&#34;&gt;view larger&lt;/a&gt;)
&lt;/div&gt;
&lt;p&gt;&lt;img style=&#34;max-width: 800px; display: block; margin-left: auto; margin-right: auto; padding-bottom: 15px;&#34; src=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/raw/master/Screenshot.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;p&gt;Bernabeu, P., &amp;amp; Lynott, D. (2020). &lt;em&gt;Web application for the simulation of experimental data&lt;/em&gt; (Version 1.4). &lt;a href=&#34;https://github.com/pablobernabeu/Experiment-simulation-app/&#34;&gt;https://github.com/pablobernabeu/Experiment-simulation-app/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Code&lt;/h3&gt;
&lt;p&gt;This web application was developed in &lt;a href=&#34;https://www.r-project.org/about.html&#34;&gt;R&lt;/a&gt; (R Core Team, 2020). The code is &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/blob/master/index.Rmd&#34;&gt;available on Github&lt;/a&gt;, where contributions may be made. The initial code for this application was influenced by Section 5.7 (&lt;a href=&#34;https://crumplab.github.io/programmingforpsych/simulating-and-analyzing-data-in-r.html#simulating-data-for-multi-factor-designs&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Simulating data for multi-factor designs&lt;/em&gt;&lt;/a&gt;) in Crump (2017). The R packages used include ‘dplyr’ (Wickham, François, Henry, &amp;amp; Müller, 2018), ‘DT’ (Xie, 2020), ‘flexdashboard’ (Iannone, Allaire, &amp;amp; Borges, 2020), ‘shiny’ (Chang, Cheng, Allaire, Xie, &amp;amp; McPherson, 2020) and ‘stringr’ (Wickham, 2019).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;options-for-development-and-local-use-of-the-app&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Options for development and local use of the app&lt;/h3&gt;
&lt;div id=&#34;option-a-using-local-rrstudio-or-rstudio-cloud-project-or-binder-rstudio-environment&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Option A) Using local R/RStudio or &lt;a href=&#34;https://rstudio.cloud/project/1739958&#34;&gt;RStudio Cloud project&lt;/a&gt; or &lt;a href=&#34;https://mybinder.org/v2/gh/pablobernabeu/Experimental-data-simulation/master?urlpath=rstudio&#34;&gt;Binder RStudio environment&lt;/a&gt;&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;[Step only necessary in R/RStudio] Install the packages in the versions used in the &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/releases&#34;&gt;latest release of this application&lt;/a&gt;, by running:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;#39;devtools&amp;#39;)
library(devtools)
install_version(&amp;#39;dplyr&amp;#39;, &amp;#39;1.0.2&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;DT&amp;#39;, &amp;#39;0.15&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;flexdashboard&amp;#39;, &amp;#39;0.5.2&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;htmltools&amp;#39;, &amp;#39;0.5.0&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;knitr&amp;#39;, &amp;#39;1.30&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;ngram&amp;#39;, &amp;#39;3.0.4&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;purrr&amp;#39;, &amp;#39;0.3.4&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;shiny&amp;#39;, &amp;#39;1.5.0&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;stringr&amp;#39;, &amp;#39;1.4.0&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;tidyr&amp;#39;, &amp;#39;1.1.2&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open the &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/blob/master/index.Rmd&#34;&gt;index.Rmd&lt;/a&gt; script.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the application by clicking on &lt;kbd&gt;▶️ Run document&lt;/kbd&gt; at the top left, or by running &lt;code&gt;rmarkdown::run(&#39;index.Rmd&#39;)&lt;/code&gt; in the console.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Click on &lt;kbd&gt;Open in Browser&lt;/kbd&gt; at the top left.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;option-b-using-dockerfile-see-instructions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Option B) Using Dockerfile (&lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation#option-b-using-dockerfile-vsochs-pull-request&#34;&gt;see instructions&lt;/a&gt;)&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgements&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Acknowledgements&lt;/h3&gt;
&lt;p&gt;Thank you to RStudio for the free hosting server used by this application, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;shinyapps.io&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div style=&#34;text-indent:-1.5em; margin-left:1.5em;&#34;&gt;
&lt;p&gt;Chang, W., Cheng, J., Allaire, J., Xie, Y., &amp;amp; McPherson, J. (2020). shiny: Web Application Framework for R. R package version 1.4.0. Available at &lt;a href=&#34;http://CRAN.R-project.org/package=shiny&#34;&gt;http://CRAN.R-project.org/package=shiny&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Crump, M. J. C. (2017). Programming for Psychologists: Data Creation and Analysis (Version 1.1). &lt;a href=&#34;https://crumplab.github.io/programmingforpsych/&#34;&gt;https://crumplab.github.io/programmingforpsych/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Iannone, R., Allaire, J. J., &amp;amp; Borges, B. (2020). Flexdashboard: R Markdown Format for Flexible Dashboards. &lt;a href=&#34;http://rmarkdown.rstudio.com/flexdashboard&#34;&gt;http://rmarkdown.rstudio.com/flexdashboard&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;R Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Wickham, H. (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0. &lt;a href=&#34;https://CRAN.R-project.org/package=stringr&#34;&gt;https://CRAN.R-project.org/package=stringr&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Wickham, H., François, R., Henry, L., &amp;amp; Müller, K. (2018). dplyr: A Grammar of Data Manipulation. R package version 0.7.6. &lt;a href=&#34;https://CRAN.R-project.org/package=dplyr&#34;&gt;https://CRAN.R-project.org/package=dplyr&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Xie, Y. (2020). DT: A Wrapper of the JavaScript Library “DataTables”. R package version 0.14. Available at &lt;a href=&#34;https://CRAN.R-project.org/package=DT&#34;&gt;https://CRAN.R-project.org/package=DT&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;contact&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Contact&lt;/h3&gt;
&lt;p&gt;To submit any questions or feedback, please post &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/issues&#34;&gt;an issue&lt;/a&gt;, or email Pablo Bernabeu at &lt;a href=&#34;mailto:pcbernabeu@gmail.com&#34;&gt;pcbernabeu@gmail.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data is present: Workshops and datathons</title>
      <link>https://pablobernabeu.github.io/2020/data-is-present-workshops-and-datathons/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2020/data-is-present-workshops-and-datathons/</guid>
      <description>


&lt;div id=&#34;enhanced-data-presentation-using-reproducible-documents-and-dashboards&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Enhanced data presentation using reproducible documents and dashboards&lt;/h2&gt;
&lt;div id=&#34;calendar&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calendar&lt;/h3&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;9%&#34; /&gt;
&lt;col width=&#34;23%&#34; /&gt;
&lt;col width=&#34;39%&#34; /&gt;
&lt;col width=&#34;27%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Date&lt;/th&gt;
&lt;th&gt;Title&lt;/th&gt;
&lt;th&gt;Event and location&lt;/th&gt;
&lt;th&gt;Registration&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;26 Nov 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pablobernabeu.github.io/talk/2020-11-26-mixed-effects-models-in-r-and-a-new-tool-for-data-simulation&#34;&gt;Mixed-effects models in R, and a new tool for data simulation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;New Tricks Seminars, Dept. Psychology, Lancaster University [online]&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;8 Oct 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pablobernabeu.github.io/talk/2020-10-08-reproducibilidad-en-torno-a-una-aplicacion-web/&#34;&gt;Reproducibilidad en torno a una aplicación web&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Reprohack en español, LatinR Conference 2020 [online]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.eventbrite.com.ar/e/reprohack-en-espanol-latinr-2020-tickets-121741832097&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;13 Aug 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/pablobernabeu/CarpentryCon-2020-workshop-Open-Data-Reproducibility&#34;&gt;Open data and reproducibility: R Markdown, data dashboards and Binder v2.1&lt;/a&gt; (co-led with Florencia D’Andrea)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://2020.carpentrycon.org/schedule/&#34;&gt;CarpentryCon&lt;/a&gt;, &lt;a href=&#34;mailto:CarpentryCon@Home&#34; class=&#34;email&#34;&gt;CarpentryCon@Home&lt;/a&gt;, The Carpentries [online]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://2020.carpentrycon.org/schedule/&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;26 July 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/pablobernabeu/UKCLC2020-workshop-Open-data-and-reproducibility&#34;&gt;Open data and reproducibility: R Markdown, data dashboards and Binder&lt;/a&gt; (co-led with Eirini Zormpa)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.ukclc2020.com/pre-conference&#34;&gt;UK Cognitive Linguistics Conference&lt;/a&gt;, University of Birmingham [online]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.ukclc2020.com/registration&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;6 May 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pablobernabeu.github.io/PDF/LU-May-Markdown-workshop-programme.pdf&#34;&gt;R Markdown&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Lancaster University [online]&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Event cancelled&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://newcastle2020.satrdays.org/&#34;&gt;Open data and reproducibility 2.0&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://newcastle2020.satrdays.org/&#34;&gt;SatRday Newcastle upon Tyne&lt;/a&gt;, Newcastle University&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://newcastle2020.satrdays.org/&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Background&lt;/h3&gt;
&lt;p&gt;This project offers free activities to learn and practise reproducible data presentation. &lt;a href=&#34;https://www.software.ac.uk/about/fellows/pablo-bernabeu&#34;&gt;Pablo Bernabeu&lt;/a&gt; organises these events in the context of a &lt;a href=&#34;https://www.software.ac.uk/programmes-and-events/fellowship-programme&#34;&gt;Software Sustainability Institute Fellowship&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;open-source-software&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Open-source software&lt;/h4&gt;
&lt;p&gt;Programming languages such as &lt;a href=&#34;https://www.r-project.org/about.html&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt; offer free, powerful resources for data processing, visualisation and analysis. Experience in these programs is highly valued in data-intensive disciplines.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;open-data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Open data&lt;/h4&gt;
&lt;p&gt;Original data has become a &lt;a href=&#34;https://www.nature.com/articles/d41586-019-01506-x&#34;&gt;public good in many research fields&lt;/a&gt; thanks to cultural and technological advances. On the internet, we can find innumerable data sets from sources such as scientific journals and repositories (e.g., &lt;a href=&#34;https://osf.io&#34;&gt;OSF&lt;/a&gt;), local and national governments (e.g., &lt;a href=&#34;https://data.london.gov.uk/&#34;&gt;London&lt;/a&gt;, UK [&lt;a href=&#34;https://www.ukdataservice.ac.uk/&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://data.gov.uk/&#34;&gt;2&lt;/a&gt;]), non-governmental organisations (e.g., &lt;a href=&#34;https://data.world/datasets/ngo&#34;&gt;data.world&lt;/a&gt;), etc. Researchers inside and outside academia nowadays share a lot of their data under attribution licences (e.g., &lt;a href=&#34;https://creativecommons.org/&#34;&gt;Creative Commons&lt;/a&gt;, the UK &lt;a href=&#34;http://www.nationalarchives.gov.uk/doc/open-government-licence/version/1/&#34;&gt;Open Government Licence&lt;/a&gt;, etc.). This allows any external analysts to access these raw data, create (additional) visualisations and analyses, and share these. In society, making data more accessible can &lt;a href=&#34;https://digitalcommons.law.yale.edu/cgi/viewcontent.cgi?article=1140&amp;amp;context=yhrdlj&#34;&gt;demonstrably benefit citizens&lt;/a&gt; (despite &lt;a href=&#34;https://firstmonday.org/ojs/index.php/fm/article/view/3316/2764#author&#34;&gt;limitations&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;activities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Activities&lt;/h2&gt;
&lt;p&gt;Activities comprise free &lt;strong&gt;workshops&lt;/strong&gt; and &lt;a href=&#34;#datathons-creating-reproducible-documents-and-dashboards&#34;&gt;&lt;strong&gt;datathons&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;workshops&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Workshops&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.r-project.org&#34;&gt;R is a programming language&lt;/a&gt; greatly equipped for the creation of reproducible documents and dashboards. Four workshops are offered that cover a suite of interrelated tools—R, R Markdown, data dashboards and Binder environments—, all underlain by reproducible workflows and open-source software.&lt;/p&gt;
&lt;p&gt;Each workshop includes &lt;strong&gt;taught and practical sections&lt;/strong&gt;. The practice provides a chance for participants to experience and address common issues with the code. The level of taught sections is largely tailored to participants; similarly, practice sections are individually adaptable by means of easier and tougher tasks. The duration is also flexible, and some of the workshops can be combined into the same session.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://rstudio.com/&#34;&gt;RStudio&lt;/a&gt; interface is used in all workshops. Multi-levelled, real code examples are used. Throughout the workshops, and especially in the practice sections, individual questions will be encouraged.&lt;/p&gt;
&lt;div id=&#34;workshop-1-introduction-to-r&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 1: Introduction to R&lt;/h4&gt;
&lt;p&gt;This workshop can serve as an introduction to R or a revision. It demonstrates what can be done in R, and provides resources for individual training. Since the duration is limited, online courses are also recommended (&lt;a href=&#34;https://www.coursera.org/courses?query=r&#34;&gt;see examples&lt;/a&gt; and &lt;a href=&#34;https://learner.coursera.help/hc/en-us/articles/209819033-Apply-for-Financial-Aid-or-a-Scholarship&#34;&gt;fee waivers for full content&lt;/a&gt;).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://adv-r.had.co.nz/Data-structures.html&#34;&gt;Data structures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.sthda.com/english/wiki/installing-and-using-r-packages&#34;&gt;Packages&lt;/a&gt;: general-purpose examples (e.g., &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt;) and more specific ones (e.g., for &lt;a href=&#34;https://cran.r-project.org/web/packages/lsr/lsr.pdf&#34;&gt;statistics&lt;/a&gt; or &lt;a href=&#34;https://cran.r-project.org/web/packages/GEOmap/GEOmap.pdf&#34;&gt;geography&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/rio/vignettes/rio.html&#34;&gt;Loading and writing data, in native and foreign formats&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Wide&lt;/em&gt; format (also dubbed ‘untidy’) versus &lt;em&gt;tidy&lt;/em&gt; format (also dubbed ‘long’ or ‘narrow’). For most processes in R, &lt;a href=&#34;https://r4ds.had.co.nz/tidy-data.html&#34;&gt;data needs to be in a tidy format&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://doi.org/10.1371/journal.pbio.3000202.g001&#39;&gt;&lt;img width = &#39;25%&#39; src = &#39;https://journals.plos.org/plosbiology/article/file?id=10.1371/journal.pbio.3000202.g001&amp;type=large&#39; alt = &#39;Illustration of wide and tidy data formats, from Postma and Goedhart (2019)&#39; /&gt;&lt;/a&gt;
&lt;p align=&#34;center&#34; style=&#34;text-align:center; margin-top:-20px; margin-bottom:10px;&#34;&gt;
Image from Postma and Goedhart (2019; &lt;a href=&#34;https://doi.org/10.1371/journal.pbio.3000202.g001&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pbio.3000202.g001&lt;/a&gt;).
&lt;/p&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://psyteachr.github.io/msc-data-skills/joins.html#joins&#34;&gt;Combining data sets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cookbook-r.com/Manipulating_data/Summarizing_data/&#34;&gt;Data summaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://philmcaleer.github.io/ug2-practical/visualisation-through-ggplot2.html&#34;&gt;Plots with &lt;code&gt;ggplot2::ggplot()&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://plot.ly/ggplot2/&#34;&gt;Interactive plots with &lt;code&gt;plotly::ggplotly()&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learningstatisticswithr-bookdown.netlify.com/part-v-statistical-tools.html&#34;&gt;Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer_and_Curtin_LMEMs-2017-Psych_Methods.pdf&#34;&gt;Linear mixed-effects models&lt;/a&gt; (see also &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0749596X20300061?dgcid=coauthor#b0670&#34;&gt;a review of practices&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://swcarpentry.github.io/r-novice-inflammation/02-func-R/&#34;&gt;How functions work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Debugging&lt;/em&gt;. Code errors are known as bugs. They can tiresome, but also interesting sometimes! 😅 Some tips for the first many years of experience include: reading and investigating error messages, in both source and console windows; controlling letter case and typos; closing parentheses and inverted commas; ensuring to have the necessary packages installed and loaded; following the format required by each function. To debug, break up code into subcomponents and test each of those to find out the source of the error. Once we act on that, the best outcome is seeing the code work, but sometimes different errors overlap, in which case we may see one error disappearing before another one appears. Debugging soon leads to proficient information seeking. The search process often begins on an internet search engine and extends to user communities, package documentation, tutorials, blogs… (see &lt;a href=&#34;https://youtu.be/Nj9J5iCSMB0?t=2687&#34;&gt;video explanation&lt;/a&gt;). &lt;a href=&#34;https://adv-r.hadley.nz/debugging.html&#34;&gt;Advanced debugging tools are also available&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Vast availability of free resources on the internet, from &lt;a href=&#34;https://www.coursera.org/courses?query=r%20programming&#34;&gt;Coursera&lt;/a&gt; and other MOOC sites, &lt;a href=&#34;https://education.rstudio.com/&#34;&gt;RStudio&lt;/a&gt;, &lt;a href=&#34;https://psyteachr.github.io/&#34;&gt;University of Glasgow&lt;/a&gt;, &lt;a href=&#34;http://swcarpentry.github.io/r-novice-inflammation/&#34;&gt;Carpentries&lt;/a&gt;, etc.&lt;/li&gt;
&lt;li&gt;Community: &lt;a href=&#34;https://stackoverflow.com/&#34;&gt;StackOverflow&lt;/a&gt;, &lt;a href=&#34;https://community.rstudio.com/&#34;&gt;RStudio Community&lt;/a&gt;, &lt;a href=&#34;https://github.com&#34;&gt;Github issues&lt;/a&gt; (e.g., for R packages), etc. Using and contributing back.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rstudio.cloud/&#34;&gt;RStudio Cloud&lt;/a&gt;: a personal RStudio environment on the internet&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;workshop-2-r-markdown-documents&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 2: R Markdown documents&lt;/h4&gt;
&lt;p&gt;Set your input and output in stone using &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;R Markdown&lt;/a&gt;. The analysis reports may be enriched with website features (HTML/CSS) and published as HTML, PDF or Word documents. Moreover, with R packages such as &lt;code&gt;bookdown&lt;/code&gt;, &lt;code&gt;bookdownplus&lt;/code&gt;, &lt;code&gt;blogdown&lt;/code&gt; and &lt;code&gt;flexdashboard&lt;/code&gt;, documents can be formatted as &lt;a href=&#34;https://awesome-blogdown.com/&#34;&gt;websites&lt;/a&gt;, &lt;a href=&#34;https://bookdown.org/&#34;&gt;digital papers and books&lt;/a&gt; and &lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/&#34;&gt;data dashboards&lt;/a&gt;. Other useful packages include &lt;code&gt;rmarkdown&lt;/code&gt;, &lt;code&gt;knitr&lt;/code&gt;, &lt;code&gt;DT&lt;/code&gt;, &lt;code&gt;kableExtra&lt;/code&gt; and &lt;code&gt;ggplot2&lt;/code&gt;. Further background: &lt;a href=&#34;https://www.youtube.com/watch?v=Nj9J5iCSMB0&#34;&gt;presentation by Michael Frank&lt;/a&gt;, &lt;a href=&#34;https://www.eddjberry.com/talks/reproducible-writing-with-rmarkdown.html#1&#34;&gt;slides by Ed Berry&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As well as facilitating the reproducibility of analyses and results to third parties, R Markdown is helpful &lt;em&gt;during the creation&lt;/em&gt; of a report. In particular, it reduces the chances of errors and the number of repetitive tasks. For instance, any part of the data can be inputted in the text directly from the source, rather than manually copying it (e.g., &lt;code&gt;`r mean(dat[dat$location==&#39;Havana&#39;, &#39;measure&#39;])`&lt;/code&gt; (&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/r-code.html&#34;&gt;expand&lt;/a&gt;). Thus, if and when the analysis needs to be changed or updated, the report can be automatically updated at the click of a button. In another area, the captions for figures and tables can be automatised using &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook/cross-ref.html&#34;&gt;cross-reference labels&lt;/a&gt; (e.g., Table &lt;code&gt;\@ref(tab:mtcars)&lt;/code&gt;). This secures the match between the text and the captions of figures and tables, and it automatically updates the numbering whenever and wherever a new figure or table is introduced.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://bookdownplus.netlify.com/portfolio/&#39;&gt;&lt;img width = &#39;50%&#39; src = &#39;https://github.com/pablobernabeu/bookdownplus/blob/master/inst2/copernicus/showcase/copernicus2.png?raw=true&#39; alt = &#39;Example of paper created with bookdownplus (image retrieved from R bookdownplus package)&#39;/&gt;&lt;/a&gt;
&lt;p align=&#34;center&#34; style=&#34;text-align:center; margin-top:-30px; margin-bottom:30px;&#34;&gt;
Image from bookdownplus package (&lt;a href=&#34;https://bookdownplus.netlify.com/portfolio/&#34; class=&#34;uri&#34;&gt;https://bookdownplus.netlify.com/portfolio/&lt;/a&gt;).
&lt;/p&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;workshop-3-introduction-to-data-dashboards&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 3: Introduction to data dashboards&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01782/full&#34;&gt;Data dashboards are web applications used to visualise data&lt;/a&gt; in detail through tables and plots. They assist in explaining and accounting for our data processing and analysis. They don’t require any coding from the end user. While most dashboards and web applications present existing data, a few of them serve the purpose of creating or simulating new data (see &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation&#34;&gt;example&lt;/a&gt;).&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/&#39;&gt; &lt;img width = &#39;90%&#39; src = &#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/8.png&#39; alt = &#39;Illustration of the usage of dashboards alongside data repositories&#39; /&gt; &lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;These all-reproducible dashboards are published as websites, and thus, they can include hyperlinks and downloadable files. Some of the R packages used are &lt;code&gt;knitr&lt;/code&gt;, &lt;code&gt;DT&lt;/code&gt;, &lt;code&gt;kableExtra&lt;/code&gt;, &lt;code&gt;reactable&lt;/code&gt;, &lt;code&gt;ggplot2&lt;/code&gt;, &lt;code&gt;plotly&lt;/code&gt;, &lt;code&gt;rmarkdown&lt;/code&gt;, &lt;code&gt;flexdashboard&lt;/code&gt; and &lt;code&gt;shiny&lt;/code&gt;. The aim of this workshop is to practise creating different forms of dashboards—&lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/&#34;&gt;Flexdashboard&lt;/a&gt; and &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;Shiny&lt;/a&gt;—the latter of which offers &lt;a href=&#34;https://mastering-shiny.org/&#34;&gt;greater features&lt;/a&gt;, and to practise also with the hosting platforms fitting each type—such as personal websites, &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;, &lt;a href=&#34;https://mybinder.org/&#34;&gt;Binder&lt;/a&gt;, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;Shinyapps&lt;/a&gt; and &lt;a href=&#34;https://rstudio.com/products/shiny/shiny-server/&#34;&gt;custom servers&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A great thing about dashboards is that they may be made very simple, but they can also be taken to the next level using some HTML, CSS or Javascript code (on top of the back-end code present in the R packages used), which is addressed in the next workshop.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;workshop-4-binder-environments-and-improving-data-dashboards&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 4: Binder environments and improving data dashboards&lt;/h4&gt;
&lt;div id=&#34;binder&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Binder&lt;/h5&gt;
&lt;p&gt;&lt;a href=&#34;https://mybinder.org/&#34;&gt;Binder&lt;/a&gt; is a tool to facilitate public access to software environments—for instance, by publishing an RStudio environment on the internet. Binder can also host Shiny apps. It is generously free &lt;a href=&#34;https://discourse.jupyter.org/t/mybinder-org-cost-updates/2426&#34;&gt;for users&lt;/a&gt;. After looking at the &lt;a href=&#34;https://github.com/binder-examples/r&#34;&gt;nuts and bolts of a deployment&lt;/a&gt;, participants will be able to deploy their own Binder environments and check the result by the end of the workshop. For this purpose, it’s recommended to have data and R code ready, ideally in a GitHub repository.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;improving-data-dashboards&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Improving data dashboards&lt;/h5&gt;
&lt;p&gt;We will practise how to improve the functionality of dashboards using some HTML, CSS and Javascript code, which is &lt;a href=&#34;https://www.w3schools.com/whatis/&#34;&gt;the basis of websites&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!-- Javascript function to enable a hovering tooltip --&amp;gt;
&amp;lt;script&amp;gt;
$(document).ready(function(){
$(&amp;#39;[data-toggle=&amp;quot;tooltip1&amp;quot;]&amp;#39;).tooltip();
});
&amp;lt;/script&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://shiny.rstudio.com/gallery/&#39;&gt; &lt;img align = &#39;center&#39; width = &#39;60%&#39; src = &#39;https://raw.githubusercontent.com/pablobernabeu/data-is-present/master/dashboard%20gif.gif&#39; alt = &#39;Examples of data dashboards&#39; /&gt; &lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trade-offs-among-dashboards&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Trade-offs among dashboards&lt;/h5&gt;
&lt;p&gt;Next, we will practise with three dashboard types—&lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/using.html&#34;&gt;Flexdashboard&lt;/a&gt;, &lt;a href=&#34;https://shiny.rstudio.com/tutorial/&#34;&gt;Shiny&lt;/a&gt; and &lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/shiny.html&#34;&gt;Flexdashboard-Shiny&lt;/a&gt;—and with the suitable hosting platforms. Firstly, the strength of Flexdashboard (&lt;a href=&#34;http://rpubs.com/pcbernabeu/Dutch-modality-exclusivity-norms&#34;&gt;example&lt;/a&gt;) is its basis on R Markdown, yielding an unmatched user interface (&lt;em&gt;front-end&lt;/em&gt;). Secondly, the strength of Shiny (&lt;a href=&#34;https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment/&#34;&gt;example&lt;/a&gt;) is the input reactivity (&lt;em&gt;back-end&lt;/em&gt;) it offers, allowing users to download sections of data they select, in various formats. Last, Flexdashboard-Shiny (&lt;a href=&#34;https://pablobernabeu.shinyapps.io/dutch-modality-exclusivity-norms/&#34;&gt;example&lt;/a&gt;) combines the best of both worlds.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ &lt;b&gt; Flexdashboard &lt;/b&gt; ★
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ ★ &lt;b&gt; Shiny &lt;/b&gt; ★ ★
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ ★ ★ &lt;b&gt; Flexdashboard-Shiny &lt;/b&gt; ★ ★ ★
&lt;/p&gt;
&lt;p&gt;Flexdashboard types are rendered as an HTML document—simple websites—, and can therefore be easily published on personal sites or &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;. This is convenient because no special hosting is required. In contrast, Shiny and Flexdashboard-Shiny types offer greater features, but require Shiny servers. Fortunately, the shinyapps.io server is available for free, up to some &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;usage limit&lt;/a&gt;. This server can host any of the three dashboards mentioned here. Another good option is presented by Binder environments, which can host the Shiny-type dashboards with no (explicit) limit. Yet, the Flexdashboard-Shiny type cannot be hosted in this server (&lt;a href=&#34;https://github.com/jupyter/repo2docker/issues/799&#34;&gt;as of January 2020, at least&lt;/a&gt;). Consequently, greater functionality may come at a cost for dashboards that have any considerable traffic, whereas dashboards with low traffic may do well on shinyapps.io. Knowing these trade-offs can help navigate &lt;a href=&#34;https://support.rstudio.com/hc/en-us/articles/217592947-What-are-the-limits-of-the-shinyapps-io-Free-plan-&#34;&gt;usage limits&lt;/a&gt;, save on web hosting fees, and increase the availability of our dashboards online, as we can offer fall-back versions on different platforms, as in the example below:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;… &lt;em&gt;&lt;a href=&#34;https://pablobernabeu.shinyapps.io/dutch-modality-exclusivity-norms/&#34;&gt;preferred-dashboard&lt;/a&gt; (in case of downtime, please visit this &lt;a href=&#34;http://rpubs.com/pcbernabeu/Dutch-modality-exclusivity-norms&#34;&gt;alternative&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Transforming dashboards into the different versions can be as easy as enabling or disabling some features, especially input reactivity. For instance, if we want to downgrade a Flexdashboard-Shiny to a Flexdashboard, to publish it outside of a Shiny server (see &lt;a href=&#34;https://github.com/pablobernabeu/Modality-exclusivity-norms-Bernabeu-et-al/blob/master/Dutch-modality-exclusivity-norms-RPubs.Rmd&#34;&gt;example&lt;/a&gt;), we must delete &lt;code&gt;runtime:shiny&lt;/code&gt; from the header, and disable reactive features, as below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
```r
# Number of words selected on sidebar
# reactive(cat(paste0(&amp;#39;Words selected below: &amp;#39;, nrow(selected_props()))))
```&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;free-accounts-and-tips&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Free accounts and tips&lt;/h5&gt;
&lt;p&gt;Hosting sites have specific terms of use. For instance, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;shinyapps.io&lt;/a&gt; has a free starter license with limited use. Free apps can handle a large but limited amount of data, and up to five apps may be created. Beyond this, RStudio offers a wide range of subscriptions starting at $9/month.&lt;/p&gt;
&lt;p&gt;Memory and traffic limits of the free shinyapps.io account can sometimes present problems when heavy data data sets are used, or there are many visits to the app. The memory overload issue is often flagged as &lt;code&gt;Shiny cannot use on-disk bookmarking&lt;/code&gt;, whereas excessive traffic may see the app not loading. Fortunately, usage limits need not always require a paid subscription or a &lt;a href=&#34;https://www.r-bloggers.com/alternative-approaches-to-scaling-shiny-with-rstudio-shiny-server-shinyproxy-or-custom-architecture/&#34;&gt;custom server&lt;/a&gt;, thanks to the following workarounds:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;develop app locally as far as possible, and only deploy to shinyapps.io only at the last stage;&lt;/li&gt;
&lt;li&gt;prune data set, leaving only the necessary data;&lt;/li&gt;
&lt;li&gt;if necessary, unlink data by splitting it into different sets, reducing computational demands;&lt;/li&gt;
&lt;li&gt;if necessary, use various apps (five are allowed in each free shinyapps.io account);&lt;/li&gt;
&lt;li&gt;if necessary, link from the app to a PDF with visualisations requiring heavy, interlinked data. High-resolution plots can be rendered into a PDF document in a snap, using code such as below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;
pdf(&amp;#39;List of plots per page&amp;#39;, width = 13, height = 5)
print(plot1)
print(plot2)
# ...
print(plot150)
dev.off()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conveniently, all text in a PDF—even in plots—is indexed, so it can be searched [ Ctrl+f / Cmd+f / 🔍 ] (see &lt;a href=&#34;https://osf.io/2tpxn/&#34;&gt;example&lt;/a&gt;). Furthermore, you may also &lt;a href=&#34;http://www.ilovepdf.com/&#34;&gt;merge the rendered PDF with any other documents&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;prerequisites-and-suggestions-for-participation-in-each-workshop&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Prerequisites and suggestions for participation in each workshop&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Necessary:&lt;/em&gt; laptop or computer with &lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;https://rstudio.com/products/rstudio/download/&#34;&gt;RStudio&lt;/a&gt; installed, or access to &lt;a href=&#34;https://rstudio.cloud/&#34;&gt;RStudio Cloud&lt;/a&gt;; familiarity with the content of the preceding workshops through the web links herein.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Suggested:&lt;/em&gt; having your own data and R code ready (on a Github repository if participating in Workshop 4); participation in some of the preceding workshops.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;datathons-creating-reproducible-documents-and-dashboards&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Datathons: creating reproducible documents and dashboards&lt;/h3&gt;
&lt;p&gt;In these coding meetups, participants collaborate to create reproducible documents or dashboards using the data and software they prefer (see &lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/tree/master/examples-documents-dashboards&#34;&gt;examples&lt;/a&gt;). Since the work can be split across different people and sections, some nice products may be achieved within a session. Any programming languages may be used.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Data used:&lt;/strong&gt; academic or non-academic data of your own or from open-access sources such as &lt;a href=&#34;https://osf.io&#34;&gt;OSF&lt;/a&gt;, scientific journals, governments, international institutions, NGOs, etc.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inspired by the great &lt;a href=&#34;https://reprohack.github.io/reprohack-hq/&#34;&gt;Reprohacks&lt;/a&gt;, content suggestions are encouraged. That is, if you’d like to have a reproducible document or dashboard created for a certain, open-access data set, please let us know, and some participants may take it on. Suggestions may be posted as &lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/issues&#34;&gt;issues&lt;/a&gt; or emailed to &lt;a href=&#34;mailto:p.bernabeu@lancaster.ac.uk&#34; class=&#34;email&#34;&gt;p.bernabeu@lancaster.ac.uk&lt;/a&gt;.
&lt;br&gt;&lt;/br&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Purposes&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;collaborating to visualise data in novel ways using reproducible documents or interactive dashboards. For this purpose, participants sometimes draw on additional data to look at a bigger picture;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;reflecting on the process by reviewing the techniques applied and challenges encountered.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt; A key aspect of datathons is the creation of output. Documents and dashboards are (co-)authored by the participants who work on them, who can then publish them on their websites, or on &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;, &lt;a href=&#34;https://mybinder.org/&#34;&gt;Binder&lt;/a&gt;, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;Shinyapps&lt;/a&gt; or &lt;a href=&#34;https://rstudio.com/products/shiny/shiny-server/&#34;&gt;custom servers&lt;/a&gt;. Time constraints notwithstanding, a lot of this output may be very enticing for further development by the same participants, or even by other people if the code is shared online. Just like with data, an attribution licence can be attached to the code.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;prerequisites-and-suggestions-for-participation-in-datathons&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Prerequisites and suggestions for participation in datathons&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Necessary:&lt;/em&gt; basic knowledge of reproducible documents or dashboards.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Suggested:&lt;/em&gt; familiarity with the development of reproducible documents or dashboards; an idea about the data you’d like to work with and the kind of document or dashboard you want to create.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;contact&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Contact&lt;/h2&gt;
&lt;p&gt;Please submit any queries or requests by &lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/issues&#34;&gt;posting an issue&lt;/a&gt; or emailing &lt;a href=&#34;mailto:p.bernabeu@lancaster.ac.uk&#34; class=&#34;email&#34;&gt;p.bernabeu@lancaster.ac.uk&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Event-related potentials: Why and how I used them</title>
      <link>https://pablobernabeu.github.io/2020/event-related-potentials-why-and-how-i-used-them/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2020/event-related-potentials-why-and-how-i-used-them/</guid>
      <description>&lt;p&gt;Event-related potentials (ERPs) offer a unique insight in the study of human cognition. Let&#39;s look at their reason-to-be for the purposes of research, and how they are defined and processed. Most of this content is based on my master&#39;s thesis, which I could fortunately conduct at the Max Planck Institute for Psycholinguistics (see &lt;a href=&#39;https://psyarxiv.com/5gjvk/&#39;&gt;thesis&lt;/a&gt; or &lt;a href=&#39;https://psyarxiv.com/a5pcz/&#39;&gt;conference paper&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;electroencephalography&#34;&gt;Electroencephalography&lt;/h2&gt;
&lt;p&gt;The brain produces electrical activity all the time, which can be measured via electrodes on the scalp—a method known as electroencephalography (EEG). These pulses are produced for every one of our states and actions, in a voltage at a micro (µ) scale, typically between 10 µV (0.000010) and 100 µV (0.000100; Aurlien et al., 2004). The overlapping pulses happen at extremely high frequencies; indeed, the signal can be measured once per millisecond. The high frequency of this signal is very interesting for the study of some cognitive processes in particular, for which the time course is (or may be) critical. One such example is conceptual processing, namely, the process of understanding the meaning of words.&lt;/p&gt;
&lt;p&gt;Research has revealed the relation between certain EEG patterns and cognitive states and functions. Brain activity includes dozens of types, but broadly, it can be divided into neural oscillations and event-related potentials. Specific oscillations (also known as brain waves) are associated with &lt;em&gt;states&lt;/em&gt; such as wakefulness, sleep, arousal, relaxation, etc. (Roohi-Azizi et al., 2017). Event-related potentials instead represent more finite &lt;em&gt;events&lt;/em&gt;, such as the presentation of as a stimulus. In cognitive neuroscience, both oscillations and ERPs are studied, whereas in cognitive psychology, ERPs are much more common than oscillations. Let&#39;s dive into ERPs below.&lt;/p&gt;
&lt;h3 id=&#34;event-related-potentials&#34;&gt;Event-related potentials&lt;/h3&gt;
&lt;p&gt;In the lab, ERPs are elicited using controlled designs. In each trial, a series of stimuli are presented. At a fixed point therein, an EEG measurement begins and spans for a certain period. In turn, in the analysis, this measurement period is divided into &lt;em&gt;time windows&lt;/em&gt;, which often correspond to specific ERP components (e.g., N400 window).&lt;/p&gt;
&lt;p&gt;In psycholinguistics, for instance, a typical scenario is the presentation of words, and ERPs are systematically &lt;em&gt;time-locked&lt;/em&gt; to the same position in consecutive trials, often the onset of a word. By this means, the experimental manipulation is collected, and the non-experimental variation—&amp;lsquo;noise&amp;rsquo;—is largely cancelled out by the aggregation of multiple trials that share the experimental manipulation.&lt;/p&gt;
&lt;p&gt;The chief reason to employ the ERP method is the measurement of cognitive processes online, that is, precisely as they unfold. This is fitting in the context of language comprehension, where some important processes last for less than a second.&lt;/p&gt;
&lt;h2 id=&#34;time-course-of-word-processing&#34;&gt;Time course of word processing&lt;/h2&gt;
&lt;p&gt;Processing a word takes around 800 milliseconds (ms). Within that time, earlier processes (compared to later ones) have been ascribed greater relevance to the core process of understanding a word (Mahon &amp;amp; Caramazza, 2008). This assumes that broader processes start only after more immediate ones have started (but see Lebois, Wilson‐Mendenhall &amp;amp; Barsalou, 2014). The most immediate process is the recognition of a string of letters, which seems to start within 90 ms post word onset in early auditory cortex and the Visual Word Form Area (Willems et al., 2016). Then ensue further, fundamental stages known as &lt;em&gt;lexical&lt;/em&gt; and &lt;em&gt;semantic&lt;/em&gt; processes. Lexical processing is the identification of a string of letters as a known word, and it happens within around 160 ms post word onset. Next, at around 200 ms, we may see the beginning of semantic processing, which denotes a further step in the cognitive analysis of the word that is akin to &lt;em&gt;meaning&lt;/em&gt; (Hauk, 2016). These processes may overlap, as indeed suggested by the sensitivity of the N400 ERP (see also next section) to both lexical and semantic tasks (Kutas &amp;amp; Federmeier, 2011). Both processes also likely extend further in the processing timeline (Hauk, 2016). In spite of this overlap, however, lexical and semantic processing have often been linked to different cognitive phenomena. For instance, tasks promoting semantic processing (e.g., semantic decision, whereby participants describe words as concrete or abstract) have been found to engage sensorimotor simulation of the word&#39;s meaning (known as &lt;em&gt;embodiment&lt;/em&gt;) more strongly than lexical tasks do (Connell &amp;amp; Lynott, 2013; Pexman et al., 2019; Sato et al., 2008).&lt;/p&gt;
&lt;p&gt;Once the lexical and semantic stages have emerged, post-lexical, post-semantic processes follow (Mahon &amp;amp; Caramazza, 2008). These are mental imagery and episodic memory processes—both with an approximate emergence around 270 ms after word onset. The gradual progression from the identification of a word up to accessing its broadest meaning is an important anchoring point in the current research on the alleged embodiment of meaning comprehension, even if we might hope to count on more definitive threshold points (Hauk, 2016).&lt;/p&gt;
&lt;p&gt;Word processing data are mainly based on written word processing, but spoken words are processed quite similarly, if slightly faster (Leonard et al., 2016; Pulvermüller et al., 2005; Shtyrov et al., 2004).&lt;/p&gt;
&lt;p&gt;The bigger take-home messages would be: (1) the processing of meaning might only start at around 160 ms post word onset, and (2) processes outside of meaning comprehension might only start at around 270 ms. These working references must be taken with some caution because particular semantic effects have been found at different stages (e.g., the conceptual modality switch, as in Hald et al., 2011; Collins, Pecher et al., 2011). Indeed, in an influential critique of blooming findings on embodiment, Mahon and Caramazza (2008) argued that even early effects might possibly be explained in terms of non-embodied processing. They contended that working memory processes that were ancillary rather than semantic could be quickly engaged with the function of ‘colouring’ a concept, not building it up. To further complicate the matter, we do not have absolute certainty on the later section of the time course. Thus, as Hauk (2016) reviews, the different stages likely overlap at certain points, with different degrees of relevance. For instance, lexical processing may continue even once semantic processing has started, but would naturally become less relevant. Indeed, the relation among these processes is likely more of a continuum than a set of clear-cut modules. In a nutshell, the time course is important with some experimental effects in word processing, and, to that extent, we depend on our knowledge of the basic time course of word processing.&lt;/p&gt;
&lt;h2 id=&#34;the-conceptual-modality-switch-paradigm-and-its-time-course&#34;&gt;The conceptual modality switch paradigm and its time course&lt;/h2&gt;
&lt;p&gt;In demonstrating the relevance of embodied cognition, a sizeable series of studies have shown that reading about different conceptual modalities (e.g., auditory ‘loud bell’ followed by visual ‘pink background’) incurs processing costs (Pecher et al., 2003). Importantly, this manipulation does not concern the presentation mode of the stimulus, maintained constant, but the intrinsic semantic modality of the stimulus concepts. The conceptual modality switch effect has often been replicated (Pecher et al., 2004; Solomon &amp;amp; Barsalou, 2004; Marques, 2006; Vermeulen et al., 2007; van Dantzig et al., 2008; Lynott &amp;amp; Connell, 2009; Ambrosi et al., 2011; Collins et al., 2011; Hald et al., 2011; Hald et al., 2013; Scerrati et al., 2015).&lt;/p&gt;
&lt;p&gt;Bernabeu, Willems and Louwerse (2017) addressed a caveat with the time course of the conceptual modality switch paradigm. In previous experiments, trials presented a concept word followed by a property word. ERPs were time-locked to the latter property word. This design may have left uncontrolled a switch produced already at the concept. Indeed, the property word was already supposed to be in the particular modality of the trial. That pitfall could have had two consequences: loss of power and loss of certainty on the time course of the effect. Thus, Bernabeu et al. created a design in which ERPs were time-locked to the first word in target trials (see some &lt;a href=&#34;https://www.researchgate.net/post/Conceptual_modality_switch_effect_measured_at_first_word&#34;&gt;early input from researchers online&lt;/a&gt;). The purpose of this relocation was not to completely annul the possibility of post-core sensory processes, but to increase the time accuracy by measuring the modality switch from the point at which it is elicited.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;designoverview.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Implementing this design had an ancillary effect on the measurement of response times. A psycholinguistic experiment like this one requires controlling fundamental variables such as word frequency and length, by matching the means of these variables across experimental conditions. This must be controlled in the target words at least. As it is often the case, this control was only possible in the target words—the first one in target trials—, but it was not possible in the second word, which is the crucial one for response times. Response times could still be measured, but comparisons across conditions were not fully warranted. In sum, this was an ERP design.&lt;/p&gt;
&lt;h2 id=&#34;erp-components&#34;&gt;ERP components&lt;/h2&gt;
&lt;p&gt;When the ERP signal is plotted, it displays multiple wave shapes, or &lt;em&gt;waveforms&lt;/em&gt;, each with a peak flanked by falling tails. Each of these waves often corresponds to an ERP component, which is what cognitive scientists are often interested in.&lt;/p&gt;
&lt;p&gt;Multiple components are known, each having been found to consistently peak around specific points in time during a cognitive process. The peak is one of several features characterising each component. A sketch list is shown below (van Hell &amp;amp; Kroll, 2013).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Polarity:&lt;/strong&gt; The component either peaks in the positive or the negative pole of the signal. This polarity is relative to the &lt;em&gt;baseline&lt;/em&gt; point that is created in the preprocessing stage (see below);&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Latency:&lt;/strong&gt; the time course of the component, encompassing an onset, a peak and an overall duration. Time windows are normally set to match relevant components (e.g., the N400 window, etc.);&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Amplitude:&lt;/strong&gt; the voltage reached at a given time (e.g., the peak) or for a certain period (e.g., a time window);&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalp distribution, or topography:&lt;/strong&gt; the areas on the scalp (the scalp being a reasonable proxy for the brain) in which the component appears;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Functional role:&lt;/strong&gt; the cognitive functions that have been consistently associated with the component.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Examples of components in language processing include the N400, consistently linked to semantic processing, that is, seeking the meaning of words or sentences. The N400 is characterised by a large, negative amplitude peaking at around 400 ms post word onset, primarily found in central and posterior sites. N400 &lt;em&gt;effects&lt;/em&gt;, which are comparisons of the N400 component in different experimental conditions, have consistently appeared under violations of semantic expectations, i.e., related to meaning and events (Kutas &amp;amp; Federmeier, 2011; Swaab et al., 2012). Another well-known component in language is the P600, linked to syntactic processing, which allows the comprehension of sentences (Swaab et al., 2012). Other examples of components include lateralized readiness potentials, signalling motor preparation (Mordkoff &amp;amp; Gianaros, 2000), and the P3b component, which appears in the context of responses (van Vliet et al., 2014). Both of the latter components are relevant to researchers across domains, who often need to ward off &lt;em&gt;contamination&lt;/em&gt; from these components in their experiments. In Bernabeu et al.&amp;lsquo;s experiment, for instance, part of the reason why ERPs were time-locked to the first word in target trials was to prevent contamination from these components.&lt;/p&gt;
&lt;p&gt;ERP data sets are large, being the product of the number of electrodes (also called &amp;lsquo;channels&amp;rsquo;) times the number of time points, times the number of experimental conditions, and times the number of participants. In recent studies, the number of trials often adds to that product, whereas in previous experiments, the trials tended to be aggregated in each condition.&lt;/p&gt;
&lt;h2 id=&#34;eeg-montage&#34;&gt;EEG montage&lt;/h2&gt;
&lt;p&gt;The EEG montage is an important factor. The options are broadly characterised by three parameters of the electrodes:&lt;/p&gt;
&lt;p style=&#34;margin-left: 30px; line-height: 1.2; padding-bottom: 12px; padding-left: 15px; float: right; display: block;&#34;&gt;&lt;img src=&#34;EEG MPI open day photo.jpg&#34; alt=&#34;Pablo Bernabeu, 2015&#34; width=&#34;170px&#34; style=&#39;padding-bottom: 15px; margin-bottom: 0px;&#39; /&gt;&lt;span style=&#34;font-size: small; padding-left: 5px; padding-top: 0px; margin-top: 0px;&#34;&gt;Brainwaves exposed at an open day.&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Active / passive:&lt;/strong&gt; Active electrodes amplify the EEG signal directly at the scalp, whereas passive electrodes require the raw signal to be sent through a lead (i.e., a wire) and up to an amplifier. In the latter case, the lead acts as an antenna, picking up ambient electrical noise. This noise can hinder the subtle measurement of interest. This problem is solved by active electrodes (Laszlo et al., 2014).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Number:&lt;/strong&gt; Traditionally, montages with 32, 64 or 128 electrodes have been used. The more electrodes, the &amp;lsquo;denser&amp;rsquo; the montage, and the higher the spatial resolution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Wet / dry:&lt;/strong&gt; In some montages, the electrical conductance on the electrodes&amp;rsquo; contact point must be increased using some fluid solutions, such as a specific gel (often commercialised by the companies that also make EEG apparatuses). Conversely, other electrodes function in a dry way. Ensuring the proper conductance on wet electrodes has traditionally been very time-consuming for experimenters, often taking over half an hour of wiggling a blunt syringe distributing the saline solution around the tip. Traditionally, wet electrodes produced more reliable data than dry ones, but &lt;em&gt;the times they are a&#39;changing&lt;/em&gt; (di Flumeri, 2019).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An EEG/ERP experiment is time-consuming. The preparation (especially conductance-prompting with wet montages) and post-experiment procedures (especially washing the EEG cap) often take four or five times as long as the experiment proper. These procedures are especially long for higher-density, wet montages.&lt;/p&gt;
&lt;h2 id=&#34;preprocessing-erps&#34;&gt;Preprocessing ERPs&lt;/h2&gt;
&lt;p&gt;ERPs are not the first signal collected in experiments. They are obtained after considerable, systematic preprocessing of the EEG signal.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#39;https://www.researchgate.net/post/EEG_error_datasets_missing_channels_Its_strange_because_they_were_recorded_well_and_faulty_files_are_quite_as_heavy_as_the_good_ones_Any_ideas&#39;&gt;&lt;img src=&#34;https://www.researchgate.net/profile/Nikolay_Novitskiy/post/EEG_error_datasets_missing_channels_Its_strange_because_they_were_recorded_well_and_faulty_files_are_quite_as_heavy_as_the_good_ones_Any_ideas/attachment/59d6391b79197b8077996520/AS%3A400433085468672%401472482095219/image/41_64ch.png&#34; alt=&#34;Brain Vision waveforms&#34; width=&#39;70%&#39;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For the Bernabeu et al. study, I used Brain Vision software, and followed the &lt;a href=&#34;https://erpinfo.org/resources&#34;&gt;tutorials from the well-known ERP Boot Camp&lt;/a&gt; of Steve Luck and Emily Kappenman. I applied the following pipeline separately for each participant:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;labeling channels (64 electrodes);&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;creating channel groups (anterior and posterior);&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;re-referencing the signal offline to the right mastoid (RM), having referenced online to the left mastoid (Ref);&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#39;EEG montage.png&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;
&lt;p&gt;separating my three experimental conditions;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ocular correction for blinks and significant, vertical or horizontal movements of the eyes (seminal method by Gratton et al., 1983, which is the default in Brain Vision);&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;baseline correction, which is a standardisation based on a certain period immediately before the onset of the target manipulation;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;further correction of artifacts such as motor action potentials (or lateralised readiness potentials) resulting from even the subtlest muscle activity.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This pipeline is reflected in the &lt;a href=&#34;https://osf.io/98fs6/&#34;&gt;scripts exported from Brain Vision&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  &amp;lt;Nodes&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMatch&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection/baselinecorr&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection/baselinecorr/baselinecorr_artif&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection/baselinecorr/baselinecorr_artif/minave_TotalMatch&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection/baselinecorr&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection/baselinecorr/baselinecorr_artif&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection/baselinecorr/baselinecorr_artif/minave_EmbodiedMismatch&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection/baselinecorr&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection/baselinecorr/baselinecorr_artif&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection/baselinecorr/baselinecorr_artif/minave_TotalMismatch&amp;lt;/string&amp;gt;
  &amp;lt;/Nodes
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;p&gt;Word reading ERPs can look somewhat like this after the preprocessing (&lt;a href=&#39;https://osf.io/bz7ae/&#39;&gt;plots made in R&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;Four main waveform plots stacked.png&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;To visualise these waveforms throughout the different sections of the data, a &lt;a href=&#34;https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment&#34;&gt;dashboard is available&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;statistical-analysis&#34;&gt;Statistical analysis&lt;/h2&gt;
&lt;p&gt;With the myriad repeated measures involved in EEG, linear mixed-effects models are a good option, allowing the registration of electrodes and time points in the error term per participant (and trial, too, if these are not aggregated). The analysis I performed, in R, is &lt;a href=&#34;https://osf.io/sx3nw/&#34;&gt;available&lt;/a&gt; (plots visible by downloading the file from the aforementioned link).&lt;/p&gt;
&lt;style&gt;.embed-responsive{position:relative;height:100%;}.embed-responsive iframe{position:absolute;height:100%;}&lt;/style&gt;&lt;script&gt;window.jQuery || document.write(&#39;&lt;script src=&#34;//code.jquery.com/jquery-1.11.2.min.js&#34;&gt;\x3C/script&gt;&#39;) &lt;/script&gt;&lt;link href=&#34;https://mfr.osf.io/static/css/mfr.css&#34; media=&#34;all&#34; rel=&#34;stylesheet&#34;&gt;&lt;div id=&#34;mfrIframe&#34; class=&#34;mfr mfr-file&#34;&gt;&lt;/div&gt;&lt;script src=&#34;https://mfr.osf.io/static/js/mfr.js&#34;&gt;&lt;/script&gt; &lt;script&gt;var mfrRender = new mfr.Render(&#34;mfrIframe&#34;, &#34;https://mfr.osf.io/render?url=https://osf.io/sx3nw/?direct%26mode=render%26action=download%26mode=render&#34;);&lt;/script&gt;
&lt;br&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Event-related potentials fulfil an important role in cognitive neuroscience and psychology, only surpassed by magnetic electroencephalography (MEG), which unites high temporal and spatial resolution. Learning how to use ERPs is demanding but even more rewarding. It certainly does not make for fast science, but allows the measurement of experimental effects online, that is, as they unfold.&lt;/p&gt;
&lt;p&gt;You can learn about and overcome multiple challenges. One of the issues I faced once regarded some channels (electrodes) that appeared to be missing from the data. I posted a &lt;a href=&#34;https://www.researchgate.net/post/EEG_error_datasets_missing_channels_Its_strange_because_they_were_recorded_well_and_faulty_files_are_quite_as_heavy_as_the_good_ones_Any_ideas&#34;&gt;question on ResearchGate&lt;/a&gt;, and emailed Brain Products, the maker of Brain Vision Recorder, which I was using.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hi everyone,&lt;/p&gt;
&lt;p&gt;If you could please give me a hand with this error, I would be very grateful. I have EEG from a psychological experiment, recorded with BrainVision Recorder, and being analyzed with BrainVision Analyzer 2. Most of the recordings are perfectly fine, but a few present a big error. Out of 64 original electrodes, only two appear. These are the right mastoid (RM) and the left eye sensor (LEOG). Both are bipolar electrodes. RM is to be re-referenced to the online reference electrode, while LEOG is to be re-referenced to the right eye electrode.&lt;/p&gt;
&lt;p&gt;I just can&#39;t fathom the error because all electrodes worked fine during the recording. Also, the data sets with the error are quite as heavy in terms of bytes as those without the error. Further, why should the RM and LEOG channels remain perfectly well as they do?&lt;/p&gt;
&lt;p&gt;This issue might seem like a simple zoom I&#39;ve bypassed, or similar&amp;hellip; But unfortunately the channels are just not there. I&#39;ve confirmed it as I tried to copy the pipeline from the good data sets onto the faulty ones, where I got the error &amp;lsquo;No channels enabled.&amp;rsquo; In case you had access to the BVA analysis software, please find the raw files for one of the faulty data sets here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;p&gt;Thanks to invaluable help from a &lt;a href=&#34;https://www.researchgate.net/post/EEG_error_datasets_missing_channels_Its_strange_because_they_were_recorded_well_and_faulty_files_are_quite_as_heavy_as_the_good_ones_Any_ideas&#34;&gt;ResearchGate contributor&lt;/a&gt; and the Brain Products team, I could put the pieces back together.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Update: Problem solved.&lt;/p&gt;
&lt;p&gt;As Nikolay said, the error originated in Recorder (I had used the workspace from the previous experimenter), and the problem was solved by setting the label and position of each channel.&lt;/p&gt;
&lt;p&gt;I tried editing the .vhdr file in raw (it seemed nice and quick to directly assign the channel names as labels) but i didn&#39;t quite find the way. Therefore, with a tip from the Brain Products team, I went about it within the program.&lt;/p&gt;
&lt;p&gt;First, I used the transform function &amp;lsquo;Edit channels&amp;rsquo; to rename all labels and set each within their coordinates. I did that for just one subject (it doesn&#39;t take as long as it sounds). Afterwards, I created a &amp;lsquo;History template&amp;rsquo; out of that process, and copied it to all other nodes.
At any rate, never getting out of the comfort workspace again&amp;hellip; :D&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;div style = &#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Ambrosi, S., Kalenine, S., Blaye, A., &amp;amp; Bonthoux, F. (2011). Modality switching cost during property verification by 7 years of age. &lt;em&gt;International Journal of Behavioral Development, 35&lt;/em&gt;, 1, 78-83.&lt;/p&gt;
&lt;p&gt;Aurlien, H., Gjerde, I., Aarseth, J., Eldøen, G., Karlsen, B., Skeidsvoll, H., &amp;amp; Gilhus, N. (2003).
EEG background activity described by a large computerized database. &lt;em&gt;Clinical Neurophysiology, 115&lt;/em&gt;, 665–673.&lt;/p&gt;
&lt;p&gt;Bernabeu, P., Willems, R. M., &amp;amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, &amp;amp; E. J. Davelaar (Eds.), &lt;em&gt;Proceedings of the 39th Annual Conference of the Cognitive Science Society&lt;/em&gt; (pp. 1629-1634). Austin, TX: Cognitive Science Society.&lt;/p&gt;
&lt;p&gt;Collins, J., Pecher, D., Zeelenberg, R., &amp;amp; Coulson, S. (2011). Modality switching in a property verification task: an ERP study of what happens when candles flicker after high heels click. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Connell, L., &amp;amp; Lynott, D. (2013). Flexible and fast: Linguistic shortcut affects both shallow and deep conceptual processing. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 20&lt;/em&gt;, 542-550.&lt;/p&gt;
&lt;p&gt;Di Flumeri, G., Aricò, P., Borghini, G., Sciaraffa, N., Di Florio, A., &amp;amp; Babiloni, F. (2019). The Dry Revolution: Evaluation of Three Different EEG Dry Electrode Types in Terms of Signal Spectral Features, Mental States Classification and Usability. &lt;em&gt;Sensors (Basel, Switzerland), 19&lt;/em&gt;(6), 1365.&lt;/p&gt;
&lt;p&gt;Gratton, G., Coles, M. G., &amp;amp; Donchin, E. (1983). A new method for offline removal of ocular artefact. &lt;em&gt;Electroencephalography and Clinical Neurophysiology, 55&lt;/em&gt;, 4, 468-484.&lt;/p&gt;
&lt;p&gt;Hald, L. A., Hocking, I., Vernon, D., Marshall, J.-A., &amp;amp; Garnham, A. (2013). Exploring modality switching effects in negated sentences: further evidence for grounded representations. &lt;em&gt;Frontiers in Psychology, 4&lt;/em&gt;, 93.&lt;/p&gt;
&lt;p&gt;Hald, L. A., Marshall, J.-A., Janssen, D. P., &amp;amp; Garnham, A. (2011). Switching modalities in a sentence verification task: ERP evidence for embodied language processing. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Hauk, O. (2016). Only time will tell—Why temporal information is essential for our neuroscientific understanding of semantics. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;, 4, 1072-1079.&lt;/p&gt;
&lt;p&gt;Kutas, M., &amp;amp; Federmeier, K. D. (2011). Thirty years and counting: finding meaning in the N400 component of the event-related brain potential (ERP). &lt;em&gt;Annual Review of Psychology, 62&lt;/em&gt;, 621–647.&lt;/p&gt;
&lt;p&gt;Laszlo, S., Ruiz-Blondet, M., Khalifian, N., Chu, F., &amp;amp; Jin, Z. (2014). A direct comparison of active and passive amplification electrodes in the same amplifier system. &lt;em&gt;Journal of Neuroscience Methods, 235&lt;/em&gt;, 298-307.&lt;/p&gt;
&lt;p&gt;Lebois, L. A. M., Wilson-Mendenhall, C. D., &amp;amp; Barsalou, L. W. (2014). Are automatic conceptual cores the gold standard of semantic processing? The context-dependence of spatial meaning in grounded congruency effects. &lt;em&gt;Cognitive Science, 39&lt;/em&gt;, 8, 1764-801.&lt;/p&gt;
&lt;p&gt;Leonard, M. K., Baud, M. O., Sjerps, M. J., &amp;amp; Chang, E. F. (2016). Perceptual restoration of masked speech in human cortex. &lt;em&gt;Nature Communications, 7&lt;/em&gt;, 13619.&lt;/p&gt;
&lt;p&gt;Luck, S. J. &amp;amp; Kappenman, E.S. (Eds.), &lt;em&gt;Oxford Handbook of Event-Related Potential Components&lt;/em&gt;. New York: Oxford University Press&lt;/p&gt;
&lt;p&gt;Mahon, B.Z., &amp;amp; Caramazza, A. (2008). A critical look at the Embodied Cognition Hypothesis and a new proposal for grounding conceptual content. &lt;em&gt;Journal of Physiology - Paris, 102&lt;/em&gt;, 59-70.&lt;/p&gt;
&lt;p&gt;Marques, J. F. (2006). Specialization and semantic organization: Evidence for multiple semantics linked to sensory modalities. *&lt;em&gt;Memory &amp;amp; Cognition, 34&lt;/em&gt;, 1, 60-67.&lt;/p&gt;
&lt;p&gt;Mordkoff, J. T., &amp;amp; Gianaros, P. J. (2000). Detecting the onset of the lateralized readiness potential: A comparison of available methods and procedures. &lt;em&gt;Psychophysiology, 37&lt;/em&gt;(3), 347–360.&lt;/p&gt;
&lt;p&gt;Pecher, D., Zeelenberg, R., &amp;amp; Barsalou, L. W. (2003). Verifying different-modality properties for concepts produces switching costs. &lt;em&gt;Psychological Science, 14&lt;/em&gt;, 2, 119-24.&lt;/p&gt;
&lt;p&gt;____ (2004). Sensorimotor simulations underlie conceptual representations: Modality-specific effects of prior activation. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 11&lt;/em&gt;, 1, 164-167.&lt;/p&gt;
&lt;p&gt;Pexman, P. M., Muraki, E. J., Sidhu, D. M., Siakaluk, P. D., &amp;amp; Yap, M. J. (2019). Quantifying sensorimotor experience: Body-object interaction ratings for more than 9,000 English words. &lt;em&gt;Behavior Research Methods, 51&lt;/em&gt;, 453-466.&lt;/p&gt;
&lt;p&gt;Pulvermüller, F., Shtyrov, Y., &amp;amp; Hauk, O. (2009). Understanding in an instant: Neurophysiological evidence for mechanistic language circuits in the brain. &lt;em&gt;Brain and Language, 110&lt;/em&gt;, 2, 81–94.&lt;/p&gt;
&lt;p&gt;Roohi-Azizi, M., Azimi, L., Heysieattalab, S., &amp;amp; Aamidfar, M. (2017). Changes of the brain&#39;s bioelectrical activity in cognition, consciousness, and some mental disorders. &lt;em&gt;Medical journal of the Islamic Republic of Iran, 31&lt;/em&gt;, 53.&lt;/p&gt;
&lt;p&gt;Sato, M., Mengarelli, M., Riggio, L., Gallese, V., &amp;amp; Buccino, G. (2008). Task related modulation of the motor system during language processing. &lt;em&gt;Brain and Language, 105&lt;/em&gt;, 83–90.&lt;/p&gt;
&lt;p&gt;Scerrati, E., Baroni, G., Borghi, A. M., Galatolo, R., Lugli, L., &amp;amp; Nicoletti, R. (2015). The modality-switch effect: visually and aurally presented prime sentences activate our senses. &lt;em&gt;Frontiers in Psychology, 6&lt;/em&gt;, 1668.&lt;/p&gt;
&lt;p&gt;Shtyrov, Y., Hauk, O., &amp;amp; Pulvermüller, F. (2004). Distributed neuronal networks for encoding category-specific semantic information: the mismatch negativity to action words. &lt;em&gt;European Journal of Neuroscience, 1&lt;/em&gt;, 4, 1083–1092.&lt;/p&gt;
&lt;p&gt;Solomon, K. O., &amp;amp; Barsalou, L. W. (2004). Perceptual simulation in property verification. &lt;em&gt;Memory &amp;amp; Cognition, 32&lt;/em&gt;, 244-259.&lt;/p&gt;
&lt;p&gt;Swaab, T.Y., Ledoux, K., Camblin, C.C., &amp;amp; Boudewyn, M.A. (2012) Language related ERP components. (Book Chapter). In Luck, S. J. &amp;amp; Kappenman, E.S. (Eds.), &lt;em&gt;Oxford Handbook of Event-Related Potential Components&lt;/em&gt; (pp. 397-440). New York: Oxford University Press&lt;/p&gt;
&lt;p&gt;Van Dantzig, S., Pecher, D., Zeelenberg, R., &amp;amp; Barsalou, L. W. (2008). Perceptual processing affects conceptual processing. &lt;em&gt;Cognitive Science, 32&lt;/em&gt;, 579–590.&lt;/p&gt;
&lt;p&gt;Van Hell, J. G., &amp;amp; Kroll, J. F. (2013). Using electrophysiological measures to track the mapping of words to concepts in the bilingual brain: a focus on translation. In J. Altarriba &amp;amp; L. Isurin (Eds.), &lt;em&gt;Memory, Language, and Bilingualism: Theoretical and Applied Approaches&lt;/em&gt; (pp. 126-160). New York: Cambridge University Press.&lt;/p&gt;
&lt;p&gt;Van Vliet, M., Manyakov, N., Storms, G., Fias, W., Wiersema, J., &amp;amp; Van Hulle, M. (2014). Response-Related Potentials during semantic priming: the effect of a speeded button response task on ERPs. &lt;em&gt;PLoS One, 9&lt;/em&gt;, 2, e87650.&lt;/p&gt;
&lt;p&gt;Vermeulen, N., Niedenthal, P. M., &amp;amp; Luminet, O. (2007). Switching between sensory and affective systems incurs processing costs. &lt;em&gt;Cognitive Science, 31&lt;/em&gt;, 1, 183-192.&lt;/p&gt;
&lt;p&gt;Willems, R. M., Frank, S. L., Nijhoff, A. D., Hagoort, P., &amp;amp; Van den Bosch, A. (2016). Prediction during natural language comprehension. &lt;em&gt;Cerebral Cortex, 26&lt;/em&gt;, 6, 2506-2516.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dutch modality exclusivity norms for 336 properties and 411 concepts</title>
      <link>https://pablobernabeu.github.io/publication/dutch-modality-exclusivity-norms-for-336-properties-and-411-concepts/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/publication/dutch-modality-exclusivity-norms-for-336-properties-and-411-concepts/</guid>
      <description>&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;div style = &#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P. (2018). &lt;em&gt;Dutch modality exclusivity norms for 336 properties and 411 concepts&lt;/em&gt;. PsyArXiv. &lt;a href=&#34;https://doi.org/10.31234/osf.io/s2c5h&#34;&gt;https://doi.org/10.31234/osf.io/s2c5h&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;h3 id=&#34;related-references&#34;&gt;Related references&lt;/h3&gt;
&lt;div style = &#34;text-indent: -2em; margin-left: 2em; color: darkgrey;&#34;&gt;
&lt;p&gt;Anderson, A. J., Binder, J. R., Fernandino, L., Humphries, C. J., Conant, L. L., Aguilar, M., Wang, X., Doko, D., &amp;amp; Raizada, R. D. S. (2016). Predicting Neural Activity Patterns Associated with Sentences Using a Neurobiologically Motivated Model of Semantic Representation. &lt;em&gt;Cerebral Cortex&lt;/em&gt;, cercor;bhw240v1. &lt;a href=&#34;https://doi.org/10.1093/cercor/bhw240&#34;&gt;https://doi.org/10.1093/cercor/bhw240&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Anderson, A. J., Binder, J. R., Fernandino, L., Humphries, C. J., Conant, L. L., Raizada, R. D. S., Lin, F., &amp;amp; Lalor, E. C. (2019). An Integrated Neural Decoder of Linguistic and Experiential Meaning. &lt;em&gt;The Journal of Neuroscience&lt;/em&gt;, &lt;em&gt;39&lt;/em&gt;(45), 8969&amp;ndash;8987. &lt;a href=&#34;https://doi.org/10.1523/JNEUROSCI.2575-18.2019&#34;&gt;https://doi.org/10.1523/JNEUROSCI.2575-18.2019&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Anderson, A. J., &amp;amp; Lin, F. (2019). How pattern information analyses of semantic brain activity elicited in language comprehension could contribute to the early identification of Alzheimer&#39;s Disease. &lt;em&gt;NeuroImage: Clinical&lt;/em&gt;, &lt;em&gt;22&lt;/em&gt;, 101788. &lt;a href=&#34;https://doi.org/10.1016/j.nicl.2019.101788&#34;&gt;https://doi.org/10.1016/j.nicl.2019.101788&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bagli, M. (2023). How to Point with Language: English Source-Based Language to Describe Taste Qualities. &lt;em&gt;Lublin Studies in Modern Languages and Literature&lt;/em&gt;, &lt;em&gt;42&lt;/em&gt;(2), 31&amp;ndash;46. &lt;a href=&#34;https://doi.org/10.17951/lsmll.2023.47.2.31-46&#34;&gt;https://doi.org/10.17951/lsmll.2023.47.2.31-46&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Banks, B., Borghi, A. M., Fargier, R., Fini, C., Jonauskaite, D., Mazzuca, C., Montalti, M., Villani, C., &amp;amp; Woodin, G. (2023). Consensus Paper: Current Perspectives on Abstract Concepts and Future Research Directions. &lt;em&gt;Journal of Cognition&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;(1), 62. &lt;a href=&#34;https://doi.org/10.5334/joc.238&#34;&gt;https://doi.org/10.5334/joc.238&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bolognesi, M., Burgers, C., &amp;amp; Caselli, T. (2020). On abstraction: Decoupling conceptual concreteness and categorical specificity. &lt;em&gt;Cognitive Processing&lt;/em&gt;, &lt;em&gt;21&lt;/em&gt;(3), 365&amp;ndash;381. &lt;a href=&#34;https://doi.org/10.1007/s10339-020-00965-9&#34;&gt;https://doi.org/10.1007/s10339-020-00965-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Borghi, A. M., Mazzuca, C., Gervasi, A. M., Mannella, F., &amp;amp; Tummolini, L. (2023). Grounded cognition can be multimodal all the way down. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, 1&amp;ndash;5. &lt;a href=&#34;https://doi.org/10.1080/23273798.2023.2210238&#34;&gt;https://doi.org/10.1080/23273798.2023.2210238&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bottini, R., Morucci, P., D&#39;Urso, A., Collignon, O., &amp;amp; Crepaldi, D. (2022). The concreteness advantage in lexical decision does not depend on perceptual simulations. &lt;em&gt;Journal of Experimental Psychology: General&lt;/em&gt;, &lt;em&gt;151&lt;/em&gt;(3), 731&amp;ndash;738. &lt;a href=&#34;https://doi.org/10.1037/xge0001090&#34;&gt;https://doi.org/10.1037/xge0001090&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bruffaerts, R., De Deyne, S., Meersmans, K., Liuzzi, A. G., Storms, G., &amp;amp; Vandenberghe, R. (2019). Redefining the resolution of semantic knowledge in the brain: Advances made by the introduction of models of semantics in neuroimaging. &lt;em&gt;Neuroscience &amp;amp; Biobehavioral Reviews&lt;/em&gt;, &lt;em&gt;103&lt;/em&gt;, 3&amp;ndash;13. &lt;a href=&#34;https://doi.org/10.1016/j.neubiorev.2019.05.015&#34;&gt;https://doi.org/10.1016/j.neubiorev.2019.05.015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Caballero, R., &amp;amp; Paradis, C. (2020). Soundscapes in English and Spanish: A corpus investigation of verb constructions. &lt;em&gt;Language and Cognition&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(4), 705&amp;ndash;728. &lt;a href=&#34;https://doi.org/10.1017/langcog.2020.19&#34;&gt;https://doi.org/10.1017/langcog.2020.19&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Caballero, R., &amp;amp; Paradis, C. (2023). Sharing Perceptual Experiences through Language. &lt;em&gt;Journal of Intelligence&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt;(7), 129. &lt;a href=&#34;https://doi.org/10.3390/jintelligence11070129&#34;&gt;https://doi.org/10.3390/jintelligence11070129&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Calzavarini, F. (2023). Rethinking modality-specificity in the cognitive neuroscience of concrete word meaning: A position paper. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, 1&amp;ndash;23. &lt;a href=&#34;https://doi.org/10.1080/23273798.2023.2173789&#34;&gt;https://doi.org/10.1080/23273798.2023.2173789&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Carney, J. (2020). Thinking avant la lettre: A Review of 4E Cognition. &lt;em&gt;Evolutionary Studies in Imaginative Culture&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(1), 77&amp;ndash;90. &lt;a href=&#34;https://doi.org/10.26613/esic.4.1.172&#34;&gt;https://doi.org/10.26613/esic.4.1.172&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Charmhun Jo, Sun-A Kim, &amp;amp; Chu-Ren Huang. (2022). Linguistic synesthesia in Korean: A compound word-based study of cross-modal directionality. &lt;em&gt;Linguistic Research&lt;/em&gt;, &lt;em&gt;39&lt;/em&gt;(2), 275&amp;ndash;296. &lt;a href=&#34;https://doi.org/10.17250/KHISLI.39.2.202206.002&#34;&gt;https://doi.org/10.17250/KHISLI.39.2.202206.002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chedid, G., Brambati, S. M., Bedetti, C., Rey, A. E., Wilson, M. A., &amp;amp; Vallet, G. T. (2019). Visual and auditory perceptual strength norms for 3,596 French nouns and their relationship with other psycholinguistic variables. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;51&lt;/em&gt;(5), 2094&amp;ndash;2105. &lt;a href=&#34;https://doi.org/10.3758/s13428-019-01254-w&#34;&gt;https://doi.org/10.3758/s13428-019-01254-w&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chedid, G., Wilson, M. A., Bedetti, C., Rey, A. E., Vallet, G. T., &amp;amp; Brambati, S. M. (2019). Norms of conceptual familiarity for 3,596 French nouns and their contribution in lexical decision. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;51&lt;/em&gt;(5), 2238&amp;ndash;2247. &lt;a href=&#34;https://doi.org/10.3758/s13428-018-1106-8&#34;&gt;https://doi.org/10.3758/s13428-018-1106-8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chen, I.-H., Zhao, Q., Long, Y., Lu, Q., &amp;amp; Huang, C.-R. (2019). Mandarin Chinese modality exclusivity norms. &lt;em&gt;PLOS ONE&lt;/em&gt;, &lt;em&gt;14&lt;/em&gt;(2), e0211336. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0211336&#34;&gt;https://doi.org/10.1371/journal.pone.0211336&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Connell, L. (2019). What have labels ever done for us? The linguistic shortcut in conceptual processing. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;34&lt;/em&gt;(10), 1308&amp;ndash;1318. &lt;a href=&#34;https://doi.org/10.1080/23273798.2018.1471512&#34;&gt;https://doi.org/10.1080/23273798.2018.1471512&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Connell, L., Lynott, D., &amp;amp; Banks, B. (2018). Interoception: The forgotten modality in perceptual grounding of abstract and concrete concepts. &lt;em&gt;Philosophical Transactions of the Royal Society B: Biological Sciences&lt;/em&gt;, &lt;em&gt;373&lt;/em&gt;(1752), 20170143. &lt;a href=&#34;https://doi.org/10.1098/rstb.2017.0143&#34;&gt;https://doi.org/10.1098/rstb.2017.0143&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Corciulo, S., Bioglio, L., Basile, V., Patti, V., &amp;amp; Damiano, R. (2023). The DEEP Sensorium: A multidimensional approach to sensory domain labelling. &lt;em&gt;Companion Proceedings of the ACM Web Conference 2023&lt;/em&gt;, 661&amp;ndash;668. &lt;a href=&#34;https://doi.org/10.1145/3543873.3587631&#34;&gt;https://doi.org/10.1145/3543873.3587631&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Davis, C. P., &amp;amp; Yee, E. (2021). Building semantic memory from embodied and distributional language experience. &lt;em&gt;WIREs Cognitive Science&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(5), e1555. &lt;a href=&#34;https://doi.org/10.1002/wcs.1555&#34;&gt;https://doi.org/10.1002/wcs.1555&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;De Deyne, S., Navarro, D. J., Perfors, A., Brysbaert, M., &amp;amp; Storms, G. (2019). The &amp;ldquo;Small World of Words&amp;rdquo; English word association norms for over 12,000 cue words. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;51&lt;/em&gt;(3), 987&amp;ndash;1006. &lt;a href=&#34;https://doi.org/10.3758/s13428-018-1115-7&#34;&gt;https://doi.org/10.3758/s13428-018-1115-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dellantonio, S., &amp;amp; Pastore, L. (2017). The &amp;lsquo;Proprioceptive&amp;rsquo; Component of Abstract Concepts. In S. Dellantonio &amp;amp; L. Pastore, &lt;em&gt;Internal Perception&lt;/em&gt; (Vol. 40, pp. 297&amp;ndash;357). Springer Berlin Heidelberg. &lt;a href=&#34;https://doi.org/10.1007/978-3-662-55763-1_6&#34;&gt;https://doi.org/10.1007/978-3-662-55763-1_6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Díez-Álamo, A. M., Díez, E., Alonso, M. Á., Vargas, C. A., &amp;amp; Fernandez, A. (2018). Normative ratings for perceptual and motor attributes of 750 object concepts in Spanish. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;50&lt;/em&gt;(4), 1632&amp;ndash;1644. &lt;a href=&#34;https://doi.org/10.3758/s13428-017-0970-y&#34;&gt;https://doi.org/10.3758/s13428-017-0970-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Díez-Álamo, A. M., Díez, E., Wojcik, D. Z., Alonso, M. A., &amp;amp; Fernandez, A. (2019). Sensory experience ratings for 5,500 Spanish words. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;51&lt;/em&gt;(3), 1205&amp;ndash;1215. &lt;a href=&#34;https://doi.org/10.3758/s13428-018-1057-0&#34;&gt;https://doi.org/10.3758/s13428-018-1057-0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dove, G. (2021). The Challenges of Abstract Concepts. In M. D. Robinson &amp;amp; L. E. Thomas (Eds.), &lt;em&gt;Handbook of Embodied Psychology&lt;/em&gt; (pp. 171&amp;ndash;195). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-78471-3_8&#34;&gt;https://doi.org/10.1007/978-3-030-78471-3_8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dymarska, A., Connell, L., &amp;amp; Banks, B. (2023). More is not necessarily better: How different aspects of sensorimotor experience affect recognition memory for words. &lt;em&gt;Journal of Experimental Psychology: Learning, Memory, and Cognition&lt;/em&gt;, &lt;em&gt;49&lt;/em&gt;(10), 1572&amp;ndash;1587. &lt;a href=&#34;https://doi.org/10.1037/xlm0001265&#34;&gt;https://doi.org/10.1037/xlm0001265&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fischer, M. H., &amp;amp; Shaki, S. (2018). Number concepts: Abstract and embodied. &lt;em&gt;Philosophical Transactions of the Royal Society B: Biological Sciences&lt;/em&gt;, &lt;em&gt;373&lt;/em&gt;(1752), 20170125. &lt;a href=&#34;https://doi.org/10.1098/rstb.2017.0125&#34;&gt;https://doi.org/10.1098/rstb.2017.0125&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fishman, A. (2022). The picture looks like my music sounds: Directional preferences in synesthetic metaphors in the absence of lexical factors. &lt;em&gt;Language and Cognition&lt;/em&gt;, &lt;em&gt;14&lt;/em&gt;(2), 208&amp;ndash;227. &lt;a href=&#34;https://doi.org/10.1017/langcog.2022.2&#34;&gt;https://doi.org/10.1017/langcog.2022.2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Gangemi, A. (2020). Closing the Loop between knowledge patterns in cognition and the Semantic Web. &lt;em&gt;Semantic Web&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt;(1), 139&amp;ndash;151. &lt;a href=&#34;https://doi.org/10.3233/SW-190383&#34;&gt;https://doi.org/10.3233/SW-190383&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ghandhari, M., Fini, C., Da Rold, F., &amp;amp; Borghi, A. M. (2020). Different kinds of embodied language: A comparison between Italian and Persian languages. &lt;em&gt;Brain and Cognition&lt;/em&gt;, &lt;em&gt;142&lt;/em&gt;, 105581. &lt;a href=&#34;https://doi.org/10.1016/j.bandc.2020.105581&#34;&gt;https://doi.org/10.1016/j.bandc.2020.105581&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Gijssels, T., &amp;amp; Casasanto, D. (2020). Hand-use norms for Dutch and English manual action verbs: Implicit measures from a pantomime task. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;52&lt;/em&gt;(4), 1744&amp;ndash;1767. &lt;a href=&#34;https://doi.org/10.3758/s13428-020-01347-x&#34;&gt;https://doi.org/10.3758/s13428-020-01347-x&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Harpaintner, M., Sim, E.-J., Trumpp, N. M., Ulrich, M., &amp;amp; Kiefer, M. (2020). The grounding of abstract concepts in the motor and visual system: An fMRI study. &lt;em&gt;Cortex&lt;/em&gt;, &lt;em&gt;124&lt;/em&gt;, 1&amp;ndash;22. &lt;a href=&#34;https://doi.org/10.1016/j.cortex.2019.10.014&#34;&gt;https://doi.org/10.1016/j.cortex.2019.10.014&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Harpaintner, M., Trumpp, N. M., &amp;amp; Kiefer, M. (2018). The Semantic Content of Abstract Concepts: A Property Listing Study of 296 Abstract Words. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;, 1748. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2018.01748&#34;&gt;https://doi.org/10.3389/fpsyg.2018.01748&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Harpaintner, M., Trumpp, N. M., &amp;amp; Kiefer, M. (2022). Time course of brain activity during the processing of motor- and vision-related abstract concepts: Flexibility and task dependency. &lt;em&gt;Psychological Research&lt;/em&gt;, &lt;em&gt;86&lt;/em&gt;(8), 2560&amp;ndash;2582. &lt;a href=&#34;https://doi.org/10.1007/s00426-020-01374-5&#34;&gt;https://doi.org/10.1007/s00426-020-01374-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hartman, J., &amp;amp; Paradis, C. (2023). The language of sound: Events and meaning multitasking of words. &lt;em&gt;Cognitive Linguistics&lt;/em&gt;, &lt;em&gt;34&lt;/em&gt;(3&amp;ndash;4), 445&amp;ndash;477. &lt;a href=&#34;https://doi.org/10.1515/cog-2022-0006&#34;&gt;https://doi.org/10.1515/cog-2022-0006&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hörberg, T., Larsson, M., &amp;amp; Olofsson, J. K. (2022). The Semantic Organization of the English Odor Vocabulary. &lt;em&gt;Cognitive Science&lt;/em&gt;, &lt;em&gt;46&lt;/em&gt;(11), e13205. &lt;a href=&#34;https://doi.org/10.1111/cogs.13205&#34;&gt;https://doi.org/10.1111/cogs.13205&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Huang, C.-R., &amp;amp; Xiong, J. (2019). Linguistic synaesthesia in Chinese. In C.-R. Huang, Z. Jing-Schmidt, &amp;amp; B. Meisterernst (Eds.), &lt;em&gt;The Routledge Handbook of Chinese Applied Linguistics&lt;/em&gt; (1st ed., pp. 294&amp;ndash;312). Routledge. &lt;a href=&#34;https://doi.org/10.4324/9781315625157-20&#34;&gt;https://doi.org/10.4324/9781315625157-20&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Iatropoulos, G., Herman, P., Lansner, A., Karlgren, J., Larsson, M., &amp;amp; Olofsson, J. K. (2018). The language of smell: Connecting linguistic and psychophysical properties of odor descriptors. &lt;em&gt;Cognition&lt;/em&gt;, &lt;em&gt;178&lt;/em&gt;, 37&amp;ndash;49. &lt;a href=&#34;https://doi.org/10.1016/j.cognition.2018.05.007&#34;&gt;https://doi.org/10.1016/j.cognition.2018.05.007&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Jacobs, A. M., &amp;amp; Kinder, A. (2017). &lt;em&gt;&amp;ldquo;The Brain Is the Prisoner of Thought&amp;rdquo;&lt;/em&gt;: A Machine-Learning Assisted Quantitative Narrative Analysis of Literary Metaphors for Use in Neurocognitive Poetics. &lt;em&gt;Metaphor and Symbol&lt;/em&gt;, &lt;em&gt;32&lt;/em&gt;(3), 139&amp;ndash;160. &lt;a href=&#34;https://doi.org/10.1080/10926488.2017.1338015&#34;&gt;https://doi.org/10.1080/10926488.2017.1338015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Jo, C. (2022). Linguistic Synesthesia in Korean: Universality and Variation. &lt;em&gt;SAGE Open&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(3), 215824402211178. &lt;a href=&#34;https://doi.org/10.1177/21582440221117804&#34;&gt;https://doi.org/10.1177/21582440221117804&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Johns, B. T. (2022). Accounting for item-level variance in recognition memory: Comparing word frequency and contextual diversity. &lt;em&gt;Memory &amp;amp; Cognition&lt;/em&gt;, &lt;em&gt;50&lt;/em&gt;(5), 1013&amp;ndash;1032. &lt;a href=&#34;https://doi.org/10.3758/s13421-021-01249-z&#34;&gt;https://doi.org/10.3758/s13421-021-01249-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Jones, L. L., Wurm, L. H., Calcaterra, R. D., &amp;amp; Ofen, N. (2017). Integrative Priming of Compositional and Locative Relations. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2017.00359&#34;&gt;https://doi.org/10.3389/fpsyg.2017.00359&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Julich-Warpakowski, N., &amp;amp; Pérez Sobrino, P. (2023). Introduction: Current challenges in metaphor research. &lt;em&gt;Metaphor and the Social World&lt;/em&gt;, &lt;em&gt;13&lt;/em&gt;(1), 1&amp;ndash;15. &lt;a href=&#34;https://doi.org/10.1075/msw.00026.jul&#34;&gt;https://doi.org/10.1075/msw.00026.jul&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kaiser, E. (2021). Consequences of Sensory Modality for Perspective-Taking: Comparing Visual, Olfactory and Gustatory Perception. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;, 701486. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2021.701486&#34;&gt;https://doi.org/10.3389/fpsyg.2021.701486&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kernot, D., Bossomaier, T., &amp;amp; Bradbury, R. (2017). Novel Text Analysis for Investigating Personality: Identifying the Dark Lady in Shakespeare&#39;s Sonnets. &lt;em&gt;Journal of Quantitative Linguistics&lt;/em&gt;, &lt;em&gt;24&lt;/em&gt;(4), 255&amp;ndash;272. &lt;a href=&#34;https://doi.org/10.1080/09296174.2017.1304049&#34;&gt;https://doi.org/10.1080/09296174.2017.1304049&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kernot, D., Bossomaier, T., &amp;amp; Bradbury, R. (2018). Using Shakespeare&#39;s Sotto Voce to Determine True Identity From Text. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;, 289. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2018.00289&#34;&gt;https://doi.org/10.3389/fpsyg.2018.00289&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kernot, D., Bossomaier, T., &amp;amp; Bradbury, R. (2019). The Stylometric Impacts of Ageing and Life Events on Identity. &lt;em&gt;Journal of Quantitative Linguistics&lt;/em&gt;, &lt;em&gt;26&lt;/em&gt;(1), 1&amp;ndash;21. &lt;a href=&#34;https://doi.org/10.1080/09296174.2017.1405719&#34;&gt;https://doi.org/10.1080/09296174.2017.1405719&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Khatin-Zadeh, O., Hu, J., Banaruee, H., &amp;amp; Marmolejo-Ramos, F. (2023). How emotions are metaphorically embodied: Measuring hand and head action strengths of typical emotional states. &lt;em&gt;Cognition and Emotion&lt;/em&gt;, &lt;em&gt;37&lt;/em&gt;(3), 486&amp;ndash;498. &lt;a href=&#34;https://doi.org/10.1080/02699931.2023.2181314&#34;&gt;https://doi.org/10.1080/02699931.2023.2181314&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kiefer, M., Pielke, L., &amp;amp; Trumpp, N. M. (2022). Differential temporo-spatial pattern of electrical brain activity during the processing of abstract concepts related to mental states and verbal associations. &lt;em&gt;NeuroImage&lt;/em&gt;, &lt;em&gt;252&lt;/em&gt;, 119036. &lt;a href=&#34;https://doi.org/10.1016/j.neuroimage.2022.119036&#34;&gt;https://doi.org/10.1016/j.neuroimage.2022.119036&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kim, M.-K., Müller, H. M., &amp;amp; Weiss, S. (2021). What you &amp;ldquo;mean&amp;rdquo; is not what I &amp;ldquo;mean&amp;rdquo;: Categorization of verbs by Germans and Koreans using the semantic differential. &lt;em&gt;Lingua&lt;/em&gt;, &lt;em&gt;252&lt;/em&gt;, 103012. &lt;a href=&#34;https://doi.org/10.1016/j.lingua.2020.103012&#34;&gt;https://doi.org/10.1016/j.lingua.2020.103012&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Koblet, O., &amp;amp; Purves, R. S. (2020). From online texts to Landscape Character Assessment: Collecting and analysing first-person landscape perception computationally. &lt;em&gt;Landscape and Urban Planning&lt;/em&gt;, &lt;em&gt;197&lt;/em&gt;, 103757. &lt;a href=&#34;https://doi.org/10.1016/j.landurbplan.2020.103757&#34;&gt;https://doi.org/10.1016/j.landurbplan.2020.103757&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Körner, A., Castillo, M., Drijvers, L., Fischer, M. H., Günther, F., Marelli, M., Platonova, O., Rinaldi, L., Shaki, S., Trujillo, J. P., Tsaregorodtseva, O., &amp;amp; Glenberg, A. M. (2023). Embodied Processing at Six Linguistic Granularity Levels: A Consensus Paper. &lt;em&gt;Journal of Cognition&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;(1), 60. &lt;a href=&#34;https://doi.org/10.5334/joc.231&#34;&gt;https://doi.org/10.5334/joc.231&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Krishna, P. P., Arulmozi, S., &amp;amp; Mishra, R. K. (2022). &amp;ldquo;Do You See and Hear More? A Study on Telugu Perception Verbs.&amp;rdquo; &lt;em&gt;Journal of Psycholinguistic Research&lt;/em&gt;, &lt;em&gt;51&lt;/em&gt;(3), 473&amp;ndash;484. &lt;a href=&#34;https://doi.org/10.1007/s10936-021-09827-7&#34;&gt;https://doi.org/10.1007/s10936-021-09827-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kumcu, A. (2021). Linguistic Synesthesia in Turkish: A Corpus-based Study of Crossmodal Directionality. &lt;em&gt;Metaphor and Symbol&lt;/em&gt;, &lt;em&gt;36&lt;/em&gt;(4), 241&amp;ndash;255. &lt;a href=&#34;https://doi.org/10.1080/10926488.2021.1921557&#34;&gt;https://doi.org/10.1080/10926488.2021.1921557&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lau, S. H., Huang, Y., Ferreira, V. S., &amp;amp; Vul, E. (2019). Perceptual features predict word frequency asymmetry across modalities. &lt;em&gt;Attention, Perception, &amp;amp; Psychophysics&lt;/em&gt;, &lt;em&gt;81&lt;/em&gt;(4), 1076&amp;ndash;1087. &lt;a href=&#34;https://doi.org/10.3758/s13414-019-01682-y&#34;&gt;https://doi.org/10.3758/s13414-019-01682-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lee, J., &amp;amp; Shin, J.-A. (2023). The cross-linguistic comparison of perceptual strength norms for Korean, English and L2 English. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;14&lt;/em&gt;, 1188909. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2023.1188909&#34;&gt;https://doi.org/10.3389/fpsyg.2023.1188909&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Li, M., Lu, Q., Long, Y., &amp;amp; Gui, L. (2017). Inferring Affective Meanings of Words from Word Embedding. &lt;em&gt;IEEE Transactions on Affective Computing&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;(4), 443&amp;ndash;456. &lt;a href=&#34;https://doi.org/10.1109/TAFFC.2017.2723012&#34;&gt;https://doi.org/10.1109/TAFFC.2017.2723012&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Littlemore, J., Sobrino, P. P., Houghton, D., Shi, J., &amp;amp; Winter, B. (2018). What makes a good metaphor? A cross-cultural study of computer-generated metaphor appreciation. &lt;em&gt;Metaphor and Symbol&lt;/em&gt;, &lt;em&gt;33&lt;/em&gt;(2), 101&amp;ndash;122. &lt;a href=&#34;https://doi.org/10.1080/10926488.2018.1434944&#34;&gt;https://doi.org/10.1080/10926488.2018.1434944&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, W., Bansal, D., Daruna, A., &amp;amp; Chernova, S. (2023). Learning instance-level N-ary semantic knowledge at scale for robots operating in everyday environments. &lt;em&gt;Autonomous Robots&lt;/em&gt;, &lt;em&gt;47&lt;/em&gt;(5), 529&amp;ndash;547. &lt;a href=&#34;https://doi.org/10.1007/s10514-023-10099-4&#34;&gt;https://doi.org/10.1007/s10514-023-10099-4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, W., Bansal, D., Daruna, A., &amp;amp; Chernova, S. (2021, July 12). Learning Instance-Level N-Ary Semantic Knowledge At Scale For Robots Operating in Everyday Environments. &lt;em&gt;Robotics: Science and Systems XVII&lt;/em&gt;. Robotics: Science and Systems 2021. &lt;a href=&#34;https://doi.org/10.15607/RSS.2021.XVII.035&#34;&gt;https://doi.org/10.15607/RSS.2021.XVII.035&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Long, Y., Xiang, R., Lu, Q., Huang, C.-R., &amp;amp; Li, M. (2021). Improving Attention Model Based on Cognition Grounded Data for Sentiment Analysis. &lt;em&gt;IEEE Transactions on Affective Computing&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(4), 900&amp;ndash;912. &lt;a href=&#34;https://doi.org/10.1109/TAFFC.2019.2903056&#34;&gt;https://doi.org/10.1109/TAFFC.2019.2903056&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lynott, D., Connell, L., Brysbaert, M., Brand, J., &amp;amp; Carney, J. (2020). The Lancaster Sensorimotor Norms: Multidimensional measures of perceptual and action strength for 40,000 English words. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;52&lt;/em&gt;(3), 1271&amp;ndash;1291. &lt;a href=&#34;https://doi.org/10.3758/s13428-019-01316-z&#34;&gt;https://doi.org/10.3758/s13428-019-01316-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Mahmood, A., &amp;amp; Yeganegi, S. (2023). Lexical sophistication and crowdfunding outcomes. &lt;em&gt;Venture Capital&lt;/em&gt;, 1&amp;ndash;32. &lt;a href=&#34;https://doi.org/10.1080/13691066.2023.2265565&#34;&gt;https://doi.org/10.1080/13691066.2023.2265565&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Marson, F., Paoletti, P., Naor-Ziv, R., Carducci, F., &amp;amp; Ben-Soussan, T. D. (2023). Embodied empathy and abstract concepts&amp;rsquo; concreteness: Evidence from contemplative practices. In &lt;em&gt;Progress in Brain Research&lt;/em&gt; (Vol. 277, pp. 181&amp;ndash;209). Elsevier. &lt;a href=&#34;https://doi.org/10.1016/bs.pbr.2022.12.005&#34;&gt;https://doi.org/10.1016/bs.pbr.2022.12.005&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Márton, Z. C., Türker, S., Rink, C., Brucker, M., Kriegel, S., Bodenmüller, T., &amp;amp; Riedel, S. (2018). Improving object orientation estimates by considering multiple viewpoints: Orientation histograms of symmetries and measurement models for view selection. &lt;em&gt;Autonomous Robots&lt;/em&gt;, &lt;em&gt;42&lt;/em&gt;(2), 423&amp;ndash;442. &lt;a href=&#34;https://doi.org/10.1007/s10514-017-9633-1&#34;&gt;https://doi.org/10.1007/s10514-017-9633-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Miceli, A., Wauthia, E., Kandana Arachchige, K., Lefebvre, L., Ris, L., &amp;amp; Simoes Loureiro, I. (2023). Perceptual strength influences lexical decision in Alzheimer&#39;s disease. &lt;em&gt;Journal of Neurolinguistics&lt;/em&gt;, &lt;em&gt;68&lt;/em&gt;, 101144. &lt;a href=&#34;https://doi.org/10.1016/j.jneuroling.2023.101144&#34;&gt;https://doi.org/10.1016/j.jneuroling.2023.101144&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Miceli, A., Wauthia, E., Lefebvre, L., Ris, L., &amp;amp; Simoes Loureiro, I. (2021). Perceptual and Interoceptive Strength Norms for 270 French Words. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;, 667271. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2021.667271&#34;&gt;https://doi.org/10.3389/fpsyg.2021.667271&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Miceli, A., Wauthia, E., Lefebvre, L., Vallet, G. T., Ris, L., &amp;amp; Loureiro, I. S. (2022). Differences related to aging in sensorimotor knowledge: Investigation of perceptual strength and body object interaction. &lt;em&gt;Archives of Gerontology and Geriatrics&lt;/em&gt;, &lt;em&gt;102&lt;/em&gt;, 104715. &lt;a href=&#34;https://doi.org/10.1016/j.archger.2022.104715&#34;&gt;https://doi.org/10.1016/j.archger.2022.104715&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Miklashevsky, A. (2018). Perceptual Experience Norms for 506 Russian Nouns: Modality Rating, Spatial Localization, Manipulability, Imageability and Other Variables. &lt;em&gt;Journal of Psycholinguistic Research&lt;/em&gt;, &lt;em&gt;47&lt;/em&gt;(3), 641&amp;ndash;661. &lt;a href=&#34;https://doi.org/10.1007/s10936-017-9548-1&#34;&gt;https://doi.org/10.1007/s10936-017-9548-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Morucci, P., Bottini, R., &amp;amp; Crepaldi, D. (2019). Augmented Modality Exclusivity Norms for Concrete and Abstract Italian Property Words. &lt;em&gt;Journal of Cognition&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;(1), 42. &lt;a href=&#34;https://doi.org/10.5334/joc.88&#34;&gt;https://doi.org/10.5334/joc.88&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Okuno, H. Y., &amp;amp; Guedes, G. (2020). Automatic XML creation for Multisensorial Books. &lt;em&gt;2020 XV Conferencia Latinoamericana de Tecnologias de Aprendizaje (LACLO)&lt;/em&gt;, 1&amp;ndash;6. &lt;a href=&#34;https://doi.org/10.1109/LACLO50806.2020.9381139&#34;&gt;https://doi.org/10.1109/LACLO50806.2020.9381139&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pathak, A., Velasco, C., Petit, O., &amp;amp; Calvert, G. A. (2019). Going to great lengths in the pursuit of luxury: How longer brand names can enhance the luxury perception of a brand. &lt;em&gt;Psychology &amp;amp; Marketing&lt;/em&gt;, &lt;em&gt;36&lt;/em&gt;(10), 951&amp;ndash;963. &lt;a href=&#34;https://doi.org/10.1002/mar.21247&#34;&gt;https://doi.org/10.1002/mar.21247&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pérez-Sánchez, M. Á., Stadthagen-Gonzalez, H., Guasch, M., Hinojosa, J. A., Fraga, I., Marín, J., &amp;amp; Ferré, P. (2021). EmoPro &amp;ndash; Emotional prototypicality for 1286 Spanish words: Relationships with affective and psycholinguistic variables. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;53&lt;/em&gt;(5), 1857&amp;ndash;1875. &lt;a href=&#34;https://doi.org/10.3758/s13428-020-01519-9&#34;&gt;https://doi.org/10.3758/s13428-020-01519-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Perlman, M., Little, H., Thompson, B., &amp;amp; Thompson, R. L. (2018). Iconicity in Signed and Spoken Vocabulary: A Comparison Between American Sign Language, British Sign Language, English, and Spanish. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;, 1433. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2018.01433&#34;&gt;https://doi.org/10.3389/fpsyg.2018.01433&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pexman, P. M., Muraki, E., Sidhu, D. M., Siakaluk, P. D., &amp;amp; Yap, M. J. (2019). Quantifying sensorimotor experience: Body&amp;ndash;object interaction ratings for more than 9,000 English words. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;51&lt;/em&gt;(2), 453&amp;ndash;466. &lt;a href=&#34;https://doi.org/10.3758/s13428-018-1171-z&#34;&gt;https://doi.org/10.3758/s13428-018-1171-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Plekhanov Russian University of Economics, Simonenko, M. A., Kazaryan, S. Y., &amp;amp; Plekhanov Russian University of Economics. (2023). Synaesthetic metaphor and its reproduction in Russian-to-English translation: A frame-based study. &lt;em&gt;RESEARCH RESULT Theoretical and Applied Linguistics&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;(3). &lt;a href=&#34;https://doi.org/10.18413/2313-8912-2023-9-3-0-2&#34;&gt;https://doi.org/10.18413/2313-8912-2023-9-3-0-2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pollock, L. (2018). Statistical and methodological problems with concreteness and other semantic variables: A list memory experiment case study. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;50&lt;/em&gt;(3), 1198&amp;ndash;1216. &lt;a href=&#34;https://doi.org/10.3758/s13428-017-0938-y&#34;&gt;https://doi.org/10.3758/s13428-017-0938-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Popović Stijačić, M., &amp;amp; Filipović Đurđević, D. (2022). Perceptual richness of words and its role in free and cued recall. &lt;em&gt;Primenjena Psihologija&lt;/em&gt;, &lt;em&gt;15&lt;/em&gt;(3), 355&amp;ndash;381. &lt;a href=&#34;https://doi.org/10.19090/pp.v15i3.2400&#34;&gt;https://doi.org/10.19090/pp.v15i3.2400&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Purves, R. S., Striedl, P., Kong, I., &amp;amp; Majid, A. (2023). Conceptualizing Landscapes Through Language: The Role of Native Language and Expertise in the Representation of Waterbody Related Terms. &lt;em&gt;Topics in Cognitive Science&lt;/em&gt;, &lt;em&gt;15&lt;/em&gt;(3), 560&amp;ndash;583. &lt;a href=&#34;https://doi.org/10.1111/tops.12652&#34;&gt;https://doi.org/10.1111/tops.12652&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Raj, R., Hörberg, T., Lindroos, R., Larsson, M., Herman, P., Laukka, E. J., &amp;amp; Olofsson, J. K. (2023). Odor identification errors reveal cognitive aspects of age-associated smell loss. &lt;em&gt;Cognition&lt;/em&gt;, &lt;em&gt;236&lt;/em&gt;, 105445. &lt;a href=&#34;https://doi.org/10.1016/j.cognition.2023.105445&#34;&gt;https://doi.org/10.1016/j.cognition.2023.105445&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Repetto, C., Rodella, C., Conca, F., Santi, G. C., &amp;amp; Catricalà, E. (2022). The Italian Sensorimotor Norms: Perception and action strength measures for 959 words. &lt;em&gt;Behavior Research Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3758/s13428-022-02004-1&#34;&gt;https://doi.org/10.3758/s13428-022-02004-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rey, A. E., Riou, B., Vallet, G. T., &amp;amp; Versace, R. (2017). The automatic visual simulation of words: A memory reactivated mask slows down conceptual access. &lt;em&gt;Canadian Journal of Experimental Psychology / Revue Canadienne de Psychologie Expérimentale&lt;/em&gt;, &lt;em&gt;71&lt;/em&gt;(1), 14&amp;ndash;22. &lt;a href=&#34;https://doi.org/10.1037/cep0000100&#34;&gt;https://doi.org/10.1037/cep0000100&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Reymore, L. (2022). Characterizing prototypical musical instrument timbres with timbre trait profiles. &lt;em&gt;Musicae Scientiae&lt;/em&gt;, &lt;em&gt;26&lt;/em&gt;(3), 648&amp;ndash;674. &lt;a href=&#34;https://doi.org/10.1177/10298649211001523&#34;&gt;https://doi.org/10.1177/10298649211001523&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;San Roque, L., Kendrick, K. H., Norcliffe, E., &amp;amp; Majid, A. (2018). Universal meaning extensions of perception verbs are grounded in interaction. &lt;em&gt;Cognitive Linguistics&lt;/em&gt;, &lt;em&gt;29&lt;/em&gt;(3), 371&amp;ndash;406. &lt;a href=&#34;https://doi.org/10.1515/cog-2017-0034&#34;&gt;https://doi.org/10.1515/cog-2017-0034&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Scerrati, E., Lugli, L., Nicoletti, R., &amp;amp; Borghi, A. M. (2017). The Multilevel Modality-Switch Effect: What Happens When We See the Bees Buzzing and Hear the Diamonds Glistening. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, &lt;em&gt;24&lt;/em&gt;(3), 798&amp;ndash;803. &lt;a href=&#34;https://doi.org/10.3758/s13423-016-1150-2&#34;&gt;https://doi.org/10.3758/s13423-016-1150-2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Schulte Im Walde, S., &amp;amp; Frassinelli, D. (2022). Distributional Measures of Semantic Abstraction. &lt;em&gt;Frontiers in Artificial Intelligence&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;, 796756. &lt;a href=&#34;https://doi.org/10.3389/frai.2021.796756&#34;&gt;https://doi.org/10.3389/frai.2021.796756&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Sidhu, D. M., &amp;amp; Pexman, P. M. (2018). Lonely sensational icons: Semantic neighbourhood density, sensory experience and iconicity. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;33&lt;/em&gt;(1), 25&amp;ndash;31. &lt;a href=&#34;https://doi.org/10.1080/23273798.2017.1358379&#34;&gt;https://doi.org/10.1080/23273798.2017.1358379&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Speed, L. J., &amp;amp; Brybaert, M. (2022). Dutch sensory modality norms. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;54&lt;/em&gt;(3), 1306&amp;ndash;1318. &lt;a href=&#34;https://doi.org/10.3758/s13428-021-01656-9&#34;&gt;https://doi.org/10.3758/s13428-021-01656-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Speed, L. J., &amp;amp; Majid, A. (2017). Dutch modality exclusivity norms: Simulating perceptual modality in space. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;49&lt;/em&gt;(6), 2204&amp;ndash;2218. &lt;a href=&#34;https://doi.org/10.3758/s13428-017-0852-3&#34;&gt;https://doi.org/10.3758/s13428-017-0852-3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Speed, L. J., &amp;amp; Majid, A. (2018). An Exception to Mental Simulation: No Evidence for Embodied Odor Language. &lt;em&gt;Cognitive Science&lt;/em&gt;, &lt;em&gt;42&lt;/em&gt;(4), 1146&amp;ndash;1178. &lt;a href=&#34;https://doi.org/10.1111/cogs.12593&#34;&gt;https://doi.org/10.1111/cogs.12593&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Speed, L. J., &amp;amp; Majid, A. (2020). Grounding language in the neglected senses of touch, taste, and smell. &lt;em&gt;Cognitive Neuropsychology&lt;/em&gt;, &lt;em&gt;37&lt;/em&gt;(5&amp;ndash;6), 363&amp;ndash;392. &lt;a href=&#34;https://doi.org/10.1080/02643294.2019.1623188&#34;&gt;https://doi.org/10.1080/02643294.2019.1623188&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Speed, L. J., Papies, E. K., &amp;amp; Majid, A. (2023). Mental simulation across sensory modalities predicts attractiveness of food concepts. &lt;em&gt;Journal of Experimental Psychology: Applied&lt;/em&gt;, &lt;em&gt;29&lt;/em&gt;(3), 557&amp;ndash;571. &lt;a href=&#34;https://doi.org/10.1037/xap0000461&#34;&gt;https://doi.org/10.1037/xap0000461&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Strik Lievers, F., &amp;amp; Winter, B. (2018). Sensory language across lexical categories. &lt;em&gt;Lingua&lt;/em&gt;, &lt;em&gt;204&lt;/em&gt;, 45&amp;ndash;61. &lt;a href=&#34;https://doi.org/10.1016/j.lingua.2017.11.002&#34;&gt;https://doi.org/10.1016/j.lingua.2017.11.002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Su, C., Wang, X., Wang, Z., &amp;amp; Chen, Y. (2019). A model of synesthetic metaphor interpretation based on cross-modality similarity. &lt;em&gt;Computer Speech &amp;amp; Language&lt;/em&gt;, &lt;em&gt;58&lt;/em&gt;, 1&amp;ndash;16. &lt;a href=&#34;https://doi.org/10.1016/j.csl.2019.03.003&#34;&gt;https://doi.org/10.1016/j.csl.2019.03.003&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tatiya, G., Hosseini, R., Hughes, M. C., &amp;amp; Sinapov, J. (2019). Sensorimotor Cross-Behavior Knowledge Transfer for Grounded Category Recognition. &lt;em&gt;2019 Joint IEEE 9th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)&lt;/em&gt;, 1&amp;ndash;6. &lt;a href=&#34;https://doi.org/10.1109/DEVLRN.2019.8850715&#34;&gt;https://doi.org/10.1109/DEVLRN.2019.8850715&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tatiya, G., Hosseini, R., Hughes, M. C., &amp;amp; Sinapov, J. (2020). A Framework for Sensorimotor Cross-Perception and Cross-Behavior Knowledge Transfer for Object Categorization. &lt;em&gt;Frontiers in Robotics and AI&lt;/em&gt;, &lt;em&gt;7&lt;/em&gt;, 522141. &lt;a href=&#34;https://doi.org/10.3389/frobt.2020.522141&#34;&gt;https://doi.org/10.3389/frobt.2020.522141&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tatiya, G., &amp;amp; Sinapov, J. (2019). Deep Multi-Sensory Object Category Recognition Using Interactive Behavioral Exploration. &lt;em&gt;2019 International Conference on Robotics and Automation (ICRA)&lt;/em&gt;, 7872&amp;ndash;7878. &lt;a href=&#34;https://doi.org/10.1109/ICRA.2019.8794095&#34;&gt;https://doi.org/10.1109/ICRA.2019.8794095&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Teodorescu, H.-N., &amp;amp; Bolea, S. C. (2019). Text Sectioning based on Stylometric Distances. &lt;em&gt;2019 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)&lt;/em&gt;, 1&amp;ndash;6. &lt;a href=&#34;https://doi.org/10.1109/SPED.2019.8906616&#34;&gt;https://doi.org/10.1109/SPED.2019.8906616&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thomason, J., Padmakumar, A., Sinapov, J., Walker, N., Jiang, Y., Yedidsion, H., Hart, J., Stone, P., &amp;amp; Mooney, R. (2020). Jointly Improving Parsing and Perception for Natural Language Commands through Human-Robot Dialog. &lt;em&gt;Journal of Artificial Intelligence Research&lt;/em&gt;, &lt;em&gt;67&lt;/em&gt;, 327&amp;ndash;374. &lt;a href=&#34;https://doi.org/10.1613/jair.1.11485&#34;&gt;https://doi.org/10.1613/jair.1.11485&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tjuka, A., Forkel, R., &amp;amp; List, J.-M. (2021). Linking norms, ratings, and relations of words and concepts across multiple language varieties. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;54&lt;/em&gt;(2), 864&amp;ndash;884. &lt;a href=&#34;https://doi.org/10.3758/s13428-021-01650-1&#34;&gt;https://doi.org/10.3758/s13428-021-01650-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tomsk State University, Rezanova, Z. I., Nekrasova, E. D., Tomsk State University, Miklashevsky, А. А., &amp;amp; Tomsk State University. (2018). INVESTIGATION OF PSYCHO-LINGUISTIC AND COGNITIVE ASPECTS OF LANGUAGE CONTACTING IN THE PROJECT &amp;ldquo;LINGUISTIC AND ETHNOCULTURAL DIVERSITY OF SOUTHERN SIBERIA IN SYNCHRONY AND DIAHRONY: INTERACTION OF LANGUAGES AND CULTURES.&amp;rdquo; &lt;em&gt;Rusin&lt;/em&gt;, &lt;em&gt;52&lt;/em&gt;, 107&amp;ndash;117. &lt;a href=&#34;https://doi.org/10.17223/18572685/52/8&#34;&gt;https://doi.org/10.17223/18572685/52/8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tomsk State University, Vladimirova, V. E., Rezanova, Z. I., Tomsk State University, Korshunova, I. S., &amp;amp; Tomsk State University. (2022). Ethno-linguistic contact as reflected in language cognition: Does bilingualism affect subjective assessments of perceptual semantics?*. &lt;em&gt;Rusin&lt;/em&gt;, &lt;em&gt;70&lt;/em&gt;, 214&amp;ndash;231. &lt;a href=&#34;https://doi.org/10.17223/18572685/70/12&#34;&gt;https://doi.org/10.17223/18572685/70/12&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Troche, J., Crutch, S. J., &amp;amp; Reilly, J. (2017). Defining a Conceptual Topography of Word Concreteness: Clustering Properties of Emotion, Sensation, and Magnitude among 750 English Words. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;, 1787. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2017.01787&#34;&gt;https://doi.org/10.3389/fpsyg.2017.01787&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Van De Weijer, J., Bianchi, I., &amp;amp; Paradis, C. (2023). Sensory modality profiles of antonyms. &lt;em&gt;Language and Cognition&lt;/em&gt;, 1&amp;ndash;15. &lt;a href=&#34;https://doi.org/10.1017/langcog.2023.20&#34;&gt;https://doi.org/10.1017/langcog.2023.20&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Vergallito, A., Petilli, M. A., &amp;amp; Marelli, M. (2020). Perceptual modality norms for 1,121 Italian words: A comparison with concreteness and imageability scores and an analysis of their impact in word processing tasks. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;52&lt;/em&gt;(4), 1599&amp;ndash;1616. &lt;a href=&#34;https://doi.org/10.3758/s13428-019-01337-8&#34;&gt;https://doi.org/10.3758/s13428-019-01337-8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Verheyen, S., De Deyne, S., Linsen, S., &amp;amp; Storms, G. (2020). Lexicosemantic, affective, and distributional norms for 1,000 Dutch adjectives. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;52&lt;/em&gt;(3), 1108&amp;ndash;1121. &lt;a href=&#34;https://doi.org/10.3758/s13428-019-01303-4&#34;&gt;https://doi.org/10.3758/s13428-019-01303-4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Vigliocco, G., Zhang, Y., Del Maschio, N., Todd, R., &amp;amp; Tuomainen, J. (2020). Electrophysiological signatures of English onomatopoeia. &lt;em&gt;Language and Cognition&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(1), 15&amp;ndash;35. &lt;a href=&#34;https://doi.org/10.1017/langcog.2019.38&#34;&gt;https://doi.org/10.1017/langcog.2019.38&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Villani, C., D&#39;Ascenzo, S., Borghi, A. M., Roversi, C., Benassi, M., &amp;amp; Lugli, L. (2022). Is justice grounded? How expertise shapes conceptual representation of institutional concepts. &lt;em&gt;Psychological Research&lt;/em&gt;, &lt;em&gt;86&lt;/em&gt;(8), 2434&amp;ndash;2450. &lt;a href=&#34;https://doi.org/10.1007/s00426-021-01492-8&#34;&gt;https://doi.org/10.1007/s00426-021-01492-8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Villani, C., Lugli, L., Liuzza, M. T., &amp;amp; Borghi, A. M. (2019). Varieties of abstract concepts and their multiple dimensions. &lt;em&gt;Language and Cognition&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt;(3), 403&amp;ndash;430. &lt;a href=&#34;https://doi.org/10.1017/langcog.2019.23&#34;&gt;https://doi.org/10.1017/langcog.2019.23&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wan, M., Su, Q., Ahrens, K., &amp;amp; Huang, C.-R. (2023). Perceptional and actional enrichment for metaphor detection with sensorimotor norms. &lt;em&gt;Natural Language Engineering&lt;/em&gt;, 1&amp;ndash;29. &lt;a href=&#34;https://doi.org/10.1017/S135132492300044X&#34;&gt;https://doi.org/10.1017/S135132492300044X&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wang, X., Su, C., &amp;amp; Chen, Y. (2019). A Method of Abstractness Ratings for Chinese Concepts. In A. Lotfi, H. Bouchachia, A. Gegov, C. Langensiepen, &amp;amp; M. McGinnity (Eds.), &lt;em&gt;Advances in Computational Intelligence Systems&lt;/em&gt; (Vol. 840, pp. 217&amp;ndash;226). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-319-97982-3_18&#34;&gt;https://doi.org/10.1007/978-3-319-97982-3_18&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wang, Y., &amp;amp; Zeng, Y. (2022a). Multisensory Concept Learning Framework Based on Spiking Neural Networks. &lt;em&gt;Frontiers in Systems Neuroscience&lt;/em&gt;, &lt;em&gt;16&lt;/em&gt;, 845177. &lt;a href=&#34;https://doi.org/10.3389/fnsys.2022.845177&#34;&gt;https://doi.org/10.3389/fnsys.2022.845177&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wang, Y., &amp;amp; Zeng, Y. (2022b). Statistical Analysis of Multisensory and Text-Derived Representations on Concept Learning. &lt;em&gt;Frontiers in Computational Neuroscience&lt;/em&gt;, &lt;em&gt;16&lt;/em&gt;, 861265. &lt;a href=&#34;https://doi.org/10.3389/fncom.2022.861265&#34;&gt;https://doi.org/10.3389/fncom.2022.861265&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wicke, P., &amp;amp; Bolognesi, M. (2020). Emoji-based semantic representations for abstract and concrete concepts. &lt;em&gt;Cognitive Processing&lt;/em&gt;, &lt;em&gt;21&lt;/em&gt;(4), 615&amp;ndash;635. &lt;a href=&#34;https://doi.org/10.1007/s10339-020-00971-x&#34;&gt;https://doi.org/10.1007/s10339-020-00971-x&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B. (2019). &lt;em&gt;Statistics for Linguists: An Introduction Using R&lt;/em&gt; (1st ed.). Routledge. &lt;a href=&#34;https://doi.org/10.4324/9781315165547&#34;&gt;https://doi.org/10.4324/9781315165547&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B. (2022). Mapping the landscape of exploratory and confirmatory data analysis in linguistics. In D. Tay &amp;amp; M. X. Pan (Eds.), &lt;em&gt;Data Analytics in Cognitive Linguistics&lt;/em&gt; (pp. 13&amp;ndash;48). De Gruyter. &lt;a href=&#34;https://doi.org/10.1515/9783110687279-002&#34;&gt;https://doi.org/10.1515/9783110687279-002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B. (2023). Abstract concepts and emotion: Cross-linguistic evidence and arguments against affective embodiment. &lt;em&gt;Philosophical Transactions of the Royal Society B: Biological Sciences&lt;/em&gt;, &lt;em&gt;378&lt;/em&gt;(1870), 20210368. &lt;a href=&#34;https://doi.org/10.1098/rstb.2021.0368&#34;&gt;https://doi.org/10.1098/rstb.2021.0368&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B., Lupyan, G., Perry, L. K., Dingemanse, M., &amp;amp; Perlman, M. (2023). Iconicity ratings for 14,000+ English words. &lt;em&gt;Behavior Research Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3758/s13428-023-02112-6&#34;&gt;https://doi.org/10.3758/s13428-023-02112-6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B., &amp;amp; Perlman, M. (2021). Size sound symbolism in the English lexicon. &lt;em&gt;Glossa: A Journal of General Linguistics&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;(1). &lt;a href=&#34;https://doi.org/10.5334/gjgl.1646&#34;&gt;https://doi.org/10.5334/gjgl.1646&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B., Perlman, M., &amp;amp; Majid, A. (2018). Vision dominates in perceptual language: English sensory vocabulary is optimized for usage. &lt;em&gt;Cognition&lt;/em&gt;, &lt;em&gt;179&lt;/em&gt;, 213&amp;ndash;220. &lt;a href=&#34;https://doi.org/10.1016/j.cognition.2018.05.008&#34;&gt;https://doi.org/10.1016/j.cognition.2018.05.008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B., Perlman, M., Perry, L. K., &amp;amp; Lupyan, G. (2017). Which words are most iconic?: Iconicity in English sensory words. &lt;em&gt;Interaction Studies. Social Behaviour and Communication in Biological and Artificial Systems&lt;/em&gt;, &lt;em&gt;18&lt;/em&gt;(3), 443&amp;ndash;464. &lt;a href=&#34;https://doi.org/10.1075/is.18.3.07win&#34;&gt;https://doi.org/10.1075/is.18.3.07win&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B., Sóskuthy, M., Perlman, M., &amp;amp; Dingemanse, M. (2022). Trilled /r/ is associated with roughness, linking sound and touch across spoken languages. &lt;em&gt;Scientific Reports&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(1), 1035. &lt;a href=&#34;https://doi.org/10.1038/s41598-021-04311-7&#34;&gt;https://doi.org/10.1038/s41598-021-04311-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B., &amp;amp; Strik-Lievers, F. (2023). Semantic distance predicts metaphoricity and creativity judgments in synesthetic metaphors. &lt;em&gt;Metaphor and the Social World&lt;/em&gt;, &lt;em&gt;13&lt;/em&gt;(1), 59&amp;ndash;80. &lt;a href=&#34;https://doi.org/10.1075/msw.00029.win&#34;&gt;https://doi.org/10.1075/msw.00029.win&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wu, C., &amp;amp; Mu, X. (2023). Sensory experience ratings (SERs) for 1,130 Chinese words: Relationships with other semantic and lexical psycholinguistic variables. &lt;em&gt;Linguistics Vanguard&lt;/em&gt;, &lt;em&gt;0&lt;/em&gt;(0). &lt;a href=&#34;https://doi.org/10.1515/lingvan-2022-0083&#34;&gt;https://doi.org/10.1515/lingvan-2022-0083&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Xiong, J., &amp;amp; Huang, C.-R. (2018). Somewhere in COLDNESS Lies Nibbāna: Lexical Manifestations of COLDNESS. In J.-F. Hong, Q. Su, &amp;amp; J.-S. Wu (Eds.), &lt;em&gt;Chinese Lexical Semantics&lt;/em&gt; (Vol. 11173, pp. 70&amp;ndash;81). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-04015-4_6&#34;&gt;https://doi.org/10.1007/978-3-030-04015-4_6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yin Zhong, &amp;amp; Chu-Ren Huang. (2020). Sweetness or Mouthfeel: A corpus-based study of the conceptualization of taste. &lt;em&gt;Linguistic Research&lt;/em&gt;, &lt;em&gt;37&lt;/em&gt;(3), 359&amp;ndash;387. &lt;a href=&#34;https://doi.org/10.17250/KHISLI.37.3.202012.001&#34;&gt;https://doi.org/10.17250/KHISLI.37.3.202012.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zeng, Y., Zhao, D., Zhao, F., Shen, G., Dong, Y., Lu, E., Zhang, Q., Sun, Y., Liang, Q., Zhao, Y., Zhao, Z., Fang, H., Wang, Y., Li, Y., Liu, X., Du, C., Kong, Q., Ruan, Z., &amp;amp; Bi, W. (2023). BrainCog: A spiking neural network based, brain-inspired cognitive intelligence engine for brain-inspired AI and brain simulation. &lt;em&gt;Patterns&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(8), 100789. &lt;a href=&#34;https://doi.org/10.1016/j.patter.2023.100789&#34;&gt;https://doi.org/10.1016/j.patter.2023.100789&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhang, X., Amiri, S., Sinapov, J., Thomason, J., Stone, P., &amp;amp; Zhang, S. (2023). Multimodal embodied attribute learning by robots for object-centric action policies. &lt;em&gt;Autonomous Robots&lt;/em&gt;, &lt;em&gt;47&lt;/em&gt;(5), 505&amp;ndash;528. &lt;a href=&#34;https://doi.org/10.1007/s10514-023-10098-5&#34;&gt;https://doi.org/10.1007/s10514-023-10098-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhang, X., Sinapov, J., &amp;amp; Zhang, S. (2021, July 12). Planning Multimodal Exploratory Actions for Online Robot Attribute Learning. &lt;em&gt;Robotics: Science and Systems XVII&lt;/em&gt;. Robotics: Science and Systems 2021. &lt;a href=&#34;https://doi.org/10.15607/RSS.2021.XVII.005&#34;&gt;https://doi.org/10.15607/RSS.2021.XVII.005&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q. (2020a). From Linguistic Synaesthesia to Conceptual Metaphor Theory. In Q. Zhao, &lt;em&gt;Embodied Conceptualization or Neural Realization&lt;/em&gt; (Vol. 10, pp. 115&amp;ndash;128). Springer Singapore. &lt;a href=&#34;https://doi.org/10.1007/978-981-32-9315-1_7&#34;&gt;https://doi.org/10.1007/978-981-32-9315-1_7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q. (2020b). Methodology: A Corpus-Driven Approach. In Q. Zhao, &lt;em&gt;Embodied Conceptualization or Neural Realization&lt;/em&gt; (Vol. 10, pp. 19&amp;ndash;34). Springer Singapore. &lt;a href=&#34;https://doi.org/10.1007/978-981-32-9315-1_2&#34;&gt;https://doi.org/10.1007/978-981-32-9315-1_2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q., Ahrens, K., &amp;amp; Huang, C.-R. (2022). Linguistic synesthesia is metaphorical: A lexical-conceptual account. &lt;em&gt;Cognitive Linguistics&lt;/em&gt;, &lt;em&gt;33&lt;/em&gt;(3), 553&amp;ndash;583. &lt;a href=&#34;https://doi.org/10.1515/cog-2021-0098&#34;&gt;https://doi.org/10.1515/cog-2021-0098&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q., Huang, C.-R., &amp;amp; Ahrens, K. (2019). Directionality of linguistic synesthesia in Mandarin: A corpus-based study. &lt;em&gt;Lingua&lt;/em&gt;, &lt;em&gt;232&lt;/em&gt;, 102744. &lt;a href=&#34;https://doi.org/10.1016/j.lingua.2019.102744&#34;&gt;https://doi.org/10.1016/j.lingua.2019.102744&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q., &amp;amp; Long, Y. (2022). A Diachronic Study on Linguistic Synesthesia in Chinese. In M. Dong, Y. Gu, &amp;amp; J.-F. Hong (Eds.), &lt;em&gt;Chinese Lexical Semantics&lt;/em&gt; (Vol. 13250, pp. 84&amp;ndash;94). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-031-06547-7_6&#34;&gt;https://doi.org/10.1007/978-3-031-06547-7_6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q., Long, Y., &amp;amp; Huang, C.-R. (2020). Linguistic Synaesthesia of Mandarin Sensory Adjectives: Corpus-Based and Experimental Approaches. In J.-F. Hong, Y. Zhang, &amp;amp; P. Liu (Eds.), &lt;em&gt;Chinese Lexical Semantics&lt;/em&gt; (Vol. 11831, pp. 139&amp;ndash;146). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-38189-9_14&#34;&gt;https://doi.org/10.1007/978-3-030-38189-9_14&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhong, Y., Ahrens, K., &amp;amp; Huang, C.-R. (2023). Entity, event, and sensory modalities: An onto-cognitive account of sensory nouns. &lt;em&gt;Humanities and Social Sciences Communications&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 255. &lt;a href=&#34;https://doi.org/10.1057/s41599-023-01677-z&#34;&gt;https://doi.org/10.1057/s41599-023-01677-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhong, Y., Huang, C.-R., &amp;amp; Dong, S. (2022). Bodily sensation and embodiment: A corpus-based study of gustatory vocabulary in Mandarin Chinese. &lt;em&gt;Journal of Chinese Linguistics&lt;/em&gt;, &lt;em&gt;50&lt;/em&gt;(1), 196&amp;ndash;230. &lt;a href=&#34;https://doi.org/10.1353/jcl.2022.0008&#34;&gt;https://doi.org/10.1353/jcl.2022.0008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhong, Y., Wan, M., Ahrens, K., &amp;amp; Huang, C.-R. (2022). Sensorimotor norms for Chinese nouns and their relationship with orthographic and semantic variables. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;37&lt;/em&gt;(8), 1000&amp;ndash;1022. &lt;a href=&#34;https://doi.org/10.1080/23273798.2022.2035416&#34;&gt;https://doi.org/10.1080/23273798.2022.2035416&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhu, S., Wang, X., &amp;amp; Liu, P. (2021). Who Killed Sanmao and Virginia Woolf? A Comparative Study of Writers with Suicidal Attempt Based on a Quantitative Linguistic Method. In M. Liu, C. Kit, &amp;amp; Q. Su (Eds.), &lt;em&gt;Chinese Lexical Semantics&lt;/em&gt; (Vol. 12278, pp. 408&amp;ndash;420). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-81197-6_34&#34;&gt;https://doi.org/10.1007/978-3-030-81197-6_34&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Naive principal component analysis in R</title>
      <link>https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Principal_component_analysis&#34;&gt;Principal Component Analysis (PCA)&lt;/a&gt; is a technique used to find the core components that underlie different variables. It comes in very useful whenever doubts arise about the true origin of three or more variables. There are two main methods for performing a PCA: naive or less naive. In the naive method, you first check some conditions in your data which will determine the essentials of the analysis. In the less-naive method, you set those yourself based on whatever prior information or purposes you had. The latter method is appropriate when you already have enough information about the intercorrelations, or when you are required to select a specific number of components. I will tackle the naive method, mainly by following the guidelines in &lt;a href=&#34;https://freethegeogbooks.files.wordpress.com/2016/08/book-for-r-language-stats.pdf&#34;&gt;Field, Miles, and Field (2012)&lt;/a&gt;, with updated code where necessary. A &lt;a href=&#34;https://freethegeogbooks.files.wordpress.com/2016/08/book-for-r-language-stats.pdf&#34;&gt;manual by Charles M. Friel&lt;/a&gt; (Sam Houston State University) was also useful.&lt;/p&gt;
&lt;p&gt;The ‘naive’ approach is characterized by a first stage that checks whether the PCA should actually be performed with your current variables, or if some should be removed. The variables that are accepted are taken to a second stage which identifies the number of principal components that seem to underlie your set of variables.&lt;/p&gt;
&lt;div id=&#34;stage-1.-determine-whether-pca-is-appropriate-at-all-considering-the-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;STAGE 1. Determine whether PCA is appropriate at all, considering the variables&lt;/h2&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;45%&#34; src=&#34;https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/1.jpg&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Variables should be &lt;strong&gt;inter-correlated enough but not too much.&lt;/strong&gt; Field et al. (2012) provide some thresholds, suggesting that no variable should have many correlations below .30, or &lt;em&gt;any&lt;/em&gt; correlation at all above .90. Thus, in the example here, variable Q06 should probably be excluded from the PCA.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Bartlett’s test&lt;/strong&gt;, on the nature of the intercorrelations, should be significant. Significance suggests that the variables are not an ‘identity matrix’ in which correlations are a sampling error.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;KMO&lt;/strong&gt; (Kaiser-Meyer-Olkin), a measure of sampling adequacy based on common variance (so similar purpose as Bartlett’s). As Field et al. review, ‘values between .5 and .7 are mediocre, values between .7 and .8 are good, values between .8 and .9 are great and values above .9 are superb’ (p. 761). There’s a general score as well as one per variable. The general one will often be good, whereas the individual scores may more likely fail. Any variable with a score below .5 should probably be removed, and the test should be run again.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Determinant:&lt;/strong&gt; A formula about multicollinearity. The result should preferably fall below .00001.
Note that some of these tests are run on the dataframe and others on a correlation matrix of the data, as distinguished below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Necessary libraries
library(ltm)
library(lattice)
library(psych)
library(car)
library(pastecs)
library(scales)
library(ggplot2)
library(arules)
library(plyr)
library(Rmisc)
library(GPArotation)
library(gdata)
library(MASS)
library(qpcR)
library(dplyr)
library(gtools)
library(Hmisc)

# Select variables of interest for the PCA
dataset = mydata[, c(&amp;#39;select_var1&amp;#39;, &amp;#39;select_var1&amp;#39;, 
  &amp;#39;select_var2&amp;#39;, &amp;#39;select_var3&amp;#39;, &amp;#39;select_var4&amp;#39;, 
  &amp;#39;select_var5&amp;#39;, &amp;#39;select_var6&amp;#39;, &amp;#39;select_var7&amp;#39;)]

# Create matrix: some tests will require it
data_matrix = cor(dataset, use = &amp;#39;complete.obs&amp;#39;)

# See intercorrelations
round(data_matrix, 2)

# Bartlett&amp;#39;s
cortest.bartlett(dataset)

# KMO (Kaiser-Meyer-Olkin)
KMO(data_matrix)

# Determinant
det(data_matrix)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-2.-identify-number-of-components-aka-factors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;STAGE 2. Identify number of components (aka factors)&lt;/h2&gt;
&lt;p&gt;In this stage, principal components (formally called ‘factors’ at this stage) are identified among the set of variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The identification is done through a basic, ‘unrotated’ PCA. The number of components set a priori must equal the number of variables that are being tested.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Start off with unrotated PCA

pc1 = psych::principal(dataset, nfactors = length(dataset), rotate=&amp;quot;none&amp;quot;)
pc1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is an example result:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  ## Principal Components Analysis
  ## Call: psych::principal(r = eng_prop, nfactors = 3, rotate = &amp;quot;none&amp;quot;)
  ## Standardized loadings (pattern matrix) based upon correlation matrix
  ##           PC1   PC2  PC3 h2       u2 com
  ## Aud_eng -0.89  0.13 0.44  1 -2.2e-16 1.5
  ## Hap_eng  0.64  0.75 0.15  1  1.1e-16 2.0
  ## Vis_eng  0.81 -0.46 0.36  1 -4.4e-16 2.0
  ## 
  ##                        PC1  PC2  PC3
  ## SS loadings           1.87 0.79 0.34
  ## Proportion Var        0.62 0.26 0.11
  ## Cumulative Var        0.62 0.89 1.00
  ## Proportion Explained  0.62 0.26 0.11
  ## Cumulative Proportion 0.62 0.89 1.00
  ## 
  ## Mean item complexity =  1.9
  ## Test of the hypothesis that 3 components are sufficient.
  ## 
  ## The root mean square of the residuals (RMSR) is  0 
  ##  with the empirical chi square  0  with prob &amp;lt;  NA 
  ## 
  ## Fit based upon off diagonal values = 1&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Among the columns, there are first the correlations between variables and components, followed by a column (h2) with the &lt;strong&gt;‘communalities’&lt;/strong&gt;. If less factors than variables had been selected, communality values would be below 1. Then there is the uniqueness column (u2): &lt;strong&gt;uniqueness&lt;/strong&gt; is equal to 1 minus the communality. Next is ‘com’, which reflects the &lt;strong&gt;complexity&lt;/strong&gt; with which a variable relates to the principal components. Those components are precisely found below. The first row contains the sums of squared loadings, or eigenvalues, namely, the total variance explained by each linear component. This value corresponds to the number of units explained out of all possible factors (which were three in the above example). The rows below all cut from the same cloth. &lt;em&gt;Proportion var&lt;/em&gt; = variance explained over a total of 1. This is the result of dividing the eigenvalue by the number of components. Multiply by 100 and you get the percentage of total variance explained, which becomes useful. In the example, 99% of the variance has been explained. Aside from the meddling maths, we should actually expect 100% there because the number of factors equaled the number of variables. &lt;em&gt;Cumulative var:&lt;/em&gt; variance added consecutively up to the last component. &lt;em&gt;Proportion explained:&lt;/em&gt; variance explained over what has actually been explained (only when variables = factors is this the same as Proportion var). &lt;em&gt;Cumulative proportion:&lt;/em&gt; the actually explained variance added consecutively up to the last component (Field et al., 2012).&lt;/p&gt;
&lt;p&gt;According to Field et al. (2012), two criteria can be used to determine the number of components that should be carried forward to the next stage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SS (sum of squares) loadings, with two possible cut-off points. On the one hand, following Kaiser’s threshold, we should select components with SS loadings &amp;gt; 1. In the example result shown above, only PC1 meets this criterion. A more lenient alternative is Kaiser’s threshold, whereby SS loadings &amp;gt; .7 are accepted. In the example result above, PC2 meet this criterion too.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Scree plot: retain as many components as the number of points after the point of inflection. To create a scree plot, call:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plot(pc1$values, type = &amp;#39;b&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;35%&#34; src=&#34;https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/2.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Imagine a straight line &lt;strong&gt;from the first point on the right.&lt;/strong&gt; Once this line bends considerably, count the points after the bend and up to the last point on the left. The number of points is the number of components to select. The example here is probably the most complicated (two components were finally chosen), but often it’s easier &lt;a href=&#34;https://www.google.nl/search?sca_esv=582945116&amp;amp;q=select+principal+components+scree+plot+point+inflection&amp;amp;tbm=isch&#34;&gt;see examples&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Based on either or both criteria, select the definitive number of components.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-3.-run-definitive-pca&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;STAGE 3. Run definitive PCA&lt;/h2&gt;
&lt;p&gt;Run a very similar command as you did before, but now with a more advanced method. The first PCA, a heuristic one, worked essentially on the inter-correlations. The definitive PCA, in contrast, will implement a prior shuffling known as ‘rotation’, to ensure that the result is robust enough (just like cards are shuffled). Explained variance is captured better this way. The go-to rotation method is the orthogonal, or ‘varimax’ (though others may be considered too).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Now with varimax rotation, Kaiser-normalized by default:
pc2 = psych::principal(dataset, nfactors=2, rotate = &amp;quot;varimax&amp;quot;, 
scores = TRUE)
pc2
pc2$loadings

# Sanity check
pc2$residual
pc2$fit
pc2$communality&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to Field et al. (2012), we would want:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Less than half of &lt;strong&gt;residuals&lt;/strong&gt; with absolute values &amp;gt; 0.05&lt;/li&gt;
&lt;li&gt;Model &lt;strong&gt;fit&lt;/strong&gt; &amp;gt; .9&lt;/li&gt;
&lt;li&gt;All &lt;strong&gt;communalities&lt;/strong&gt; &amp;gt; .7&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If any of this fails, consider changing the number of factors. Next, the rotated components that have been ‘extracted’ from the core of the set of variables can be added to the dataset. This would enable the use of these components as new variables that might prove powerful and useful (as in &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.1551-6709.2010.01157.x/full&#34;&gt;this research&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dataset = cbind(dataset, pc2$scores)
summary(dataset$RC1, dataset$RC2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-4.-determine-ascription-of-each-variable-to-components&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;STAGE 4. Determine ascription of each variable to components&lt;/h2&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;55%&#34; src=&#34;https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/3.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Check the main summary by just calling pc2, and see how each variable correlates with the rotated components. This is essential because it reveals how variables load on each component, or in other words, to which component a variable belongs. For instance, the table shown here belongs to a study about the meaning of words (Bernabeu, 2018). These results suggest that the visual and haptic modalities of words are quite related, whereas the auditory modality is relatively unique. When the analysis works out well, a cut-off point of &lt;em&gt;r&lt;/em&gt; = .8 may be applied for considering a variable as part of a component.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-5.-enjoy-the-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;STAGE 5. Enjoy the plot&lt;/h2&gt;
&lt;p&gt;The plot is perhaps the coolest part about PCA. It really makes an awesome illustration of the power of data analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ggplot(eng_props,
  aes(RC1, RC2, label = as.character(main_eng))) + stat_density2d (color = &amp;quot;gray87&amp;quot;) +
  geom_text(size = ifelse(eng_props$word_eng %in% w_set, 12, 7),
	fontface = ifelse(eng_props$word_eng %in% w_set, &amp;#39;bold&amp;#39;, &amp;#39;plain&amp;#39;)) +
  geom_point(data=eng_props[eng_props$word_eng %in% w_set,], pch=21, fill=NA, size=14, stroke=2, alpha=.6) +
  labs(subtitle=&amp;#39;(Data from Lynott &amp;amp; Connell, 2009)&amp;#39;, x = &amp;quot;Varimax-rotated Principal Component 1&amp;quot;, 
	y = &amp;quot;Varimax-rotated Principal Component 2&amp;quot;) +	theme_bw() +   
  theme( plot.background = element_blank(), panel.grid.major = element_blank(),
	panel.grid.minor = element_blank(), panel.border = element_blank(),
  	axis.line = element_line(color = &amp;#39;black&amp;#39;),
	axis.title.x = element_text(colour = &amp;#39;black&amp;#39;, size = 23, margin=margin(15,15,15,15)),
	axis.title.y = element_text(colour = &amp;#39;black&amp;#39;, size = 23, margin=margin(15,15,15,15)),
	axis.text.x = element_text(size=16), axis.text.y  = element_text(size=16),
	plot.title = element_text(hjust = 0.5, size = 32, face = &amp;quot;bold&amp;quot;, margin=margin(15,15,15,15)),
	plot.subtitle = element_text(hjust = 0.5, size = 20, margin=margin(2,15,15,15)) ) +
  geom_label_repel(data = eng_props[eng_props$word_eng %in% w_set,], aes(label = word_eng), size = 8, 
	alpha = 0.77, color = &amp;#39;black&amp;#39;, box.padding = 1.5 )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is an example combining PCA plots with code similar to the above. These plots illustrate something further with regard to the relationships among modalities. In property words, the different modalities spread out more clearly than they do in concept words. This makes sense because in language, properties define concepts (Bernabeu, 2018).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An example of these analyses is &lt;a href=&#34;https://pablobernabeu.shinyapps.io/Dutch-modality-exclusivity-norms&#34;&gt;available in available in this RStudio environment&lt;/a&gt;, in the &lt;code&gt;norms.R&lt;/code&gt; script.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P. (2018). &lt;em&gt;Dutch modality exclusivity norms for 336 properties and 411 concepts&lt;/em&gt;. PsyArXiv. &lt;a href=&#34;https://doi.org/10.31234/osf.io/s2c5h&#34; class=&#34;uri&#34;&gt;https://doi.org/10.31234/osf.io/s2c5h&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Field, A. P., Miles, J., &amp;amp; Field, Z. (2012). &lt;em&gt;Discovering Statistics Using R&lt;/em&gt;. London, UK: Sage.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Web application: Dutch modality exclusivity norms</title>
      <link>https://pablobernabeu.github.io/applications-and-dashboards/bernabeu-2018-modalitynorms/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/applications-and-dashboards/bernabeu-2018-modalitynorms/</guid>
      <description>&lt;a href=&#39;https://pablobernabeu.shinyapps.io/Dutch-modality-exclusivity-norms&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #196F27; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt; 
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;i class=&#34;fas fa-mouse-pointer&#34;&gt;&lt;/i&gt;&lt;/span&gt;&amp;nbsp; Complete web application &lt;font style=&#39;font-size:60%;&#39;&gt;&lt;i&gt;Flexdashboard-Shiny&lt;/i&gt;&lt;/font&gt; &lt;/h3&gt;&lt;/button&gt;&lt;/a&gt;
&lt;br&gt;
&lt;br&gt;
&lt;a href=&#39;https://pablobernabeu.github.io/dashboards/Dutch-modality-exclusivity-norms/d.html&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #4CAF50; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt; 
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;i class=&#34;fas fa-mouse-pointer&#34;&gt;&lt;/i&gt;&lt;/span&gt;&amp;nbsp; Reduced dashboard &lt;font style=&#39;font-size:60%;&#39;&gt;&lt;i&gt;Flexdashboard&lt;/i&gt;&lt;/font&gt; &lt;/h3&gt;&lt;/button&gt;&lt;/a&gt; &amp;nbsp; 
&lt;br&gt;
&lt;br&gt;
&lt;p&gt;This web application presents linguistic data over several tabs. The code combines the great front-end of Flexdashboard—based on R Markdown and yielding an unmatched user interface—, with the great back-end of Shiny—allowing users to download sections of data they select, in various formats.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A nice find was the &amp;lsquo;reactable&amp;rsquo; package, which implements Javascript under the hood to allow the use of colours, bar charts, etc.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  
Auditory = colDef(header = with_tooltip(&#39;Auditory Rating&#39;,
                                        &#39;Mean rating of each word on the auditory modality across participants.&#39;),
                  cell = function(value) {
                    width &amp;lt;- paste0(value / max(table_data$Auditory) * 100, &amp;quot;%&amp;quot;)
                    value = sprintf(&amp;quot;%.2f&amp;quot;, round(value,2))  # Round to two digits, keeping trailing zeros
                    bar_chart(value, width = width, fill = &#39;#ff3030&#39;)
                    },
                  align = &#39;left&#39;),
  
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One of the hardest nuts to crack was allowing the full functionality of tables—i.e, scaling to screen, frozen header, and vertical and horizontal scrolling—whilst having tweaked the vertical/horizontal orientation of the dashboard sections. Initial clashes were sorted by adjusting the section&#39;s CSS styles&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Table {#table style=&amp;quot;background-color:#FCFCFC;&amp;quot;}
=======================================================================
  
Inputs {.sidebar style=&#39;position:fixed; padding-top: 65px; padding-bottom:30px;&#39;}
-----------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and by also adjusting the reactable settings.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  
renderReactable({
  reactable(selected_words(),
            defaultSorted = list(cat = &#39;desc&#39;, word = &#39;asc&#39;),
            defaultColDef = colDef(footerStyle = list(fontWeight = &amp;quot;bold&amp;quot;)),
            height = 840, striped = TRUE, pagination = FALSE, highlight = TRUE,
  
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A nice feature, especially suited to Flexdashboard, was the use of different formats across tabs. Whereas the Info tab presents long text using HTML and CSS styling, along with rmarkdown code output, the other tabs rely more strongly on Javascript features, enabled by R packages such as ‘shiny’ and sweetalert (e.g., allowing modal dialogs—pop-ups), reactable and plotly (e.g., allowing information opened by hovering—tooltips).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```{r}
  
# reactive for the word bar
highlighted_properties = reactive(input$highlighted_properties)
  
renderPlotly({
 ggplotly(
  ggplot( selected_props(), aes(RC1, RC2, label = as.character(word), color = main, 
    # Html tags below used for format. Decimals rounded to two.
    text = paste0(&#39; &#39;, &#39;&amp;lt;span style=&amp;quot;padding-top:3px; padding-bottom:3px; font-size:2.2em; color:#EEEEEE&amp;quot;&amp;gt;&#39;, capitalize(word), &#39;&amp;lt;/span&amp;gt; &#39;, &#39;&amp;lt;br&amp;gt;&#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Dominant modality: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, main, &#39; &#39;,
     &#39; &#39;, &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Modality exclusivity: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Exclusivity, 2)), &#39;% &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Perceptual strength: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Perceptualstrength, 2)),
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Auditory rating: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Auditory, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Haptic rating: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Haptic, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Visual rating: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Visual, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Concreteness (Brysbaert et al., 2014): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, 
       sprintf(&amp;quot;%.2f&amp;quot;, round(concrete_Brysbaertetal2014, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Number of letters: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, letters, &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Number of phonemes (DutchPOND): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, 
     round(phonemes_DUTCHPOND, 2), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Contextual diversity (lg10CD SUBTLEX-NL): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;,
       sprintf(&amp;quot;%.2f&amp;quot;, round(freq_lg10CD_SUBTLEXNL, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Word frequency (lg10WF SUBTLEX-NL): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;,
       sprintf(&amp;quot;%.2f&amp;quot;, round(freq_lg10WF_SUBTLEXNL, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Lemma frequency (CELEX): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, 
       sprintf(&amp;quot;%.2f&amp;quot;, round(freq_CELEX_lem, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Phonological neighbourhood size (DutchPOND): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, 
     round(phon_neighbours_DUTCHPOND, 2), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Orthographic neighbourhood size (DutchPOND): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;,
     round(orth_neighbours_DUTCHPOND, 2), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Age of acquisition (Brysbaert et al., 2014): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;,
     sprintf(&amp;quot;%.2f&amp;quot;, round(AoA_Brysbaertetal2014, 2)), &#39; &#39;, &#39;&amp;lt;br&amp;gt; &#39;
     ) ) ) +
  geom_text(size = ifelse(selected_props()$word %in% highlighted_properties(), 7,
             ifelse(is.null(highlighted_properties()), 3, 2.8)),
      fontface = ifelse(selected_props()$word %in% highlighted_properties(), &#39;bold&#39;, &#39;plain&#39;)) +
geom_point(alpha = 0) +  # This geom_point helps to colour the tooltip according to the dominant modality
scale_colour_manual(values = colours, drop = FALSE) + theme_bw() + ggtitle(&#39;Property words&#39;) +
labs(x = &#39;Varimax-rotated Principal Component 1&#39;, y = &#39;Varimax-rotated Principal Component 2&#39;) +
guides(color = guide_legend(title = &#39;Main&amp;lt;br&amp;gt;modality&#39;)) +
theme( plot.background = element_blank(), panel.grid.major = element_blank(),
   panel.grid.minor = element_blank(), panel.border = element_blank(),
   axis.line = element_line(color = &#39;black&#39;), plot.title = element_text(size = 14, hjust = .5),
   axis.title.x = element_text(colour = &#39;black&#39;, size = 12, margin = margin(15,15,0,15)),
   axis.title.y = element_text(colour = &#39;black&#39;, size = 12, margin = margin(0,15,15,5)),
   axis.text.x = element_text(size = 8), axis.text.y  = element_text(size = 8),
   legend.background = element_rect(size = 2), legend.position = &#39;none&#39;,
 legend.title = element_blank(),
 legend.text = element_text(colour = colours, size = 13) ),
tooltip = &#39;text&#39;
)
})
  
# For download, save plot without the interactive &#39;plotly&#39; part
  
properties_png = reactive({ ggplot(selected_props(), aes(RC1, RC2, color = main, label = as.character(word))) +
geom_text(show.legend = FALSE, size = ifelse(selected_props()$word %in% highlighted_properties(), 7,
         ifelse(is.null(highlighted_properties()), 3, 2.8)),
      fontface = ifelse(selected_props()$word %in% highlighted_properties(), &#39;bold&#39;, &#39;plain&#39;)) +
geom_point(alpha = 0) + scale_colour_manual(values = colours, drop = FALSE) + theme_bw() +
guides(color = guide_legend(title = &#39;Main&amp;lt;br&amp;gt;modality&#39;, override.aes = list(size = 7, alpha = 1))) +
ggtitle( paste0(&#39;Properties&#39;, &#39; (showing &#39;, nrow(selected_props()), &#39; out of &#39;, nrow(props), &#39;)&#39;) ) + 
labs(x = &#39;Varimax-rotated Principal Component 1&#39;, y = &#39;Varimax-rotated Principal Component 2&#39;) +
theme( plot.background = element_blank(), panel.grid.major = element_blank(),
   panel.grid.minor = element_blank(), panel.border = element_blank(),
   axis.line = element_line(color = &#39;black&#39;), plot.title = element_text(size = 17, hjust = .5, margin = margin(3,3,7,3)),
   axis.title.x = element_text(colour = &#39;black&#39;, size = 12, margin = margin(10,10,2,10)),
   axis.title.y = element_text(colour = &#39;black&#39;, size = 12, margin = margin(10,10,10,5)),
   axis.text.x = element_text(size = 8), axis.text.y  = element_text(size = 8),
   legend.background = element_rect(size = 2), legend.position = &#39;right&#39;,
   legend.title = element_blank(), legend.text = element_text(size = 15))
})
  
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The only instance in which I drew on javascript code outside R packages was to enable tooltips beyond the packages’ limits—for instance, in the side bar. This javascript feature is created at the top of the script, in the head area.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!-- Javascript function to enable a hovering tooltip --&amp;gt;
&amp;lt;script&amp;gt;
$(document).ready(function(){
   $(&#39;[data-toggle=&amp;quot;tooltip1&amp;quot;]&#39;).tooltip();
});
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the side bar, I added a reactive mean for each variable, complementing the range selector.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;reactive(cat(paste0(&#39;Mean = &#39;, 
  sprintf(&amp;quot;%.2f&amp;quot;, round(mean(selected_words()$Exclusivity),2)))))
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2 id=&#34;static-version-published-on-rpubs&#34;&gt;Static version published on RPubs&lt;/h2&gt;
&lt;p&gt;A reduced, &lt;a href=&#34;https://rpubs.com/pcbernabeu/Dutch-modality-exclusivity-norms&#34;&gt;&lt;em&gt;static&lt;/em&gt; version&lt;/a&gt; was also created to increase the availability of the content. Removing some reactivity features allows the dashboard to be published as a standard website (i.e., on a personal website, on &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;, etc.), without the need for a back-end Shiny server. Note that this type of website is dubbed &amp;lsquo;static&amp;rsquo;, but it can retain multiple interactive features thanks to Javascript-based tools under the hood, allowed by R packages such as &lt;code&gt;leaflet&lt;/code&gt; for maps, &lt;code&gt;DT&lt;/code&gt; for tables, &lt;code&gt;plotly&lt;/code&gt; for plots, etc.&lt;/p&gt;
&lt;p&gt;To create the Flexdashboard-only version departing from the Flexdashboard-Shiny version, I deleted &lt;code&gt;runtime: shiny&lt;/code&gt; from the YAML header, and disabled Shiny reactive inputs and objects, as below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```{r}
# Number of words selected on sidebar
# reactive(cat(paste0(&#39;Words selected below: &#39;, nrow(selected_props()))))
```
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;div style = &#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P. (2018). Dutch modality exclusivity norms for 336 properties and 411 concepts [Web application]. Retrieved from &lt;a href=&#34;https://pablobernabeu.shinyapps.io/Dutch-Modality-Exclusivity-Norms&#34;&gt;https://pablobernabeu.shinyapps.io/Dutch-Modality-Exclusivity-Norms&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;br&gt;
</description>
    </item>
    
    <item>
      <title>Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs</title>
      <link>https://pablobernabeu.github.io/publication/bernabeu-etal-2017/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/publication/bernabeu-etal-2017/</guid>
      <description>&lt;h3 id=&#34;bonus-a-conference-poster-with-further-analyses-nbspa-hrefhttpsmfrosfiorenderurlhttpsosfiodj52ndirect26moderender26actiondownload26moderenderi-classfas-fa-external-link-altia&#34;&gt;Bonus: a conference poster with further analyses  &lt;a href=&#39;https://mfr.osf.io/render?url=https://osf.io/dj52n/?direct%26mode=render%26action=download%26mode=render&#39;&gt;&lt;i class=&#34;fas fa-external-link-alt&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;style&gt;.embed-responsive{position:relative;height:100%;}.embed-responsive iframe{position:absolute;height:100%;}&lt;/style&gt;&lt;script&gt;window.jQuery || document.write(&#39;&lt;script src=&#34;//code.jquery.com/jquery-1.11.2.min.js&#34;&gt;\x3C/script&gt;&#39;) &lt;/script&gt;&lt;link href=&#34;https://mfr.osf.io/static/css/mfr.css&#34; media=&#34;all&#34; rel=&#34;stylesheet&#34;&gt;&lt;div id=&#34;mfrIframe&#34; class=&#34;mfr mfr-file&#34;&gt;&lt;/div&gt;&lt;script src=&#34;https://mfr.osf.io/static/js/mfr.js&#34;&gt;&lt;/script&gt; &lt;script&gt;var mfrRender = new mfr.Render(&#34;mfrIframe&#34;, &#34;https://mfr.osf.io/render?url=https://osf.io/dj52n/?direct%26mode=render%26action=download%26mode=render&#34;);&lt;/script&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;div style = &#34;text-indent: -2em; margin-left: 2em;&#34;&gt;
&lt;p&gt;Bernabeu, P., Willems, R. M., &amp;amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, &amp;amp; E. J. Davelaar (Eds.), &lt;em&gt;Proceedings of the 39th Annual Conference of the Cognitive Science Society&lt;/em&gt; (pp. 1629-1634). Austin, TX: Cognitive Science Society. &lt;a href=&#34;https://doi.org/10.31234/osf.io/a5pcz&#34;&gt;https://doi.org/10.31234/osf.io/a5pcz&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;br&gt;
&lt;h3 id=&#34;related-references&#34;&gt;Related references&lt;/h3&gt;
&lt;div style = &#34;text-indent: -2em; margin-left: 2em; color: darkgrey;&#34;&gt;
&lt;p&gt;Ali Qurbi, E. (2022). Ambiguous Word Processing among Second Language Learners. &lt;em&gt;The Canadian Modern Language Review&lt;/em&gt;, &lt;em&gt;78&lt;/em&gt;(2), 151&amp;ndash;173. &lt;a href=&#34;https://doi.org/10.3138/cmlr-2020-0115&#34;&gt;https://doi.org/10.3138/cmlr-2020-0115&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Amenta, S., Crepaldi, D., &amp;amp; Marelli, M. (2020). Consistency measures individuate dissociating semantic modulations in priming paradigms: A new look on semantics in the processing of (complex) words. &lt;em&gt;Quarterly Journal of Experimental Psychology&lt;/em&gt;, &lt;em&gt;73&lt;/em&gt;(10), 1546&amp;ndash;1563. &lt;a href=&#34;https://doi.org/10.1177/1747021820927663&#34;&gt;https://doi.org/10.1177/1747021820927663&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Amsel, B. D., Kutas, M., &amp;amp; Coulson, S. (2017). Projectors, associators, visual imagery, and the time course of visual processing in grapheme-color synesthesia. &lt;em&gt;Cognitive Neuroscience&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;(4), 206&amp;ndash;223. &lt;a href=&#34;https://doi.org/10.1080/17588928.2017.1353492&#34;&gt;https://doi.org/10.1080/17588928.2017.1353492&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Argiris, G., Rumiati, R. I., &amp;amp; Crepaldi, D. (2021). No fruits without color: Cross-modal priming and EEG reveal different roles for different features across semantic categories. &lt;em&gt;PLOS ONE&lt;/em&gt;, &lt;em&gt;16&lt;/em&gt;(4), e0234219. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0234219&#34;&gt;https://doi.org/10.1371/journal.pone.0234219&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Avramova, Y. R., De Pelsmacker, P., &amp;amp; Dens, N. (2017). Brand placement in text: The short- and long-term effects of placement modality and need for cognition. &lt;em&gt;International Journal of Advertising&lt;/em&gt;, &lt;em&gt;36&lt;/em&gt;(5), 682&amp;ndash;704. &lt;a href=&#34;https://doi.org/10.1080/02650487.2017.1335041&#34;&gt;https://doi.org/10.1080/02650487.2017.1335041&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bagli, M. (2023). How to Point with Language: English Source-Based Language to Describe Taste Qualities. &lt;em&gt;Lublin Studies in Modern Languages and Literature&lt;/em&gt;, &lt;em&gt;42&lt;/em&gt;(2), 31&amp;ndash;46. &lt;a href=&#34;https://doi.org/10.17951/lsmll.2023.47.2.31-46&#34;&gt;https://doi.org/10.17951/lsmll.2023.47.2.31-46&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Banks, B., Wingfield, C., &amp;amp; Connell, L. (2021). Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production. &lt;em&gt;Cognitive Science&lt;/em&gt;, &lt;em&gt;45&lt;/em&gt;(10), e13055. &lt;a href=&#34;https://doi.org/10.1111/cogs.13055&#34;&gt;https://doi.org/10.1111/cogs.13055&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barros García, B. (2017). In Other Words: Reformulation Strategies in Dostoevskii&#39;s Literary Works. &lt;em&gt;Russian Literature&lt;/em&gt;, &lt;em&gt;91&lt;/em&gt;, 1&amp;ndash;25. &lt;a href=&#34;https://doi.org/10.1016/j.ruslit.2017.09.001&#34;&gt;https://doi.org/10.1016/j.ruslit.2017.09.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barsalou, L. W. (2017a). Cognitively Plausible Theories of Concept Composition. In J. A. Hampton &amp;amp; Y. Winter (Eds.), &lt;em&gt;Compositionality and Concepts in Linguistics and Psychology&lt;/em&gt; (Vol. 3, pp. 9&amp;ndash;30). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-319-45977-6_2&#34;&gt;https://doi.org/10.1007/978-3-319-45977-6_2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barsalou, L. W. (2017b). What does semantic tiling of the cortex tell us about semantics? &lt;em&gt;Neuropsychologia&lt;/em&gt;, &lt;em&gt;105&lt;/em&gt;, 18&amp;ndash;38. &lt;a href=&#34;https://doi.org/10.1016/j.neuropsychologia.2017.04.011&#34;&gt;https://doi.org/10.1016/j.neuropsychologia.2017.04.011&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barsalou, L. W. (2020). Challenges and Opportunities for Grounding Cognition. &lt;em&gt;Journal of Cognition&lt;/em&gt;, &lt;em&gt;3&lt;/em&gt;(1), 31. &lt;a href=&#34;https://doi.org/10.5334/joc.116&#34;&gt;https://doi.org/10.5334/joc.116&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bartl, S., &amp;amp; Lahey, E. (2023). &amp;lsquo;As the title implies&amp;rsquo;: How readers talk about titles in Amazon book reviews. &lt;em&gt;Language and Literature: International Journal of Stylistics&lt;/em&gt;, &lt;em&gt;32&lt;/em&gt;(2), 209&amp;ndash;230. &lt;a href=&#34;https://doi.org/10.1177/09639470221147788&#34;&gt;https://doi.org/10.1177/09639470221147788&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Baumann, A., Hofmann, K., Marakasova, A., Neidhardt, J., &amp;amp; Wissik, T. (2023). Semantic micro-dynamics as a reflex of occurrence frequency: A semantic networks approach. &lt;em&gt;Cognitive Linguistics&lt;/em&gt;, &lt;em&gt;34&lt;/em&gt;(3&amp;ndash;4), 533&amp;ndash;568. &lt;a href=&#34;https://doi.org/10.1515/cog-2022-0008&#34;&gt;https://doi.org/10.1515/cog-2022-0008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bechtold, L., Bellebaum, C., Egan, S., Tettamanti, M., &amp;amp; Ghio, M. (2019). The role of experience for abstract concepts: Expertise modulates the electrophysiological correlates of mathematical word processing. &lt;em&gt;Brain and Language&lt;/em&gt;, &lt;em&gt;188&lt;/em&gt;, 1&amp;ndash;10. &lt;a href=&#34;https://doi.org/10.1016/j.bandl.2018.10.002&#34;&gt;https://doi.org/10.1016/j.bandl.2018.10.002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bechtold, L., Bellebaum, C., &amp;amp; Ghio, M. (2023). When a Sunny Day Gives You Butterflies: An Electrophysiological Investigation of Concreteness and Context Effects in Semantic Word Processing. &lt;em&gt;Journal of Cognitive Neuroscience&lt;/em&gt;, &lt;em&gt;35&lt;/em&gt;(2), 241&amp;ndash;258. &lt;a href=&#34;https://doi.org/10.1162/jocn_a_01942&#34;&gt;https://doi.org/10.1162/jocn_a_01942&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bechtold, L., Bellebaum, C., Hoffman, P., &amp;amp; Ghio, M. (2021). Corroborating behavioral evidence for the interplay of representational richness and semantic control in semantic word processing. &lt;em&gt;Scientific Reports&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt;(1), 6184. &lt;a href=&#34;https://doi.org/10.1038/s41598-021-85711-7&#34;&gt;https://doi.org/10.1038/s41598-021-85711-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bidet-Ildei, C., Gimenes, M., Toussaint, L., Almecija, Y., &amp;amp; Badets, A. (2017). Sentence plausibility influences the link between action words and the perception of biological human movements. &lt;em&gt;Psychological Research&lt;/em&gt;, &lt;em&gt;81&lt;/em&gt;(4), 806&amp;ndash;813. &lt;a href=&#34;https://doi.org/10.1007/s00426-016-0776-z&#34;&gt;https://doi.org/10.1007/s00426-016-0776-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bidet-Ildei, C., Gimenes, M., Toussaint, L., Beauprez, S.-A., &amp;amp; Badets, A. (2017). Painful semantic context modulates the relationship between action words and biological movement perception. &lt;em&gt;Journal of Cognitive Psychology&lt;/em&gt;, &lt;em&gt;29&lt;/em&gt;(7), 821&amp;ndash;831. &lt;a href=&#34;https://doi.org/10.1080/20445911.2017.1322093&#34;&gt;https://doi.org/10.1080/20445911.2017.1322093&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bolognesi, M., &amp;amp; Strik Lievers, F. (2020). How language and image construct synaesthetic metaphors in print advertising. &lt;em&gt;Visual Communication&lt;/em&gt;, &lt;em&gt;19&lt;/em&gt;(4), 431&amp;ndash;457. &lt;a href=&#34;https://doi.org/10.1177/1470357218782001&#34;&gt;https://doi.org/10.1177/1470357218782001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Borghesani, V., &amp;amp; Piazza, M. (2017). The neuro-cognitive representations of symbols: The case of concrete words. &lt;em&gt;Neuropsychologia&lt;/em&gt;, &lt;em&gt;105&lt;/em&gt;, 4&amp;ndash;17. &lt;a href=&#34;https://doi.org/10.1016/j.neuropsychologia.2017.06.026&#34;&gt;https://doi.org/10.1016/j.neuropsychologia.2017.06.026&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Borghi, A. M., &amp;amp; Barsalou, L. (2021). Perspective in the conceptualization of categories. &lt;em&gt;Psychological Research&lt;/em&gt;, &lt;em&gt;85&lt;/em&gt;(2), 697&amp;ndash;719. &lt;a href=&#34;https://doi.org/10.1007/s00426-019-01269-0&#34;&gt;https://doi.org/10.1007/s00426-019-01269-0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Borghi, A. M., Binkofski, F., Castelfranchi, C., Cimatti, F., Scorolli, C., &amp;amp; Tummolini, L. (2017). The challenge of abstract concepts. &lt;em&gt;Psychological Bulletin&lt;/em&gt;, &lt;em&gt;143&lt;/em&gt;(3), 263&amp;ndash;292. &lt;a href=&#34;https://doi.org/10.1037/bul0000089&#34;&gt;https://doi.org/10.1037/bul0000089&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Borghi, A. M., Mazzuca, C., Gervasi, A. M., Mannella, F., &amp;amp; Tummolini, L. (2023). Grounded cognition can be multimodal all the way down. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, 1&amp;ndash;5. &lt;a href=&#34;https://doi.org/10.1080/23273798.2023.2210238&#34;&gt;https://doi.org/10.1080/23273798.2023.2210238&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bottini, R., Ferraro, S., Nigri, A., Cuccarini, V., Bruzzone, M. G., &amp;amp; Collignon, O. (2020). Brain Regions Involved in Conceptual Retrieval in Sighted and Blind People. &lt;em&gt;Journal of Cognitive Neuroscience&lt;/em&gt;, &lt;em&gt;32&lt;/em&gt;(6), 1009&amp;ndash;1025. &lt;a href=&#34;https://doi.org/10.1162/jocn_a_01538&#34;&gt;https://doi.org/10.1162/jocn_a_01538&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bruffaerts, R., De Deyne, S., Meersmans, K., Liuzzi, A. G., Storms, G., &amp;amp; Vandenberghe, R. (2019). Redefining the resolution of semantic knowledge in the brain: Advances made by the introduction of models of semantics in neuroimaging. &lt;em&gt;Neuroscience &amp;amp; Biobehavioral Reviews&lt;/em&gt;, &lt;em&gt;103&lt;/em&gt;, 3&amp;ndash;13. &lt;a href=&#34;https://doi.org/10.1016/j.neubiorev.2019.05.015&#34;&gt;https://doi.org/10.1016/j.neubiorev.2019.05.015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Caballero, R., &amp;amp; Paradis, C. (2020). Soundscapes in English and Spanish: A corpus investigation of verb constructions. &lt;em&gt;Language and Cognition&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(4), 705&amp;ndash;728. &lt;a href=&#34;https://doi.org/10.1017/langcog.2020.19&#34;&gt;https://doi.org/10.1017/langcog.2020.19&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Caballero, R., &amp;amp; Paradis, C. (2023). Sharing Perceptual Experiences through Language. &lt;em&gt;Journal of Intelligence&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt;(7), 129. &lt;a href=&#34;https://doi.org/10.3390/jintelligence11070129&#34;&gt;https://doi.org/10.3390/jintelligence11070129&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cantarero, K., Parzuchowski, M., &amp;amp; Dukala, K. (2017). White Lies in Hand: Are Other-Oriented Lies Modified by Hand Gestures? Possibly Not. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;, 814. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2017.00814&#34;&gt;https://doi.org/10.3389/fpsyg.2017.00814&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Carney, J. (2020). Thinking avant la lettre: A Review of 4E Cognition. &lt;em&gt;Evolutionary Studies in Imaginative Culture&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(1), 77&amp;ndash;90. &lt;a href=&#34;https://doi.org/10.26613/esic.4.1.172&#34;&gt;https://doi.org/10.26613/esic.4.1.172&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Caterina Villani &amp;amp; Luisa Lugli. (2020). L&#39;effetto Simon e il suo decorso temporale con stimoli linguistici non spaziali. &lt;em&gt;Giornale italiano di psicologia&lt;/em&gt;, &lt;em&gt;1&lt;/em&gt;, 305&amp;ndash;314. &lt;a href=&#34;https://doi.org/10.1421/96612&#34;&gt;https://doi.org/10.1421/96612&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chan, S. (2022). Dynamics of nominal classification systems in language processing. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;37&lt;/em&gt;(6), 671&amp;ndash;685. &lt;a href=&#34;https://doi.org/10.1080/23273798.2021.2011331&#34;&gt;https://doi.org/10.1080/23273798.2021.2011331&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Charmhun Jo, Sun-A Kim, &amp;amp; Chu-Ren Huang. (2022). Linguistic synesthesia in Korean: A compound word-based study of cross-modal directionality. &lt;em&gt;Linguistic Research&lt;/em&gt;, &lt;em&gt;39&lt;/em&gt;(2), 275&amp;ndash;296. &lt;a href=&#34;https://doi.org/10.17250/KHISLI.39.2.202206.002&#34;&gt;https://doi.org/10.17250/KHISLI.39.2.202206.002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chedid, G., Brambati, S. M., Bedetti, C., Rey, A. E., Wilson, M. A., &amp;amp; Vallet, G. T. (2019). Visual and auditory perceptual strength norms for 3,596 French nouns and their relationship with other psycholinguistic variables. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;51&lt;/em&gt;(5), 2094&amp;ndash;2105. &lt;a href=&#34;https://doi.org/10.3758/s13428-019-01254-w&#34;&gt;https://doi.org/10.3758/s13428-019-01254-w&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chen, I.-H., Zhao, Q., Long, Y., Lu, Q., &amp;amp; Huang, C.-R. (2019). Mandarin Chinese modality exclusivity norms. &lt;em&gt;PLOS ONE&lt;/em&gt;, &lt;em&gt;14&lt;/em&gt;(2), e0211336. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0211336&#34;&gt;https://doi.org/10.1371/journal.pone.0211336&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Choi, U., Sung, Y., &amp;amp; Ogawa, S. (2020). Measurement of ultra‐fast signal progression related to face processing by 7T fMRI. &lt;em&gt;Human Brain Mapping&lt;/em&gt;, &lt;em&gt;41&lt;/em&gt;(7), 1754&amp;ndash;1764. &lt;a href=&#34;https://doi.org/10.1002/hbm.24907&#34;&gt;https://doi.org/10.1002/hbm.24907&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chwilla, D. J. (2022). Context effects in language comprehension: The role of emotional state and attention on semantic and syntactic processing. &lt;em&gt;Frontiers in Human Neuroscience&lt;/em&gt;, &lt;em&gt;16&lt;/em&gt;, 1014547. &lt;a href=&#34;https://doi.org/10.3389/fnhum.2022.1014547&#34;&gt;https://doi.org/10.3389/fnhum.2022.1014547&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cirillo, G., Strijkers, K., Runnqvist, E., &amp;amp; Baus, C. (2023). Effects of Shared Attention on joint language production across processing stages. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, 1&amp;ndash;12. &lt;a href=&#34;https://doi.org/10.1080/23273798.2023.2260021&#34;&gt;https://doi.org/10.1080/23273798.2023.2260021&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cochrane, B. A., &amp;amp; Milliken, B. (2019). Imagined event files: An interplay between imagined and perceived objects. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, &lt;em&gt;26&lt;/em&gt;(2), 538&amp;ndash;544. &lt;a href=&#34;https://doi.org/10.3758/s13423-019-01572-2&#34;&gt;https://doi.org/10.3758/s13423-019-01572-2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Connell, L. (2019). What have labels ever done for us? The linguistic shortcut in conceptual processing. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;34&lt;/em&gt;(10), 1308&amp;ndash;1318. &lt;a href=&#34;https://doi.org/10.1080/23273798.2018.1471512&#34;&gt;https://doi.org/10.1080/23273798.2018.1471512&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Connell, L., Lynott, D., &amp;amp; Banks, B. (2018). Interoception: The forgotten modality in perceptual grounding of abstract and concrete concepts. &lt;em&gt;Philosophical Transactions of the Royal Society B: Biological Sciences&lt;/em&gt;, &lt;em&gt;373&lt;/em&gt;(1752), 20170143. &lt;a href=&#34;https://doi.org/10.1098/rstb.2017.0143&#34;&gt;https://doi.org/10.1098/rstb.2017.0143&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Corciulo, S., Bioglio, L., Basile, V., Patti, V., &amp;amp; Damiano, R. (2023). The DEEP Sensorium: A multidimensional approach to sensory domain labelling. &lt;em&gt;Companion Proceedings of the ACM Web Conference 2023&lt;/em&gt;, 661&amp;ndash;668. &lt;a href=&#34;https://doi.org/10.1145/3543873.3587631&#34;&gt;https://doi.org/10.1145/3543873.3587631&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Courson, M., Macoir, J., &amp;amp; Tremblay, P. (2018). A facilitating role for the primary motor cortex in action sentence processing. &lt;em&gt;Behavioural Brain Research&lt;/em&gt;, &lt;em&gt;336&lt;/em&gt;, 244&amp;ndash;249. &lt;a href=&#34;https://doi.org/10.1016/j.bbr.2017.09.019&#34;&gt;https://doi.org/10.1016/j.bbr.2017.09.019&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Davis, C. P., Joergensen, G. H., Boddy, P., Dowling, C., &amp;amp; Yee, E. (2020). Making It Harder to &amp;ldquo;See&amp;rdquo; Meaning: The More You See Something, the More Its Conceptual Representation Is Susceptible to Visual Interference. &lt;em&gt;Psychological Science&lt;/em&gt;, &lt;em&gt;31&lt;/em&gt;(5), 505&amp;ndash;517. &lt;a href=&#34;https://doi.org/10.1177/0956797620910748&#34;&gt;https://doi.org/10.1177/0956797620910748&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Davis, C. P., &amp;amp; Yee, E. (2021). Building semantic memory from embodied and distributional language experience. &lt;em&gt;WIREs Cognitive Science&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(5), e1555. &lt;a href=&#34;https://doi.org/10.1002/wcs.1555&#34;&gt;https://doi.org/10.1002/wcs.1555&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Davis, J. D., Coulson, S., Arnold, A. J., &amp;amp; Winkielman, P. (2021). Dynamic Grounding of Concepts: Implications for Emotion and Social Cognition. In M. D. Robinson &amp;amp; L. E. Thomas (Eds.), &lt;em&gt;Handbook of Embodied Psychology&lt;/em&gt; (pp. 23&amp;ndash;42). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-78471-3_2&#34;&gt;https://doi.org/10.1007/978-3-030-78471-3_2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;De Deyne, S., Brysbaert, M., &amp;amp; Elgort, I. (2023). Chapter 7. Cross-language influences in L2 semantic and conceptual representation and processing. In I. Elgort, A. Siyanova-Chanturia, &amp;amp; M. Brysbaert (Eds.), &lt;em&gt;Bilingual Processing and Acquisition&lt;/em&gt; (Vol. 16, pp. 152&amp;ndash;186). John Benjamins Publishing Company. &lt;a href=&#34;https://doi.org/10.1075/bpa.16.07ded&#34;&gt;https://doi.org/10.1075/bpa.16.07ded&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dellantonio, S., &amp;amp; Pastore, L. (2017). The &amp;lsquo;Proprioceptive&amp;rsquo; Component of Abstract Concepts. In S. Dellantonio &amp;amp; L. Pastore, &lt;em&gt;Internal Perception&lt;/em&gt; (Vol. 40, pp. 297&amp;ndash;357). Springer Berlin Heidelberg. &lt;a href=&#34;https://doi.org/10.1007/978-3-662-55763-1_6&#34;&gt;https://doi.org/10.1007/978-3-662-55763-1_6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Díez-Álamo, A. M., Díez, E., Alonso, M. Á., Vargas, C. A., &amp;amp; Fernandez, A. (2018). Normative ratings for perceptual and motor attributes of 750 object concepts in Spanish. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;50&lt;/em&gt;(4), 1632&amp;ndash;1644. &lt;a href=&#34;https://doi.org/10.3758/s13428-017-0970-y&#34;&gt;https://doi.org/10.3758/s13428-017-0970-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dove, G. (2018). Language as a disruptive technology: Abstract concepts, embodiment and the flexible mind. &lt;em&gt;Philosophical Transactions of the Royal Society B: Biological Sciences&lt;/em&gt;, &lt;em&gt;373&lt;/em&gt;(1752), 20170135. &lt;a href=&#34;https://doi.org/10.1098/rstb.2017.0135&#34;&gt;https://doi.org/10.1098/rstb.2017.0135&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dove, G. (2021). The Challenges of Abstract Concepts. In M. D. Robinson &amp;amp; L. E. Thomas (Eds.), &lt;em&gt;Handbook of Embodied Psychology&lt;/em&gt; (pp. 171&amp;ndash;195). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-78471-3_8&#34;&gt;https://doi.org/10.1007/978-3-030-78471-3_8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dove, G., Barca, L., Tummolini, L., &amp;amp; Borghi, A. M. (2022). Words have a weight: Language as a source of inner grounding and flexibility in abstract concepts. &lt;em&gt;Psychological Research&lt;/em&gt;, &lt;em&gt;86&lt;/em&gt;(8), 2451&amp;ndash;2467. &lt;a href=&#34;https://doi.org/10.1007/s00426-020-01438-6&#34;&gt;https://doi.org/10.1007/s00426-020-01438-6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dupont, W., Papaxanthis, C., Lebon, F., &amp;amp; Madden-Lombardi, C. (2022). Does the Motor Cortex Want the Full Story? The Influence of Sentence Context on Corticospinal Excitability in Action Language Processing. &lt;em&gt;Neuroscience&lt;/em&gt;, &lt;em&gt;506&lt;/em&gt;, 58&amp;ndash;67. &lt;a href=&#34;https://doi.org/10.1016/j.neuroscience.2022.10.022&#34;&gt;https://doi.org/10.1016/j.neuroscience.2022.10.022&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dutriaux, L., Dahiez, X., &amp;amp; Gyselinck, V. (2019). How to change your memory of an object with a posture and a verb. &lt;em&gt;Quarterly Journal of Experimental Psychology&lt;/em&gt;, &lt;em&gt;72&lt;/em&gt;(5), 1112&amp;ndash;1118. &lt;a href=&#34;https://doi.org/10.1177/1747021818785096&#34;&gt;https://doi.org/10.1177/1747021818785096&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dymarska, A., Connell, L., &amp;amp; Banks, B. (2022). Linguistic Bootstrapping Allows More Real-world Object Concepts to Be Held in Mind. &lt;em&gt;Collabra: Psychology&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;(1), 40171. &lt;a href=&#34;https://doi.org/10.1525/collabra.40171&#34;&gt;https://doi.org/10.1525/collabra.40171&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dymarska, A., Connell, L., &amp;amp; Banks, B. (2023). More is not necessarily better: How different aspects of sensorimotor experience affect recognition memory for words. &lt;em&gt;Journal of Experimental Psychology: Learning, Memory, and Cognition&lt;/em&gt;, &lt;em&gt;49&lt;/em&gt;(10), 1572&amp;ndash;1587. &lt;a href=&#34;https://doi.org/10.1037/xlm0001265&#34;&gt;https://doi.org/10.1037/xlm0001265&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Edmiston, P., &amp;amp; Lupyan, G. (2017). Visual interference disrupts visual knowledge. &lt;em&gt;Journal of Memory and Language&lt;/em&gt;, &lt;em&gt;92&lt;/em&gt;, 281&amp;ndash;292. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2016.07.002&#34;&gt;https://doi.org/10.1016/j.jml.2016.07.002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Elisa Scerrati, Cristina Iani, Luisa Lugli, &amp;amp; Sandro Rubichi. (2019). C&#39;è un effetto di potenziamento dell&#39;azione con oggetti bimanuali? &lt;em&gt;Giornale italiano di psicologia&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;, 987&amp;ndash;996. &lt;a href=&#34;https://doi.org/10.1421/95573&#34;&gt;https://doi.org/10.1421/95573&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Elpidorou, A., &amp;amp; Dove, G. (2018). &lt;em&gt;Consciousness and Physicalism: A Defense of a Research Program&lt;/em&gt; (1st ed.). Routledge. &lt;a href=&#34;https://doi.org/10.4324/9781315682075&#34;&gt;https://doi.org/10.4324/9781315682075&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Espino, O., &amp;amp; Byrne, R. M. J. (2018). Thinking About the Opposite of What Is Said: Counterfactual Conditionals and Symbolic or Alternate Simulations of Negation. &lt;em&gt;Cognitive Science&lt;/em&gt;, &lt;em&gt;42&lt;/em&gt;(8), 2459&amp;ndash;2501. &lt;a href=&#34;https://doi.org/10.1111/cogs.12677&#34;&gt;https://doi.org/10.1111/cogs.12677&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fabrizio Calzavarini. (2023). Comprensione e cervello. &lt;em&gt;Sistemi intelligenti&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;, 251&amp;ndash;276. &lt;a href=&#34;https://doi.org/10.1422/107149&#34;&gt;https://doi.org/10.1422/107149&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fischer, J., &amp;amp; Mahon, B. Z. (2021). What tool representation, intuitive physics, and action have in common: The brain&#39;s first-person physics engine. &lt;em&gt;Cognitive Neuropsychology&lt;/em&gt;, &lt;em&gt;38&lt;/em&gt;(7&amp;ndash;8), 455&amp;ndash;467. &lt;a href=&#34;https://doi.org/10.1080/02643294.2022.2106126&#34;&gt;https://doi.org/10.1080/02643294.2022.2106126&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Galetzka, C. (2017). The Story So Far: How Embodied Cognition Advances Our Understanding of Meaning-Making. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;, 1315. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2017.01315&#34;&gt;https://doi.org/10.3389/fpsyg.2017.01315&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Gálvez-García, G., Aldunate, N., Bascour-Sandoval, C., Martínez-Molina, A., Peña, J., &amp;amp; Barramuño, M. (2020). Muscle activation in semantic processing: An electromyography approach. &lt;em&gt;Biological Psychology&lt;/em&gt;, &lt;em&gt;152&lt;/em&gt;, 107881. &lt;a href=&#34;https://doi.org/10.1016/j.biopsycho.2020.107881&#34;&gt;https://doi.org/10.1016/j.biopsycho.2020.107881&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Gao, C., Baucom, L. B., Kim, J., Wang, J., Wedell, D. H., &amp;amp; Shinkareva, S. V. (2019). Distinguishing abstract from concrete concepts in supramodal brain regions. &lt;em&gt;Neuropsychologia&lt;/em&gt;, &lt;em&gt;131&lt;/em&gt;, 102&amp;ndash;110. &lt;a href=&#34;https://doi.org/10.1016/j.neuropsychologia.2019.05.032&#34;&gt;https://doi.org/10.1016/j.neuropsychologia.2019.05.032&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Gatti, D., Marelli, M., Vecchi, T., &amp;amp; Rinaldi, L. (2022). Spatial Representations Without Spatial Computations. &lt;em&gt;Psychological Science&lt;/em&gt;, &lt;em&gt;33&lt;/em&gt;(11), 1947&amp;ndash;1958. &lt;a href=&#34;https://doi.org/10.1177/09567976221094863&#34;&gt;https://doi.org/10.1177/09567976221094863&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Gijssels, T., &amp;amp; Casasanto, D. (2020). Hand-use norms for Dutch and English manual action verbs: Implicit measures from a pantomime task. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;52&lt;/em&gt;(4), 1744&amp;ndash;1767. &lt;a href=&#34;https://doi.org/10.3758/s13428-020-01347-x&#34;&gt;https://doi.org/10.3758/s13428-020-01347-x&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Grand, G., Blank, I. A., Pereira, F., &amp;amp; Fedorenko, E. (2022). Semantic projection recovers rich human knowledge of multiple object features from word embeddings. &lt;em&gt;Nature Human Behaviour&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;(7), 975&amp;ndash;987. &lt;a href=&#34;https://doi.org/10.1038/s41562-022-01316-8&#34;&gt;https://doi.org/10.1038/s41562-022-01316-8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Grant, L. D., &amp;amp; Weissman, D. H. (2023). The binary structure of event files generalizes to abstract features: A nonhierarchical explanation of task set boundaries for the congruency sequence effect. &lt;em&gt;Journal of Experimental Psychology: Learning, Memory, and Cognition&lt;/em&gt;, &lt;em&gt;49&lt;/em&gt;(7), 1033&amp;ndash;1050. &lt;a href=&#34;https://doi.org/10.1037/xlm0001148&#34;&gt;https://doi.org/10.1037/xlm0001148&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Greco, A. (2021). Spatial and Motor Aspects in the &amp;ldquo;Action-Sentence Compatibility Effect.&amp;rdquo; &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;, 647899. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2021.647899&#34;&gt;https://doi.org/10.3389/fpsyg.2021.647899&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Guerra, E., &amp;amp; Knoeferle, P. (2018). Semantic Interference and Facilitation: Understanding the Integration of Spatial Distance and Conceptual Similarity During Sentence Reading. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;, 718. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2018.00718&#34;&gt;https://doi.org/10.3389/fpsyg.2018.00718&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Günther, F., Petilli, M. A., Vergallito, A., &amp;amp; Marelli, M. (2022). Images of the unseen: Extrapolating visual representations for abstract and concrete words in a data-driven computational model. &lt;em&gt;Psychological Research&lt;/em&gt;, &lt;em&gt;86&lt;/em&gt;(8), 2512&amp;ndash;2532. &lt;a href=&#34;https://doi.org/10.1007/s00426-020-01429-7&#34;&gt;https://doi.org/10.1007/s00426-020-01429-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Günther, F., Rinaldi, L., &amp;amp; Marelli, M. (2019). Vector-Space Models of Semantic Representation From a Cognitive Perspective: A Discussion of Common Misconceptions. &lt;em&gt;Perspectives on Psychological Science&lt;/em&gt;, &lt;em&gt;14&lt;/em&gt;(6), 1006&amp;ndash;1033. &lt;a href=&#34;https://doi.org/10.1177/1745691619861372&#34;&gt;https://doi.org/10.1177/1745691619861372&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hafri, A., Trueswell, J. C., &amp;amp; Strickland, B. (2018). Encoding of event roles from visual scenes is rapid, spontaneous, and interacts with higher-level visual processing. &lt;em&gt;Cognition&lt;/em&gt;, &lt;em&gt;175&lt;/em&gt;, 36&amp;ndash;52. &lt;a href=&#34;https://doi.org/10.1016/j.cognition.2018.02.011&#34;&gt;https://doi.org/10.1016/j.cognition.2018.02.011&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hardy, B. W. (2021). Embodied Cognition in Communication Science. &lt;em&gt;Communication Theory&lt;/em&gt;, &lt;em&gt;31&lt;/em&gt;(4), 633&amp;ndash;653. &lt;a href=&#34;https://doi.org/10.1093/ct/qtaa003&#34;&gt;https://doi.org/10.1093/ct/qtaa003&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Harpaintner, M., Sim, E.-J., Trumpp, N. M., Ulrich, M., &amp;amp; Kiefer, M. (2020). The grounding of abstract concepts in the motor and visual system: An fMRI study. &lt;em&gt;Cortex&lt;/em&gt;, &lt;em&gt;124&lt;/em&gt;, 1&amp;ndash;22. &lt;a href=&#34;https://doi.org/10.1016/j.cortex.2019.10.014&#34;&gt;https://doi.org/10.1016/j.cortex.2019.10.014&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Harpaintner, M., Trumpp, N. M., &amp;amp; Kiefer, M. (2018). The Semantic Content of Abstract Concepts: A Property Listing Study of 296 Abstract Words. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;, 1748. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2018.01748&#34;&gt;https://doi.org/10.3389/fpsyg.2018.01748&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Harpaintner, M., Trumpp, N. M., &amp;amp; Kiefer, M. (2022). Time course of brain activity during the processing of motor- and vision-related abstract concepts: Flexibility and task dependency. &lt;em&gt;Psychological Research&lt;/em&gt;, &lt;em&gt;86&lt;/em&gt;(8), 2560&amp;ndash;2582. &lt;a href=&#34;https://doi.org/10.1007/s00426-020-01374-5&#34;&gt;https://doi.org/10.1007/s00426-020-01374-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hartman, J., &amp;amp; Paradis, C. (2023). The language of sound: Events and meaning multitasking of words. &lt;em&gt;Cognitive Linguistics&lt;/em&gt;, &lt;em&gt;34&lt;/em&gt;(3&amp;ndash;4), 445&amp;ndash;477. &lt;a href=&#34;https://doi.org/10.1515/cog-2022-0006&#34;&gt;https://doi.org/10.1515/cog-2022-0006&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hauk, O., Giraud, A.-L., &amp;amp; Clarke, A. (2017). Brain oscillations in language comprehension. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;32&lt;/em&gt;(5), 533&amp;ndash;535. &lt;a href=&#34;https://doi.org/10.1080/23273798.2017.1297842&#34;&gt;https://doi.org/10.1080/23273798.2017.1297842&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hauk, O., Jackson, R. L., &amp;amp; Rahimi, S. (2023). Transforming the neuroscience of language: Estimating pattern-to-pattern transformations of brain activity. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, 1&amp;ndash;16. &lt;a href=&#34;https://doi.org/10.1080/23273798.2023.2226268&#34;&gt;https://doi.org/10.1080/23273798.2023.2226268&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hauk, O., Magnabosco, F., &amp;amp; Law, R. (2023). Can we separate semantic representations from computations? A commentary on Calzavarini (2023). &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, 1&amp;ndash;4. &lt;a href=&#34;https://doi.org/10.1080/23273798.2023.2226269&#34;&gt;https://doi.org/10.1080/23273798.2023.2226269&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Heim, E. M. (2017a). &lt;em&gt;Adoption in Galatians and Romans: Contemporary Metaphor Theories and the Pauline Huiothesia Metaphors&lt;/em&gt;. BRILL. &lt;a href=&#34;https://doi.org/10.1163/9789004339873&#34;&gt;https://doi.org/10.1163/9789004339873&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Heim, E. M. (2017b). &lt;em&gt;Adoption in Galatians and Romans: Contemporary Metaphor Theories and the Pauline Huiothesia Metaphors&lt;/em&gt;. BRILL. &lt;a href=&#34;https://doi.org/10.1163/9789004339873&#34;&gt;https://doi.org/10.1163/9789004339873&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hoeben Mannaert, L. N., Dijkstra, K., &amp;amp; Zwaan, R. A. (2020). Object combination in mental simulations. &lt;em&gt;Quarterly Journal of Experimental Psychology&lt;/em&gt;, &lt;em&gt;73&lt;/em&gt;(11), 1796&amp;ndash;1806. &lt;a href=&#34;https://doi.org/10.1177/1747021820933214&#34;&gt;https://doi.org/10.1177/1747021820933214&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hoffman, P., McClelland, J. L., &amp;amp; Lambon Ralph, M. A. (2018). Concepts, control, and context: A connectionist account of normal and disordered semantic cognition. &lt;em&gt;Psychological Review&lt;/em&gt;, &lt;em&gt;125&lt;/em&gt;(3), 293&amp;ndash;328. &lt;a href=&#34;https://doi.org/10.1037/rev0000094&#34;&gt;https://doi.org/10.1037/rev0000094&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hohol, M. (2019). &lt;em&gt;Foundations of Geometric Cognition&lt;/em&gt; (1st ed.). Routledge. &lt;a href=&#34;https://doi.org/10.4324/9780429056291&#34;&gt;https://doi.org/10.4324/9780429056291&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Holman, A., &amp;amp; Gîrbă, A. (2019). The match in orientation between verbal context and object accelerates change detection. &lt;em&gt;Psihologija&lt;/em&gt;, &lt;em&gt;52&lt;/em&gt;(1), 93&amp;ndash;105. &lt;a href=&#34;https://doi.org/10.2298/PSI180412033H&#34;&gt;https://doi.org/10.2298/PSI180412033H&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hörberg, T., Larsson, M., &amp;amp; Olofsson, J. K. (2022). The Semantic Organization of the English Odor Vocabulary. &lt;em&gt;Cognitive Science&lt;/em&gt;, &lt;em&gt;46&lt;/em&gt;(11), e13205. &lt;a href=&#34;https://doi.org/10.1111/cogs.13205&#34;&gt;https://doi.org/10.1111/cogs.13205&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Huang, C.-R., &amp;amp; Xiong, J. (2019). Linguistic synaesthesia in Chinese. In C.-R. Huang, Z. Jing-Schmidt, &amp;amp; B. Meisterernst (Eds.), &lt;em&gt;The Routledge Handbook of Chinese Applied Linguistics&lt;/em&gt; (1st ed., pp. 294&amp;ndash;312). Routledge. &lt;a href=&#34;https://doi.org/10.4324/9781315625157-20&#34;&gt;https://doi.org/10.4324/9781315625157-20&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Iatropoulos, G., Herman, P., Lansner, A., Karlgren, J., Larsson, M., &amp;amp; Olofsson, J. K. (2018). The language of smell: Connecting linguistic and psychophysical properties of odor descriptors. &lt;em&gt;Cognition&lt;/em&gt;, &lt;em&gt;178&lt;/em&gt;, 37&amp;ndash;49. &lt;a href=&#34;https://doi.org/10.1016/j.cognition.2018.05.007&#34;&gt;https://doi.org/10.1016/j.cognition.2018.05.007&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Iriguchi, M., Fujimura, R., Koda, H., &amp;amp; Masataka, N. (2019). Traffic symbol recognition modulates bodily actions. &lt;em&gt;PLOS ONE&lt;/em&gt;, &lt;em&gt;14&lt;/em&gt;(3), e0214281. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0214281&#34;&gt;https://doi.org/10.1371/journal.pone.0214281&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Jo, C. (2022). Linguistic Synesthesia in Korean: Universality and Variation. &lt;em&gt;SAGE Open&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(3), 215824402211178. &lt;a href=&#34;https://doi.org/10.1177/21582440221117804&#34;&gt;https://doi.org/10.1177/21582440221117804&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Johns, B. T. (2022). Accounting for item-level variance in recognition memory: Comparing word frequency and contextual diversity. &lt;em&gt;Memory &amp;amp; Cognition&lt;/em&gt;, &lt;em&gt;50&lt;/em&gt;(5), 1013&amp;ndash;1032. &lt;a href=&#34;https://doi.org/10.3758/s13421-021-01249-z&#34;&gt;https://doi.org/10.3758/s13421-021-01249-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Jones, L. L., Wurm, L. H., Calcaterra, R. D., &amp;amp; Ofen, N. (2017). Integrative Priming of Compositional and Locative Relations. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2017.00359&#34;&gt;https://doi.org/10.3389/fpsyg.2017.00359&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Julich-Warpakowski, N., &amp;amp; Pérez Sobrino, P. (2023). Introduction: Current challenges in metaphor research. &lt;em&gt;Metaphor and the Social World&lt;/em&gt;, &lt;em&gt;13&lt;/em&gt;(1), 1&amp;ndash;15. &lt;a href=&#34;https://doi.org/10.1075/msw.00026.jul&#34;&gt;https://doi.org/10.1075/msw.00026.jul&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kabbach, A., &amp;amp; Herbelot, A. (2021). Avoiding Conflict: When Speaker Coordination Does Not Require Conceptual Agreement. &lt;em&gt;Frontiers in Artificial Intelligence&lt;/em&gt;, &lt;em&gt;3&lt;/em&gt;, 523920. &lt;a href=&#34;https://doi.org/10.3389/frai.2020.523920&#34;&gt;https://doi.org/10.3389/frai.2020.523920&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kaiser, E. (2021). Consequences of Sensory Modality for Perspective-Taking: Comparing Visual, Olfactory and Gustatory Perception. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;, 701486. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2021.701486&#34;&gt;https://doi.org/10.3389/fpsyg.2021.701486&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kaup, B., Ulrich, R., Bausenhart, K. M., Bryce, D., Butz, M. V., Dignath, D., Dudschig, C., Franz, V. H., Friedrich, C., Gawrilow, C., Heller, J., Huff, M., Hütter, M., Janczyk, M., Leuthold, H., Mallot, H., Nürk, H.-C., Ramscar, M., Said, N., &amp;hellip; Wong, H. Y. (2023). Modal and amodal cognition: An overarching principle in various domains of psychology. &lt;em&gt;Psychological Research&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1007/s00426-023-01878-w&#34;&gt;https://doi.org/10.1007/s00426-023-01878-w&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Keogh, R., &amp;amp; Pearson, J. (2017). The perceptual and phenomenal capacity of mental imagery. &lt;em&gt;Cognition&lt;/em&gt;, &lt;em&gt;162&lt;/em&gt;, 124&amp;ndash;132. &lt;a href=&#34;https://doi.org/10.1016/j.cognition.2017.02.004&#34;&gt;https://doi.org/10.1016/j.cognition.2017.02.004&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kernot, D., Bossomaier, T., &amp;amp; Bradbury, R. (2017). Novel Text Analysis for Investigating Personality: Identifying the Dark Lady in Shakespeare&#39;s Sonnets. &lt;em&gt;Journal of Quantitative Linguistics&lt;/em&gt;, &lt;em&gt;24&lt;/em&gt;(4), 255&amp;ndash;272. &lt;a href=&#34;https://doi.org/10.1080/09296174.2017.1304049&#34;&gt;https://doi.org/10.1080/09296174.2017.1304049&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kernot, D., Bossomaier, T., &amp;amp; Bradbury, R. (2018). Using Shakespeare&#39;s Sotto Voce to Determine True Identity From Text. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;, 289. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2018.00289&#34;&gt;https://doi.org/10.3389/fpsyg.2018.00289&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kernot, D., Bossomaier, T., &amp;amp; Bradbury, R. (2019). The Stylometric Impacts of Ageing and Life Events on Identity. &lt;em&gt;Journal of Quantitative Linguistics&lt;/em&gt;, &lt;em&gt;26&lt;/em&gt;(1), 1&amp;ndash;21. &lt;a href=&#34;https://doi.org/10.1080/09296174.2017.1405719&#34;&gt;https://doi.org/10.1080/09296174.2017.1405719&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Keus, K., &amp;amp; Harde, R. (2022). &amp;ldquo;She Wished Someone Would Help Them&amp;rdquo;: PTSD and Empathy in the Six of Crows Duology. &lt;em&gt;Children&#39;s Literature in Education&lt;/em&gt;, &lt;em&gt;53&lt;/em&gt;(1), 130&amp;ndash;146. &lt;a href=&#34;https://doi.org/10.1007/s10583-021-09441-0&#34;&gt;https://doi.org/10.1007/s10583-021-09441-0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Khatin-Zadeh, O., Hu, J., Banaruee, H., &amp;amp; Marmolejo-Ramos, F. (2023). How emotions are metaphorically embodied: Measuring hand and head action strengths of typical emotional states. &lt;em&gt;Cognition and Emotion&lt;/em&gt;, &lt;em&gt;37&lt;/em&gt;(3), 486&amp;ndash;498. &lt;a href=&#34;https://doi.org/10.1080/02699931.2023.2181314&#34;&gt;https://doi.org/10.1080/02699931.2023.2181314&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kim, M.-K., Müller, H. M., &amp;amp; Weiss, S. (2021). What you &amp;ldquo;mean&amp;rdquo; is not what I &amp;ldquo;mean&amp;rdquo;: Categorization of verbs by Germans and Koreans using the semantic differential. &lt;em&gt;Lingua&lt;/em&gt;, &lt;em&gt;252&lt;/em&gt;, 103012. &lt;a href=&#34;https://doi.org/10.1016/j.lingua.2020.103012&#34;&gt;https://doi.org/10.1016/j.lingua.2020.103012&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Klomberg, B., Schilhab, T., &amp;amp; Burke, M. (2022). &lt;em&gt;Picturing Fiction through Embodied Cognition: Drawn Representations and Viewpoint in Literary Texts&lt;/em&gt; (1st ed.). Routledge. &lt;a href=&#34;https://doi.org/10.4324/9781003225300&#34;&gt;https://doi.org/10.4324/9781003225300&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Knoeferle, P. (2019). Predicting (variability of) context effects in language comprehension. &lt;em&gt;Journal of Cultural Cognitive Science&lt;/em&gt;, &lt;em&gt;3&lt;/em&gt;(2), 141&amp;ndash;158. &lt;a href=&#34;https://doi.org/10.1007/s41809-019-00025-5&#34;&gt;https://doi.org/10.1007/s41809-019-00025-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Koblet, O., &amp;amp; Purves, R. S. (2020). From online texts to Landscape Character Assessment: Collecting and analysing first-person landscape perception computationally. &lt;em&gt;Landscape and Urban Planning&lt;/em&gt;, &lt;em&gt;197&lt;/em&gt;, 103757. &lt;a href=&#34;https://doi.org/10.1016/j.landurbplan.2020.103757&#34;&gt;https://doi.org/10.1016/j.landurbplan.2020.103757&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kompa, N. A. (2021). Language and embodiment&amp;mdash;Or the cognitive benefits of abstract representations. &lt;em&gt;Mind &amp;amp; Language&lt;/em&gt;, &lt;em&gt;36&lt;/em&gt;(1), 27&amp;ndash;47. &lt;a href=&#34;https://doi.org/10.1111/mila.12266&#34;&gt;https://doi.org/10.1111/mila.12266&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kompa, N. A., &amp;amp; Mueller, J. L. (2022). Inner speech as a cognitive tool&amp;mdash;Or what is the point of talking to oneself? &lt;em&gt;Philosophical Psychology&lt;/em&gt;, 1&amp;ndash;24. &lt;a href=&#34;https://doi.org/10.1080/09515089.2022.2112164&#34;&gt;https://doi.org/10.1080/09515089.2022.2112164&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Körner, A., Castillo, M., Drijvers, L., Fischer, M. H., Günther, F., Marelli, M., Platonova, O., Rinaldi, L., Shaki, S., Trujillo, J. P., Tsaregorodtseva, O., &amp;amp; Glenberg, A. M. (2023). Embodied Processing at Six Linguistic Granularity Levels: A Consensus Paper. &lt;em&gt;Journal of Cognition&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;(1), 60. &lt;a href=&#34;https://doi.org/10.5334/joc.231&#34;&gt;https://doi.org/10.5334/joc.231&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Krishna, P. P., Arulmozi, S., &amp;amp; Mishra, R. K. (2022). &amp;ldquo;Do You See and Hear More? A Study on Telugu Perception Verbs.&amp;rdquo; &lt;em&gt;Journal of Psycholinguistic Research&lt;/em&gt;, &lt;em&gt;51&lt;/em&gt;(3), 473&amp;ndash;484. &lt;a href=&#34;https://doi.org/10.1007/s10936-021-09827-7&#34;&gt;https://doi.org/10.1007/s10936-021-09827-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kuhnke, P., Beaupain, M. C., Arola, J., Kiefer, M., &amp;amp; Hartwigsen, G. (2023). Meta-analytic evidence for a novel hierarchical model of conceptual processing. &lt;em&gt;Neuroscience &amp;amp; Biobehavioral Reviews&lt;/em&gt;, &lt;em&gt;144&lt;/em&gt;, 104994. &lt;a href=&#34;https://doi.org/10.1016/j.neubiorev.2022.104994&#34;&gt;https://doi.org/10.1016/j.neubiorev.2022.104994&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kuhnke, P., Kiefer, M., &amp;amp; Hartwigsen, G. (2021). Task-Dependent Functional and Effective Connectivity during Conceptual Processing. &lt;em&gt;Cerebral Cortex&lt;/em&gt;, &lt;em&gt;31&lt;/em&gt;(7), 3475&amp;ndash;3493. &lt;a href=&#34;https://doi.org/10.1093/cercor/bhab026&#34;&gt;https://doi.org/10.1093/cercor/bhab026&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kumcu, A. (2021). Linguistic Synesthesia in Turkish: A Corpus-based Study of Crossmodal Directionality. &lt;em&gt;Metaphor and Symbol&lt;/em&gt;, &lt;em&gt;36&lt;/em&gt;(4), 241&amp;ndash;255. &lt;a href=&#34;https://doi.org/10.1080/10926488.2021.1921557&#34;&gt;https://doi.org/10.1080/10926488.2021.1921557&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lai, V. T., Hubbard, R., Ku, L.-C., &amp;amp; Pfeifer, V. (2023). Electrophysiology of Non-Literal Language. In M. Grimaldi, E. Brattico, &amp;amp; Y. Shtyrov (Eds.), &lt;em&gt;Language Electrified&lt;/em&gt; (Vol. 202, pp. 613&amp;ndash;646). Springer US. &lt;a href=&#34;https://doi.org/10.1007/978-1-0716-3263-5_19&#34;&gt;https://doi.org/10.1007/978-1-0716-3263-5_19&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Landrigan, J.-F., &amp;amp; Mirman, D. (2018). The cost of switching between taxonomic and thematic semantics. &lt;em&gt;Memory &amp;amp; Cognition&lt;/em&gt;, &lt;em&gt;46&lt;/em&gt;(2), 191&amp;ndash;203. &lt;a href=&#34;https://doi.org/10.3758/s13421-017-0757-5&#34;&gt;https://doi.org/10.3758/s13421-017-0757-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Large, A.-C., Bach, C., &amp;amp; Calvet, G. (2018). CONTACT: A Human Centered Approach of Multimodal Flight Deck Design and Evaluation. In D. Harris (Ed.), &lt;em&gt;Engineering Psychology and Cognitive Ergonomics&lt;/em&gt; (Vol. 10906, pp. 593&amp;ndash;604). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-319-91122-9_48&#34;&gt;https://doi.org/10.1007/978-3-319-91122-9_48&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lee, J., &amp;amp; Shin, J.-A. (2023). The cross-linguistic comparison of perceptual strength norms for Korean, English and L2 English. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;14&lt;/em&gt;, 1188909. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2023.1188909&#34;&gt;https://doi.org/10.3389/fpsyg.2023.1188909&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Li, M., Lu, Q., Long, Y., &amp;amp; Gui, L. (2017). Inferring Affective Meanings of Words from Word Embedding. &lt;em&gt;IEEE Transactions on Affective Computing&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;(4), 443&amp;ndash;456. &lt;a href=&#34;https://doi.org/10.1109/TAFFC.2017.2723012&#34;&gt;https://doi.org/10.1109/TAFFC.2017.2723012&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lin, K., &amp;amp; Chan, S. (2019). When senses meet functions: An amodal stage in conceptual processing. &lt;em&gt;Journal of Cognitive Psychology&lt;/em&gt;, &lt;em&gt;31&lt;/em&gt;(1), 64&amp;ndash;75. &lt;a href=&#34;https://doi.org/10.1080/20445911.2018.1560299&#34;&gt;https://doi.org/10.1080/20445911.2018.1560299&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Littlemore, J. (2019). &lt;em&gt;Metaphors in the Mind: Sources of Variation in Embodied Metaphor&lt;/em&gt; (1st ed.). Cambridge University Press. &lt;a href=&#34;https://doi.org/10.1017/9781108241441&#34;&gt;https://doi.org/10.1017/9781108241441&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, S., Zhou, M., &amp;amp; Li, Y. (2019). Internet use experience influence individuals&amp;rsquo; lexical decision performance by changing their body representation. &lt;em&gt;Computers in Human Behavior&lt;/em&gt;, &lt;em&gt;91&lt;/em&gt;, 157&amp;ndash;166. &lt;a href=&#34;https://doi.org/10.1016/j.chb.2018.09.021&#34;&gt;https://doi.org/10.1016/j.chb.2018.09.021&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, W., Bansal, D., Daruna, A., &amp;amp; Chernova, S. (2023). Learning instance-level N-ary semantic knowledge at scale for robots operating in everyday environments. &lt;em&gt;Autonomous Robots&lt;/em&gt;, &lt;em&gt;47&lt;/em&gt;(5), 529&amp;ndash;547. &lt;a href=&#34;https://doi.org/10.1007/s10514-023-10099-4&#34;&gt;https://doi.org/10.1007/s10514-023-10099-4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, W., Bansal, D., Daruna, A., &amp;amp; Chernova, S. (2021, July 12). Learning Instance-Level N-Ary Semantic Knowledge At Scale For Robots Operating in Everyday Environments. &lt;em&gt;Robotics: Science and Systems XVII&lt;/em&gt;. Robotics: Science and Systems 2021. &lt;a href=&#34;https://doi.org/10.15607/RSS.2021.XVII.035&#34;&gt;https://doi.org/10.15607/RSS.2021.XVII.035&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Long, Y., Xiang, R., Lu, Q., Huang, C.-R., &amp;amp; Li, M. (2021). Improving Attention Model Based on Cognition Grounded Data for Sentiment Analysis. &lt;em&gt;IEEE Transactions on Affective Computing&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(4), 900&amp;ndash;912. &lt;a href=&#34;https://doi.org/10.1109/TAFFC.2019.2903056&#34;&gt;https://doi.org/10.1109/TAFFC.2019.2903056&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Louwerse, M. M. (2018). Knowing the Meaning of a Word by the Linguistic and Perceptual Company It Keeps. &lt;em&gt;Topics in Cognitive Science&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(3), 573&amp;ndash;589. &lt;a href=&#34;https://doi.org/10.1111/tops.12349&#34;&gt;https://doi.org/10.1111/tops.12349&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lupyan, G., &amp;amp; Winter, B. (2018). Language is more abstract than you think, or, why aren&#39;t languages more iconic? &lt;em&gt;Philosophical Transactions of the Royal Society B: Biological Sciences&lt;/em&gt;, &lt;em&gt;373&lt;/em&gt;(1752), 20170137. &lt;a href=&#34;https://doi.org/10.1098/rstb.2017.0137&#34;&gt;https://doi.org/10.1098/rstb.2017.0137&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lynott, D., Connell, L., Brysbaert, M., Brand, J., &amp;amp; Carney, J. (2020). The Lancaster Sensorimotor Norms: Multidimensional measures of perceptual and action strength for 40,000 English words. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;52&lt;/em&gt;(3), 1271&amp;ndash;1291. &lt;a href=&#34;https://doi.org/10.3758/s13428-019-01316-z&#34;&gt;https://doi.org/10.3758/s13428-019-01316-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lynott, D., Walsh, M., McEnery, T., Connell, L., Cross, L., &amp;amp; O&#39;Brien, K. (2019). Are You What You Read? Predicting Implicit Attitudes to Immigration Based on Linguistic Distributional Cues From Newspaper Readership; A Pre-registered Study. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;, 842. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2019.00842&#34;&gt;https://doi.org/10.3389/fpsyg.2019.00842&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Malhi, S. K., &amp;amp; Buchanan, L. (2018). A test of the symbol interdependency hypothesis with both concrete and abstract stimuli. &lt;em&gt;PLOS ONE&lt;/em&gt;, &lt;em&gt;13&lt;/em&gt;(3), e0192719. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0192719&#34;&gt;https://doi.org/10.1371/journal.pone.0192719&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Mangels, J. A., Rodriguez, S., Ochakovskaya, Y., &amp;amp; Guerra-Carrillo, B. (2017). Achievement Goal Task Framing and Fit With Personal Goals Modulate the Neurocognitive Response to Corrective Feedback. &lt;em&gt;AERA Open&lt;/em&gt;, &lt;em&gt;3&lt;/em&gt;(3), 233285841772087. &lt;a href=&#34;https://doi.org/10.1177/2332858417720875&#34;&gt;https://doi.org/10.1177/2332858417720875&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Marchetti, R., Vaugoyeau, M., Colé, P., &amp;amp; Assaiante, C. (2022). A sensorimotor representation impairment in dyslexic adults: A specific profile of comorbidity. &lt;em&gt;Neuropsychologia&lt;/em&gt;, &lt;em&gt;165&lt;/em&gt;, 108134. &lt;a href=&#34;https://doi.org/10.1016/j.neuropsychologia.2021.108134&#34;&gt;https://doi.org/10.1016/j.neuropsychologia.2021.108134&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Marmeleira, J., &amp;amp; Duarte Santos, G. (2019). Do Not Neglect the Body and Action: The Emergence of Embodiment Approaches to Understanding Human Development. &lt;em&gt;Perceptual and Motor Skills&lt;/em&gt;, &lt;em&gt;126&lt;/em&gt;(3), 410&amp;ndash;445. &lt;a href=&#34;https://doi.org/10.1177/0031512519834389&#34;&gt;https://doi.org/10.1177/0031512519834389&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Marson, F., Paoletti, P., Naor-Ziv, R., Carducci, F., &amp;amp; Ben-Soussan, T. D. (2023). Embodied empathy and abstract concepts&amp;rsquo; concreteness: Evidence from contemplative practices. In &lt;em&gt;Progress in Brain Research&lt;/em&gt; (Vol. 277, pp. 181&amp;ndash;209). Elsevier. &lt;a href=&#34;https://doi.org/10.1016/bs.pbr.2022.12.005&#34;&gt;https://doi.org/10.1016/bs.pbr.2022.12.005&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Márton, Z. C., Türker, S., Rink, C., Brucker, M., Kriegel, S., Bodenmüller, T., &amp;amp; Riedel, S. (2018). Improving object orientation estimates by considering multiple viewpoints: Orientation histograms of symmetries and measurement models for view selection. &lt;em&gt;Autonomous Robots&lt;/em&gt;, &lt;em&gt;42&lt;/em&gt;(2), 423&amp;ndash;442. &lt;a href=&#34;https://doi.org/10.1007/s10514-017-9633-1&#34;&gt;https://doi.org/10.1007/s10514-017-9633-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;McRae, K., Nedjadrasul, D., Pau, R., Lo, B. P., &amp;amp; King, L. (2018). Abstract Concepts and Pictures of Real‐World Situations Activate One Another. &lt;em&gt;Topics in Cognitive Science&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(3), 518&amp;ndash;532. &lt;a href=&#34;https://doi.org/10.1111/tops.12328&#34;&gt;https://doi.org/10.1111/tops.12328&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meng, X., Sun, C., Du, B., Liu, L., Zhang, Y., Dong, Q., Georgiou, G. K., &amp;amp; Nan, Y. (2022). The development of brain rhythms at rest and its impact on vocabulary acquisition. &lt;em&gt;Developmental Science&lt;/em&gt;, &lt;em&gt;25&lt;/em&gt;(2), e13157. &lt;a href=&#34;https://doi.org/10.1111/desc.13157&#34;&gt;https://doi.org/10.1111/desc.13157&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Miceli, A., Wauthia, E., Kandana Arachchige, K., Lefebvre, L., Ris, L., &amp;amp; Simoes Loureiro, I. (2023). Perceptual strength influences lexical decision in Alzheimer&#39;s disease. &lt;em&gt;Journal of Neurolinguistics&lt;/em&gt;, &lt;em&gt;68&lt;/em&gt;, 101144. &lt;a href=&#34;https://doi.org/10.1016/j.jneuroling.2023.101144&#34;&gt;https://doi.org/10.1016/j.jneuroling.2023.101144&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Miceli, A., Wauthia, E., Lefebvre, L., Ris, L., &amp;amp; Simoes Loureiro, I. (2021). Perceptual and Interoceptive Strength Norms for 270 French Words. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;, 667271. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2021.667271&#34;&gt;https://doi.org/10.3389/fpsyg.2021.667271&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Miceli, A., Wauthia, E., Lefebvre, L., Vallet, G. T., Ris, L., &amp;amp; Loureiro, I. S. (2022). Differences related to aging in sensorimotor knowledge: Investigation of perceptual strength and body object interaction. &lt;em&gt;Archives of Gerontology and Geriatrics&lt;/em&gt;, &lt;em&gt;102&lt;/em&gt;, 104715. &lt;a href=&#34;https://doi.org/10.1016/j.archger.2022.104715&#34;&gt;https://doi.org/10.1016/j.archger.2022.104715&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Miklashevsky, A. (2018). Perceptual Experience Norms for 506 Russian Nouns: Modality Rating, Spatial Localization, Manipulability, Imageability and Other Variables. &lt;em&gt;Journal of Psycholinguistic Research&lt;/em&gt;, &lt;em&gt;47&lt;/em&gt;(3), 641&amp;ndash;661. &lt;a href=&#34;https://doi.org/10.1007/s10936-017-9548-1&#34;&gt;https://doi.org/10.1007/s10936-017-9548-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Miller, J., Brookie, K., Wales, S., Wallace, S., &amp;amp; Kaup, B. (2018). Embodied cognition: Is activation of the motor cortex essential for understanding action verbs? &lt;em&gt;Journal of Experimental Psychology: Learning, Memory, and Cognition&lt;/em&gt;, &lt;em&gt;44&lt;/em&gt;(3), 335&amp;ndash;370. &lt;a href=&#34;https://doi.org/10.1037/xlm0000451&#34;&gt;https://doi.org/10.1037/xlm0000451&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Minervino, R. A., Martín, A., Tavernini, L. M., &amp;amp; Trench, M. (2018). The Understanding of Visual Metaphors by the Congenitally Blind. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;, 1242. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2018.01242&#34;&gt;https://doi.org/10.3389/fpsyg.2018.01242&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Moretti, S., &amp;amp; Greco, A. (2018). Truth is in the head. A nod and shake compatibility effect. &lt;em&gt;Acta Psychologica&lt;/em&gt;, &lt;em&gt;185&lt;/em&gt;, 203&amp;ndash;218. &lt;a href=&#34;https://doi.org/10.1016/j.actpsy.2018.02.010&#34;&gt;https://doi.org/10.1016/j.actpsy.2018.02.010&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Moretti, S., &amp;amp; Greco, A. (2020). Nodding and shaking of the head as simulated approach and avoidance responses. &lt;em&gt;Acta Psychologica&lt;/em&gt;, &lt;em&gt;203&lt;/em&gt;, 102988. &lt;a href=&#34;https://doi.org/10.1016/j.actpsy.2019.102988&#34;&gt;https://doi.org/10.1016/j.actpsy.2019.102988&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Morucci, P., Bottini, R., &amp;amp; Crepaldi, D. (2019). Augmented Modality Exclusivity Norms for Concrete and Abstract Italian Property Words. &lt;em&gt;Journal of Cognition&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;(1), 42. &lt;a href=&#34;https://doi.org/10.5334/joc.88&#34;&gt;https://doi.org/10.5334/joc.88&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Mueller, C. J., White, C. N., &amp;amp; Kuchinke, L. (2017). Electrophysiological correlates of the drift diffusion model in visual word recognition. &lt;em&gt;Human Brain Mapping&lt;/em&gt;, &lt;em&gt;38&lt;/em&gt;(11), 5616&amp;ndash;5627. &lt;a href=&#34;https://doi.org/10.1002/hbm.23753&#34;&gt;https://doi.org/10.1002/hbm.23753&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Muraki, E. J., Doyle, A., Protzner, A. B., &amp;amp; Pexman, P. M. (2023). Context matters: How do task demands modulate the recruitment of sensorimotor information during language processing? &lt;em&gt;Frontiers in Human Neuroscience&lt;/em&gt;, &lt;em&gt;16&lt;/em&gt;, 976954. &lt;a href=&#34;https://doi.org/10.3389/fnhum.2022.976954&#34;&gt;https://doi.org/10.3389/fnhum.2022.976954&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Muraki, E. J., Speed, L. J., &amp;amp; Pexman, P. M. (2023). Insights into embodied cognition and mental imagery from aphantasia. &lt;em&gt;Nature Reviews Psychology&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;(10), 591&amp;ndash;605. &lt;a href=&#34;https://doi.org/10.1038/s44159-023-00221-9&#34;&gt;https://doi.org/10.1038/s44159-023-00221-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Okuno, H. Y., &amp;amp; Guedes, G. (2020). Automatic XML creation for Multisensorial Books. &lt;em&gt;2020 XV Conferencia Latinoamericana de Tecnologias de Aprendizaje (LACLO)&lt;/em&gt;, 1&amp;ndash;6. &lt;a href=&#34;https://doi.org/10.1109/LACLO50806.2020.9381139&#34;&gt;https://doi.org/10.1109/LACLO50806.2020.9381139&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ostarek, M., &amp;amp; Vigliocco, G. (2017). Reading sky and seeing a cloud: On the relevance of events for perceptual simulation. &lt;em&gt;Journal of Experimental Psychology: Learning, Memory, and Cognition&lt;/em&gt;, &lt;em&gt;43&lt;/em&gt;(4), 579&amp;ndash;590. &lt;a href=&#34;https://doi.org/10.1037/xlm0000318&#34;&gt;https://doi.org/10.1037/xlm0000318&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pecher, D. (2018). Curb Your Embodiment. &lt;em&gt;Topics in Cognitive Science&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(3), 501&amp;ndash;517. &lt;a href=&#34;https://doi.org/10.1111/tops.12311&#34;&gt;https://doi.org/10.1111/tops.12311&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pérez-Gay Juárez, F., Labrecque, D., &amp;amp; Frak, V. (2019). Assessing language-induced motor activity through Event Related Potentials and the Grip Force Sensor, an exploratory study. &lt;em&gt;Brain and Cognition&lt;/em&gt;, &lt;em&gt;135&lt;/em&gt;, 103572. &lt;a href=&#34;https://doi.org/10.1016/j.bandc.2019.05.010&#34;&gt;https://doi.org/10.1016/j.bandc.2019.05.010&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pérez-Sánchez, M. Á., Stadthagen-Gonzalez, H., Guasch, M., Hinojosa, J. A., Fraga, I., Marín, J., &amp;amp; Ferré, P. (2021). EmoPro &amp;ndash; Emotional prototypicality for 1286 Spanish words: Relationships with affective and psycholinguistic variables. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;53&lt;/em&gt;(5), 1857&amp;ndash;1875. &lt;a href=&#34;https://doi.org/10.3758/s13428-020-01519-9&#34;&gt;https://doi.org/10.3758/s13428-020-01519-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Perfecto, H., Galak, J., Simmons, J. P., &amp;amp; Nelson, L. D. (2017). Rejecting a bad option feels like choosing a good one. &lt;em&gt;Journal of Personality and Social Psychology&lt;/em&gt;, &lt;em&gt;113&lt;/em&gt;(5), 659&amp;ndash;670. &lt;a href=&#34;https://doi.org/10.1037/pspa0000092&#34;&gt;https://doi.org/10.1037/pspa0000092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Perlman, M., Little, H., Thompson, B., &amp;amp; Thompson, R. L. (2018). Iconicity in Signed and Spoken Vocabulary: A Comparison Between American Sign Language, British Sign Language, English, and Spanish. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;, 1433. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2018.01433&#34;&gt;https://doi.org/10.3389/fpsyg.2018.01433&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pexman, P. M. (2019). The role of embodiment in conceptual development. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;34&lt;/em&gt;(10), 1274&amp;ndash;1283. &lt;a href=&#34;https://doi.org/10.1080/23273798.2017.1303522&#34;&gt;https://doi.org/10.1080/23273798.2017.1303522&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pexman, P. M., Muraki, E., Sidhu, D. M., Siakaluk, P. D., &amp;amp; Yap, M. J. (2019). Quantifying sensorimotor experience: Body&amp;ndash;object interaction ratings for more than 9,000 English words. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;51&lt;/em&gt;(2), 453&amp;ndash;466. &lt;a href=&#34;https://doi.org/10.3758/s13428-018-1171-z&#34;&gt;https://doi.org/10.3758/s13428-018-1171-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Piai, V., &amp;amp; Zheng, X. (2019). Speaking waves: Neuronal oscillations in language production. In &lt;em&gt;Psychology of Learning and Motivation&lt;/em&gt; (Vol. 71, pp. 265&amp;ndash;302). Elsevier. &lt;a href=&#34;https://doi.org/10.1016/bs.plm.2019.07.002&#34;&gt;https://doi.org/10.1016/bs.plm.2019.07.002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Plekhanov Russian University of Economics, Simonenko, M. A., Kazaryan, S. Y., &amp;amp; Plekhanov Russian University of Economics. (2023). Synaesthetic metaphor and its reproduction in Russian-to-English translation: A frame-based study. &lt;em&gt;RESEARCH RESULT Theoretical and Applied Linguistics&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;(3). &lt;a href=&#34;https://doi.org/10.18413/2313-8912-2023-9-3-0-2&#34;&gt;https://doi.org/10.18413/2313-8912-2023-9-3-0-2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Popović Stijačić, M., &amp;amp; Filipović Đurđević, D. (2022). Perceptual richness of words and its role in free and cued recall. &lt;em&gt;Primenjena Psihologija&lt;/em&gt;, &lt;em&gt;15&lt;/em&gt;(3), 355&amp;ndash;381. &lt;a href=&#34;https://doi.org/10.19090/pp.v15i3.2400&#34;&gt;https://doi.org/10.19090/pp.v15i3.2400&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pulvermüller, F. (2018a). Neural reuse of action perception circuits for language, concepts and communication. &lt;em&gt;Progress in Neurobiology&lt;/em&gt;, &lt;em&gt;160&lt;/em&gt;, 1&amp;ndash;44. &lt;a href=&#34;https://doi.org/10.1016/j.pneurobio.2017.07.001&#34;&gt;https://doi.org/10.1016/j.pneurobio.2017.07.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pulvermüller, F. (2018b). Neurobiological Mechanisms for Semantic Feature Extraction and Conceptual Flexibility. &lt;em&gt;Topics in Cognitive Science&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(3), 590&amp;ndash;620. &lt;a href=&#34;https://doi.org/10.1111/tops.12367&#34;&gt;https://doi.org/10.1111/tops.12367&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Purves, R. S., Striedl, P., Kong, I., &amp;amp; Majid, A. (2023). Conceptualizing Landscapes Through Language: The Role of Native Language and Expertise in the Representation of Waterbody Related Terms. &lt;em&gt;Topics in Cognitive Science&lt;/em&gt;, &lt;em&gt;15&lt;/em&gt;(3), 560&amp;ndash;583. &lt;a href=&#34;https://doi.org/10.1111/tops.12652&#34;&gt;https://doi.org/10.1111/tops.12652&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Radvansky, G. A. (2017). &lt;em&gt;Human Memory&lt;/em&gt; (3rd ed.). Routledge. &lt;a href=&#34;https://doi.org/10.4324/9781315542768&#34;&gt;https://doi.org/10.4324/9781315542768&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Radvansky, G. A. (2021). &lt;em&gt;Human Memory&lt;/em&gt; (4th ed.). Routledge. &lt;a href=&#34;https://doi.org/10.4324/9780429287039&#34;&gt;https://doi.org/10.4324/9780429287039&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rahimi, S., Farahibozorg, S.-R., Jackson, R., &amp;amp; Hauk, O. (2022). Task modulation of spatiotemporal dynamics in semantic brain networks: An EEG/MEG study. &lt;em&gt;NeuroImage&lt;/em&gt;, &lt;em&gt;246&lt;/em&gt;, 118768. &lt;a href=&#34;https://doi.org/10.1016/j.neuroimage.2021.118768&#34;&gt;https://doi.org/10.1016/j.neuroimage.2021.118768&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Raia, R. (2023). An analysis of conceptual ambiguities in the debate on the format of concepts. &lt;em&gt;Phenomenology and the Cognitive Sciences&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1007/s11097-023-09938-7&#34;&gt;https://doi.org/10.1007/s11097-023-09938-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Raj, R., Hörberg, T., Lindroos, R., Larsson, M., Herman, P., Laukka, E. J., &amp;amp; Olofsson, J. K. (2023). Odor identification errors reveal cognitive aspects of age-associated smell loss. &lt;em&gt;Cognition&lt;/em&gt;, &lt;em&gt;236&lt;/em&gt;, 105445. &lt;a href=&#34;https://doi.org/10.1016/j.cognition.2023.105445&#34;&gt;https://doi.org/10.1016/j.cognition.2023.105445&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Reggin, L. D., Gómez Franco, L. E., Horchak, O. V., Labrecque, D., Lana, N., Rio, L., &amp;amp; Vigliocco, G. (2023). Consensus Paper: Situated and Embodied Language Acquisition. &lt;em&gt;Journal of Cognition&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;(1), 63. &lt;a href=&#34;https://doi.org/10.5334/joc.308&#34;&gt;https://doi.org/10.5334/joc.308&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Repetto, C., Rodella, C., Conca, F., Santi, G. C., &amp;amp; Catricalà, E. (2022). The Italian Sensorimotor Norms: Perception and action strength measures for 959 words. &lt;em&gt;Behavior Research Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3758/s13428-022-02004-1&#34;&gt;https://doi.org/10.3758/s13428-022-02004-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rey, A. E., Riou, B., Vallet, G. T., &amp;amp; Versace, R. (2017). The automatic visual simulation of words: A memory reactivated mask slows down conceptual access. &lt;em&gt;Canadian Journal of Experimental Psychology / Revue Canadienne de Psychologie Expérimentale&lt;/em&gt;, &lt;em&gt;71&lt;/em&gt;(1), 14&amp;ndash;22. &lt;a href=&#34;https://doi.org/10.1037/cep0000100&#34;&gt;https://doi.org/10.1037/cep0000100&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Reymore, L. (2022). Characterizing prototypical musical instrument timbres with timbre trait profiles. &lt;em&gt;Musicae Scientiae&lt;/em&gt;, &lt;em&gt;26&lt;/em&gt;(3), 648&amp;ndash;674. &lt;a href=&#34;https://doi.org/10.1177/10298649211001523&#34;&gt;https://doi.org/10.1177/10298649211001523&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Robinson, M. D., Fetterman, A. K., Meier, B. P., Persich, M. R., &amp;amp; Waters, M. R. (2021). Embodied Perspectives on Personality. In M. D. Robinson &amp;amp; L. E. Thomas (Eds.), &lt;em&gt;Handbook of Embodied Psychology&lt;/em&gt; (pp. 477&amp;ndash;498). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-78471-3_21&#34;&gt;https://doi.org/10.1007/978-3-030-78471-3_21&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Robinson, M. D., &amp;amp; Thomas, L. E. (2021). Introduction to Embodied Psychology: Thinking, Feeling, and Acting. In M. D. Robinson &amp;amp; L. E. Thomas (Eds.), &lt;em&gt;Handbook of Embodied Psychology&lt;/em&gt; (pp. 1&amp;ndash;19). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-78471-3_1&#34;&gt;https://doi.org/10.1007/978-3-030-78471-3_1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;San Roque, L., Kendrick, K. H., Norcliffe, E., &amp;amp; Majid, A. (2018). Universal meaning extensions of perception verbs are grounded in interaction. &lt;em&gt;Cognitive Linguistics&lt;/em&gt;, &lt;em&gt;29&lt;/em&gt;(3), 371&amp;ndash;406. &lt;a href=&#34;https://doi.org/10.1515/cog-2017-0034&#34;&gt;https://doi.org/10.1515/cog-2017-0034&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Scerrati, E., D&#39;Ascenzo, S., Nicoletti, R., Villani, C., &amp;amp; Lugli, L. (2022). Assessing Interpersonal Proximity Evaluation in the COVID-19 Era: Evidence From the Affective Priming Task. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;13&lt;/em&gt;, 901730. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2022.901730&#34;&gt;https://doi.org/10.3389/fpsyg.2022.901730&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Scerrati, E., Iani, C., Lugli, L., Nicoletti, R., &amp;amp; Rubichi, S. (2020). Do my hands prime your hands? The hand-to-response correspondence effect. &lt;em&gt;Acta Psychologica&lt;/em&gt;, &lt;em&gt;203&lt;/em&gt;, 103012. &lt;a href=&#34;https://doi.org/10.1016/j.actpsy.2020.103012&#34;&gt;https://doi.org/10.1016/j.actpsy.2020.103012&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Scerrati, E., Iani, C., &amp;amp; Rubichi, S. (2021). Does the Activation of Motor Information Affect Semantic Processing? In L. Bechberger, K.-U. Kühnberger, &amp;amp; M. Liu (Eds.), &lt;em&gt;Concepts in Action&lt;/em&gt; (Vol. 9, pp. 153&amp;ndash;166). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-69823-2_7&#34;&gt;https://doi.org/10.1007/978-3-030-69823-2_7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Scerrati, E., Lugli, L., Nicoletti, R., &amp;amp; Borghi, A. M. (2017). The Multilevel Modality-Switch Effect: What Happens When We See the Bees Buzzing and Hear the Diamonds Glistening. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, &lt;em&gt;24&lt;/em&gt;(3), 798&amp;ndash;803. &lt;a href=&#34;https://doi.org/10.3758/s13423-016-1150-2&#34;&gt;https://doi.org/10.3758/s13423-016-1150-2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Scherer, D., &amp;amp; Wentura, D. (2018). Combining the Post-Cue Task and the Perceptual Identification Task to Assess Parallel Activation and Mutual Facilitation of Related Primes and Targets. &lt;em&gt;Experimental Psychology&lt;/em&gt;, &lt;em&gt;65&lt;/em&gt;(2), 84&amp;ndash;97. &lt;a href=&#34;https://doi.org/10.1027/1618-3169/a000396&#34;&gt;https://doi.org/10.1027/1618-3169/a000396&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Schilhab, T. (2017). &lt;em&gt;Derived Embodiment in Abstract Language&lt;/em&gt;. Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-319-56056-4&#34;&gt;https://doi.org/10.1007/978-3-319-56056-4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Schulte Im Walde, S., &amp;amp; Frassinelli, D. (2022). Distributional Measures of Semantic Abstraction. &lt;em&gt;Frontiers in Artificial Intelligence&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;, 796756. &lt;a href=&#34;https://doi.org/10.3389/frai.2021.796756&#34;&gt;https://doi.org/10.3389/frai.2021.796756&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Seghier, M. L. (2023). Multiple functions of the angular gyrus at high temporal resolution. &lt;em&gt;Brain Structure and Function&lt;/em&gt;, &lt;em&gt;228&lt;/em&gt;(1), 7&amp;ndash;46. &lt;a href=&#34;https://doi.org/10.1007/s00429-022-02512-y&#34;&gt;https://doi.org/10.1007/s00429-022-02512-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Šetić Beg, M. (2021). Uloga utjelovljenja u razumijevanju pojmova. &lt;em&gt;Psihologijske Teme&lt;/em&gt;, &lt;em&gt;30&lt;/em&gt;(2), 371&amp;ndash;395. &lt;a href=&#34;https://doi.org/10.31820/pt.30.2.12&#34;&gt;https://doi.org/10.31820/pt.30.2.12&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Shen, G., Wang, R., Yang, M., &amp;amp; Xie, J. (2022). Chinese Children with Congenital and Acquired Blindness Represent Concrete Concepts in Vertical Space through Tactile Perception. &lt;em&gt;International Journal of Environmental Research and Public Health&lt;/em&gt;, &lt;em&gt;19&lt;/em&gt;(17), 11055. &lt;a href=&#34;https://doi.org/10.3390/ijerph191711055&#34;&gt;https://doi.org/10.3390/ijerph191711055&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Sidhu, D. M., &amp;amp; Pexman, P. M. (2018). Lonely sensational icons: Semantic neighbourhood density, sensory experience and iconicity. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;33&lt;/em&gt;(1), 25&amp;ndash;31. &lt;a href=&#34;https://doi.org/10.1080/23273798.2017.1358379&#34;&gt;https://doi.org/10.1080/23273798.2017.1358379&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Speed, L. J., &amp;amp; Brybaert, M. (2022). Dutch sensory modality norms. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;54&lt;/em&gt;(3), 1306&amp;ndash;1318. &lt;a href=&#34;https://doi.org/10.3758/s13428-021-01656-9&#34;&gt;https://doi.org/10.3758/s13428-021-01656-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Speed, L. J., &amp;amp; Majid, A. (2017). Dutch modality exclusivity norms: Simulating perceptual modality in space. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;49&lt;/em&gt;(6), 2204&amp;ndash;2218. &lt;a href=&#34;https://doi.org/10.3758/s13428-017-0852-3&#34;&gt;https://doi.org/10.3758/s13428-017-0852-3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Speed, L. J., &amp;amp; Majid, A. (2018). An Exception to Mental Simulation: No Evidence for Embodied Odor Language. &lt;em&gt;Cognitive Science&lt;/em&gt;, &lt;em&gt;42&lt;/em&gt;(4), 1146&amp;ndash;1178. &lt;a href=&#34;https://doi.org/10.1111/cogs.12593&#34;&gt;https://doi.org/10.1111/cogs.12593&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Speed, L. J., &amp;amp; Majid, A. (2020). Grounding language in the neglected senses of touch, taste, and smell. &lt;em&gt;Cognitive Neuropsychology&lt;/em&gt;, &lt;em&gt;37&lt;/em&gt;(5&amp;ndash;6), 363&amp;ndash;392. &lt;a href=&#34;https://doi.org/10.1080/02643294.2019.1623188&#34;&gt;https://doi.org/10.1080/02643294.2019.1623188&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Speed, L. J., Papies, E. K., &amp;amp; Majid, A. (2023). Mental simulation across sensory modalities predicts attractiveness of food concepts. &lt;em&gt;Journal of Experimental Psychology: Applied&lt;/em&gt;, &lt;em&gt;29&lt;/em&gt;(3), 557&amp;ndash;571. &lt;a href=&#34;https://doi.org/10.1037/xap0000461&#34;&gt;https://doi.org/10.1037/xap0000461&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Sri-Ganeshan, M. (2021). Modality Effect. In M. Raz &amp;amp; P. Pouryahya (Eds.), &lt;em&gt;Decision Making in Emergency Medicine&lt;/em&gt; (pp. 215&amp;ndash;220). Springer Singapore. &lt;a href=&#34;https://doi.org/10.1007/978-981-16-0143-9_34&#34;&gt;https://doi.org/10.1007/978-981-16-0143-9_34&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Strik Lievers, F., &amp;amp; Winter, B. (2018). Sensory language across lexical categories. &lt;em&gt;Lingua&lt;/em&gt;, &lt;em&gt;204&lt;/em&gt;, 45&amp;ndash;61. &lt;a href=&#34;https://doi.org/10.1016/j.lingua.2017.11.002&#34;&gt;https://doi.org/10.1016/j.lingua.2017.11.002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Su, C., Wang, X., Wang, Z., &amp;amp; Chen, Y. (2019). A model of synesthetic metaphor interpretation based on cross-modality similarity. &lt;em&gt;Computer Speech &amp;amp; Language&lt;/em&gt;, &lt;em&gt;58&lt;/em&gt;, 1&amp;ndash;16. &lt;a href=&#34;https://doi.org/10.1016/j.csl.2019.03.003&#34;&gt;https://doi.org/10.1016/j.csl.2019.03.003&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Szychowska, M., &amp;amp; Wiens, S. (2021). Visual load effects on the auditory steady-state responses to 20-, 40-, and 80-Hz amplitude-modulated tones. &lt;em&gt;Physiology &amp;amp; Behavior&lt;/em&gt;, &lt;em&gt;228&lt;/em&gt;, 113240. &lt;a href=&#34;https://doi.org/10.1016/j.physbeh.2020.113240&#34;&gt;https://doi.org/10.1016/j.physbeh.2020.113240&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tarai, S., Bit, A., Kumar, R., &amp;amp; Savekar, A. (2021). Processing of party symbols and names predicts the results of 2019 Indian parliamentary election: Analysing psycholinguistic behavioural incongruency effects. &lt;em&gt;Psychology of Language and Communication&lt;/em&gt;, &lt;em&gt;25&lt;/em&gt;(1), 264&amp;ndash;295. &lt;a href=&#34;https://doi.org/10.2478/plc-2021-0012&#34;&gt;https://doi.org/10.2478/plc-2021-0012&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tatiya, G., Hosseini, R., Hughes, M. C., &amp;amp; Sinapov, J. (2019). Sensorimotor Cross-Behavior Knowledge Transfer for Grounded Category Recognition. &lt;em&gt;2019 Joint IEEE 9th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)&lt;/em&gt;, 1&amp;ndash;6. &lt;a href=&#34;https://doi.org/10.1109/DEVLRN.2019.8850715&#34;&gt;https://doi.org/10.1109/DEVLRN.2019.8850715&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tatiya, G., Hosseini, R., Hughes, M. C., &amp;amp; Sinapov, J. (2020). A Framework for Sensorimotor Cross-Perception and Cross-Behavior Knowledge Transfer for Object Categorization. &lt;em&gt;Frontiers in Robotics and AI&lt;/em&gt;, &lt;em&gt;7&lt;/em&gt;, 522141. &lt;a href=&#34;https://doi.org/10.3389/frobt.2020.522141&#34;&gt;https://doi.org/10.3389/frobt.2020.522141&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tatiya, G., &amp;amp; Sinapov, J. (2019). Deep Multi-Sensory Object Category Recognition Using Interactive Behavioral Exploration. &lt;em&gt;2019 International Conference on Robotics and Automation (ICRA)&lt;/em&gt;, 7872&amp;ndash;7878. &lt;a href=&#34;https://doi.org/10.1109/ICRA.2019.8794095&#34;&gt;https://doi.org/10.1109/ICRA.2019.8794095&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Teodorescu, H.-N., &amp;amp; Bolea, S. C. (2019). Text Sectioning based on Stylometric Distances. &lt;em&gt;2019 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)&lt;/em&gt;, 1&amp;ndash;6. &lt;a href=&#34;https://doi.org/10.1109/SPED.2019.8906616&#34;&gt;https://doi.org/10.1109/SPED.2019.8906616&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thomason, J., Padmakumar, A., Sinapov, J., Walker, N., Jiang, Y., Yedidsion, H., Hart, J., Stone, P., &amp;amp; Mooney, R. (2020). Jointly Improving Parsing and Perception for Natural Language Commands through Human-Robot Dialog. &lt;em&gt;Journal of Artificial Intelligence Research&lt;/em&gt;, &lt;em&gt;67&lt;/em&gt;, 327&amp;ndash;374. &lt;a href=&#34;https://doi.org/10.1613/jair.1.11485&#34;&gt;https://doi.org/10.1613/jair.1.11485&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tillman, R., &amp;amp; Louwerse, M. (2018). Estimating Emotions Through Language Statistics and Embodied Cognition. &lt;em&gt;Journal of Psycholinguistic Research&lt;/em&gt;, &lt;em&gt;47&lt;/em&gt;(1), 159&amp;ndash;167. &lt;a href=&#34;https://doi.org/10.1007/s10936-017-9522-y&#34;&gt;https://doi.org/10.1007/s10936-017-9522-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tjuka, A., Forkel, R., &amp;amp; List, J.-M. (2021). Linking norms, ratings, and relations of words and concepts across multiple language varieties. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;54&lt;/em&gt;(2), 864&amp;ndash;884. &lt;a href=&#34;https://doi.org/10.3758/s13428-021-01650-1&#34;&gt;https://doi.org/10.3758/s13428-021-01650-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tomsk State University, Vladimirova, V. E., Rezanova, Z. I., Tomsk State University, Korshunova, I. S., &amp;amp; Tomsk State University. (2022). Ethno-linguistic contact as reflected in language cognition: Does bilingualism affect subjective assessments of perceptual semantics?*. &lt;em&gt;Rusin&lt;/em&gt;, &lt;em&gt;70&lt;/em&gt;, 214&amp;ndash;231. &lt;a href=&#34;https://doi.org/10.17223/18572685/70/12&#34;&gt;https://doi.org/10.17223/18572685/70/12&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Utsumi, A. (2020). Exploring What Is Encoded in Distributional Word Vectors: A Neurobiologically Motivated Analysis. &lt;em&gt;Cognitive Science&lt;/em&gt;, &lt;em&gt;44&lt;/em&gt;(6), e12844. &lt;a href=&#34;https://doi.org/10.1111/cogs.12844&#34;&gt;https://doi.org/10.1111/cogs.12844&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Valenzuela, J. (2017). &lt;em&gt;Meaning in English: An Introduction&lt;/em&gt; (1st ed.). Cambridge University Press. &lt;a href=&#34;https://doi.org/10.1017/9781316156278&#34;&gt;https://doi.org/10.1017/9781316156278&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Van De Weijer, J., Bianchi, I., &amp;amp; Paradis, C. (2023). Sensory modality profiles of antonyms. &lt;em&gt;Language and Cognition&lt;/em&gt;, 1&amp;ndash;15. &lt;a href=&#34;https://doi.org/10.1017/langcog.2023.20&#34;&gt;https://doi.org/10.1017/langcog.2023.20&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Vergallito, A., Petilli, M. A., &amp;amp; Marelli, M. (2020). Perceptual modality norms for 1,121 Italian words: A comparison with concreteness and imageability scores and an analysis of their impact in word processing tasks. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;52&lt;/em&gt;(4), 1599&amp;ndash;1616. &lt;a href=&#34;https://doi.org/10.3758/s13428-019-01337-8&#34;&gt;https://doi.org/10.3758/s13428-019-01337-8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Verheyen, S., De Deyne, S., Linsen, S., &amp;amp; Storms, G. (2020). Lexicosemantic, affective, and distributional norms for 1,000 Dutch adjectives. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;52&lt;/em&gt;(3), 1108&amp;ndash;1121. &lt;a href=&#34;https://doi.org/10.3758/s13428-019-01303-4&#34;&gt;https://doi.org/10.3758/s13428-019-01303-4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Vigliocco, G., Zhang, Y., Del Maschio, N., Todd, R., &amp;amp; Tuomainen, J. (2020). Electrophysiological signatures of English onomatopoeia. &lt;em&gt;Language and Cognition&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(1), 15&amp;ndash;35. &lt;a href=&#34;https://doi.org/10.1017/langcog.2019.38&#34;&gt;https://doi.org/10.1017/langcog.2019.38&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Volfart, A., Rice, G. E., Lambon Ralph, M. A., &amp;amp; Rossion, B. (2021). Implicit, automatic semantic word categorisation in the left occipito-temporal cortex as revealed by fast periodic visual stimulation. &lt;em&gt;NeuroImage&lt;/em&gt;, &lt;em&gt;238&lt;/em&gt;, 118228. &lt;a href=&#34;https://doi.org/10.1016/j.neuroimage.2021.118228&#34;&gt;https://doi.org/10.1016/j.neuroimage.2021.118228&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;W, D., C, P., C, M.-L., &amp;amp; F, L. (2023). Imagining and reading actions: Towards similar motor representations. &lt;em&gt;Heliyon&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;(2), e13426. &lt;a href=&#34;https://doi.org/10.1016/j.heliyon.2023.e13426&#34;&gt;https://doi.org/10.1016/j.heliyon.2023.e13426&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wajnerman Paz, A. (2018). A Defense of an Amodal Number System. &lt;em&gt;Philosophies&lt;/em&gt;, &lt;em&gt;3&lt;/em&gt;(2), 13. &lt;a href=&#34;https://doi.org/10.3390/philosophies3020013&#34;&gt;https://doi.org/10.3390/philosophies3020013&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wajnerman Paz, A. (2019). Using neural response properties to draw the distinction between modal and amodal representations. &lt;em&gt;Philosophical Psychology&lt;/em&gt;, &lt;em&gt;32&lt;/em&gt;(3), 301&amp;ndash;331. &lt;a href=&#34;https://doi.org/10.1080/09515089.2018.1563677&#34;&gt;https://doi.org/10.1080/09515089.2018.1563677&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wajnerman-Paz, A., &amp;amp; Rojas-Líbano, D. (2022). On the role of contextual factors in cognitive neuroscience experiments: A mechanistic approach. &lt;em&gt;Synthese&lt;/em&gt;, &lt;em&gt;200&lt;/em&gt;(5), 402. &lt;a href=&#34;https://doi.org/10.1007/s11229-022-03870-0&#34;&gt;https://doi.org/10.1007/s11229-022-03870-0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wan, M., Su, Q., Ahrens, K., &amp;amp; Huang, C.-R. (2023). Perceptional and actional enrichment for metaphor detection with sensorimotor norms. &lt;em&gt;Natural Language Engineering&lt;/em&gt;, 1&amp;ndash;29. &lt;a href=&#34;https://doi.org/10.1017/S135132492300044X&#34;&gt;https://doi.org/10.1017/S135132492300044X&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wang, L., Chen, Q., Chen, Y., &amp;amp; Zhong, R. (2019). The Effect of Sweet Taste on Romantic Semantic Processing: An ERP Study. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;, 1573. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2019.01573&#34;&gt;https://doi.org/10.3389/fpsyg.2019.01573&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wang, Y., &amp;amp; Zeng, Y. (2022a). Multisensory Concept Learning Framework Based on Spiking Neural Networks. &lt;em&gt;Frontiers in Systems Neuroscience&lt;/em&gt;, &lt;em&gt;16&lt;/em&gt;, 845177. &lt;a href=&#34;https://doi.org/10.3389/fnsys.2022.845177&#34;&gt;https://doi.org/10.3389/fnsys.2022.845177&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wang, Y., &amp;amp; Zeng, Y. (2022b). Statistical Analysis of Multisensory and Text-Derived Representations on Concept Learning. &lt;em&gt;Frontiers in Computational Neuroscience&lt;/em&gt;, &lt;em&gt;16&lt;/em&gt;, 861265. &lt;a href=&#34;https://doi.org/10.3389/fncom.2022.861265&#34;&gt;https://doi.org/10.3389/fncom.2022.861265&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wassenburg, S. I., De Koning, B. B., De Vries, M. H., Boonstra, A. M., &amp;amp; Van Der Schoot, M. (2017). Gender differences in mental simulation during sentence and word processing. &lt;em&gt;Journal of Research in Reading&lt;/em&gt;, &lt;em&gt;40&lt;/em&gt;(3), 274&amp;ndash;296. &lt;a href=&#34;https://doi.org/10.1111/1467-9817.12066&#34;&gt;https://doi.org/10.1111/1467-9817.12066&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wentura, D. (2019). Cognition and emotion: On paradigms and metaphors. &lt;em&gt;Cognition and Emotion&lt;/em&gt;, &lt;em&gt;33&lt;/em&gt;(1), 85&amp;ndash;93. &lt;a href=&#34;https://doi.org/10.1080/02699931.2019.1567464&#34;&gt;https://doi.org/10.1080/02699931.2019.1567464&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wingfield, C., &amp;amp; Connell, L. (2022a). Sensorimotor distance: A grounded measure of semantic similarity for 800 million concept pairs. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;55&lt;/em&gt;(7), 3416&amp;ndash;3432. &lt;a href=&#34;https://doi.org/10.3758/s13428-022-01965-7&#34;&gt;https://doi.org/10.3758/s13428-022-01965-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wingfield, C., &amp;amp; Connell, L. (2022b). Understanding the role of linguistic distributional knowledge in cognition. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;37&lt;/em&gt;(10), 1220&amp;ndash;1270. &lt;a href=&#34;https://doi.org/10.1080/23273798.2022.2069278&#34;&gt;https://doi.org/10.1080/23273798.2022.2069278&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, A., Dudschig, C., Miller, J., Ulrich, R., &amp;amp; Kaup, B. (2022). The action-sentence compatibility effect (ACE): Meta-analysis of a benchmark finding for embodiment. &lt;em&gt;Acta Psychologica&lt;/em&gt;, &lt;em&gt;230&lt;/em&gt;, 103712. &lt;a href=&#34;https://doi.org/10.1016/j.actpsy.2022.103712&#34;&gt;https://doi.org/10.1016/j.actpsy.2022.103712&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B. (2019). &lt;em&gt;Statistics for Linguists: An Introduction Using R&lt;/em&gt; (1st ed.). Routledge. &lt;a href=&#34;https://doi.org/10.4324/9781315165547&#34;&gt;https://doi.org/10.4324/9781315165547&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B. (2022). Mapping the landscape of exploratory and confirmatory data analysis in linguistics. In D. Tay &amp;amp; M. X. Pan (Eds.), &lt;em&gt;Data Analytics in Cognitive Linguistics&lt;/em&gt; (pp. 13&amp;ndash;48). De Gruyter. &lt;a href=&#34;https://doi.org/10.1515/9783110687279-002&#34;&gt;https://doi.org/10.1515/9783110687279-002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B. (2023). Abstract concepts and emotion: Cross-linguistic evidence and arguments against affective embodiment. &lt;em&gt;Philosophical Transactions of the Royal Society B: Biological Sciences&lt;/em&gt;, &lt;em&gt;378&lt;/em&gt;(1870), 20210368. &lt;a href=&#34;https://doi.org/10.1098/rstb.2021.0368&#34;&gt;https://doi.org/10.1098/rstb.2021.0368&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B., Fischer, M. H., Scheepers, C., &amp;amp; Myachykov, A. (2023). More is Better: English Language Statistics are Biased Toward Addition. &lt;em&gt;Cognitive Science&lt;/em&gt;, &lt;em&gt;47&lt;/em&gt;(4), e13254. &lt;a href=&#34;https://doi.org/10.1111/cogs.13254&#34;&gt;https://doi.org/10.1111/cogs.13254&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B., Perlman, M., &amp;amp; Majid, A. (2018). Vision dominates in perceptual language: English sensory vocabulary is optimized for usage. &lt;em&gt;Cognition&lt;/em&gt;, &lt;em&gt;179&lt;/em&gt;, 213&amp;ndash;220. &lt;a href=&#34;https://doi.org/10.1016/j.cognition.2018.05.008&#34;&gt;https://doi.org/10.1016/j.cognition.2018.05.008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B., Perlman, M., Perry, L. K., &amp;amp; Lupyan, G. (2017). Which words are most iconic?: Iconicity in English sensory words. &lt;em&gt;Interaction Studies. Social Behaviour and Communication in Biological and Artificial Systems&lt;/em&gt;, &lt;em&gt;18&lt;/em&gt;(3), 443&amp;ndash;464. &lt;a href=&#34;https://doi.org/10.1075/is.18.3.07win&#34;&gt;https://doi.org/10.1075/is.18.3.07win&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B., Sóskuthy, M., Perlman, M., &amp;amp; Dingemanse, M. (2022). Trilled /r/ is associated with roughness, linking sound and touch across spoken languages. &lt;em&gt;Scientific Reports&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(1), 1035. &lt;a href=&#34;https://doi.org/10.1038/s41598-021-04311-7&#34;&gt;https://doi.org/10.1038/s41598-021-04311-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B., &amp;amp; Strik-Lievers, F. (2023). Semantic distance predicts metaphoricity and creativity judgments in synesthetic metaphors. &lt;em&gt;Metaphor and the Social World&lt;/em&gt;, &lt;em&gt;13&lt;/em&gt;(1), 59&amp;ndash;80. &lt;a href=&#34;https://doi.org/10.1075/msw.00029.win&#34;&gt;https://doi.org/10.1075/msw.00029.win&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wu, C., &amp;amp; Mu, X. (2023). Sensory experience ratings (SERs) for 1,130 Chinese words: Relationships with other semantic and lexical psycholinguistic variables. &lt;em&gt;Linguistics Vanguard&lt;/em&gt;, &lt;em&gt;0&lt;/em&gt;(0). &lt;a href=&#34;https://doi.org/10.1515/lingvan-2022-0083&#34;&gt;https://doi.org/10.1515/lingvan-2022-0083&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Xiong, J., &amp;amp; Huang, C.-R. (2018). Somewhere in COLDNESS Lies Nibbāna: Lexical Manifestations of COLDNESS. In J.-F. Hong, Q. Su, &amp;amp; J.-S. Wu (Eds.), &lt;em&gt;Chinese Lexical Semantics&lt;/em&gt; (Vol. 11173, pp. 70&amp;ndash;81). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-04015-4_6&#34;&gt;https://doi.org/10.1007/978-3-030-04015-4_6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Xu, Y., Vignali, L., Sigismondi, F., Crepaldi, D., Bottini, R., &amp;amp; Collignon, O. (2023). Similar object shape representation encoded in the inferolateral occipitotemporal cortex of sighted and early blind people. &lt;em&gt;PLOS Biology&lt;/em&gt;, &lt;em&gt;21&lt;/em&gt;(7), e3001930. &lt;a href=&#34;https://doi.org/10.1371/journal.pbio.3001930&#34;&gt;https://doi.org/10.1371/journal.pbio.3001930&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yan, J., Li, W., Zhang, T., Zhang, J., Jin, Z., &amp;amp; Li, L. (2023). Structural and functional neural substrates underlying the concreteness effect. &lt;em&gt;Brain Structure and Function&lt;/em&gt;, &lt;em&gt;228&lt;/em&gt;(6), 1493&amp;ndash;1510. &lt;a href=&#34;https://doi.org/10.1007/s00429-023-02668-1&#34;&gt;https://doi.org/10.1007/s00429-023-02668-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yasuda, M., Stins, J. F., &amp;amp; Higuchi, T. (2017). Effect of Constrained Arm Posture on the Processing of Action Verbs. &lt;em&gt;Frontiers in Neuroscience&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3389/fnins.2017.00057&#34;&gt;https://doi.org/10.3389/fnins.2017.00057&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yin Zhong, &amp;amp; Chu-Ren Huang. (2020). Sweetness or Mouthfeel: A corpus-based study of the conceptualization of taste. &lt;em&gt;Linguistic Research&lt;/em&gt;, &lt;em&gt;37&lt;/em&gt;(3), 359&amp;ndash;387. &lt;a href=&#34;https://doi.org/10.17250/KHISLI.37.3.202012.001&#34;&gt;https://doi.org/10.17250/KHISLI.37.3.202012.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zappa, A., Mestre, D., Pergandi, J.-M., Bolger, D., &amp;amp; Frenck-Mestre, C. (2022). Cross-linguistic gender congruency effects during lexical access in novice L2 learners: Evidence from ERPs. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;37&lt;/em&gt;(9), 1073&amp;ndash;1098. &lt;a href=&#34;https://doi.org/10.1080/23273798.2022.2039726&#34;&gt;https://doi.org/10.1080/23273798.2022.2039726&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zeng, Y., Zhao, D., Zhao, F., Shen, G., Dong, Y., Lu, E., Zhang, Q., Sun, Y., Liang, Q., Zhao, Y., Zhao, Z., Fang, H., Wang, Y., Li, Y., Liu, X., Du, C., Kong, Q., Ruan, Z., &amp;amp; Bi, W. (2023). BrainCog: A spiking neural network based, brain-inspired cognitive intelligence engine for brain-inspired AI and brain simulation. &lt;em&gt;Patterns&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(8), 100789. &lt;a href=&#34;https://doi.org/10.1016/j.patter.2023.100789&#34;&gt;https://doi.org/10.1016/j.patter.2023.100789&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhang, M., Wu, X., &amp;amp; Wang, A. (2021). Crossmodal Nonspatial Repetition Inhibition Due to Modality Shift. &lt;em&gt;Perception&lt;/em&gt;, &lt;em&gt;50&lt;/em&gt;(2), 116&amp;ndash;128. &lt;a href=&#34;https://doi.org/10.1177/0301006620988209&#34;&gt;https://doi.org/10.1177/0301006620988209&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhang, X., Amiri, S., Sinapov, J., Thomason, J., Stone, P., &amp;amp; Zhang, S. (2023). Multimodal embodied attribute learning by robots for object-centric action policies. &lt;em&gt;Autonomous Robots&lt;/em&gt;, &lt;em&gt;47&lt;/em&gt;(5), 505&amp;ndash;528. &lt;a href=&#34;https://doi.org/10.1007/s10514-023-10098-5&#34;&gt;https://doi.org/10.1007/s10514-023-10098-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhang, X., Sinapov, J., &amp;amp; Zhang, S. (2021, July 12). Planning Multimodal Exploratory Actions for Online Robot Attribute Learning. &lt;em&gt;Robotics: Science and Systems XVII&lt;/em&gt;. Robotics: Science and Systems 2021. &lt;a href=&#34;https://doi.org/10.15607/RSS.2021.XVII.005&#34;&gt;https://doi.org/10.15607/RSS.2021.XVII.005&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhang, Y., Mirman, D., &amp;amp; Hoffman, P. (2023). Taxonomic and thematic relations rely on different types of semantic features: Evidence from an fMRI meta-analysis and a semantic priming study. &lt;em&gt;Brain and Language&lt;/em&gt;, &lt;em&gt;242&lt;/em&gt;, 105287. &lt;a href=&#34;https://doi.org/10.1016/j.bandl.2023.105287&#34;&gt;https://doi.org/10.1016/j.bandl.2023.105287&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q. (2020a). From Linguistic Synaesthesia to Conceptual Metaphor Theory. In Q. Zhao, &lt;em&gt;Embodied Conceptualization or Neural Realization&lt;/em&gt; (Vol. 10, pp. 115&amp;ndash;128). Springer Singapore. &lt;a href=&#34;https://doi.org/10.1007/978-981-32-9315-1_7&#34;&gt;https://doi.org/10.1007/978-981-32-9315-1_7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q. (2020b). Methodology: A Corpus-Driven Approach. In Q. Zhao, &lt;em&gt;Embodied Conceptualization or Neural Realization&lt;/em&gt; (Vol. 10, pp. 19&amp;ndash;34). Springer Singapore. &lt;a href=&#34;https://doi.org/10.1007/978-981-32-9315-1_2&#34;&gt;https://doi.org/10.1007/978-981-32-9315-1_2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q., Ahrens, K., &amp;amp; Huang, C.-R. (2022). Linguistic synesthesia is metaphorical: A lexical-conceptual account. &lt;em&gt;Cognitive Linguistics&lt;/em&gt;, &lt;em&gt;33&lt;/em&gt;(3), 553&amp;ndash;583. &lt;a href=&#34;https://doi.org/10.1515/cog-2021-0098&#34;&gt;https://doi.org/10.1515/cog-2021-0098&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q., Huang, C.-R., &amp;amp; Ahrens, K. (2019). Directionality of linguistic synesthesia in Mandarin: A corpus-based study. &lt;em&gt;Lingua&lt;/em&gt;, &lt;em&gt;232&lt;/em&gt;, 102744. &lt;a href=&#34;https://doi.org/10.1016/j.lingua.2019.102744&#34;&gt;https://doi.org/10.1016/j.lingua.2019.102744&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q., &amp;amp; Long, Y. (2022). A Diachronic Study on Linguistic Synesthesia in Chinese. In M. Dong, Y. Gu, &amp;amp; J.-F. Hong (Eds.), &lt;em&gt;Chinese Lexical Semantics&lt;/em&gt; (Vol. 13250, pp. 84&amp;ndash;94). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-031-06547-7_6&#34;&gt;https://doi.org/10.1007/978-3-031-06547-7_6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q., Long, Y., &amp;amp; Huang, C.-R. (2020). Linguistic Synaesthesia of Mandarin Sensory Adjectives: Corpus-Based and Experimental Approaches. In J.-F. Hong, Y. Zhang, &amp;amp; P. Liu (Eds.), &lt;em&gt;Chinese Lexical Semantics&lt;/em&gt; (Vol. 11831, pp. 139&amp;ndash;146). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-38189-9_14&#34;&gt;https://doi.org/10.1007/978-3-030-38189-9_14&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, T., Huang, Y., Chen, D., Jiao, L., Marmolejo-Ramos, F., Wang, R., &amp;amp; Xie, J. (2020). The modality switching costs of Chinese&amp;ndash;English bilinguals in the processing of L1 and L2. &lt;em&gt;Quarterly Journal of Experimental Psychology&lt;/em&gt;, &lt;em&gt;73&lt;/em&gt;(3), 396&amp;ndash;412. &lt;a href=&#34;https://doi.org/10.1177/1747021819878089&#34;&gt;https://doi.org/10.1177/1747021819878089&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhong, J., Peniak, M., Tani, J., Ogata, T., &amp;amp; Cangelosi, A. (2019). Sensorimotor input as a language generalisation tool: A neurorobotics model for generation and generalisation of noun-verb combinations with sensorimotor inputs. &lt;em&gt;Autonomous Robots&lt;/em&gt;, &lt;em&gt;43&lt;/em&gt;(5), 1271&amp;ndash;1290. &lt;a href=&#34;https://doi.org/10.1007/s10514-018-9793-7&#34;&gt;https://doi.org/10.1007/s10514-018-9793-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhong, Y., Huang, C.-R., &amp;amp; Dong, S. (2022). Bodily sensation and embodiment: A corpus-based study of gustatory vocabulary in Mandarin Chinese. &lt;em&gt;Journal of Chinese Linguistics&lt;/em&gt;, &lt;em&gt;50&lt;/em&gt;(1), 196&amp;ndash;230. &lt;a href=&#34;https://doi.org/10.1353/jcl.2022.0008&#34;&gt;https://doi.org/10.1353/jcl.2022.0008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhong, Y., Wan, M., Ahrens, K., &amp;amp; Huang, C.-R. (2022). Sensorimotor norms for Chinese nouns and their relationship with orthographic and semantic variables. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;37&lt;/em&gt;(8), 1000&amp;ndash;1022. &lt;a href=&#34;https://doi.org/10.1080/23273798.2022.2035416&#34;&gt;https://doi.org/10.1080/23273798.2022.2035416&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhu, S., Wang, X., &amp;amp; Liu, P. (2021). Who Killed Sanmao and Virginia Woolf? A Comparative Study of Writers with Suicidal Attempt Based on a Quantitative Linguistic Method. In M. Liu, C. Kit, &amp;amp; Q. Su (Eds.), &lt;em&gt;Chinese Lexical Semantics&lt;/em&gt; (Vol. 12278, pp. 408&amp;ndash;420). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-81197-6_34&#34;&gt;https://doi.org/10.1007/978-3-030-81197-6_34&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Гарах, Ж. В., Ребрейкина, А. Б., Стрелец, В. Б., Голикова, А. В., &amp;amp; Зайцева, Ю. С. (2019). НЕЙРОФИЗИОЛОГИЧЕСКИЕ МЕХАНИЗМЫ ЧТЕНИЯ. &lt;em&gt;Журнал высшей нервной деятельности им И П Павлова&lt;/em&gt;, &lt;em&gt;69&lt;/em&gt;(3), 294&amp;ndash;313. &lt;a href=&#34;https://doi.org/10.1134/S0044467719030055&#34;&gt;https://doi.org/10.1134/S0044467719030055&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>At Greg, 8 am</title>
      <link>https://pablobernabeu.github.io/2017/at-greg-8-am/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2017/at-greg-8-am/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2017/at-greg-8-am/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The clock strikes a certain hour, below all the Greg’s teaspoons at play. Results o’clock. The usual, please.&lt;/p&gt;
&lt;p&gt;Usual table. &lt;code&gt;summaryby&lt;/code&gt; (having to get the first peek in the cafeteria can only add zest). &lt;code&gt;summaryBy(RT ~ list(Ptp, Group, Cond), behdata, FUN=summary)&lt;/code&gt;. So, hardly any of the 95% Confidence Intervals contain 0. Does this really mean…?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‘For example, the hypothesis of equality of population means will be rejected at the 0.05 level if and only if a 95% CI for the mean difference does not contain 0.’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;— Dallal (2002; &lt;a href=&#34;http://www.jerrydallal.com/lhsp/pval.htm&#34; class=&#34;uri&#34;&gt;http://www.jerrydallal.com/lhsp/pval.htm&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Of course. The CI just has that and more. The window is showing a chilly 1999 morning. Let’s see the summary again. Wee standard deviations. By card, please.&lt;/p&gt;
&lt;p&gt;Mmm, the air outside is worth gingering up…&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The trials!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The assumption of independence spoils another morning.&lt;/p&gt;
&lt;p&gt;This new data consisted of response times (RT) that had been collected over several trials. The single dependent variable, RT, was accompanied by other variables which could be analyzed as independent variables. These included &lt;em&gt;Group&lt;/em&gt;, &lt;em&gt;Trial Number&lt;/em&gt;, and a within-subjects &lt;em&gt;Condition&lt;/em&gt;. &lt;strong&gt;What had to be done first off, in order to take the usual table?&lt;/strong&gt; &lt;em&gt;The trials!&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;assumption-of-independence-of-observations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assumption of independence of observations&lt;/h2&gt;
&lt;p&gt;One must account for any redundant measures below the level of participants (the experimental trials, in this case), so that the sample size (&lt;em&gt;N&lt;/em&gt;) used for any summary statistics match the number of participants (or the largest group, &lt;em&gt;n&lt;/em&gt;). Why? This is a &lt;a href=&#34;https://stats.stackexchange.com/questions/130019/standard-error-for-aggregated-proportions&#34;&gt;central assumption in statistics&lt;/a&gt;: observations must be independent. We can observe the independence assumption differently, depending on whether we’re summarizing data or performing statistical tests.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For &lt;em&gt;descriptive tables and plots&lt;/em&gt; (involving Standard Error/Deviation, Confidence Intervals, etc), &lt;em&gt;the data ought to be aggregated to the level from which you want to generalize&lt;/em&gt;. That level is—in this case and very often—&lt;em&gt;participants&lt;/em&gt;. Trials do not normally serve for statistical generalization (they’re good for experimental validity). This realization may come as a bummer if you have first seen the effect sizes in the un-aggregated data. The mirage (see red lines on the left table below) is caused by an inflated &lt;em&gt;N&lt;/em&gt; (cf. red lines on the right-hand table). As an illustration, the tables below summarize data with an actual sample &lt;em&gt;n&lt;/em&gt; = 23. However, the table on the right includes repeated measures that should have been aggregated, massively inflating &lt;em&gt;n&lt;/em&gt;. The inflation of the sample size equals the product of all repeated measures that failed to be aggregated under participants.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;inflated.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#39;30%&#39; src=&#39;SD.jpg&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Measures of variance such as the Standard Deviation divide by the sample size. Thus, the larger the sample (N), the smaller the Standard Deviation, Standard Error, Confidence Interval…—that is, the variation or noise.&lt;/p&gt;
&lt;p&gt;Aggregating is a snap. For example, with the aggregate() function in R, you just have to include all of your variables except that or those of the repeated measures:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;behdata_aggreg = aggregate(behdata$RT, list(behdata$Ptp, behdata$Group, behdata$Cond), 
  data=behdata, FUN=mean)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;In statistical tests, repeated measures below the participant level–e.g., trials–normally must be either factored in or aggregated. Barr and colleagues provide an easy, focused &lt;a href=&#34;http://talklab.psy.gla.ac.uk/simgen/faq.html#sec-3&#34;&gt;guide on this procedure&lt;/a&gt;. This is necessary because when the N in the analyses is augmented by unaccounted, redundant observations, &lt;em&gt;the famous assumption of independence of observations is violated&lt;/em&gt;, and the results may be invalid, as &lt;a href=&#34;https://arxiv.org/pdf/1601.01126.pdf&#34;&gt;Vasishth and Nicenboim (2016, p. 3)&lt;/a&gt; put it:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;‘if we were to do a t-test on the unaggregated data, we would violate the independence assumption and the result of the t-test would be invalid.’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now, usually the repetitions that concern us are the multiple trials or items in experiments, or other sub-participant measures. So what about participants–what are they never aggregated? &lt;a href=&#34;http://tandfonline.com.sci-hub.cc/doi/abs/10.1080/01933922.2016.1264520?journalCode=usgw20&#34;&gt;McCarthy, Whittaker, Boyle, and Eyal (2017, p.10)&lt;/a&gt; note:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‘It has also been proposed that researchers aggregate the responses of participants within the same group and use the groups/clusters as the unit of analysis (Stevens, 2007). However, because this would result in losing sample size at the participant level, this approach is not optimal given the already small numbers of groups typically studied in group work research.’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;different-procedure-in-linear-mixed-effects-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Different procedure in linear mixed-effects models&lt;/h2&gt;
&lt;p&gt;Aggregation is no longer necessary, where linear mixed-effects models can be used. These models allow us to &lt;a href=&#34;http://talklab.psy.gla.ac.uk/simgen/faq.html#sec-3&#34;&gt;account for any clusters (Participants, Trials, Items…) by signing them into the error term&lt;/a&gt; (&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer_and_Curtin_LMEMs-2017-Psych_Methods.pdf&#34;&gt;Brauer &amp;amp; Curtin, 2017&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs</title>
      <link>https://pablobernabeu.github.io/2017/modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing-evidence-from-erps/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2017/modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing-evidence-from-erps/</guid>
      <description>&lt;p&gt;Research has extensively investigated whether conceptual processing is modality-specific—that is, whether meaning is processed to a large extent on the basis of perceptual and motor affordances (Barsalou, 2016). This possibility challenges long-established theories. It suggests a strong link between physical experience and language which is not borne out of the paradigmatic arbitrariness of words (see Lockwood, Dingemanse, &amp;amp; Hagoort, 2016). Modality-specificity also clashes with models of language that have no link to sensory and motor systems (Barsalou, 2016).&lt;/p&gt;
&lt;a href=&#39;https://pablobernabeu.github.io/publication/bernabeu-etal-2017/&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #4CAF50; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt; 
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;i class=&#34;fas fa-mouse-pointer&#34;&gt;&lt;/i&gt;&lt;/span&gt;&amp;nbsp; Conference paper &lt;/h3&gt;&lt;/button&gt;&lt;/a&gt; &amp;nbsp; 
&lt;a href=&#39;https://pablobernabeu.github.io/publication/bernabeu-2017-mphil-thesis/&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #196F27; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt; 
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;i class=&#34;fas fa-mouse-pointer&#34;&gt;&lt;/i&gt;&lt;/span&gt;&amp;nbsp; Master&#39;s thesis &lt;/h3&gt;&lt;/button&gt;&lt;/a&gt; &amp;nbsp; 
&lt;br&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/post/Conceptual_modality_switch_effect_measured_at_first_word&#34;&gt;Early discussion on ResearchGate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://osf.io/97unm/&#34;&gt;Data and code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pablobernabeu.github.io/applications-and-dashboards/bernabeu-etal-2017-modalityswitch/&#34;&gt;Data dashboard&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the Conceptual Modality Switch (CMS) paradigm, participants perform a property verification task, deciding whether certain property words can reasonably describe concept words. Covertly, the conceptual modality of consecutive trials is manipulated in order to produce specific switches in conceptual modality. For instance, after the trial &lt;em&gt;Soundless Answer&lt;/em&gt;, which is primarily auditory, the following trial may match in modality—&lt;em&gt;Loud Welcome&lt;/em&gt;—or mismatch—&lt;em&gt;Fine Selection&lt;/em&gt; (visual).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;designoverview.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Modality switches incur processing costs, as reflected in brain signals linked to semantic violation, and in longer response times (RTs) (Scerrati, Lugli, Nicoletti, &amp;amp; Borghi, 2016). This effect suggests that perceptual features of concepts are accessed during conceptual processing. More recently, however, the CMS effect was reanalysed using a non-perceptual alternative. Louwerse and Connell (2011) found that language statistics (the co-occurrence of words in a language) were able to approximately predict visual/haptic, olfactory/gustatory, and auditory modalities, but not the subtler differences between visual and haptic and between olfactory and gustatory, which seemed to be reserved for perceptual simulations. Moreover, faster response times (RTs) were best explained by language statistics, whereas slower RTs were best explained by perceptual simulations.&lt;/p&gt;
&lt;p&gt;The time course of word processing is important. Research suggests that word processing spans one second, during which different processes—semantic and post-semantic—gradually accumulate (Hauk, 2016). The later an effect, the more reasons to question it. Yet, having an early emergence does not either make an effect lexicosemantic, as the meaning encoded could have gone through working memory before activating the actual system of interest, e.g., sensorimotor (Mahon &amp;amp; Caramazza, 2008). Research also suggests that modal systems may contribute to conceptual processing early on—within 200 ms (Vukovic, Feurra, Shpektor, Myachykov, &amp;amp; Shtyrov, 2017). Thus, measuring effects online may prove valuable.&lt;/p&gt;
&lt;h2 id=&#34;experiment&#34;&gt;Experiment&lt;/h2&gt;
&lt;p&gt;Bernabeu, Willems and Louwerse (2017) investigated whether CMS reflects a functionally relevant process of simulation or instead arises only after basic conceptual processing has been attained. We also examined whether different processing systems, amodal and modal, may compatibly operate.&lt;/p&gt;
&lt;p&gt;We measured CMS online by time-locking Event-Related brain Potentials (ERPs) to the onset of the first word in the target trials, in order to assess how strongly CMS may be influenced by post-semantic processes. Previous research would predict an increase in the CMS effect over time because earlier processing is relatively amodal (Louwerse &amp;amp; Hutchinson, 2012).&lt;/p&gt;
&lt;p&gt;We tested the compatibility of amodal and modal processing by drawing on Louwerse and Connell’s (2011) findings. In this conceptual replication, we split participants into a Quick and a Slow group based on RT. Maintaining CMS as a within-subjects factor, we predicted that the larger modality switches (e.g., auditory to visual) would be picked up equally by both groups, whereas the subtler switches (e.g., haptic to visual) would be picked up only—or more clearly—by the Slow group.&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;The stimuli were normed (Bernabeu, Louwerse, &amp;amp; Willems, in prep.). Three CMS conditions were created—Auditory-to-visual, Haptic-to-visual, Visual-to-visual—, each with 36 target trials. The property verification task was pretested valid (&lt;em&gt;N&lt;/em&gt; = 19).&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;All participants but one responded correctly to over half of the trials, with an overall accuracy of 63%.&lt;/p&gt;
&lt;p&gt;ERPs showed a CMS effect from time window 1 on, larger after 350 ms. It appeared with both switch conditions, and was characterized by a more negative amplitude for the switch conditions compared to the no-switch condition. It was generally stronger in the posterior brain regions, and in the Slow group. The results are illustrated in the figure below, which includes 95% Confidence Intervals and time windows.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;stackERPs.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#39;50%&#39; src=&#39;results.jpg&#39; /&gt;&lt;/p&gt;
&lt;p&gt;The analysis was done with Linear Mixed Effects models. Final models presented good fits, with R&lt;sup&gt;2&lt;/sup&gt; ranging from .748 to .862. First, the CMS effect in time window 1 was confirmed significant. Such an early emergence is unprecedented in the CMS literature, and it may have been enabled by the time-locking of ERPs to the first word in target trials. In this time window, the only process not lexicosemantic is possibly working memory (Hauk, 2016), and therefore this early emergence adds support to the possibility that CMS was directly caused by perceptual simulation.&lt;/p&gt;
&lt;p&gt;Whereas in time window 1, the effect was circumscribed to an interaction with Brain Area, by Time Window 2, a main effect of CMS emerged. In Windows 3 and 4, the only experimental effect was CMS.&lt;/p&gt;
&lt;p&gt;Bonferroni-corrected, planned ANOVA contrasts into CMS conditions revealed that the no-switch condition differed significantly from the switch conditions. By contrast, the switch conditions (Haptic-to-visual and Auditory-to-visual) hardly differed from each other, underscoring the CMS effect.&lt;/p&gt;
&lt;p&gt;Although the interaction of Group and CMS was only significant in Time Windows 1 and 2, Windows 2 to 4 presented a pattern fitting our predictions (Louwerse &amp;amp; Connell, 2011). While the Slow group picked up the switches across all modalities similarly, the Quick group picked up the Auditory-to-visual switch more clearly than the Haptic-to-visual switch.&lt;/p&gt;
&lt;h3 id=&#34;statistical-analysishttpsosfiosx3nw&#34;&gt;&lt;a href=&#34;https://osf.io/sx3nw/&#34;&gt;Statistical analysis&lt;/a&gt;&lt;/h3&gt;
&lt;style&gt;.embed-responsive{position:relative;height:100%;}.embed-responsive iframe{position:absolute;height:100%;}&lt;/style&gt;&lt;script&gt;window.jQuery || document.write(&#39;&lt;script src=&#34;//code.jquery.com/jquery-1.11.2.min.js&#34;&gt;\x3C/script&gt;&#39;) &lt;/script&gt;&lt;link href=&#34;https://mfr.osf.io/static/css/mfr.css&#34; media=&#34;all&#34; rel=&#34;stylesheet&#34;&gt;&lt;div id=&#34;mfrIframe&#34; class=&#34;mfr mfr-file&#34;&gt;&lt;/div&gt;&lt;script src=&#34;https://mfr.osf.io/static/js/mfr.js&#34;&gt;&lt;/script&gt; &lt;script&gt;var mfrRender = new mfr.Render(&#34;mfrIframe&#34;, &#34;https://mfr.osf.io/render?url=https://osf.io/sx3nw/?direct%26mode=render%26action=download%26mode=render&#34;);&lt;/script&gt;
&lt;h2 id=&#34;discussion&#34;&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Results broadly suggest that cognition may operate on qualitatively different systems for the same task. In conceptual processing, one of these systems appears to be modality-independent, potentially based on linguistic co-occurrences, whereas another system is modality-specific, linked to physical experience.&lt;/p&gt;
&lt;p&gt;A conference poster with further analyses is &lt;a href=&#34;https://mfr.osf.io/render?url=https://osf.io/dj52n/?direct%26mode=render%26action=download%26mode=render&#34;&gt;also available&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;div style = &#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Barsalou, L. W. (2016). On staying grounded and avoiding quixotic dead ends. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Bernabeu, P., Louwerse, M. M., &amp;amp; Willems, R. M. (in prep.). Modality exclusivity norms for 747 properties and concepts in Dutch: a replication of English. Retrieved from &lt;a href=&#34;https://osf.io/brkjw/&#34;&gt;https://osf.io/brkjw/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P., Willems, R. M., &amp;amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, &amp;amp; E. J. Davelaar (Eds.), &lt;em&gt;Proceedings of the 39th Annual Conference of the Cognitive Science Society&lt;/em&gt; (pp. 1629-1634). Austin, TX: Cognitive Science Society. &lt;a href=&#34;https://doi.org/10.31234/osf.io/a5pcz&#34;&gt;https://doi.org/10.31234/osf.io/a5pcz&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hauk, O. (2016). Only time will tell—Why temporal information is essential for our neuroscientific understanding of semantics. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;, 4, 1072-1079.&lt;/p&gt;
&lt;p&gt;Lockwood, G., Hagoort, P., &amp;amp; Dingemanse, M. (2016). How iconicity helps people learn new words: neural correlates and individual differences in sound-symbolic bootstrapping. &lt;em&gt;Collabra, 2&lt;/em&gt;, 1, 7.&lt;/p&gt;
&lt;p&gt;Louwerse, M., &amp;amp; Connell, L. (2011). A taste of words: linguistic context and perceptual simulation predict the modality of words. &lt;em&gt;Cognitive Science, 35&lt;/em&gt;, 2, 381-98.&lt;/p&gt;
&lt;p&gt;Louwerse, M., &amp;amp; Hutchinson, S. (2012). Neurological evidence linguistic processes precede perceptual simulation in conceptual processing. &lt;em&gt;Frontiers in Psychology, 3&lt;/em&gt;, 385.&lt;/p&gt;
&lt;p&gt;Mahon, B. Z., &amp;amp; Caramazza, A. (2008). A critical look at the Embodied Cognition Hypothesis and a new proposal for grounding conceptual content. &lt;em&gt;Journal of Physiology - Paris, 102&lt;/em&gt;, 59-70.&lt;/p&gt;
&lt;p&gt;Scerrati, E., Lugli, L., Nicoletti, R., &amp;amp; Borghi, A. M. (2016). The Multilevel Modality-Switch Effect: What Happens When We See the Bees Buzzing and Hear the Diamonds Glistening. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, doi:10.3758/s13423-016-1150-2.&lt;/p&gt;
&lt;p&gt;Vukovic, V., Feurra, M., Shpektor, A., Myachykov, A., &amp;amp; Shtyrov, Y. (2017). Primary motor cortex functionally contributes to language comprehension: An online rTMS study. &lt;em&gt;Neuropsychologia, 96&lt;/em&gt;, 222-229.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
