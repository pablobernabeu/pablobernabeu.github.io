<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>bias | Pablo Bernabeu</title>
    <link>https://pablobernabeu.github.io/tags/bias/</link>
      <atom:link href="https://pablobernabeu.github.io/tags/bias/index.xml" rel="self" type="application/rss+xml" />
    <description>bias</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-uk</language><copyright>© Pablo Bernabeu, 2021 — [CC BY Attribution licence](https://creativecommons.org/licenses/by/4.0/). Cookies may be used by external services such as [Disqus](https://help.disqus.com/en/articles/1717155-use-of-cookies); none set by the author of this website.</copyright><lastBuildDate>Sat, 09 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://pablobernabeu.github.io/img/pablobernabeu_image_sharing.png</url>
      <title>bias</title>
      <link>https://pablobernabeu.github.io/tags/bias/</link>
    </image>
    
    <item>
      <title>Brief Clarifications | Open Questions</title>
      <link>https://pablobernabeu.github.io/2021/brief-clarifications-open-questions-commentary-on-liu-et-al-2018/</link>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2021/brief-clarifications-open-questions-commentary-on-liu-et-al-2018/</guid>
      <description>&lt;p&gt;Liu et al. (2018) present a study that implements the conceptual modality switch (CMS) paradigm, which has been used to investigate the modality-specific nature of conceptual representations (Pecher et al., 2003). Liu et al.&amp;lsquo;s experiment uses event-related potentials (ERPs; similarly, see Bernabeu et al., 2017; Collins et al., 2011; Hald, Hocking, et al., 2013; Hald, Marshall, et al., 2011). In the design of the switch conditions, the experiment implements a corpus analysis to distinguish between purely-embodied modality switches and switches that are more liable to linguistic bootstrapping (also see Bernabeu et al., 2017; Louwerse &amp;amp; Connell, 2011). The procedure for stimulus selection was novel as well as novel; thus, it could prove useful in future studies, too. In addition, the application of bayesian statistics is an interesting and promising novelty in the present research area. All of Liu et al.&amp;lsquo;s valuable data&amp;mdash;including ERPs from a high-density montage&amp;mdash;is directly available on OSF.&lt;/p&gt;
&lt;p&gt;In reviewing the literature, Liu (2018) and Liu et al. (2018) contend that previous studies may be strongly biased due to methodological decisions in the analysis of ERPs. These decisions particularly regard the latency&amp;mdash;i.e., time windows&amp;mdash;and the topographic regions of interest&amp;mdash;i.e., subsets of electrodes. Thus, Liu et al. identify a &amp;lsquo;highly inconsistent&amp;rsquo; (p. 6) landscape in the ERP components that have been ascribed to the CMS effect in previous studies. Similarly, Liu (2018, p. 47) writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Several studies have looked for the ERP manifestations of modality switching costs (Bernabeu, Willems, &amp;amp; Louwerse, 2017; Collins, Pecher, Zeelenberg, &amp;amp; Coulson, 2011; Hald, Hocking, Vernon, Marshall, &amp;amp; Garnham, 2013; Hald, Marshall, Janssen, &amp;amp; Garnham, 2011). However, what they found was not a clear picture. Not only was a significant effect found in the time window for the N400 component, but also a so-called early N400-like effect around 300ms (Bernabeu et al., 2017; Hald et al., 2011), the N1-P2 complex around 200ms (Bernabeu et al., 2017; Hald et al., 2013, 2011), as well as the late positivity component (LPC) after 600ms (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;drastic-conclusions&#34;&gt;Drastic conclusions&lt;/h3&gt;
&lt;p&gt;Liu et al. conclude that a confirmatory research approach is not warranted, and adopt a semi-exploratory approach, creating time windows of 50 ms each, rather than windows linked to known ERP components (Swaab et al., 2012), and using bayesian statistics. Such a drastic conclusion appears to stem from the assumption that, if the CMS were robust enough, it would present in the same guise across studies. Such an assumption, however, may merit further examination, considering the multiplicity of known and unknown variables that may differ across experiments (Barsalou, 2019). This variability influences the &lt;em&gt;replication praxis&lt;/em&gt;, as it were. Indeed, Liu et al. themselves allude to one such variable (p. 7).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;These previous studies did not only examine the effect of modality switching but also other linguistic factors such as negated sentences, which could easily distort observed waveforms (Luck, 2005).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu et al.&amp;lsquo;s conclusions about the &amp;lsquo;inconsistent&amp;rsquo; results do not seem to duly weigh the differences across studies. None of the existing studies is a direct replication of another. The example quoted from Liu et al. above, regarding the presence of negated sentences, is one of the less important differences because one of the corresponding studies implemented the negation as a controlled, experimental condition, contrasting with affirmative sentences (Hald, Hocking, et al., 2013). Yet, other differences exist in virtually every aspect, from the types of modality switch used to the time-locking of ERPs. For instance, the studies vary in the implementation of the modality switches. Whereas Hald, Marshall, et al. (2011) distinguish between a switch and a non-switch condition, Bernabeu et al. analysed each type of switch separately&amp;mdash;i.e., auditory-to-visual, haptic-to-visual, visual-to-visual. Another difference across the studies is the onset point for ERPs. For instance, Bernabeu et al. (2017) time-locked ERPs to the point at which the modality switch is actually elicited in the CMS paradigm&amp;mdash;namely, the first word in target trials. In contrast, the other studies time-locked ERPs to the second word. In addition, these studies vary in their timelines&amp;mdash;i.e., presentation of words and inter-stimulus intervals&amp;mdash;, as well as in the words that were used as stimuli, in the preprocessing of ERPs, in the statistical analysis, and even in the language of testing in one case (all studies using English except Bernabeu et al., 2017, which used Dutch). Moreover, the studies differ in the sample size, ranging from ten finally-analysed participants (Hald, Marshall, et al, 2011) to 46 finally-analysed participants (Bernabeu et al., 2017). Last, the studies differ in the number of items per modality switch condition, ranging from 17 (in one of Liu et al.&amp;lsquo;s conditions) to 40 per condition (Hald, Hocking, et al., 2013; Hald, Marshall, et al., 2011). Undoubtedly, seeing larger sample sizes and more stimulus items used in ERP studies is something to promote and celebrate. Last, it may be noted that Liu et al.&amp;lsquo;s stance on the inconsistency of previous results starkly contrasts with their use of a single study&amp;mdash;Hald, Marshall, et al., 2011&amp;mdash;, with &lt;em&gt;N&lt;/em&gt; = 10, as the motivation for their sample size (Albers &amp;amp; Lakens, 2018).&lt;/p&gt;
&lt;h3 id=&#34;clarifications&#34;&gt;Clarifications&lt;/h3&gt;
&lt;p&gt;Liu et al. apply bayesian statistics reportedly to help reduce the bias that may exist in the present literature. However, the reviews of Liu (2018) and Liu et al. (2018) appear to gloss over some important aspects regarding previous studies. For instance, Liu writes (p. 53):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The inflation of the probability of Type I error leads to an over-confidence in the interpretation of the results. For example, in the studies on modality switching costs, different time windows were chosen to test the early effect of modality switching costs. While Bernabeu et al. (2017), Hald et al. (2011) and Hald et al. (2013) examined the segment of ERP waveform between 190ms and 300ms or 160ms and 215ms based on visual inspection and found significant effects, Collins et al. (2011) chose a prescribed time window between 100ms and 200ms before the analysis and did not find the effect.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu also refers to the issue of multiple tests related to the multiple time windows and electrodes we find in ERP studies (p. 59):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It is typical for ERP studies to conduct multiple comparisons (e.g., running the same ANOVA repeatedly on different subsets of data like different time windows and different groups of electrodes). This would massively increase Type I error if no post hoc correction is conducted. However, if Bonferroni or other correction is conducted, it will render the study over-conservative, thus increasing the chance of Type II error. In the present thesis, 90 electrodes will be analysed individually, with 20 time slices in each trial. That results in 1800 NHSTs for each critical variable. A correction of multiple comparison will require a critical level of 2.78 x 10ˆ-5 for each test for a family-wise critical level of .05 (and an uncorrected test will almost definitely lead to false positive results). This stringent criterion could conceivably render it meaningless any p-values we can obtain from a statistical package.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Arguably, the scenario presented by Liu, in which a researcher could conduct a purely data-driven analysis of ERP data, is extreme. The field of psycholinguistics, in general, does not have a tradition of purely data-driven analysis. Instead, it blends a humanistic background with a scientific methodology. As a result, the hypotheses and methods tend to be largely driven by the available literature. For instance, Bernabeu et al. (2017, quoted below from p. 1632) based their time windows and regions of interest on the most relevant of the preceding studies (see also Bernabeu, 2017).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Electrodes were divided into an anterior and a posterior area (also done in Hald et al., 2011). Albeit a superficial division, we found it sufficient for the research question. Time windows were selected as in Hald et al., except for the last window, which was extended up to 750 ms post word onset, instead of 700 ms, because the characteristic component of that latency tends to extend until then, as we confirmed by visual inspection of these results.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The literature-based approach follows the advice from Luck and Gaspelin (2017, p. 149), who wrote: &amp;lsquo;a researcher who wants to avoid significant but bogus effects would be advised to focus on testing a priori predictions without using the observed data to guide the selection of time windows or electrode sites&amp;rsquo; (see also Armstrong, 2014; for a more conservative stance, see Luck &amp;amp; Gaspelin, 2017). In addition, notice that the extension of the last window by 50 ms was informed by Swaab et al. (2012), who report results by which the P600 component (the main component occurring after the N400 in word reading) extended up to 800 ms.&lt;/p&gt;
&lt;p&gt;Next, Liu et al. (2018, pp. 6&amp;ndash;7) write:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;However, the findings of these components have been highly inconsistent. The N400 effect alone was found in the posterior region in some cases (Bernabeu et al., 2017; Hald et al., 2013), while in anterior region in others (Collins et al., 2011, Hald et al. (2011)).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yet, Bernabeu et al. (2017, p. 1632) had written:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In certain parts over the time course, the effect appeared in both anterior and posterior areas&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu et al. continue (p. 7):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In some cases, it was found in the typical window around 400ms (Collins et al., 2011), while in others an earlier window from 270ms to 370ms (Bernabeu et al., 2017; Hald et al., 2011).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, Bernabeu et al. (p. 1632) had written:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The ERP results revealed a CMS effect from Time Window 1 on, larger after 350 ms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Regarding the time-locking of ERPs, Liu (2018, p. 43) writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Because the properties were usually salient for the concepts, the switching costs might have already been incurred when participants were processing the concept word. Bernabeu et al. (2017), in their recent replication of previous ERP studies, reversed the order of concept and property and did not find an immediate effect from the property onset. In future studies, it is recommended to adopt the reverse order, control the concept words so that they do not automatically activate the properties before the words are shown, or analyse epochs after both the concept and property words.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above excerpt seems to reveal a misunderstanding of Bernabeu et al.&amp;lsquo;s method and results (we may assume that, by &amp;lsquo;immediate&amp;rsquo;, Liu is referring to the 200 ms point or afterwards, since that is about as immediate as it gets; see Amsel et al., 2014; Swaab et al., 2012; Van Dam et al., 2014). Bernabeu et al.&amp;lsquo;s abstract mentioned (p. 1629):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Event-Related Potentials (ERPs) were time-locked to the onset of the first word (property) in the target trials so as to measure the effect online and to avoid a within-trial confound. A switch effect was found, characterized by more negative ERP amplitudes for modality switches than no-switches. It proved significant in four typical time windows from 160 to 750 milliseconds post word onset, with greater strength in posterior brain regions, and after 350 milliseconds.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Indeed, time-locking ERPs to the beginning of the trial was one of the principal features of Bernabeu et al.&amp;lsquo;s experiment. Complementing that, the property words were placed in the first trial because they are more perceptually-loaded than concepts (Lynott &amp;amp; Connell, 2013), thus better suiting the main basis of the CMS paradigm. We know that the semantic processing of a word often commences within the first 200 ms (Amsel et al., 2014; Van Dam et al., 2014). Considering the importance of the time course in the grounding of conceptual representations (Hauk, 2016), it seems important to measure the CMS from the moment that it is elicited&amp;mdash;namely, in all experiments, from the first word of the target trial (Bernabeu, 2017; Bernabeu et al., 2017), rather than letting several hundreds of milliseconds elapse. Nonetheless, from a methodological perspective, it would be interesting to compare the two approaches in a dedicated study. This would precisely reveal the speed at which modality-specific meaning becomes activated during conceptual processing.&lt;/p&gt;
&lt;h3 id=&#34;outstanding-issues-random-effects-and-correction-for-multiple-tests&#34;&gt;Outstanding issues: Random effects and correction for multiple tests&lt;/h3&gt;
&lt;p&gt;A methodological issue affecting the statistical analysis of all the studies hereby considered (Bernabeu et al., 2017; Collins et al., 2011; Hald, Hocking, et al., 2013; Hald, Marshall, et al., 2011; Liu et al., 2018) is the absence of some applicable random effects. Some of the studies did not apply any random effects (Collins et al., 2011; Hald, Hocking, et al., 2013; Hald, Marshall, et al., 2011). Even in psycholinguistics, use of linear mixed-effects models is still increasing (Meteyard &amp;amp; Davies, 2020; Yarkoni, 2020). Yet, in those studies that did apply random effects, the corresponding  structure was not as exhaustive as it should have been, as they lacked random slopes. In Bernabeu et al. (2017), a model selection approach was applied (Matuschek et al., 2017), whereby each random effect was tested and only kept in the model if it significantly improved the fit. In Liu et al. (2018), random slopes were deemed unfeasible due to computational constraints (for background, see Brauer &amp;amp; Curtin, 2017). Applying a complete random effects structure is important for a robust statistical analysis (Barr et al., 2013; Yarkoni, 2020).&lt;/p&gt;
&lt;p&gt;Another issue is that of multiple tests. Where a small number of levels is used (e.g., time windows, topographic regions of interest) and these are informed by the literature, the advice has often been ambiguous as to whether a correction should be applied (e.g., Armstrong, 2014; for a more conservative stance, see Luck &amp;amp; Gaspelin, 2017). Indeed, no correction was applied in any of the four studies that used frequentist statistics (Bernabeu et al., 2017; Collins et al., 2011; Hald, Hocking, et al., 2013; Hald, Marshall, et al., 2011).&lt;/p&gt;
&lt;h4 id=&#34;reanalysis-of-bernabeu-et-al-2017&#34;&gt;Reanalysis of Bernabeu et al. (2017)&lt;/h4&gt;
&lt;p&gt;The results of Bernabeu et al. were reanalysed after publication using a more complete random effects structure that incorporated by-participant random slopes for the modality-switch factor&amp;mdash;i.e., &lt;code&gt;(condition | participant)&lt;/code&gt;. The results were also corrected for multiple tests using the Holm-Bonferroni correction (Holm, 1979). For this purpose, the lowest p-value in the four time windows was multiplied by 4, the next p-value by 3, the next by 2, and the highest p-value was left as it unmodified. In this stepwise correction, if a nonsignificant p-value was reached, all the subsequent p-values became nonsignificant (see &lt;a href=&#34;https://osf.io/unvfs/&#34;&gt;analysis script&lt;/a&gt;). The &lt;a href=&#34;https://osf.io/qhe5s/&#34;&gt;results&lt;/a&gt; differed from the original, slopes-free models (available &lt;a href=&#34;https://osf.io/sx3nw/&#34;&gt;script&lt;/a&gt; and &lt;a href=&#34;https://osf.io/4v38d/&#34;&gt;results&lt;/a&gt;) in that the main effect of the modality switch factor became nonsignificant in the second time window (160&amp;ndash;216 ms) and in the fourth one (500&amp;ndash;750 ms), while remaining significant in the third time window (350&amp;ndash;550 ms). Incidentally, note that main effects may not be directly interpretable as the same same variables are present interactions (Kam &amp;amp; Franzese, 2007). Yet, the interactions of modality switch with participant group (quick/slow) and scalp location (anterior/posterior) retained the same significance.&lt;/p&gt;
&lt;h3 id=&#34;open-questions&#34;&gt;Open questions&lt;/h3&gt;
&lt;p&gt;Liu (2018) and Liu et al. (2018) raise interesting and important questions. Firstly, future research may be conducted to investigate what determines the variability of ERP results&amp;mdash;in terms of ERP components, time windows and topographic regions of interest. This research could include a comparison with other measurements, such as response times, to test whether ERPs are less reliable&amp;mdash;i.e., more variable across studies&amp;mdash;than response times. Similarly, future research may investigate whether the ERP literature is more biased than literature employing other measures, such as response times. In addition, future research could investigate whether moving to exploratory, bayesian research designs is a necessary or sufficient condition to reduce bias in research and improve the precision of experimental measurements. Current alternatives to such an approach include direct (or conceptual) replications designed to achieve a higher power than previous studies (e.g., Chen et al., 2019). Arguably, policies determining funding decisions would need to change if we are to recognise the importance of direct replication. Last, future research may investigate whether &amp;lsquo;clear picture&amp;rsquo; results are realistic, desirable or necessary, and whether unclear-picture results should be eschewed; or whether, on the contrary, clear-picture results are the product of publication bias&amp;mdash;that is, the pressure to hide any aspects of a study that could challenge its acceptance by peer-reviewers or any other academics.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;div style = &#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Albers, C., &amp;amp; Lakens, D. (2018). When power analyses based on pilot data are biased: Inaccurate effect size estimators and follow-up bias. &lt;em&gt;Journal of Experimental Social Psychology, 74&lt;/em&gt;, 187–195. &lt;a href=&#34;https://doi.org/10.1016/j.jesp.2017.09.004&#34;&gt;https://doi.org/10.1016/j.jesp.2017.09.004&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Amsel, B. D., Urbach, T. P., &amp;amp; Kutas, M. (2014). Empirically grounding grounded cognition: the case of color. &lt;em&gt;Neuroimage, 99&lt;/em&gt;, 149-157. &lt;a href=&#34;https://doi.org/10.1016/j.neuroimage.2014.05.025&#34;&gt;https://doi.org/10.1016/j.neuroimage.2014.05.025&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barr, D. J., Levy, R., Scheepers, C., &amp;amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. &lt;em&gt;Journal of Memory and Language, 68&lt;/em&gt;, 255–278. &lt;a href=&#34;http://dx.doi.org/10.1016/j.jml.2012.11.001&#34;&gt;http://dx.doi.org/10.1016/j.jml.2012.11.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P. (2017). &lt;em&gt;Modality switches occur early and extend late in conceptual processing: evidence from ERPs&lt;/em&gt; [Master&#39;s thesis]. School of Humanities, Tilburg University. &lt;a href=&#34;https://psyarxiv.com/5gjvk&#34;&gt;https://psyarxiv.com/5gjvk&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P., Willems, R. M., &amp;amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, &amp;amp; E. J. Davelaar (Eds.), &lt;em&gt;Proceedings of the 39th Annual Conference of the Cognitive Science Society&lt;/em&gt; (pp. 1629-1634). Austin, TX: Cognitive Science Society. &lt;a href=&#34;https://mindmodeling.org/cogsci2017/papers/0318&#34;&gt;https://mindmodeling.org/cogsci2017/papers/0318&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chen, S., Szabelska, A., Chartier, C. R., Kekecs, Z., Lynott, D., Bernabeu, P., … Schmidt, K. (2018). &lt;em&gt;Investigating object orientation effects across 14 languages&lt;/em&gt;. PsyArXiv. &lt;a href=&#34;https://doi.org/10.31234/osf.io/t2pjv/&#34;&gt;https://doi.org/10.31234/osf.io/t2pjv/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Collins, J., Pecher, D., Zeelenberg, R., &amp;amp; Coulson, S. (2011). Modality switching in a property verification task: an ERP study of what happens when candles flicker after high heels click. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2011.00010&#34;&gt;https://doi.org/10.3389/fpsyg.2011.00010&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hald, L. A., Hocking, I., Vernon, D., Marshall, J.-A., &amp;amp; Garnham, A. (2013). Exploring modality switching effects in negated sentences: further evidence for grounded representations. &lt;em&gt;Frontiers in Psychology, 4&lt;/em&gt;, 93. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2013.00093&#34;&gt;https://doi.org/10.3389/fpsyg.2013.00093&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hald, L. A., Marshall, J.-A., Janssen, D. P., &amp;amp; Garnham, A. (2011). Switching modalities in a sentence verification task: ERP evidence for embodied language processing. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2011.00045&#34;&gt;https://doi.org/10.3389/fpsyg.2011.00045&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hauk, O. (2016). Only time will tell–why temporal information is essential for our neuroscientific understanding of semantics. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;(4), 1072-1079. &lt;a href=&#34;https://doi.org/10.3758/s13423-015-0873-9&#34;&gt;https://doi.org/10.3758/s13423-015-0873-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Holm, S. (1979). A simple sequentially rejective multiple test procedure. &lt;em&gt;Scandinavian Journal of Statistics, 6&lt;/em&gt;, 65-70. &lt;a href=&#34;http://www.jstor.org/stable/4615733&#34;&gt;http://www.jstor.org/stable/4615733&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Kam, C. D., &amp;amp; Franzese, R. J. (2007). &lt;em&gt;Modeling and interpreting interactive hypotheses in regression analysis&lt;/em&gt;. Ann Arbor, MI: University of Michigan Press.&lt;/p&gt;
&lt;p&gt;Liu, P. (2018). &lt;em&gt;Embodied-linguistic conceptual representations during metaphor processing&lt;/em&gt;. Doctoral thesis, Lancaster University, UK. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/489&#34;&gt;https://doi.org/10.17635/lancaster/thesis/489&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, P., Lynott, D., &amp;amp; Connell, L. (2018). Continuous neural activations of simulation-linguistic representations in modality switching costs. In P. Liu, &lt;em&gt;Embodied-linguistic conceptual representations during metaphor processing&lt;/em&gt;. Doctoral thesis, Lancaster University, UK. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/489&#34;&gt;https://doi.org/10.17635/lancaster/thesis/489&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Luck, S. J. (2005). Ten simple rules for designing ERP experiments. In T. C. Handy (Ed.), &lt;em&gt;Event-related potentials: A methods handbook&lt;/em&gt;.
MIT Press.&lt;/p&gt;
&lt;p&gt;Luck, S. J., &amp;amp; Gaspelin, N. (2017). How to get statistically significant effects in any ERP experiment (and why you shouldn&#39;t). &lt;em&gt;Psychophysiology, 54&lt;/em&gt;(1), 146-157. &lt;a href=&#34;https://doi.org/10.1111/psyp.12639&#34;&gt;https://doi.org/10.1111/psyp.12639&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., &amp;amp; Bates, D. (2017). Balancing type 1 error and power in linear mixed models. &lt;em&gt;Journal of Memory and Language, 94&lt;/em&gt;, 305–315. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2017.01.001&#34;&gt;https://doi.org/10.1016/j.jml.2017.01.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meteyard, L., &amp;amp; Davies, R. A. (2020). Best practice guidance for linear mixed-effects models in psychological science. &lt;em&gt;Journal of Memory and Language, 112&lt;/em&gt;, 104092. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2020.104092&#34;&gt;https://doi.org/10.1016/j.jml.2020.104092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pecher, D., Zeelenberg, R., &amp;amp; Barsalou, L. W. (2003). Verifying different-modality properties for concepts produces switching costs. &lt;em&gt;Psychological Science, 14&lt;/em&gt;, 2, 119-24. &lt;a href=&#34;https://doi.org/10.1111/1467-9280.t01-1-01429&#34;&gt;https://doi.org/10.1111/1467-9280.t01-1-01429&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Swaab, T. Y., Ledoux, K., Camblin, C. C., &amp;amp; Boudewyn, M. A. (2012). Language-related ERP components. In S. J. Luck &amp;amp; E. S. Kappenman (Eds.), &lt;em&gt;Oxford handbook of event-related potential components&lt;/em&gt; (pp. 397–440). Oxford University Press. &lt;a href=&#34;https://doi.org/10.1093/oxfordhb/9780195374148.013.0197&#34;&gt;https://doi.org/10.1093/oxfordhb/9780195374148.013.0197&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Van Dam, W. O., Brazil, I. A., Bekkering, H., &amp;amp; Rueschemeyer, S.-A. (2014). Flexibility in embodied language processing: context effects in lexical access. &lt;em&gt;Topics in Cognitive Science, 6&lt;/em&gt;(3), 407–424. &lt;a href=&#34;https://doi.org/10.1111/tops.12100&#34;&gt;https://doi.org/10.1111/tops.12100&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yarkoni, T. (2020). The generalizability crisis. &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt;, 1-37. &lt;a href=&#34;https://doi.org/10.1017/S0140525X20001685&#34;&gt;https://doi.org/10.1017/S0140525X20001685&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What&#39;s in a fluke? The problem of trust and distrust</title>
      <link>https://pablobernabeu.github.io/2020/whats-in-a-fluke/</link>
      <pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2020/whats-in-a-fluke/</guid>
      <description>
&lt;link href=&#34;index_files/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Fluke&lt;/em&gt; is a popular term for the statistical concepts of false positive or false negative results (more commonly used for false positives). A &lt;em&gt;false positive&lt;/em&gt; occurs when a result that does not exist in reality is observed in an analysis due to a methodological error (be it experimental, statistical, or otherwise). Conversely, a &lt;em&gt;false negative&lt;/em&gt; occurs when a genuine result is not observed in an analysis, due to the same kind of error. The concept of fluke is defined by its opposite: a truthful, accurate result.&lt;/p&gt;
&lt;p&gt;The label ‘fluke’ is ubiquitous where statistics is applied, from medicine to psychology and from sociology to politics.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
When I won in 2018, many dismissed our victory as a “fluke.”&lt;br&gt;&lt;br&gt;Our win was treated as an aberration, or bc my opponent “didn’t try.”&lt;br&gt;&lt;br&gt;So from the start, tonight’s race was important to me.&lt;br&gt;&lt;br&gt;Tonight we are proving that the people’s movement in NY isn’t an accident. It‘s a mandate.
&lt;/p&gt;
— Alexandria Ocasio-Cortez (&lt;span class=&#34;citation&#34;&gt;@AOC&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/AOC/status/1275618050134474761?ref_src=twsrc%5Etfw&#34;&gt;June 24, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;whats-in-the-label-flukeor-what-could-there-be&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What’s in the label ‘fluke’—or what could there be?&lt;/h2&gt;
&lt;p&gt;AOC’s tweet points at two main types of reactions to her results in the previous elections. The first type is dismissal, on the grounds of a fluke or a weak opposition. The other reaction is plain ‘aberration’. A few weeks earlier, Mark Leibovich had also delved into this issue in The New York Times (May 4, 2020, &lt;a href=&#34;https://nyti.ms/2YsJ9fb&#34; class=&#34;uri&#34;&gt;https://nyti.ms/2YsJ9fb&lt;/a&gt;):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;She believed misconceptions had taken hold about her: that she was angry and strident. That she was naïve. “That I just don’t know how this town works,” she said. “That I’m stupid. Or I’m lucky. That was a big thing the Democrats were saying. That I was a fluke. Which is basically just 10 different ways of saying she’s not supposed to be here.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The seemingly objective term ‘fluke’, with its statistical underpinning, seems to be susceptible to biased uses. We may then have to ask: Have other electoral results, comparable to AOC’s, been equally studied for signs of a fluke? Why is a certain result perceived as a fluke in the first place? From perceptual to cognitive and historical biases, people’s judgements are susceptible to visual mistakes (Zamboni, Ledgeway, McGraw, &amp;amp; Schluppeck, 2016), confirmation bias (Rajsic, Taylor, &amp;amp; Pratt, 2018) and biased records (Hug, 2003).&lt;/p&gt;
&lt;p&gt;In summary, the label ‘fluke’ may in principle be skewed by:&lt;/p&gt;
&lt;div style=&#34;padding-left: 20px;&#34;&gt;
&lt;p&gt;&lt;i class=&#34;fas fa-eye&#34;&gt;&lt;/i&gt;   the eye of the beholder&lt;/p&gt;
&lt;p&gt;&lt;i class=&#34;fas fa-brain&#34;&gt;&lt;/i&gt;   the mind of the perceiver&lt;/p&gt;
&lt;p&gt;&lt;i class=&#34;far fa-newspaper&#34;&gt;&lt;/i&gt;   the availability or lack of data&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trust-and-distrust&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trust and distrust&lt;/h2&gt;
&lt;p&gt;Problems associated with the reliance on trust and distrust have become patent even in fields that are relatively regulated against arbitrary decisions, such as higher education and academia (Barber, Hayes, Johnson, Márquez-Magaña, &amp;amp; 10,234 signatories, 2020; Milkman, Akinola, &amp;amp; Chugh, 2012, 2015).&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Barber, P. H., Hayes, T. B., Johnson, T. L., Márquez-Magaña, L., &amp;amp; 10,234 signatories (2020). Systemic racism in higher education. &lt;em&gt;Science, 369&lt;/em&gt;, 6510, 1440-1441. &lt;a href=&#34;https://doi.org/10.1126/science.abd7140&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1126/science.abd7140&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hug, S. (2003). Selection Bias in Comparative Research: The Case of Incomplete Data Sets. &lt;em&gt;Political Analysis, 11&lt;/em&gt;(3), 255-274. &lt;a href=&#34;https://doi.org/10.1093/pan/mpg014&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1093/pan/mpg014&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Milkman, K. L., Akinola, M., &amp;amp; Chugh, D. (2012). Temporal distance and discrimination: an audit study in academia. &lt;em&gt;Psychological Science, 23&lt;/em&gt;(7), 710–717. &lt;a href=&#34;https://doi.org/10.1177/0956797611434539&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/0956797611434539&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;_____ (2015). What happens before? A field experiment exploring how pay and representation differentially shape bias on the pathway into organizations. &lt;em&gt;Journal of Applied Psychology, 100&lt;/em&gt;(6), 1678–1712. &lt;a href=&#34;https://doi.org/10.1037/apl0000022&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/apl0000022&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rajsic, J., Taylor, J. E. T., &amp;amp; Pratt, J. (2018) Out of sight, out of mind: Matching bias underlies confirmatory visual search. &lt;em&gt;Attention, Perception, &amp;amp; Psychophysics, 79&lt;/em&gt;, 498–507. &lt;a href=&#34;https://doi.org/10.3758/s13414-016-1259-4&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13414-016-1259-4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zamboni, E., Ledgeway, T., McGraw, P. V., &amp;amp; Schluppeck, D. (2016). Do perceptual biases emerge early or late in visual processing? Decision-biases in motion perception. &lt;em&gt;Proceedings of the Royal Society B: Biological Sciences, 283&lt;/em&gt;(1833), 20160263. &lt;a href=&#34;https://doi.org/10.1098/rspb.2016.0263&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1098/rspb.2016.0263&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
