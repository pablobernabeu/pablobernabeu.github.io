<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Pablo Bernabeu</title>
    <link>/tags/r/</link>
      <atom:link href="/tags/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-uk</language><copyright>© Pablo Bernabeu, 2020 — [CC BY Attribution licence](https://creativecommons.org/licenses/by/4.0/). Cookies used [by Disqus to enable comments](https://help.disqus.com/en/articles/1717155-use-of-cookies).</copyright><lastBuildDate>Fri, 14 Aug 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/pablobernabeu_image_sharing.png</url>
      <title>R</title>
      <link>/tags/r/</link>
    </image>
    
    <item>
      <title>R Markdown amidst Madison parks</title>
      <link>/2020/r-markdown-amidst-madison-parks/</link>
      <pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/2020/r-markdown-amidst-madison-parks/</guid>
      <description>



</description>
    </item>
    
    <item>
      <title>Web application for the simulation of experimental data</title>
      <link>/applications-and-dashboards/experimental-data-simulation/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/applications-and-dashboards/experimental-data-simulation/</guid>
      <description>


&lt;div style=&#34;font-size: 25px; color: #614064; padding-top: 15px; padding-bottom: 10px;&#34;&gt;
&lt;i class=&#34;fas fa-chalkboard-teacher fa-xs&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt; &lt;i class=&#34;fas fa-university fa-xs&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;  Purposes
&lt;/div&gt;
&lt;p&gt;This open-source, R-based web application is suitable for educational and research purposes in experimental and quantitative sciences. It allows the &lt;strong&gt;creation of varied data sets with specified structures, such as between-group and within-participant variables, that can be categorical or continuous.&lt;/strong&gt; These features can be selected along the different tabs. In the penultimate tab, a custom summary of the current data set can be constructed. In the last tab, the list of parameters and the data set can be downloaded.&lt;/p&gt;
&lt;div style=&#34;margin-bottom:-20px; color:#665F5F;&#34;&gt;
Screenshot of the &lt;em&gt;Dependent&lt;/em&gt; tab (&lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/raw/master/Screenshot.png&#34;&gt;view larger&lt;/a&gt;)
&lt;/div&gt;
&lt;p&gt;&lt;img style=&#34;max-width: 800px; display: block; margin-left: auto; margin-right: auto; padding-bottom: 15px;&#34; src=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/raw/master/Screenshot.png&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;p&gt;Bernabeu, P., &amp;amp; Lynott, D. (2020). Web application for the simulation of experimental data (Version 0.2). Retrieved from &lt;a href=&#34;https://github.com/pablobernabeu/Experiment-simulation-app/&#34;&gt;https://github.com/pablobernabeu/Experiment-simulation-app/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Code&lt;/h3&gt;
&lt;p&gt;This web application was developed in &lt;a href=&#34;https://www.r-project.org/about.html&#34;&gt;R&lt;/a&gt; (R Core Team, 2020). The code is &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/blob/master/web-application.Rmd&#34;&gt;available on Github&lt;/a&gt;, where contributions may be made. The initial code for this application was influenced by Section 5.7 in Crump (2017; &lt;a href=&#34;https://crumplab.github.io/programmingforpsych/simulating-and-analyzing-data-in-r.html#simulating-data-for-multi-factor-designs&#34;&gt;&lt;em&gt;Simulating data for multi-factor designs&lt;/em&gt;&lt;/a&gt;). The R packages used include Shiny (Chang, Cheng, Allaire, Xie, &amp;amp; McPherson, 2020), Tidyverse (Wickham et al., 2019) and DT (Xie, 2020).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgements&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Acknowledgements&lt;/h3&gt;
&lt;p&gt;Thank you to RStudio for the free hosting server used by this application, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;shinyapps.io&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div style=&#34;text-indent:-1.5em; margin-left:1.5em;&#34;&gt;
&lt;p&gt;Chang, W., Cheng, J., Allaire, J., Xie, Y., &amp;amp; McPherson, J. (2020). shiny: Web Application Framework for R. R package version 1.4.0. Available at &lt;a href=&#34;http://CRAN.R-project.org/package=shiny&#34;&gt;http://CRAN.R-project.org/package=shiny&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Crump, M. J. C. (2017). Programming for Psychologists: Data Creation and Analysis (Version 1.1). &lt;a href=&#34;https://crumplab.github.io/programmingforpsych/&#34;&gt;https://crumplab.github.io/programmingforpsych/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;R Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D. A., François, R., … Kuhn, M. (2019). Welcome to the Tidyverse. &lt;em&gt;Journal of Open Source Software, 4&lt;/em&gt;(43), 1686. &lt;a href=&#34;https://doi.org/10.21105/joss.01686&#34;&gt;https://doi.org/10.21105/joss.01686&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Xie, Y. (2020). DT: A Wrapper of the JavaScript Library “DataTables”. R package version 0.14. Available at &lt;a href=&#34;https://CRAN.R-project.org/package=DT&#34;&gt;https://CRAN.R-project.org/package=DT&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;contact&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Contact&lt;/h3&gt;
&lt;p&gt;To submit any questions or feedback, please post &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/issues&#34;&gt;an issue&lt;/a&gt;, or email Pablo Bernabeu at &lt;a href=&#34;mailto:p.bernabeu@lancaster.ac.uk&#34;&gt;p.bernabeu@lancaster.ac.uk&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Butterfly species richness in Los Angeles</title>
      <link>/applications-and-dashboards/butterfly-species-richness-in-la/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/applications-and-dashboards/butterfly-species-richness-in-la/</guid>
      <description>&lt;a href=&#39;https://pablobernabeu.github.io/dashboards/Butterfly-species-richness-in-LA/d.html&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #196F27; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt;
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;i class=&#34;fas fa-mouse-pointer&#34;&gt;&lt;/i&gt;&lt;/span&gt;&amp;nbsp; Dashboard
      &lt;/h3&gt;&lt;/button&gt;
      &lt;/a&gt;
&lt;br&gt;
&lt;br&gt;
&lt;p&gt;This dashboard presents open data&lt;/a&gt; (&lt;a href=&#39;https://github.com/jcoliver/bioscan/blob/master/data/iNaturalist-clean-reduced.csv&#39; target=&#34;_blank&#34;&gt;iNaturalist&lt;/a&gt; and &lt;a href=&#39;https://github.com/jcoliver/bioscan/blob/master/data/BioScanDataComplete.csv&#39; target=&#34;_blank&#34;&gt;BioScan&lt;/a&gt;) from Prudic, K.L.; Oliver, J.C.; Brown, B.V.; Long, E.C. &lt;a href=&#39;https://doi.org/10.3390/insects9040186&#39; target=&#34;_blank&#34;&gt; &lt;strong&gt;Comparisons of Citizen Science Data-Gathering Approaches to Evaluate Urban Butterfly Diversity&lt;/strong&gt;. &lt;em&gt;Insects&lt;/em&gt; &lt;strong&gt;2018&lt;/strong&gt;, &lt;em&gt;9&lt;/em&gt;, 186&lt;/a&gt;. In their study, Prudic and colleagues compared citizen science with traditional methods in the measurement of butterfly populations.&lt;/p&gt;
&lt;p&gt;I developed this dashboard after reproducing the &lt;a href=&#34;https://github.com/jcoliver/bioscan&#34;&gt;analyses of the original study&lt;/a&gt; in a &lt;a href=&#34;https://github.com/reprohack/reprohack-hq/blob/master/README.md&#34;&gt;Reprohack session&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My coding tasks included transforming the data to a long format,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# There are pseudovariables, that is, observations entered as variables. Since most R processes 
# need the tidy format, convert below (see https://r4ds.had.co.nz/tidy-data.html).
# The specific numbers found through traps and crowdsourcing methods are preserved.

BioScan = BioScan %&amp;gt;% pivot_longer(
    cols = Anthocharis_sara:Vanessa_cardui, names_to = &amp;quot;Species&amp;quot;,
    values_to = &amp;quot;Number&amp;quot;, values_drop_na = TRUE
  )

# Compare
#str(BioScan)
#str(dat)
# 928 rows now; the result of 29 pseudo-variables being transposed into
# rows, interacting with 32 previous rows, i.e., 29 * 32 = 928.

&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;p&gt;merging three data sets,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# The iNaturalist data set presents a slightly different challenge from the pseudovariables found above.
# The number of animals of each species must be computed from repeated entries, per site.

iNaturalist = merge(iNaturalist, iNaturalist %&amp;gt;% count(species, site, name = &#39;Number&#39;))

&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;p&gt;and, as ever, wrangling with the format of the dashboard pages to preserve the format of a table.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Species details {style=&amp;quot;background-color: #FCFCFC;&amp;quot;}
=======================================================================

Column {style=&amp;quot;data-width:100%; position:static; height:1000px;&amp;quot;}
-----------------------------------------------------------------------

&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;h4 id=&#34;reference&#34;&gt;Reference&lt;/h4&gt;
&lt;div style = &#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P. (2020). Dashboard with data from Prudic, Oliver, Brown, &amp;amp; Long (2018), Comparisons of Citizen Science Data-Gathering Approaches to Evaluate Urban Butterfly Diversity, &lt;em&gt;Insects&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;, 186. Retrieved from &lt;a href=&#34;https://pablobernabeu.github.io/dashboards/Butterfly-species-richness-in-LA/d.html&#34;&gt;https://pablobernabeu.github.io/dashboards/Butterfly-species-richness-in-LA/d.html&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;br&gt;
</description>
    </item>
    
    <item>
      <title>Data is present: workshops and datathons</title>
      <link>/2020/data-is-present-workshops-and-datathons/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/2020/data-is-present-workshops-and-datathons/</guid>
      <description>


&lt;div id=&#34;enhanced-data-presentation-using-reproducible-documents-and-dashboards&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Enhanced data presentation using reproducible documents and dashboards&lt;/h2&gt;
&lt;div id=&#34;calendar&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calendar&lt;/h3&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;9%&#34; /&gt;
&lt;col width=&#34;23%&#34; /&gt;
&lt;col width=&#34;39%&#34; /&gt;
&lt;col width=&#34;27%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Date&lt;/th&gt;
&lt;th&gt;Title&lt;/th&gt;
&lt;th&gt;Event and location&lt;/th&gt;
&lt;th&gt;Registration&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;To follow&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://wp.lancs.ac.uk/lcicd/programme/&#34;&gt;R is for Resources, Reproducibility and oppRtunities&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://wp.lancs.ac.uk/lcicd/programme/&#34;&gt;Lancaster Conference on Infant and Early Child Development&lt;/a&gt;, Lancaster University&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;To follow&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/blob/master/N8-CIR-workshops.md/&#34;&gt;Data dashboards and Binder environments&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;N8 CIR [online]&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;To follow&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/blob/master/N8-CIR-workshops.md/&#34;&gt;Introduction to R and R Markdown&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;N8 CIR [online]&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;13 Aug 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/pablobernabeu/CarpentryCon-2020-workshop-Open-Data-Reproducibility&#34;&gt;Open data and reproducibility: R Markdown, data dashboards and Binder v2.1&lt;/a&gt; (co-led with Florencia D’Andrea)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://2020.carpentrycon.org/schedule/&#34;&gt;CarpentryCon&lt;/a&gt;, &lt;a href=&#34;mailto:CarpentryCon@Home&#34; class=&#34;email&#34;&gt;CarpentryCon@Home&lt;/a&gt;, The Carpentries [online]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://2020.carpentrycon.org/schedule/&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;26 July 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/pablobernabeu/UKCLC2020-workshop-Open-data-and-reproducibility&#34;&gt;Open data and reproducibility: R Markdown, data dashboards and Binder&lt;/a&gt; (co-led with Eirini Zormpa)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.ukclc2020.com/pre-conference&#34;&gt;UK Cognitive Linguistics Conference&lt;/a&gt;, University of Birmingham [online]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.ukclc2020.com/registration&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;6 May 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/blob/master/N8-CIR-workshops.md/&#34;&gt;R Markdown&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Lancaster University [online]&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Event cancelled&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://newcastle2020.satrdays.org/&#34;&gt;Open data and reproducibility 2.0&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://newcastle2020.satrdays.org/&#34;&gt;SatRday Newcastle upon Tyne&lt;/a&gt;, Newcastle University&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://newcastle2020.satrdays.org/&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Background&lt;/h3&gt;
&lt;p&gt;This project offers free activities to learn and practise reproducible data presentation. &lt;a href=&#34;https://www.software.ac.uk/about/fellows/pablo-bernabeu&#34;&gt;Pablo Bernabeu&lt;/a&gt; organises these events in the context of a &lt;a href=&#34;https://www.software.ac.uk/programmes-and-events/fellowship-programme&#34;&gt;Software Sustainability Institute Fellowship&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;open-source-software&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Open-source software&lt;/h4&gt;
&lt;p&gt;Programming languages such as &lt;a href=&#34;https://www.r-project.org/about.html&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt; offer free, powerful resources for data processing, visualisation and analysis. Experience in these programs is highly valued in data-intensive disciplines.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;open-data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Open data&lt;/h4&gt;
&lt;p&gt;Original data has become a &lt;a href=&#34;https://www.nature.com/articles/d41586-019-01506-x&#34;&gt;public good in many research fields&lt;/a&gt; thanks to cultural and technological advances. On the internet, we can find innumerable data sets from sources such as scientific journals and repositories (e.g., &lt;a href=&#34;https://osf.io&#34;&gt;OSF&lt;/a&gt;), local and national governments (e.g., &lt;a href=&#34;https://data.london.gov.uk/&#34;&gt;London&lt;/a&gt;, UK [&lt;a href=&#34;https://www.ukdataservice.ac.uk/&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://data.gov.uk/&#34;&gt;2&lt;/a&gt;]), non-governmental organisations (e.g., &lt;a href=&#34;https://data.world/datasets/ngo&#34;&gt;data.world&lt;/a&gt;), etc. Researchers inside and outside academia nowadays share a lot of their data under attribution licences (e.g., &lt;a href=&#34;https://creativecommons.org/&#34;&gt;Creative Commons&lt;/a&gt;, the UK &lt;a href=&#34;http://www.nationalarchives.gov.uk/doc/open-government-licence/version/1/&#34;&gt;Open Government Licence&lt;/a&gt;, etc.). This allows any external analysts to access these raw data, create (additional) visualisations and analyses, and share these. In society, making data more accessible can &lt;a href=&#34;https://digitalcommons.law.yale.edu/cgi/viewcontent.cgi?article=1140&amp;amp;context=yhrdlj&#34;&gt;demonstrably benefit citizens&lt;/a&gt; (despite &lt;a href=&#34;https://firstmonday.org/ojs/index.php/fm/article/view/3316/2764#author&#34;&gt;limitations&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;activities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Activities&lt;/h2&gt;
&lt;p&gt;Activities comprise free &lt;strong&gt;workshops&lt;/strong&gt; and &lt;a href=&#34;#datathons-creating-reproducible-documents-and-dashboards&#34;&gt;&lt;strong&gt;datathons&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;workshops&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Workshops&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.r-project.org&#34;&gt;R is a programming language&lt;/a&gt; greatly equipped for the creation of reproducible documents and dashboards. Four workshops are offered that cover a suite of interrelated tools—R, R Markdown, data dashboards and Binder environments—, all underlain by reproducible workflows and open-source software.&lt;/p&gt;
&lt;p&gt;Each workshop includes &lt;strong&gt;taught and practical sections&lt;/strong&gt;. The practice provides a chance for participants to experience and address common issues with the code. The level of taught sections is largely tailored to participants; similarly, practice sections are individually adaptable by means of easier and tougher tasks. The duration is also flexible, and some of the workshops can be combined into the same session.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://rstudio.com/&#34;&gt;RStudio&lt;/a&gt; interface is used in all workshops. Multi-levelled, real code examples are used. Throughout the workshops, and especially in the practice sections, individual questions will be encouraged.&lt;/p&gt;
&lt;div id=&#34;workshop-1-introduction-to-r&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 1: Introduction to R&lt;/h4&gt;
&lt;p&gt;This workshop can serve as an introduction to R or a revision. It demonstrates what can be done in R, and provides resources for individual training. Since the duration is limited, online courses are also recommended (&lt;a href=&#34;https://www.coursera.org/courses?query=r&#34;&gt;see examples&lt;/a&gt; and &lt;a href=&#34;https://learner.coursera.help/hc/en-us/articles/209819033-Apply-for-Financial-Aid-or-a-Scholarship&#34;&gt;fee waivers for full content&lt;/a&gt;).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://adv-r.had.co.nz/Data-structures.html&#34;&gt;Data structures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.sthda.com/english/wiki/installing-and-using-r-packages&#34;&gt;Packages&lt;/a&gt;: general-purpose examples (e.g., &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt;) and more specific ones (e.g., for &lt;a href=&#34;https://cran.r-project.org/web/packages/lsr/lsr.pdf&#34;&gt;statistics&lt;/a&gt; or &lt;a href=&#34;https://cran.r-project.org/web/packages/GEOmap/GEOmap.pdf&#34;&gt;geography&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/rio/vignettes/rio.html&#34;&gt;Loading and writing data, in native and foreign formats&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Wide&lt;/em&gt; format (also dubbed ‘untidy’) versus &lt;em&gt;tidy&lt;/em&gt; format (also dubbed ‘long’ or ‘narrow’). For most processes in R, &lt;a href=&#34;https://r4ds.had.co.nz/tidy-data.html&#34;&gt;data needs to be in a tidy format&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://doi.org/10.1371/journal.pbio.3000202.g001&#39;&gt;&lt;img width = &#39;25%&#39; src = &#39;https://journals.plos.org/plosbiology/article/file?id=10.1371/journal.pbio.3000202.g001&amp;type=large&#39; alt = &#39;Illustration of wide and tidy data formats, from Postma and Goedhart (2019)&#39; /&gt;&lt;/a&gt;
&lt;p align=&#34;center&#34; style=&#34;text-align:center; margin-top:-20px; margin-bottom:10px;&#34;&gt;
Image from Postma and Goedhart (2019; &lt;a href=&#34;https://doi.org/10.1371/journal.pbio.3000202.g001&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pbio.3000202.g001&lt;/a&gt;).
&lt;/p&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://psyteachr.github.io/msc-data-skills/joins.html#joins&#34;&gt;Combining data sets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cookbook-r.com/Manipulating_data/Summarizing_data/&#34;&gt;Data summaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://philmcaleer.github.io/ug2-practical/visualisation-through-ggplot2.html&#34;&gt;Plots with &lt;code&gt;ggplot2::ggplot()&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://plot.ly/ggplot2/&#34;&gt;Interactive plots with &lt;code&gt;plotly::ggplotly()&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learningstatisticswithr-bookdown.netlify.com/part-v-statistical-tools.html&#34;&gt;Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer_and_Curtin_LMEMs-2017-Psych_Methods.pdf&#34;&gt;Linear mixed-effects models&lt;/a&gt; (see also &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0749596X20300061?dgcid=coauthor#b0670&#34;&gt;a review of practices&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://swcarpentry.github.io/r-novice-inflammation/02-func-R/&#34;&gt;How functions work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Debugging&lt;/em&gt;. Code errors are known as bugs. They can tiresome, but also interesting sometimes! 😅 Some tips for the first many years of experience include: reading and investigating error messages, in both source and console windows; controlling letter case and typos; closing parentheses and inverted commas; ensuring to have the necessary packages installed and loaded; following the format required by each function. To debug, break up code into subcomponents and test each of those to find out the source of the error. Once we act on that, the best outcome is seeing the code work, but sometimes different errors overlap, in which case we may see one error disappearing before another one appears. Debugging soon leads to proficient information seeking. The search process often begins on an internet search engine and extends to user communities, package documentation, tutorials, blogs… (see &lt;a href=&#34;https://youtu.be/Nj9J5iCSMB0?t=2687&#34;&gt;video explanation&lt;/a&gt;). &lt;a href=&#34;https://adv-r.hadley.nz/debugging.html&#34;&gt;Advanced debugging tools are also available&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Vast availability of free resources on the internet, from &lt;a href=&#34;https://www.coursera.org/courses?query=r%20programming&#34;&gt;Coursera&lt;/a&gt; and other MOOC sites, &lt;a href=&#34;https://education.rstudio.com/&#34;&gt;RStudio&lt;/a&gt;, &lt;a href=&#34;https://psyteachr.github.io/&#34;&gt;University of Glasgow&lt;/a&gt;, &lt;a href=&#34;http://swcarpentry.github.io/r-novice-inflammation/&#34;&gt;Carpentries&lt;/a&gt;, etc.&lt;/li&gt;
&lt;li&gt;Community: &lt;a href=&#34;https://stackoverflow.com/&#34;&gt;StackOverflow&lt;/a&gt;, &lt;a href=&#34;https://community.rstudio.com/&#34;&gt;RStudio Community&lt;/a&gt;, &lt;a href=&#34;https://github.com&#34;&gt;Github issues&lt;/a&gt; (e.g., for R packages), etc. Using and contributing back.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rstudio.cloud/&#34;&gt;RStudio Cloud&lt;/a&gt;: a personal RStudio environment on the internet&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;workshop-2-r-markdown-documents&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 2: R Markdown documents&lt;/h4&gt;
&lt;p&gt;Set your input and output in stone using &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;R Markdown&lt;/a&gt;. The analysis reports may be enriched with website features (HTML/CSS) and published as HTML, PDF or Word documents. Moreover, with R packages such as &lt;code&gt;bookdown&lt;/code&gt;, &lt;code&gt;bookdownplus&lt;/code&gt;, &lt;code&gt;blogdown&lt;/code&gt; and &lt;code&gt;flexdashboard&lt;/code&gt;, documents can be formatted as &lt;a href=&#34;https://awesome-blogdown.com/&#34;&gt;websites&lt;/a&gt;, &lt;a href=&#34;https://bookdown.org/&#34;&gt;digital papers and books&lt;/a&gt; and &lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/&#34;&gt;data dashboards&lt;/a&gt;. Other useful packages include &lt;code&gt;rmarkdown&lt;/code&gt;, &lt;code&gt;knitr&lt;/code&gt;, &lt;code&gt;DT&lt;/code&gt;, &lt;code&gt;kableExtra&lt;/code&gt; and &lt;code&gt;ggplot2&lt;/code&gt;. Further background: &lt;a href=&#34;https://www.youtube.com/watch?v=Nj9J5iCSMB0&#34;&gt;presentation by Michael Frank&lt;/a&gt;, &lt;a href=&#34;https://www.eddjberry.com/talks/reproducible-writing-with-rmarkdown.html#1&#34;&gt;slides by Ed Berry&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As well as facilitating the reproducibility of analyses and results to third parties, R Markdown is helpful &lt;em&gt;during the creation&lt;/em&gt; of a report. In particular, it reduces the chances of errors and the number of repetitive tasks. For instance, any part of the data can be inputted in the text directly from the source, rather than manually copying it (e.g., &lt;code&gt;`r mean(dat[dat$location==&#39;Havana&#39;, &#39;measure&#39;])`&lt;/code&gt; (&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/r-code.html&#34;&gt;expand&lt;/a&gt;). Thus, if and when the analysis needs to be changed or updated, the report can be automatically updated at the click of a button. In another area, the captions for figures and tables can be automatised using &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook/cross-ref.html&#34;&gt;cross-reference labels&lt;/a&gt; (e.g., Table &lt;code&gt;\@ref(tab:mtcars)&lt;/code&gt;). This secures the match between the text and the captions of figures and tables, and it automatically updates the numbering whenever and wherever a new figure or table is introduced.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://bookdownplus.netlify.com/portfolio/&#39;&gt;&lt;img width = &#39;50%&#39; src = &#39;https://github.com/pablobernabeu/bookdownplus/blob/master/inst2/copernicus/showcase/copernicus2.png?raw=true&#39; alt = &#39;Example of paper created with bookdownplus (image retrieved from R bookdownplus package)&#39;/&gt;&lt;/a&gt;
&lt;p align=&#34;center&#34; style=&#34;text-align:center; margin-top:-30px; margin-bottom:30px;&#34;&gt;
Image from bookdownplus package (&lt;a href=&#34;https://bookdownplus.netlify.com/portfolio/&#34; class=&#34;uri&#34;&gt;https://bookdownplus.netlify.com/portfolio/&lt;/a&gt;).
&lt;/p&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;workshop-3-introduction-to-data-dashboards&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 3: Introduction to data dashboards&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01782/full&#34;&gt;Data dashboards are web applications used to visualise data&lt;/a&gt; in detail through tables and plots. They assist in explaining and accounting for our data processing and analysis. They don’t require any coding from the end user. While most dashboards and web applications present existing data, a few of them serve the purpose of creating or simulating new data (see &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation&#34;&gt;example&lt;/a&gt;).&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/&#39;&gt; &lt;img width = &#39;90%&#39; src = &#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/8.png&#39; alt = &#39;Illustration of the usage of dashboards alongside data repositories&#39; /&gt; &lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;These all-reproducible dashboards are published as websites, and thus, they can include hyperlinks and downloadable files. Some of the R packages used are &lt;code&gt;knitr&lt;/code&gt;, &lt;code&gt;DT&lt;/code&gt;, &lt;code&gt;kableExtra&lt;/code&gt;, &lt;code&gt;reactable&lt;/code&gt;, &lt;code&gt;ggplot2&lt;/code&gt;, &lt;code&gt;plotly&lt;/code&gt;, &lt;code&gt;rmarkdown&lt;/code&gt;, &lt;code&gt;flexdashboard&lt;/code&gt; and &lt;code&gt;shiny&lt;/code&gt;. The aim of this workshop is to practise creating different forms of dashboards—&lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/&#34;&gt;Flexdashboard&lt;/a&gt; and &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;Shiny&lt;/a&gt;—the latter of which offers &lt;a href=&#34;https://mastering-shiny.org/&#34;&gt;greater features&lt;/a&gt;, and to practise also with the hosting platforms fitting each type—such as personal websites, &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;, &lt;a href=&#34;https://mybinder.org/&#34;&gt;Binder&lt;/a&gt;, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;Shinyapps&lt;/a&gt; and &lt;a href=&#34;https://rstudio.com/products/shiny/shiny-server/&#34;&gt;custom servers&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A great thing about dashboards is that they may be made very simple, but they can also be taken to the next level using some HTML, CSS or Javascript code (on top of the back-end code present in the R packages used), which is addressed in the next workshop.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;workshop-4-binder-environments-and-improving-data-dashboards&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 4: Binder environments and improving data dashboards&lt;/h4&gt;
&lt;div id=&#34;binder&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Binder&lt;/h5&gt;
&lt;p&gt;&lt;a href=&#34;https://mybinder.org/&#34;&gt;Binder&lt;/a&gt; is a tool to facilitate public access to software environments—for instance, by publishing an RStudio environment on the internet. Binder can also host Shiny apps. It is generously free &lt;a href=&#34;https://discourse.jupyter.org/t/mybinder-org-cost-updates/2426&#34;&gt;for users&lt;/a&gt;. After looking at the &lt;a href=&#34;https://github.com/binder-examples/r&#34;&gt;nuts and bolts of a deployment&lt;/a&gt;, participants will be able to deploy their own Binder environments and check the result by the end of the workshop. For this purpose, it’s recommended to have data and R code ready, ideally in a GitHub repository.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;improving-data-dashboards&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Improving data dashboards&lt;/h5&gt;
&lt;p&gt;We will practise how to improve the functionality of dashboards using some HTML, CSS and Javascript code, which is &lt;a href=&#34;https://www.w3schools.com/whatis/&#34;&gt;the basis of websites&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!-- Javascript function to enable a hovering tooltip --&amp;gt;
&amp;lt;script&amp;gt;
$(document).ready(function(){
$(&amp;#39;[data-toggle=&amp;quot;tooltip1&amp;quot;]&amp;#39;).tooltip();
});
&amp;lt;/script&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://shiny.rstudio.com/gallery/&#39;&gt; &lt;img align = &#39;center&#39; width = &#39;60%&#39; src = &#39;https://raw.githubusercontent.com/pablobernabeu/data-is-present/master/dashboard%20gif.gif&#39; alt = &#39;Examples of data dashboards&#39; /&gt; &lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trade-offs-among-dashboards&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Trade-offs among dashboards&lt;/h5&gt;
&lt;p&gt;Next, we will practise with three dashboard types—&lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/using.html&#34;&gt;Flexdashboard&lt;/a&gt;, &lt;a href=&#34;https://shiny.rstudio.com/tutorial/&#34;&gt;Shiny&lt;/a&gt; and &lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/shiny.html&#34;&gt;Flexdashboard-Shiny&lt;/a&gt;—and with the suitable hosting platforms. Firstly, the strength of Flexdashboard (&lt;a href=&#34;http://rpubs.com/pcbernabeu/Dutch-modality-exclusivity-norms&#34;&gt;example&lt;/a&gt;) is its basis on R Markdown, yielding an unmatched user interface (&lt;em&gt;front-end&lt;/em&gt;). Secondly, the strength of Shiny (&lt;a href=&#34;https://mybinder.org/v2/gh/pablobernabeu/Modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing/master?urlpath=shiny/Shiny-app/&#34;&gt;example&lt;/a&gt;) is the input reactivity (&lt;em&gt;back-end&lt;/em&gt;) it offers, allowing users to download sections of data they select, in various formats. Last, Flexdashboard-Shiny (&lt;a href=&#34;https://pablobernabeu.shinyapps.io/dutch-modality-exclusivity-norms/&#34;&gt;example&lt;/a&gt;) combines the best of both worlds.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ &lt;b&gt; Flexdashboard &lt;/b&gt; ★
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ ★ &lt;b&gt; Shiny &lt;/b&gt; ★ ★
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ ★ ★ &lt;b&gt; Flexdashboard-Shiny &lt;/b&gt; ★ ★ ★
&lt;/p&gt;
&lt;p&gt;Flexdashboard types are rendered as an HTML document—simple websites—, and can therefore be easily published on personal sites or &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;. This is convenient because no special hosting is required. In contrast, Shiny and Flexdashboard-Shiny types offer greater features, but require Shiny servers. Fortunately, the shinyapps.io server is available for free, up to some &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;usage limit&lt;/a&gt;. This server can host any of the three dashboards mentioned here. Another good option is presented by Binder environments, which can host the Shiny-type dashboards with no (explicit) limit. Yet, the Flexdashboard-Shiny type cannot be hosted in this server (&lt;a href=&#34;https://github.com/jupyter/repo2docker/issues/799&#34;&gt;as of January 2020, at least&lt;/a&gt;). Consequently, greater functionality may come at a cost for dashboards that have any considerable traffic, whereas dashboards with low traffic may do well on shinyapps.io. Knowing these trade-offs can help navigate &lt;a href=&#34;https://support.rstudio.com/hc/en-us/articles/217592947-What-are-the-limits-of-the-shinyapps-io-Free-plan-&#34;&gt;usage limits&lt;/a&gt;, save on web hosting fees, and increase the availability of our dashboards online, as we can offer fall-back versions on different platforms, as in the example below:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;… &lt;em&gt;&lt;a href=&#34;https://pablobernabeu.shinyapps.io/dutch-modality-exclusivity-norms/&#34;&gt;preferred-dashboard&lt;/a&gt; (in case of downtime, please visit this &lt;a href=&#34;http://rpubs.com/pcbernabeu/Dutch-modality-exclusivity-norms&#34;&gt;alternative&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Transforming dashboards into the different versions can be as easy as enabling or disabling some features, especially input reactivity. For instance, if we want to downgrade a Flexdashboard-Shiny to a Flexdashboard, to publish it outside of a Shiny server (see &lt;a href=&#34;https://github.com/pablobernabeu/Modality-exclusivity-norms-Bernabeu-et-al/blob/master/Dutch-modality-exclusivity-norms-RPubs.Rmd&#34;&gt;example&lt;/a&gt;), we must delete &lt;code&gt;runtime:shiny&lt;/code&gt; from the header, and disable reactive features, as below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
```r
# Number of words selected on sidebar
# reactive(cat(paste0(&amp;#39;Words selected below: &amp;#39;, nrow(selected_props()))))
```&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;free-accounts-and-tips&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Free accounts and tips&lt;/h5&gt;
&lt;p&gt;Hosting sites have specific terms of use. For instance, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;shinyapps.io&lt;/a&gt; has a free starter license with limited use. Free apps can handle a large but limited amount of data, and up to five apps may be created. Beyond this, RStudio offers a wide range of subscriptions starting at $9/month.&lt;/p&gt;
&lt;p&gt;Memory and traffic limits of the free shinyapps.io account can sometimes present problems when heavy data data sets are used, or there are many visits to the app. The memory overload issue is often flagged as &lt;code&gt;Shiny cannot use on-disk bookmarking&lt;/code&gt;, whereas excessive traffic may see the app not loading. Fortunately, usage limits need not always require a paid subscription or a &lt;a href=&#34;https://www.r-bloggers.com/alternative-approaches-to-scaling-shiny-with-rstudio-shiny-server-shinyproxy-or-custom-architecture/&#34;&gt;custom server&lt;/a&gt;, thanks to the following workarounds:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;develop app locally as far as possible, and only deploy to shinyapps.io only at the last stage;&lt;/li&gt;
&lt;li&gt;prune data set, leaving only the necessary data;&lt;/li&gt;
&lt;li&gt;if necessary, unlink data by splitting it into different sets, reducing computational demands;&lt;/li&gt;
&lt;li&gt;if necessary, use various apps (five are allowed in each free shinyapps.io account);&lt;/li&gt;
&lt;li&gt;if necessary, link from the app to a PDF with visualisations requiring heavy, interlinked data. High-resolution plots can be rendered into a PDF document in a snap, using code such as below.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;df(‘List of plots per page’, width = 13, height = 5)
print(plot1)
print(plot2)
# …
print(plot150)
dev.off()&lt;/p&gt;
&lt;p&gt;Conveniently, all text in a PDF—even in plots—is indexed, so it can be searched [ Ctrl+f / Cmd+f / 🔍 ] (see &lt;a href=&#34;https://osf.io/2tpxn/&#34;&gt;example&lt;/a&gt;). Furthermore, you may also &lt;a href=&#34;http://www.ilovepdf.com/&#34;&gt;merge the rendered PDF with any other documents&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;prerequisites-and-suggestions-for-participation-in-each-workshop&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Prerequisites and suggestions for participation in each workshop&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Necessary:&lt;/em&gt; laptop or computer with &lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;https://rstudio.com/products/rstudio/download/&#34;&gt;RStudio&lt;/a&gt; installed, or access to &lt;a href=&#34;https://rstudio.cloud/&#34;&gt;RStudio Cloud&lt;/a&gt;; familiarity with the content of the preceding workshops through the web links herein.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Suggested:&lt;/em&gt; having your own data and R code ready (on a Github repository if participating in Workshop 4); participation in some of the preceding workshops.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;datathons-creating-reproducible-documents-and-dashboards&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Datathons: creating reproducible documents and dashboards&lt;/h3&gt;
&lt;p&gt;In these coding meetups, participants collaborate to create reproducible documents or dashboards using the data and software they prefer (see &lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/tree/master/examples-documents-dashboards&#34;&gt;examples&lt;/a&gt;). Since the work can be split across different people and sections, some nice products may be achieved within a session. Any programming languages may be used.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Data used:&lt;/strong&gt; academic or non-academic data of your own or from open-access sources such as &lt;a href=&#34;https://osf.io&#34;&gt;OSF&lt;/a&gt;, scientific journals, governments, international institutions, NGOs, etc.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inspired by the great &lt;a href=&#34;https://reprohack.github.io/reprohack-hq/&#34;&gt;Reprohacks&lt;/a&gt;, content suggestions are encouraged. That is, if you’d like to have a reproducible document or dashboard created for a certain, open-access data set, please let us know, and some participants may take it on. Suggestions may be posted as &lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/issues&#34;&gt;issues&lt;/a&gt; or emailed to &lt;a href=&#34;mailto:p.bernabeu@lancaster.ac.uk&#34; class=&#34;email&#34;&gt;p.bernabeu@lancaster.ac.uk&lt;/a&gt;.
&lt;br&gt;&lt;/br&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Purposes&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;collaborating to visualise data in novel ways using reproducible documents or interactive dashboards. For this purpose, participants sometimes draw on additional data to look at a bigger picture;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;reflecting on the process by reviewing the techniques applied and challenges encountered.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt; A key aspect of datathons is the creation of output. Documents and dashboards are (co-)authored by the participants who work on them, who can then publish them on their websites, or on &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;, &lt;a href=&#34;https://mybinder.org/&#34;&gt;Binder&lt;/a&gt;, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;Shinyapps&lt;/a&gt; or &lt;a href=&#34;https://rstudio.com/products/shiny/shiny-server/&#34;&gt;custom servers&lt;/a&gt;. Time constraints notwithstanding, a lot of this output may be very enticing for further development by the same participants, or even by other people if the code is shared online. Just like with data, an attribution licence can be attached to the code.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;prerequisites-and-suggestions-for-participation-in-datathons&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Prerequisites and suggestions for participation in datathons&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Necessary:&lt;/em&gt; basic knowledge of reproducible documents or dashboards.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Suggested:&lt;/em&gt; familiarity with the development of reproducible documents or dashboards; an idea about the data you’d like to work with and the kind of document or dashboard you want to create.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;contact&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Contact&lt;/h2&gt;
&lt;p&gt;Please submit any queries or requests by &lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/issues&#34;&gt;posting an issue&lt;/a&gt; or emailing &lt;a href=&#34;mailto:p.bernabeu@lancaster.ac.uk&#34; class=&#34;email&#34;&gt;p.bernabeu@lancaster.ac.uk&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Event-related potentials: Why and how I used them</title>
      <link>/2020/event-related-potentials-why-and-how-i-used-them/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/2020/event-related-potentials-why-and-how-i-used-them/</guid>
      <description>


&lt;p&gt;Event-related potentials (ERPs) offer a unique insight in the study of human cognition. Let’s look at their reason-to-be for the purposes of research, and how they are defined and processed. Most of this content is based on my master’s thesis (&lt;a href=&#34;https://psyarxiv.com/5gjvk/download/&#34;&gt;download&lt;/a&gt;), which I could fortunately conduct at the Max Planck Institute for Psycholinguistics (&lt;a href=&#34;https://psyarxiv.com/a5pcz/&#34;&gt;conference paper&lt;/a&gt; also available).&lt;/p&gt;
&lt;div id=&#34;electroencephalography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Electroencephalography&lt;/h2&gt;
&lt;p&gt;The brain produces electrical activity all the time, which can be measured via electrodes on the scalp—a method known as electroencephalography (EEG). These pulses are constantly produced, for every one of our states and actions. These overlapping pulses happen at extremely high frequencies; indeed, the signal can be measured once per millisecond. Of course, the voltage is in a micro (µ) scale, typically between 10 µV (0.000010) and 100 µV (0.000100) (Aurlien et al., 2004). The high frequency of this signal is very interesting for the study of some cognitive processes in particular, for which the time course is (or may be) critical. One such example is conceptual processing, namely, the process of understanding the meaning of words.&lt;/p&gt;
&lt;p&gt;Research has revealed the relation between certain EEG patterns and cognitive states and functions. Brain activity includes dozens of types, but broadly, it can be divided into neural oscillations and event-related potentials. Specific oscillations (also known as brain waves) are associated to &lt;em&gt;states&lt;/em&gt; such as wakefulness, sleep, arousal, relaxation, etc. (Roohi-Azizi, Azimi, Heysieattalab, &amp;amp; Aamidfar, 2017). Event-related potentials instead represent more finite &lt;em&gt;events&lt;/em&gt;, such the presentation of as a stimulus. In cognitive neuroscience, both oscillations and ERPs are studied, whereas in cognitive psychology, ERPs are much more common than oscillations. Let’s dive into ERPs below.&lt;/p&gt;
&lt;div id=&#34;event-related-potentials&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Event-related potentials&lt;/h3&gt;
&lt;p&gt;In the lab, ERPs are elicited using controlled designs. In each trial, a series of stimuli are presented. At a fixed point therein, an EEG measurement begins and spans for a certain period. This period is called &lt;em&gt;time window&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In psycholinguistics, for instance, a typical scenario is the presentation of words, and ERPs are systematically &lt;em&gt;time-locked&lt;/em&gt; to the same position in consecutive trials, often the onset of a word. By this means, the experimental manipulation is collected, and the non-experimental variation—‘noise’—is largely cancelled out by the aggregation of multiple trials that share the experimental manipulation.&lt;/p&gt;
&lt;p&gt;The chief reason to employ the ERP method is the measurement of cognitive processes online, that is, precisely as they unfold. This is fitting in the context of language comprehension, where important processes last for less than a second.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;time-course-of-word-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Time course of word processing&lt;/h2&gt;
&lt;p&gt;Processing a word takes around 800 milliseconds (ms). Within that time, earlier processes (compared to later ones) have been ascribed greater relevance to the core process of understanding a word (Mahon &amp;amp; Caramazza, 2008). This assumes that broader processes start only after more immediate ones have started (but see Lebois, Wilson‐Mendenhall &amp;amp; Barsalou, 2014). The most immediate process is the recognition of a string of letters, which seems to start within 90 ms post word onset in early auditory cortex and the Visual Word Form Area (Willems, Frank, Nijhoff, Hagoort, &amp;amp; van den Bosch, 2016). Then ensue further, fundamental stages known as &lt;em&gt;lexical&lt;/em&gt; and &lt;em&gt;semantic&lt;/em&gt; processes. Lexical processing is the identification of a string of letters as a known word, and it happens within around 160 ms post word onset. Next, at around 200 ms, we may see the beginning of semantic processing, which denotes a further step in the cognitive analysis of the word that is akin to &lt;em&gt;meaning&lt;/em&gt; (Hauk, 2016). These processes may overlap, as indeed suggested by the sensitivity of the N400 ERP (see also next section) to both lexical and semantic tasks (Kutas &amp;amp; Federmeier, 2011). Both processes also likely extend further in the processing timeline (Hauk, 2016). In spite of this overlap, however, lexical and semantic processing have often been linked to different cognitive phenomena. For instance, tasks promoting semantic processing (e.g., semantic decision, whereby participants describe words as concrete or abstract) have been found to engage sensorimotor simulation of the word’s meaning (known as &lt;em&gt;embodiment&lt;/em&gt;) more strongly than lexical tasks do (Connell &amp;amp; Lynott, 2013; Pexman, Muraki, Sidhu, Siakaluk, &amp;amp; Yap, 2019; Sato, Mengarelli, Riggio, Gallese, &amp;amp; Buccino, 2008).&lt;/p&gt;
&lt;p&gt;Once the lexical and semantic stages have emerged, post-lexical, post-semantic processes follow (Mahon &amp;amp; Caramazza, 2008). These are mental imagery and episodic memory processes—both with an approximate emergence around 270 ms after word onset. The gradual progression from the identification of a word up to accessing its broadest meaning is an important anchoring point in the current research on the alleged embodiment of meaning comprehension, even if we might hope to count on more definitive threshold points (Hauk, 2016).&lt;/p&gt;
&lt;p&gt;Word processing data are mainly based on written word processing, but spoken words are processed quite similarly, if slightly faster (Leonard, Baud, Sjerps, &amp;amp; Chang, 2016; Pulvermüller, Shtyrov, &amp;amp; Ilmoniemi, 2005; Shtyrov, Hauk, &amp;amp; Pulvermüller, 2004).&lt;/p&gt;
&lt;p&gt;The bigger take-home messages would be: (1) the processing of meaning might only start at around 160 ms post word onset, and (2) processes outside of meaning comprehension might only start at around 270 ms. These working references must be taken with some caution because particular semantic effects have been found at different stages (e.g., the conceptual modality switch, as in Hald, Marshall, Janssen, &amp;amp; Garnham, 2011; Collins, Pecher, Zeelenberg, &amp;amp; Coulson, 2011). Indeed, in an influential critique of blooming findings on embodiment, Mahon and Caramazza (2008) argued that even early effects might possibly be explained in terms of non-embodied processing. They contended that working memory processes that were ancillary rather than semantic could be quickly engaged with the function of ‘colouring’ a concept, not building it up. To further complicate the matter, we do not have absolute certainty on the later section of the time course. Thus, as Hauk (2016) reviews, the different stages likely overlap at certain points, with different degrees of relevance. For instance, lexical processing may continue even once semantic processing has started, but would naturally become less relevant. Indeed, the relation among these processes is likely more of a continuum than a set of clear-cut modules. In a nutshell, the time course is important with some experimental effects in word processing, and, to that extent, we depend on our knowledge of the basic time course of word processing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-conceptual-modality-switch-paradigm-and-its-time-course&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The conceptual modality switch paradigm and its time course&lt;/h2&gt;
&lt;p&gt;In demonstrating the relevance of embodied cognition, a sizeable series of studies have shown that reading about different conceptual modalities (e.g., auditory ‘loud bell’ followed by visual ‘pink background’) incurs processing costs (Pecher, Zeelenberg, &amp;amp; Barsalou, 2003). Importantly, this manipulation does not concern the presentation mode of the stimulus, maintained constant, but the intrinsic semantic modality of the stimulus concepts. The conceptual modality switch effect has often been replicated (Pecher, Zeelenberg, &amp;amp; Barsalou, 2004; Solomon &amp;amp; Barsalou, 2004; Marques, 2006; Vermeulen, Niedenthal, &amp;amp; Luminet, 2007; van Dantzig, Pecher, Zeelenberg, &amp;amp; Barsalou, 2008; Lynott &amp;amp; Connell, 2009; Ambrosi, Kalenine, Blaye, &amp;amp; Bonthoux, 2011; Collins et al., 2011; Hald et al., 2011; Hald et al., 2013; Scerrati et al., 2015).&lt;/p&gt;
&lt;p&gt;Bernabeu, Willems and Louwerse (2017) addressed a caveat with the time course of the conceptual modality switch paradigm. In previous experiments, trials presented a concept word followed by a property word. ERPs were time-locked to the latter property word. This design may have left uncontrolled a switch produced already at the concept. Indeed, the property word was already supposed to be in the particular modality of the trial. That pitfall could have had two consequences: loss of power and loss of certainty on the time course of the effect. Thus, Bernabeu et al. created a design in which ERPs were time-locked to the first word in target trials (see some &lt;a href=&#34;https://www.researchgate.net/post/Conceptual_modality_switch_effect_measured_at_first_word&#34;&gt;early input from researchers online&lt;/a&gt;). The purpose of this relocation was not to completely annul the possibility of post-core sensory processes, but to increase the time accuracy by measuring the modality switch from the point at which it is elicited.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../post/event-related-potentials/index_files/designoverview.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Implementing this design had an ancillary effect on the measurement of response times. A psycholinguistic experiment like this one requires controlling fundamental variables such as word frequency and length, by matching the means of these variables across experimental conditions. This must be controlled in the target words at least. As it is often the case, this control was only possible in the target words—the first one in target trials—, but it was not possible in the second word, which is the crucial one for response times. Response times could still be measured, but comparisons across conditions were not fully warranted. In sum, this was an ERP design.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;erp-components&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ERP components&lt;/h2&gt;
&lt;p&gt;When the ERP signal is plotted, it displays multiple wave shapes, or &lt;em&gt;waveforms&lt;/em&gt;, each with a peak flanked by falling tails. Each of these waves often corresponds to an ERP component, which is what cognitive scientists are often interested in.&lt;/p&gt;
&lt;p&gt;Multiple components are known, each having been found to consistently peak around specific points in time during a cognitive process. The peak is one of several features characterising each component. A sketch list is shown below (van Hell &amp;amp; Kroll, 2013).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Polarity:&lt;/strong&gt; The component either peaks in the positive or the negative pole of the signal. This polarity is relative to the &lt;em&gt;baseline&lt;/em&gt; point that is created in the preprocessing stage (see below);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;latency:&lt;/strong&gt; the time course of the component, encompassing an onset, a peak and an overall duration. Time windows are normally set to match relevant components (e.g., the N400 window, etc.);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;amplitude:&lt;/strong&gt; the voltage reached at a given time (e.g., the peak) or for a certain period (e.g., a time window);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;scalp distribution, or topography:&lt;/strong&gt; the areas on the scalp (the scalp being a reasonable proxy for the brain) in which the component appears;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;functional role:&lt;/strong&gt; the cognitive functions that have been consistently associated with the component.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Examples of components in language processing include the N400, consistently linked to semantic processing, that is, seeking the meaning of words or sentences. The N400 is characterised by a large, negative amplitude peaking at around 400 ms post word onset, primarily found in central and posterior sites. N400 &lt;em&gt;effects&lt;/em&gt;, which are comparisons of the N400 component in different experimental conditions, have consistently appeared under violations of semantic expectations, i.e., related to meaning and events (Kutas &amp;amp; Federmeier, 2011; Swaab, Ledoux, Camblin, &amp;amp; Boudewyn, 2012). Another well-known component in language is the P600, linked to syntactic processing, that is, the structure of sentences (Swaab et al., 2012). Other examples of components include lateralized readiness potentials, signalling motor preparation (Mordkoff &amp;amp; Gianaros, 2000), and the P3b component, which appears in the context of responses (van Vliet et al., 2014). Both the latter are relevant to researchers across domains, who often ward off contamination from these components in their experiments. In Bernabeu et al.’s experiment, for instance, time-locking ERPs to the first word in target trials was also useful to ward off contamination from these components.&lt;/p&gt;
&lt;p&gt;ERP data sets are large, being the product of the number of electrodes times the number of time points times the experimental conditions times the number participants. In recent studies, the number of trials often adds to that product, whereas in previous experiments, the trials tended to be aggregated in each condition.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;electroencephalography-montage&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Electroencephalography montage&lt;/h2&gt;
&lt;p&gt;The electroencephalography (EEG) montage is an important factor. The options are broadly characterised by three parameters of the electrodes (also called channels).&lt;/p&gt;
&lt;p style=&#34;margin-left: 30px; line-height: 1.2; padding-bottom: 12px; padding-left: 15px; float: right; display: block;&#34;&gt;
&lt;img src=&#34;../../post/event-related-potentials/index_files/EEG MPI open day photo.jpg&#34; alt=&#34;Pablo Bernabeu, 2015&#34; width=&#34;200px&#34; style=&#39;padding-bottom: 15px; margin-bottom: 0px;&#39; /&gt;&lt;span style=&#34;font-size: small; padding-left: 5px; padding-top: 0px; margin-top: 0px;&#34;&gt;Brainwaves exposed for an open day.&lt;/span&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Number:&lt;/strong&gt; Traditionally, montages with 32, 64 or 128 electrodes have been used. The larger the number, the higher the spatial resolution.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Wet / dry:&lt;/strong&gt; In some montages, the electrical conductance on the electrodes’ contact point must be increased using some fluid solutions, such as a specific gel (often commercialised by the companies that also make EEG apparatuses). Conversely, other electrodes function in a dry way. Ensuring the proper conductance on wet electrodes has traditionally been very time-consuming for experimenters, often taking over half an hour of wiggling a blunt syringe distributing the saline solution around the tip. Traditionally, wet electrodes produced more reliable data than dry ones, but &lt;em&gt;the times they are a’changing&lt;/em&gt; (di Flumeri, 2019).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Active / passive:&lt;/strong&gt; In some wet montages, the conductance-prompting job is much facilitated by the existence of a pilot light on top of each electrode, which signals the conductance level throughout the setup on the participant’s head.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An EEG/ERP experiment is time-consuming. The preparation (especially conductance-prompting with wet montages) and post-experiment procedures (especially washing the EEG cap) often take four or five times as long as the experiment proper. These procedures are especially long for higher-density, wet, passive montages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;preprocessing-erps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preprocessing ERPs&lt;/h2&gt;
&lt;p&gt;ERPs are not the first signal collected in experiments. They are obtained after considerable, systematic preprocessing of the EEG signal.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#39;https://www.researchgate.net/post/EEG_error_datasets_missing_channels_Its_strange_because_they_were_recorded_well_and_faulty_files_are_quite_as_heavy_as_the_good_ones_Any_ideas&#39;&gt;&lt;img src=&#34;https://www.researchgate.net/profile/Nikolay_Novitskiy/post/EEG_error_datasets_missing_channels_Its_strange_because_they_were_recorded_well_and_faulty_files_are_quite_as_heavy_as_the_good_ones_Any_ideas/attachment/59d6391b79197b8077996520/AS%3A400433085468672%401472482095219/image/41_64ch.png&#34; alt=&#34;Brain Vision waveforms&#34; width=&#39;70%&#39;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For the Bernabeu et al. study, I used Brain Vision software, and followed the &lt;a href=&#34;https://erpinfo.org/resources&#34;&gt;tutorials from the well-known ERP Boot Camp&lt;/a&gt; of Steve Luck and Emily Kappenman. I applied the following pipeline separately for each participant:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;labeling channels (64 electrodes);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;creating channel groups (anterior and posterior);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;re-referencing the signal offline to the right mastoid (RM), having referenced online to the left mastoid (Ref);&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;../../post/event-related-potentials/index_files/EEG%20montage.png&#34; style=&#34;width:90.0%&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;separating my three experimental conditions;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ocular correction for blinks and significant, vertical or horizontal movements of the eyes (seminal method by Gratton, Coles, &amp;amp; Donchin, 1983, which is the default in Brain Vision);&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;baseline correction, which is a standardisation based on a certain period immediately before the onset of the target manipulation;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;further correction of artifacts such as motor action potentials (or lateralised readiness potentials) resulting from even the subtlest muscle activity.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This pipeline is reflected in the &lt;a href=&#34;https://osf.io/98fs6/&#34;&gt;scripts exported from Brain Vision&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  &amp;lt;Nodes&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMatch&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection/baselinecorr&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection/baselinecorr/baselinecorr_artif&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection/baselinecorr/baselinecorr_artif/minave_TotalMatch&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection/baselinecorr&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection/baselinecorr/baselinecorr_artif&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection/baselinecorr/baselinecorr_artif/minave_EmbodiedMismatch&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection/baselinecorr&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection/baselinecorr/baselinecorr_artif&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection/baselinecorr/baselinecorr_artif/minave_TotalMismatch&amp;lt;/string&amp;gt;
  &amp;lt;/Nodes&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Word reading ERPs can look somewhat like this after the preprocessing (&lt;a href=&#34;https://osf.io/bz7ae/&#34;&gt;plots made in R&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../post/event-related-potentials/index_files/Four%20main%20waveform%20plots%20stacked.png&#34; style=&#34;width:80.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To visualise these waveforms throughout the different sections of the data, a &lt;a href=&#34;https://mybinder.org/v2/gh/pablobernabeu/Modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing/master?urlpath=shiny/Shiny-app/&#34;&gt;dashboard is available&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;statistical-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Statistical analysis&lt;/h2&gt;
&lt;p&gt;With the myriad repeated measures involved in EEG, linear mixed-effects models are a good option, allowing the registration of electrodes and time points in the error term per participant (and trial, too, if these are not aggregated). The analysis I performed, in R, is &lt;a href=&#34;https://osf.io/sx3nw/&#34;&gt;available&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.embed-responsive{position:relative;height:100%;}.embed-responsive iframe{position:absolute;height:100%;}&lt;/style&gt;
&lt;script&gt;window.jQuery || document.write(&#39;&lt;script src=&#34;//code.jquery.com/jquery-1.11.2.min.js&#34;&gt;\x3C/script&gt;&#39;) &lt;/script&gt;
&lt;link href=&#34;https://mfr.osf.io/static/css/mfr.css&#34; media=&#34;all&#34; rel=&#34;stylesheet&#34;&gt;
&lt;div id=&#34;mfrIframe&#34; class=&#34;mfr mfr-file&#34;&gt;

&lt;/div&gt;
&lt;script src=&#34;https://mfr.osf.io/static/js/mfr.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;var mfrRender = new mfr.Render(&#34;mfrIframe&#34;, &#34;https://mfr.osf.io/render?url=https://osf.io/sx3nw/?direct%26mode=render%26action=download%26mode=render&#34;);&lt;/script&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Event-related potentials fulfil an important role in cognitive neuroscience and psychology, only matched by magnetic electroencephalography (MEG), which unites high temporal and spatial resolution. Learning how to use this method is demanding but even more rewarding. It certainly does not make for fast science, but allows the measurement of experimental effects online, that is, as they unfold.&lt;/p&gt;
&lt;p&gt;You can learn about and overcome multiple challenges. One of the issues I faced once regarded some channels (electrodes) that appeared to be missing from the data. I posted a &lt;a href=&#34;https://www.researchgate.net/post/EEG_error_datasets_missing_channels_Its_strange_because_they_were_recorded_well_and_faulty_files_are_quite_as_heavy_as_the_good_ones_Any_ideas&#34;&gt;question on ResearchGate&lt;/a&gt;, and emailed Brain Products, the maker of Brain Vision Recorder, which I was using.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hi everyone,&lt;/p&gt;
&lt;p&gt;If you could please give me a hand with this error, I would be very grateful. I have EEG from a psychological experiment, recorded with BrainVision Recorder, and being analyzed with BrainVision Analyzer 2. Most of the recordings are perfectly fine, but a few present a big error. Out of 64 original electrodes, only two appear. These are the right mastoid (RM) and the left eye sensor (LEOG). Both are bipolar electrodes. RM is to be re-referenced to the online reference electrode, while LEOG is to be re-referenced to the right eye electrode.&lt;/p&gt;
&lt;p&gt;I just can’t fathom the error because all electrodes worked fine during the recording. Also, the data sets with the error are quite as heavy in terms of bytes as those without the error. Further, why should the RM and LEOG channels remain perfectly well as they do?&lt;/p&gt;
&lt;p&gt;This issue might seem like a simple zoom I’ve bypassed, or similar… But unfortunately the channels are just not there. I’ve confirmed it as I tried to copy the pipeline from the good data sets onto the faulty ones, where I got the error ‘No channels enabled.’ In case you had access to the BVA analysis software, please find the raw files for one of the faulty data sets here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Thanks to invaluable help from a &lt;a href=&#34;https://www.researchgate.net/post/EEG_error_datasets_missing_channels_Its_strange_because_they_were_recorded_well_and_faulty_files_are_quite_as_heavy_as_the_good_ones_Any_ideas&#34;&gt;ResearchGate contributor&lt;/a&gt; and the Brain Products team, I could put the pieces back together.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Update: Problem solved.&lt;/p&gt;
&lt;p&gt;As Nikolay said, the error originated in Recorder (I had used the workspace from the previous experimenter), and the problem was solved by setting the label and position of each channel.&lt;/p&gt;
&lt;p&gt;I tried editing the .vhdr file in raw (it seemed nice and quick to directly assign the channel names as labels) but i didn’t quite find the way. Therefore, with a tip from the Brain Products team, I went about it within the program.&lt;/p&gt;
&lt;p&gt;First, I used the transform function ‘Edit channels’ to rename all labels and set each within their coordinates. I did that for just one subject (it doesn’t take as long as it sounds). Afterwards, I created a ‘History template’ out of that process, and copied it to all other nodes.
At any rate, never getting out of the comfort workspace again… :D&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Ambrosi, S., Kalenine, S., Blaye, A., &amp;amp; Bonthoux, F. (2011). Modality switching cost during property verification by 7 years of age. &lt;em&gt;International Journal of Behavioral Development, 35&lt;/em&gt;, 1, 78-83.&lt;/p&gt;
&lt;p&gt;Aurlien, H., Gjerde, I., Aarseth, J., Eldøen, G., Karlsen, B., Skeidsvoll, H., &amp;amp; Gilhus, N. (2003).
EEG background activity described by a large computerized database. &lt;em&gt;Clinical Neurophysiology, 115&lt;/em&gt;, 665–673.&lt;/p&gt;
&lt;p&gt;Bernabeu, P., Willems, R. M., &amp;amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, &amp;amp; E. J. Davelaar (Eds.), &lt;em&gt;Proceedings of the 39th Annual Conference of the Cognitive Science Society&lt;/em&gt; (pp. 1629-1634). Austin, TX: Cognitive Science Society.&lt;/p&gt;
&lt;p&gt;Collins, J., Pecher, D., Zeelenberg, R., &amp;amp; Coulson, S. (2011). Modality switching in a property verification task: an ERP study of what happens when candles flicker after high heels click. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Connell, L., &amp;amp; Lynott, D. (2013). Flexible and fast: Linguistic shortcut affects both shallow and deep conceptual processing. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 20&lt;/em&gt;, 542-550.&lt;/p&gt;
&lt;p&gt;Di Flumeri, G., Aricò, P., Borghini, G., Sciaraffa, N., Di Florio, A., &amp;amp; Babiloni, F. (2019). The Dry Revolution: Evaluation of Three Different EEG Dry Electrode Types in Terms of Signal Spectral Features, Mental States Classification and Usability. &lt;em&gt;Sensors (Basel, Switzerland), 19&lt;/em&gt;(6), 1365.&lt;/p&gt;
&lt;p&gt;Gratton, G., Coles, M. G., &amp;amp; Donchin, E. (1983). A new method for offline removal of ocular artefact. &lt;em&gt;Electroencephalography and Clinical Neurophysiology, 55&lt;/em&gt;, 4, 468-484.&lt;/p&gt;
&lt;p&gt;Hald, L. A., Hocking, I., Vernon, D., Marshall, J.-A., &amp;amp; Garnham, A. (2013). Exploring modality switching effects in negated sentences: further evidence for grounded representations. &lt;em&gt;Frontiers in Psychology, 4&lt;/em&gt;, 93.&lt;/p&gt;
&lt;p&gt;Hald, L. A., Marshall, J.-A., Janssen, D. P., &amp;amp; Garnham, A. (2011). Switching modalities in a sentence verification task: ERP evidence for embodied language processing. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Hauk, O. (2016). Only time will tell—Why temporal information is essential for our neuroscientific understanding of semantics. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;, 4, 1072-1079.&lt;/p&gt;
&lt;p&gt;Kutas, M., &amp;amp; Federmeier, K. D. (2011). Thirty years and counting: finding meaning in the N400 component of the event-related brain potential (ERP). &lt;em&gt;Annual Review of Psychology, 62&lt;/em&gt;, 621–647.&lt;/p&gt;
&lt;p&gt;Lebois, L. A. M., Wilson-Mendenhall, C. D., &amp;amp; Barsalou, L. W. (2014). Are automatic conceptual cores the gold standard of semantic processing? The context-dependence of spatial meaning in grounded congruency effects. &lt;em&gt;Cognitive Science, 39&lt;/em&gt;, 8, 1764-801.&lt;/p&gt;
&lt;p&gt;Leonard, M. K., Baud, M. O., Sjerps, M. J., &amp;amp; Chang, E. F. (2016). Perceptual restoration of masked speech in human cortex. &lt;em&gt;Nature Communications, 7&lt;/em&gt;, 13619.&lt;/p&gt;
&lt;p&gt;Luck, S. J. &amp;amp; Kappenman, E.S. (Eds.), &lt;em&gt;Oxford Handbook of Event-Related Potential Components&lt;/em&gt;. New York: Oxford University Press&lt;/p&gt;
&lt;p&gt;Mahon, B.Z., &amp;amp; Caramazza, A. (2008). A critical look at the Embodied Cognition Hypothesis and a new proposal for grounding conceptual content. &lt;em&gt;Journal of Physiology - Paris, 102&lt;/em&gt;, 59-70.&lt;/p&gt;
&lt;p&gt;Marques, J. F. (2006). Specialization and semantic organization: Evidence for multiple semantics linked to sensory modalities. **Memory &amp;amp; Cognition, 34*, 1, 60-67.&lt;/p&gt;
&lt;p&gt;Mordkoff, J. T., &amp;amp; Gianaros, P. J. (2000). Detecting the onset of the lateralized readiness potential: A comparison of available methods and procedures. &lt;em&gt;Psychophysiology, 37&lt;/em&gt;(3), 347–360.&lt;/p&gt;
&lt;p&gt;Pecher, D., Zeelenberg, R., &amp;amp; Barsalou, L. W. (2003). Verifying different-modality properties for concepts produces switching costs. &lt;em&gt;Psychological Science, 14&lt;/em&gt;, 2, 119-24.&lt;/p&gt;
&lt;p&gt;____ (2004). Sensorimotor simulations underlie conceptual representations: Modality-specific effects of prior activation. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 11&lt;/em&gt;, 1, 164-167.&lt;/p&gt;
&lt;p&gt;Pexman, P. M., Muraki, E. J., Sidhu, D. M., Siakaluk, P. D., &amp;amp; Yap, M. J. (2019). Quantifying sensorimotor experience: Body-object interaction ratings for more than 9,000 English words. &lt;em&gt;Behavior Research Methods, 51&lt;/em&gt;, 453-466.&lt;/p&gt;
&lt;p&gt;Pulvermüller, F., Shtyrov, Y., &amp;amp; Hauk, O. (2009). Understanding in an instant: Neurophysiological evidence for mechanistic language circuits in the brain. &lt;em&gt;Brain and Language, 110&lt;/em&gt;, 2, 81–94.&lt;/p&gt;
&lt;p&gt;Roohi-Azizi, M., Azimi, L., Heysieattalab, S., &amp;amp; Aamidfar, M. (2017). Changes of the brain’s bioelectrical activity in cognition, consciousness, and some mental disorders. &lt;em&gt;Medical journal of the Islamic Republic of Iran, 31&lt;/em&gt;, 53.&lt;/p&gt;
&lt;p&gt;Sato, M., Mengarelli, M., Riggio, L., Gallese, V., &amp;amp; Buccino, G. (2008). Task related modulation of the motor system during language processing. &lt;em&gt;Brain and Language, 105&lt;/em&gt;, 83–90.&lt;/p&gt;
&lt;p&gt;Scerrati, E., Baroni, G., Borghi, A. M., Galatolo, R., Lugli, L., &amp;amp; Nicoletti, R. (2015). The modality-switch effect: visually and aurally presented prime sentences activate our senses. &lt;em&gt;Frontiers in Psychology, 6&lt;/em&gt;, 1668.&lt;/p&gt;
&lt;p&gt;Shtyrov, Y., Hauk, O., &amp;amp; Pulvermüller, F. (2004). Distributed neuronal networks for encoding category-specific semantic information: the mismatch negativity to action words. &lt;em&gt;European Journal of Neuroscience, 1&lt;/em&gt;, 4, 1083–1092.&lt;/p&gt;
&lt;p&gt;Solomon, K. O., &amp;amp; Barsalou, L. W. (2004). Perceptual simulation in property verification. &lt;em&gt;Memory &amp;amp; Cognition, 32&lt;/em&gt;, 244-259.&lt;/p&gt;
&lt;p&gt;Swaab, T.Y., Ledoux, K., Camblin, C.C., &amp;amp; Boudewyn, M.A. (2012) Language related ERP components. (Book Chapter). In Luck, S. J. &amp;amp; Kappenman, E.S. (Eds.), &lt;em&gt;Oxford Handbook of Event-Related Potential Components&lt;/em&gt; (pp. 397-440). New York: Oxford University Press&lt;/p&gt;
&lt;p&gt;Van Dantzig, S., Pecher, D., Zeelenberg, R., &amp;amp; Barsalou, L. W. (2008). Perceptual processing affects conceptual processing. &lt;em&gt;Cognitive Science, 32&lt;/em&gt;, 579–590.&lt;/p&gt;
&lt;p&gt;Van Hell, J. G., &amp;amp; Kroll, J. F. (2013). Using electrophysiological measures to track the mapping of words to concepts in the bilingual brain: a focus on translation. In J. Altarriba &amp;amp; L. Isurin (Eds.), &lt;em&gt;Memory, Language, and Bilingualism: Theoretical and Applied Approaches&lt;/em&gt; (pp. 126-160). New York: Cambridge University Press.&lt;/p&gt;
&lt;p&gt;Van Vliet, M., Manyakov, N., Storms, G., Fias, W., Wiersema, J., &amp;amp; Van Hulle, M. (2014). Response-Related Potentials during semantic priming: the effect of a speeded button response task on ERPs. &lt;em&gt;PLoS One, 9&lt;/em&gt;, 2, e87650.&lt;/p&gt;
&lt;p&gt;Vermeulen, N., Niedenthal, P. M., &amp;amp; Luminet, O. (2007). Switching between sensory and affective systems incurs processing costs. &lt;em&gt;Cognitive Science, 31&lt;/em&gt;, 1, 183-192.&lt;/p&gt;
&lt;p&gt;Willems, R. M., Frank, S. L., Nijhoff, A. D., Hagoort, P., &amp;amp; Van den Bosch, A. (2016). Prediction during natural language comprehension. &lt;em&gt;Cerebral Cortex, 26&lt;/em&gt;, 6, 2506-2516.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dutch modality exclusivity norms</title>
      <link>/applications-and-dashboards/bernabeu-2018-modalitynorms/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/applications-and-dashboards/bernabeu-2018-modalitynorms/</guid>
      <description>&lt;a href=&#39;../../dashboards/Dutch-modality-exclusivity-norms/d.html&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #4CAF50; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt; 
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;i class=&#34;fas fa-mouse-pointer&#34;&gt;&lt;/i&gt;&lt;/span&gt;&amp;nbsp; Reduced dashboard &lt;font style=&#39;font-size:60%;&#39;&gt;&lt;i&gt;Flexdashboard&lt;/i&gt;&lt;/font&gt; &lt;/h3&gt;&lt;/button&gt;&lt;/a&gt; &amp;nbsp; 
&lt;br&gt;
&lt;br&gt;
&lt;a href=&#39;https://pablobernabeu.shinyapps.io/Dutch-modality-exclusivity-norms&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #196F27; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt; 
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;i class=&#34;fas fa-mouse-pointer&#34;&gt;&lt;/i&gt;&lt;/span&gt;&amp;nbsp; Complete dashboard &lt;font style=&#39;font-size:60%;&#39;&gt;&lt;i&gt;Flexdashboard-Shiny&lt;/i&gt;&lt;/font&gt; &lt;/h3&gt;&lt;/button&gt;&lt;/a&gt;
&lt;br&gt;
&lt;br&gt;
&lt;p&gt;This web application presents linguistic data over several tabs. The code combines the great front-end of Flexdashboard—based on R Markdown and yielding an unmatched user interface—, with the great back-end of Shiny—allowing users to download sections of data they select, in various formats.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A cool, recent finding was the reactable package, which puts Javascript into the cells, allowing coloured bars, etc.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  
Auditory = colDef(header = with_tooltip(&#39;Auditory Rating&#39;,
                                        &#39;Mean rating of each word on the auditory modality across participants.&#39;),
                  cell = function(value) {
                    width &amp;lt;- paste0(value / max(table_data$Auditory) * 100, &amp;quot;%&amp;quot;)
                    value = sprintf(&amp;quot;%.2f&amp;quot;, round(value,2))  # Round to two digits, keeping trailing zeros
                    bar_chart(value, width = width, fill = &#39;#ff3030&#39;)
                    },
                  align = &#39;left&#39;),
  
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One of the hardest nuts to crack was allowing the full functionality of tables—i.e, scaling to screen, frozen header, and vertical and horizontal scrolling—whilst having tweaked the vertical/horizontal orientation of the dashboard sections. Initial clashes were sorted by adjusting the section&#39;s CSS styles&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Table {#table style=&amp;quot;background-color:#FCFCFC;&amp;quot;}
=======================================================================
  
Inputs {.sidebar style=&#39;position:fixed; padding-top: 65px; padding-bottom:30px;&#39;}
-----------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and by also adjusting the reactable settings.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  
renderReactable({
  reactable(selected_words(),
            defaultSorted = list(cat = &#39;desc&#39;, word = &#39;asc&#39;),
            defaultColDef = colDef(footerStyle = list(fontWeight = &amp;quot;bold&amp;quot;)),
            height = 840, striped = TRUE, pagination = FALSE, highlight = TRUE,
  
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A nice feature, especially suited to Flexdashboard, was the use of different formats across tabs. Whereas the Info tab presents long text using HTML and CSS styling, along with rmarkdown code output, the other tabs rely more strongly on Javascript features, enabled by R packages such as ‘shiny’ and sweetalert (e.g., allowing modal dialogs—pop-ups), reactable and plotly (e.g., allowing information opened by hovering—tooltips).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```{r}
  
# reactive for the word bar
highlighted_properties = reactive(input$highlighted_properties)
  
renderPlotly({
 ggplotly(
  ggplot( selected_props(), aes(RC1, RC2, label = as.character(word), color = main, 
    # Html tags below used for format. Decimals rounded to two.
    text = paste0(&#39; &#39;, &#39;&amp;lt;span style=&amp;quot;padding-top:3px; padding-bottom:3px; font-size:2.2em; color:#EEEEEE&amp;quot;&amp;gt;&#39;, capitalize(word), &#39;&amp;lt;/span&amp;gt; &#39;, &#39;&amp;lt;br&amp;gt;&#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Dominant modality: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, main, &#39; &#39;,
     &#39; &#39;, &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Modality exclusivity: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Exclusivity, 2)), &#39;% &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Perceptual strength: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Perceptualstrength, 2)),
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Auditory rating: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Auditory, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Haptic rating: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Haptic, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Visual rating: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, sprintf(&amp;quot;%.2f&amp;quot;, round(Visual, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Concreteness (Brysbaert et al., 2014): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, 
       sprintf(&amp;quot;%.2f&amp;quot;, round(concrete_Brysbaertetal2014, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Number of letters: &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, letters, &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Number of phonemes (DutchPOND): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, 
     round(phonemes_DUTCHPOND, 2), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Contextual diversity (lg10CD SUBTLEX-NL): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;,
       sprintf(&amp;quot;%.2f&amp;quot;, round(freq_lg10CD_SUBTLEXNL, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Word frequency (lg10WF SUBTLEX-NL): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;,
       sprintf(&amp;quot;%.2f&amp;quot;, round(freq_lg10WF_SUBTLEXNL, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Lemma frequency (CELEX): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, 
       sprintf(&amp;quot;%.2f&amp;quot;, round(freq_CELEX_lem, 2)), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Phonological neighbourhood size (DutchPOND): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;, 
     round(phon_neighbours_DUTCHPOND, 2), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Orthographic neighbourhood size (DutchPOND): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;,
     round(orth_neighbours_DUTCHPOND, 2), &#39; &#39;,
     &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&amp;lt;span style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt; Age of acquisition (Brysbaert et al., 2014): &amp;lt;/span&amp;gt;&amp;lt;b style=&amp;quot;color:#EEEEEE&amp;quot;&amp;gt;&#39;,
     sprintf(&amp;quot;%.2f&amp;quot;, round(AoA_Brysbaertetal2014, 2)), &#39; &#39;, &#39;&amp;lt;br&amp;gt; &#39;
     ) ) ) +
  geom_text(size = ifelse(selected_props()$word %in% highlighted_properties(), 7,
             ifelse(is.null(highlighted_properties()), 3, 2.8)),
      fontface = ifelse(selected_props()$word %in% highlighted_properties(), &#39;bold&#39;, &#39;plain&#39;)) +
geom_point(alpha = 0) +  # This geom_point helps to colour the tooltip according to the dominant modality
scale_colour_manual(values = colours, drop = FALSE) + theme_bw() + ggtitle(&#39;Property words&#39;) +
labs(x = &#39;Varimax-rotated Principal Component 1&#39;, y = &#39;Varimax-rotated Principal Component 2&#39;) +
guides(color = guide_legend(title = &#39;Main&amp;lt;br&amp;gt;modality&#39;)) +
theme( plot.background = element_blank(), panel.grid.major = element_blank(),
   panel.grid.minor = element_blank(), panel.border = element_blank(),
   axis.line = element_line(color = &#39;black&#39;), plot.title = element_text(size = 14, hjust = .5),
   axis.title.x = element_text(colour = &#39;black&#39;, size = 12, margin = margin(15,15,0,15)),
   axis.title.y = element_text(colour = &#39;black&#39;, size = 12, margin = margin(0,15,15,5)),
   axis.text.x = element_text(size = 8), axis.text.y  = element_text(size = 8),
   legend.background = element_rect(size = 2), legend.position = &#39;none&#39;,
 legend.title = element_blank(),
 legend.text = element_text(colour = colours, size = 13) ),
tooltip = &#39;text&#39;
)
})
  
# For download, save plot without the interactive &#39;plotly&#39; part
  
properties_png = reactive({ ggplot(selected_props(), aes(RC1, RC2, color = main, label = as.character(word))) +
geom_text(show.legend = FALSE, size = ifelse(selected_props()$word %in% highlighted_properties(), 7,
         ifelse(is.null(highlighted_properties()), 3, 2.8)),
      fontface = ifelse(selected_props()$word %in% highlighted_properties(), &#39;bold&#39;, &#39;plain&#39;)) +
geom_point(alpha = 0) + scale_colour_manual(values = colours, drop = FALSE) + theme_bw() +
guides(color = guide_legend(title = &#39;Main&amp;lt;br&amp;gt;modality&#39;, override.aes = list(size = 7, alpha = 1))) +
ggtitle( paste0(&#39;Properties&#39;, &#39; (showing &#39;, nrow(selected_props()), &#39; out of &#39;, nrow(props), &#39;)&#39;) ) + 
labs(x = &#39;Varimax-rotated Principal Component 1&#39;, y = &#39;Varimax-rotated Principal Component 2&#39;) +
theme( plot.background = element_blank(), panel.grid.major = element_blank(),
   panel.grid.minor = element_blank(), panel.border = element_blank(),
   axis.line = element_line(color = &#39;black&#39;), plot.title = element_text(size = 17, hjust = .5, margin = margin(3,3,7,3)),
   axis.title.x = element_text(colour = &#39;black&#39;, size = 12, margin = margin(10,10,2,10)),
   axis.title.y = element_text(colour = &#39;black&#39;, size = 12, margin = margin(10,10,10,5)),
   axis.text.x = element_text(size = 8), axis.text.y  = element_text(size = 8),
   legend.background = element_rect(size = 2), legend.position = &#39;right&#39;,
   legend.title = element_blank(), legend.text = element_text(size = 15))
})
  
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The only instance in which I drew on javascript code outside R packages was to enable tooltips beyond the packages’ limits—for instance, in the side bar. This javascript feature is created at the top of the script, in the head area.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!-- Javascript function to enable a hovering tooltip --&amp;gt;
&amp;lt;script&amp;gt;
$(document).ready(function(){
   $(&#39;[data-toggle=&amp;quot;tooltip1&amp;quot;]&#39;).tooltip();
});
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the side bar, I added a reactive mean for each variable, complementing the range selector.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;reactive(cat(paste0(&#39;Mean = &#39;, 
  sprintf(&amp;quot;%.2f&amp;quot;, round(mean(selected_words()$Exclusivity),2)))))
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;static-version-published-on-rpubs&#34;&gt;Static version published on RPubs&lt;/h2&gt;
&lt;p&gt;A reduced, &amp;lsquo;static&amp;rsquo; version was also created to increase the availability of the content. Removing some reactivity features allows the dashboard to be published as a standard website (i.e., on a personal website, on &lt;a href=&#34;rpubs.com&#34;&gt;RPubs&lt;/a&gt;, etc.), without the need for a back-end Shiny server. Note that this type of website is dubbed &amp;lsquo;static&amp;rsquo;, but it can retain multiple interactive features thanks to Javascript-based tools under the hood, allowed by R packages such as &lt;code&gt;leaflet&lt;/code&gt; for maps, &lt;code&gt;DT&lt;/code&gt; for tables, &lt;code&gt;plotly&lt;/code&gt; for plots, etc.&lt;/p&gt;
&lt;p&gt;To create the Flexdashboard-only version departing from the Flexdashboard-Shiny version, I deleted &lt;code&gt;runtime: shiny&lt;/code&gt; from the YAML header, and disabled Shiny reactive inputs and objects, as below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```{r}
# Number of words selected on sidebar
# reactive(cat(paste0(&#39;Words selected below: &#39;, nrow(selected_props()))))
```
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;div style = &#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P. (2018). Dutch modality exclusivity norms for 336 properties and 411 concepts [Data dashboard]. Retrieved from &lt;a href=&#34;https://pablobernabeu.shinyapps.io/Dutch-Modality-Exclusivity-Norms/&#34;&gt;https://pablobernabeu.shinyapps.io/Dutch-Modality-Exclusivity-Norms/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;br&gt;
</description>
    </item>
    
    <item>
      <title>Naive principal component analysis in R</title>
      <link>/2018/naive-principal-component-analysis-in-r/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/2018/naive-principal-component-analysis-in-r/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Principal_component_analysis&#34;&gt;Principal Component Analysis (PCA)&lt;/a&gt; is a technique used to find the core components that underlie different variables. It comes in very useful whenever doubts arise about the true origin of three or more variables. There are two main methods for performing a PCA: naive or less naive. In the naive method, you first check some conditions in your data which will determine the essentials of the analysis. In the less-naive method, you set those yourself based on whatever prior information or purposes you had. The latter method is appropriate when you already have enough information about the intercorrelations, or when you are required to select a specific number of components. I will tackle the naive method, mainly by following the guidelines in &lt;a href=&#34;https://freethegeogbooks.files.wordpress.com/2016/08/book-for-r-language-stats.pdf&#34;&gt;Field, Miles, and Field (2012)&lt;/a&gt;, with updated code where necessary. A &lt;a href=&#34;https://freethegeogbooks.files.wordpress.com/2016/08/book-for-r-language-stats.pdf&#34;&gt;manual by Charles M. Friel&lt;/a&gt; (Sam Houston State University) was also useful.&lt;/p&gt;
&lt;p&gt;The ‘naive’ approach is characterized by a first stage that checks whether the PCA should actually be performed with your current variables, or if some should be removed. The variables that are accepted are taken to a second stage which identifies the number of principal components that seem to underlie your set of variables.&lt;/p&gt;
&lt;div id=&#34;stage-1.-determine-whether-pca-is-appropriate-at-all-considering-the-variables&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;STAGE 1. Determine whether PCA is appropriate at all, considering the variables&lt;/h4&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;45%&#34; src=&#34;1.jpg&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Variables should be &lt;strong&gt;inter-correlated enough but not too much.&lt;/strong&gt; Field et al. (2012) provide some thresholds, suggesting that no variable should have many correlations below .30, or &lt;em&gt;any&lt;/em&gt; correlation at all above .90. Thus, in the example here, variable Q06 should probably be excluded from the PCA.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Bartlett’s test&lt;/strong&gt;, on the nature of the intercorrelations, should be significant. Significance suggests that the variables are not an ‘identity matrix’ in which correlations are a sampling error.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;KMO&lt;/strong&gt; (Kaiser-Meyer-Olkin), a measure of sampling adequacy based on common variance (so similar purpose as Bartlett’s). As Field et al. review, ‘values between .5 and .7 are mediocre, values between .7 and .8 are good, values between .8 and .9 are great and values above .9 are superb’ (p. 761). There’s a general score as well as one per variable. The general one will often be good, whereas the individual scores may more likely fail. Any variable with a score below .5 should probably be removed, and the test should be run again.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Determinant:&lt;/strong&gt; A formula about multicollinearity. The result should preferably fall below .00001.
Note that some of these tests are run on the dataframe and others on a correlation matrix of the data, as distinguished below.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;
# Necessary libraries
library(ltm)
library(lattice)
library(psych)
library(car)
library(pastecs)
library(scales)
library(ggplot2)
library(arules)
library(plyr)
library(Rmisc)
library(GPArotation)
library(gdata)
library(MASS)
library(qpcR)
library(dplyr)
library(gtools)
library(Hmisc)

# Select variables of interest for the PCA
dataset = mydata[, c(&amp;#39;select_var1&amp;#39;,&amp;#39;select_var1&amp;#39;,&amp;#39;select_var2&amp;#39;,&amp;#39;select_var3&amp;#39;,&amp;#39;select_var4&amp;#39;,&amp;#39;select_var5&amp;#39;,&amp;#39;select_var6&amp;#39;,&amp;#39;select_var7&amp;#39;)]

# Create matrix: some tests will require it
data_matrix = cor(dataset, use = &amp;#39;complete.obs&amp;#39;)

# See intercorrelations
round(data_matrix, 2)

# Bartlett&amp;#39;s
cortest.bartlett(dataset)

# KMO (Kaiser-Meyer-Olkin)
KMO(data_matrix)

# Determinant
det(data_matrix)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-2.-identify-number-of-components-aka-factors&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;STAGE 2. Identify number of components (aka factors)&lt;/h4&gt;
&lt;p&gt;In this stage, principal components (formally called ‘factors’ at this stage) are identified among the set of variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The identification is done through a basic, ‘unrotated’ PCA. The number of components set a priori must equal the number of variables that are being tested.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# Start off with unrotated PCA

pc1 = psych::principal(dataset, nfactors = length(dataset), rotate=&amp;quot;none&amp;quot;)
pc1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is an example result:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Principal Components Analysis
## Call: psych::principal(r = eng_prop, nfactors = 3, rotate = &amp;quot;none&amp;quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##           PC1   PC2  PC3 h2       u2 com
## Aud_eng -0.89  0.13 0.44  1 -2.2e-16 1.5
## Hap_eng  0.64  0.75 0.15  1  1.1e-16 2.0
## Vis_eng  0.81 -0.46 0.36  1 -4.4e-16 2.0
## 
##                        PC1  PC2  PC3
## SS loadings           1.87 0.79 0.34
## Proportion Var        0.62 0.26 0.11
## Cumulative Var        0.62 0.89 1.00
## Proportion Explained  0.62 0.26 0.11
## Cumulative Proportion 0.62 0.89 1.00
## 
## Mean item complexity =  1.9
## Test of the hypothesis that 3 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0 
##  with the empirical chi square  0  with prob &amp;lt;  NA 
## 
## Fit based upon off diagonal values = 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Among the columns, there are first the correlations between variables and components, followed by a column (h2) with the &lt;strong&gt;‘communalities’&lt;/strong&gt;. If less factors than variables had been selected, communality values would be below 1. Then there is the uniqueness column (u2): &lt;strong&gt;uniqueness&lt;/strong&gt; is equal to 1 minus the communality. Next is ‘com’, which reflects the &lt;strong&gt;complexity&lt;/strong&gt; with which a variable relates to the principal components. Those components are precisely found below. The first row contains the sums of squared loadings, or eigenvalues, namely, the total variance explained by each linear component. This value corresponds to the number of units explained out of all possible factors (which were three in the above example). The rows below all cut from the same cloth. &lt;em&gt;Proportion var&lt;/em&gt; = variance explained over a total of 1. This is the result of dividing the eigenvalue by the number of components. Multiply by 100 and you get the percentage of total variance explained, which becomes useful. In the example, 99% of the variance has been explained. Aside from the meddling maths, we should actually expect 100% there because the number of factors equaled the number of variables. &lt;em&gt;Cumulative var:&lt;/em&gt; variance added consecutively up to the last component. &lt;em&gt;Proportion explained:&lt;/em&gt; variance explained over what has actually been explained (only when variables = factors is this the same as Proportion var). &lt;em&gt;Cumulative proportion:&lt;/em&gt; the actually explained variance added consecutively up to the last component (Field et al., 2012).&lt;/p&gt;
&lt;p&gt;According to Field et al. (2012), two criteria will determine the number of components to select for the next stage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kaiser’s criterion: components with SS loadings &amp;gt; 1. In our example, only PC1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A more lenient alternative is Joliffe’s criterion, SS loadings &amp;gt; .7.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scree plot: the number of points after point of inflexion. For this plot, call:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;plot(pc1$values, type = &amp;#39;b&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;35%&#34; src=&#34;2.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Imagine a straight line &lt;strong&gt;from the first point on the right.&lt;/strong&gt; Once this line bends considerably, count the points after the bend and up to the last point on the left. The number of points is the number of components to select. The example here is probably the most complicated (two components were finally chosen), but normally it’s &lt;a href=&#34;https://www.google.nl/search?q=select+principal+components+scree+plot+point+inflexion&amp;amp;source=lnms&amp;amp;tbm=isch&amp;amp;sa=X&amp;amp;ved=0ahUKEwi00ujoto_WAhXJbVAKHbTCBAgQ_AUICigB&amp;amp;biw=1280&amp;amp;bih=619&#34;&gt;not difficult&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Based on both criteria, go ahead and select the definitive number of components.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-3.-run-definitive-pca&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;STAGE 3. Run definitive PCA&lt;/h4&gt;
&lt;p&gt;Run a very similar command as you did before, but now with a more advanced method. The first PCA, a heuristic one, worked essentially on the inter-correlations. The definitive PCA, in contrast, will implement a prior shuffling known as ‘rotation’, to ensure that the result is robust enough (just like cards are shuffled). Explained variance is captured better this way. The go-to rotation method is the orthogonal, or ‘varimax’ (though others may be considered too).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Now with varimax rotation, Kaiser-normalized by default:
pc2 = psych::principal(dataset, nfactors=2, rotate = &amp;quot;varimax&amp;quot;, 
scores = TRUE)
pc2
pc2$loadings

# Healthcheck
pc2$residual
pc2$fit
pc2$communality
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to Field et al. (2012), we would want:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Less than half of &lt;strong&gt;residuals&lt;/strong&gt; with absolute values &amp;gt; 0.05&lt;/li&gt;
&lt;li&gt;Model &lt;strong&gt;fit&lt;/strong&gt; &amp;gt; .9&lt;/li&gt;
&lt;li&gt;All &lt;strong&gt;communalities&lt;/strong&gt; &amp;gt; .7&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If any of this fails, consider changing the number of factors. Next, the rotated components that have been ‘extracted’ from the core of the set of variables can be added to the dataset. This would enable the use of these components as new variables that might prove powerful and useful (as in &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.1551-6709.2010.01157.x/full&#34;&gt;this research&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dataset = cbind(dataset, pc2$scores)
summary(dataset$RC1, dataset$RC2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-4.-determine-ascription-of-each-variable-to-components&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;STAGE 4. Determine ascription of each variable to components&lt;/h4&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;55%&#34; src=&#34;3.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Check the main summary by just calling pc2, and see how each variable correlates with the rotated components. This is essential because it reveals how variables load on each component, or in other words, to which component a variable belongs. For instance, the table shown here belongs to a &lt;a href=&#34;https://www.linkedin.com/pulse/modality-exclusivity-norms-336-properties-411-dutch-english-bernabeu/?published=t&amp;amp;lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3BRIskxEq5Rgq59xHemwzpdw%3D%3D&#34;&gt;study about meaning of words&lt;/a&gt;. These results suggest that the visual and haptic modalities of words are quite related, whereas the auditory modality is relatively unique. When the analysis works out well, a cut-off point of &lt;em&gt;r&lt;/em&gt; = .8 may be applied for considering a variable as part of a component.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-5.-enjoy-the-plot&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;STAGE 5. Enjoy the plot&lt;/h4&gt;
&lt;p&gt;The plot is perhaps the coolest part about PCA. It really makes an awesome illustration of the power of data analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ggplot(eng_props,
  aes(RC1, RC2, label = as.character(main_eng))) + stat_density2d (color = &amp;quot;gray87&amp;quot;) +
  geom_text(size = ifelse(eng_props$word_eng %in% w_set, 12, 7),
    fontface = ifelse(eng_props$word_eng %in% w_set, &amp;#39;bold&amp;#39;, &amp;#39;plain&amp;#39;)) +
  geom_point(data=eng_props[eng_props$word_eng %in% w_set,], pch=21, fill=NA, size=14, stroke=2, alpha=.6) +
  labs(subtitle=&amp;#39;(Data from Lynott &amp;amp; Connell, 2009)&amp;#39;, x = &amp;quot;Varimax-rotated Principal Component 1&amp;quot;, 
    y = &amp;quot;Varimax-rotated Principal Component 2&amp;quot;) +  theme_bw() +   
  theme( plot.background = element_blank(), panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(), panel.border = element_blank(),
    axis.line = element_line(color = &amp;#39;black&amp;#39;),
    axis.title.x = element_text(colour = &amp;#39;black&amp;#39;, size = 23, margin=margin(15,15,15,15)),
    axis.title.y = element_text(colour = &amp;#39;black&amp;#39;, size = 23, margin=margin(15,15,15,15)),
    axis.text.x = element_text(size=16), axis.text.y  = element_text(size=16),
    plot.title = element_text(hjust = 0.5, size = 32, face = &amp;quot;bold&amp;quot;, margin=margin(15,15,15,15)),
    plot.subtitle = element_text(hjust = 0.5, size = 20, margin=margin(2,15,15,15)) ) +
  geom_label_repel(data = eng_props[eng_props$word_eng %in% w_set,], aes(label = word_eng), size = 8, 
    alpha = 0.77, color = &amp;#39;black&amp;#39;, box.padding = 1.5 )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is an example combining PCA plots with code similar to the above. These plots illustrate something further with regard to the relationships among modalities. In property words, the different modalities spread out more clearly than they do in concept words. This makes sense because in language, properties define concepts (&lt;a href=&#34;https://www.linkedin.com/pulse/modality-exclusivity-norms-336-properties-411-dutch-english-bernabeu?published=t&amp;amp;lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3BRIskxEq5Rgq59xHemwzpdw%3D%3D&#34;&gt;see more&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An example of these analyses is &lt;a href=&#34;https://mybinder.org/v2/gh/pablobernabeu/Modality-exclusivity-norms-747-Dutch-English-replication/master?urlpath=rstudio&#34;&gt;available in available in this RStudio environment&lt;/a&gt;, in the &lt;code&gt;norms.R&lt;/code&gt; script.&lt;/p&gt;
&lt;p&gt;References&lt;/p&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Field, A. P., Miles, J., &amp;amp; Field, Z. (2012). &lt;em&gt;Discovering Statistics Using R&lt;/em&gt;. London, UK: Sage.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>At Greg, 8 am</title>
      <link>/2017/at-greg-8-am/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>/2017/at-greg-8-am/</guid>
      <description>


&lt;p&gt;The clock strikes a certain hour, below all the Greg’s teaspoons at play. Results o’clock. The usual, please.&lt;/p&gt;
&lt;p&gt;Usual table. &lt;code&gt;summaryby&lt;/code&gt; (having to get the first peek in the cafeteria can only add zest). &lt;code&gt;summaryBy(RT ~ list(Ptp, Group, Cond), behdata, FUN=summary)&lt;/code&gt;. So, hardly any of the 95% Confidence Intervals contain 0. Does this really mean…?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‘For example, the hypothesis of equality of population means will be rejected at the 0.05 level if and only if a 95% CI for the mean difference does not contain 0.’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;— Dallal (2002; &lt;a href=&#34;http://www.jerrydallal.com/lhsp/pval.htm&#34; class=&#34;uri&#34;&gt;http://www.jerrydallal.com/lhsp/pval.htm&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Of course. The CI just has that and more. The window is showing a chilly 1999 morning. Let’s see the summary again. Wee standard deviations. By card, please.&lt;/p&gt;
&lt;p&gt;Mmm, the air outside is worth gingering up…&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The trials!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The assumption of independence spoils another morning.&lt;/p&gt;
&lt;p&gt;This new data consisted of response times (RT) that had been collected over several trials. The single dependent variable, RT, was accompanied by other variables which could be analyzed as independent variables. These included &lt;em&gt;Group&lt;/em&gt;, &lt;em&gt;Trial Number&lt;/em&gt;, and a within-subjects &lt;em&gt;Condition&lt;/em&gt;. &lt;strong&gt;What had to be done first off, in order to take the usual table?&lt;/strong&gt; &lt;em&gt;The trials!&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;assumption-of-independence-of-observations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assumption of independence of observations&lt;/h2&gt;
&lt;p&gt;One must account for any redundant measures below the level of participants (the experimental trials, in this case), so that the sample size (&lt;em&gt;N&lt;/em&gt;) used for any summary statistics match the number of participants (or the largest group, &lt;em&gt;n&lt;/em&gt;). Why? This is a &lt;a href=&#34;https://stats.stackexchange.com/questions/130019/standard-error-for-aggregated-proportions&#34;&gt;central assumption in statistics&lt;/a&gt;: observations must be independent. We can observe the independence assumption differently, depending on whether we’re summarizing data or performing statistical tests.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For &lt;em&gt;descriptive tables and plots&lt;/em&gt; (involving Standard Error/Deviation, Confidence Intervals, etc), &lt;em&gt;the data ought to be aggregated to the level from which you want to generalize&lt;/em&gt;. That level is—in this case and very often—&lt;em&gt;participants&lt;/em&gt;. Trials do not normally serve for statistical generalization (they’re good for experimental validity). This realization may come as a bummer if you have first seen the effect sizes in the un-aggregated data. The mirage (see red lines on the left table below) is caused by an inflated &lt;em&gt;N&lt;/em&gt; (cf. red lines on the right-hand table). As an illustration, the tables below summarize data with an actual sample &lt;em&gt;n&lt;/em&gt; = 23. However, the table on the right includes repeated measures that should have been aggregated, massively inflating &lt;em&gt;n&lt;/em&gt;. The inflation of the sample size equals the product of all repeated measures that failed to be aggregated under participants.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;inflated.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#39;30%&#39; src=&#39;SD.jpg&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Measures of variance such as the Standard Deviation divide by the sample size. Thus, the larger the sample (N), the smaller the Standard Deviation, Standard Error, Confidence Interval…—that is, the variation or noise.&lt;/p&gt;
&lt;p&gt;Aggregating is a snap. For example, with the aggregate() function in R, you just have to include all of your variables except that or those of the repeated measures:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;behdata_aggreg = aggregate(behdata$RT, list(behdata$Ptp, behdata$Group, behdata$Cond), 
  data=behdata, FUN=mean)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;In statistical tests, repeated measures below the participant level–e.g., trials–normally must be either factored in or aggregated. Barr and colleagues provide an easy, focused &lt;a href=&#34;http://talklab.psy.gla.ac.uk/simgen/faq.html#sec-3&#34;&gt;guide on this procedure&lt;/a&gt;. This is necessary because when the N in the analyses is augmented by unaccounted, redundant observations, &lt;em&gt;the famous assumption of independence of observations is violated&lt;/em&gt;, and the results may be invalid, as &lt;a href=&#34;https://arxiv.org/pdf/1601.01126.pdf&#34;&gt;Vasishth and Nicenboim (2016, p. 3)&lt;/a&gt; put it:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;‘if we were to do a t-test on the unaggregated data, we would violate the independence assumption and the result of the t-test would be invalid.’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now, usually the repetitions that concern us are the multiple trials or items in experiments, or other sub-participant measures. So what about participants–what are they never aggregated? &lt;a href=&#34;http://tandfonline.com.sci-hub.cc/doi/abs/10.1080/01933922.2016.1264520?journalCode=usgw20&#34;&gt;McCarthy, Whittaker, Boyle, and Eyal (2017, p.10)&lt;/a&gt; note:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‘It has also been proposed that researchers aggregate the responses of participants within the same group and use the groups/clusters as the unit of analysis (Stevens, 2007). However, because this would result in losing sample size at the participant level, this approach is not optimal given the already small numbers of groups typically studied in group work research.’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;different-procedure-in-linear-mixed-effects-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Different procedure in linear mixed-effects models&lt;/h2&gt;
&lt;p&gt;Aggregation is no longer necessary, where linear mixed-effects models can be used. These models allow us to &lt;a href=&#34;http://talklab.psy.gla.ac.uk/simgen/faq.html#sec-3&#34;&gt;account for any clusters (Participants, Trials, Items…) by signing them into the error term&lt;/a&gt; (&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer_and_Curtin_LMEMs-2017-Psych_Methods.pdf&#34;&gt;Brauer &amp;amp; Curtin, 2017&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Modality switch effects emerge early and increase throughout conceptual processing: evidence from ERPs</title>
      <link>/applications-and-dashboards/bernabeu-etal-2017-modalityswitch/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>/applications-and-dashboards/bernabeu-etal-2017-modalityswitch/</guid>
      <description>&lt;a href=&#39;https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment/&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #4CAF50; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt; 
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;i class=&#34;fas fa-mouse-pointer&#34;&gt;&lt;/i&gt;&lt;/span&gt;&amp;nbsp; Dashboard &lt;/h3&gt;&lt;/button&gt;&lt;/a&gt; &amp;nbsp; 
&lt;a href=&#39;https://mybinder.org/v2/gh/pablobernabeu/Modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing/0a5542658914a6ed01cf8e96252c48bb5bcf8f18?urlpath=shiny/Shiny-app/&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #196F27; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;font-size:90%; margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt; 
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;i class=&#34;fas fa-mouse-pointer&#34;&gt;&lt;/i&gt;&lt;/span&gt;&amp;nbsp; Alternative in case of downtime &lt;/h3&gt;&lt;/button&gt;&lt;/a&gt; &amp;nbsp; 
&lt;br&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;Content&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The data is from a psychology experiment on the comprehension of words, in which electroencephalographic (EEG) responses were measured. The data are presented in plots spanning 800 milliseconds (the duration of word processing). The aim of this Shiny app is to facilitate the exploration of the data by researchers and the public. Users can delve into the different sections of the data. In a hierarchical order, these sections are groups of participants, individual participants, brain areas, and electrodes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shiny apps in science&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;By creating this app, I tried to reach beyond the scope of current &lt;em&gt;open science&lt;/em&gt;, which is often confined to files shared on data repositories. I believe that Shiny apps will become general practice in science within a few years (&lt;a href=&#34;http://www.research.lancs.ac.uk/portal/en/activities/presenting-data-interactively-online-using-r-shiny(c9ce06ac-987e-4141-9121-016f6ee6d16b).html&#34;&gt;see blog post or slides for more information&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Technical details&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I made use of tabs on the top of the dashboard in order to keep the side bar from having too many widgets. I adjusted the appearance of these tabs, and by means of &amp;lsquo;reactivity&amp;rsquo; conditions, also modified the inputs in the sidebar depending on the active tab.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mainPanel(

	tags$style(HTML(&#39;
	    .tabbable &amp;gt; .nav &amp;gt; li &amp;gt; a                  		{background-color:white; color:#3E454E}
	    .tabbable &amp;gt; .nav &amp;gt; li &amp;gt; a:hover            		{background-color:#002555; color:white}
	    .tabbable &amp;gt; .nav &amp;gt; li[class=active] &amp;gt; a 		{background-color:#ECF4FF; color:black}
	    .tabbable &amp;gt; .nav &amp;gt; li[class=active] &amp;gt; a:hover	{background-color:#E7F1FF; color:black}
	&#39;)),

	tabsetPanel(id=&#39;tabvals&#39;,

            tabPanel(value=1, h4(strong(&#39;Group &amp;amp; Electrode&#39;)), br(), plotOutput(&#39;plot_GroupAndElectrode&#39;),
			h5(a(strong(&#39;See plots with 95% Confidence Intervals&#39;), href=&#39;https://osf.io/2tpxn/&#39;,
			target=&#39;_blank&#39;), style=&#39;text-decoration: underline;&#39;), 
			downloadButton(&#39;downloadPlot.1&#39;, &#39;Download HD plot&#39;), br(), br(),
			# EEG montage
			img(src=&#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&#39;, height=500, width=1000)),

            tabPanel(value=2, h4(strong(&#39;Participant &amp;amp; Area&#39;)), br(), plotOutput(&#39;plot_ParticipantAndLocation&#39;),
			h5(a(strong(&#39;See plots with 95% Confidence Intervals&#39;), href=&#39;https://osf.io/86ch9/&#39;,
			target=&#39;_blank&#39;), style=&#39;text-decoration: underline;&#39;), 
			downloadButton(&#39;downloadPlot.2&#39;, &#39;Download HD plot&#39;), br(), br(),
			# EEG montage
			img(src=&#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&#39;, height=500, width=1000)),

            tabPanel(value=3, h4(strong(&#39;Participant &amp;amp; Electrode&#39;)), br(), plotOutput(&#39;plot_ParticipantAndElectrode&#39;),
			br(), downloadButton(&#39;downloadPlot.3&#39;, &#39;Download HD plot&#39;), br(), br(),
			# EEG montage
			img(src=&#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&#39;, height=500, width=1000)),

            tabPanel(value=4, h4(strong(&#39;OLD Group &amp;amp; Electrode&#39;)), br(), plotOutput(&#39;plot_OLDGroupAndElectrode&#39;),
			h5(a(strong(&#39;See plots with 95% Confidence Intervals&#39;), href=&#39;https://osf.io/dvs2z/&#39;,
			target=&#39;_blank&#39;), style=&#39;text-decoration: underline;&#39;), 
			downloadButton(&#39;downloadPlot.4&#39;, &#39;Download HD plot&#39;), br(), br(),
			# EEG montage
			img(src=&#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&#39;, height=500, width=1000))
	),
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data set was fairly large, considering the fact that it&#39;s hosted with the free plan. In order to lighten the processing, I split the data into various files, reducing the total size. Furthermore, I outsourced a particularly heavy set of plots (those with Confidence Intervals) to PDF files, to which I linked in the app.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;h5(a(strong(&#39;See plots with 95% Confidence Intervals&#39;), href=&#39;https://osf.io/dvs2z/&#39;,
			target=&#39;_blank&#39;), style=&#39;text-decoration: underline;&#39;),
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also used web links to the published paper and raw data, as well as to the &lt;em&gt;server&lt;/em&gt; and &lt;em&gt;ui&lt;/em&gt; scripts. These files, along with the data, are publicly available &lt;a href=&#34;https://osf.io/97unm/&#34;&gt;in this repository&lt;/a&gt;; they may be accessed within the &amp;ldquo;Files&amp;rdquo; section, by opening the folders &amp;ldquo;ERPs&amp;rdquo; -&amp;gt; &amp;ldquo;Analyses of ERPs averaged across trials&amp;rdquo; -&amp;gt; &amp;ldquo;Shiny app&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Another feature I added was the download button.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# From server.R script

spec_title = paste0(&#39;ERP waveforms for &#39;, input$var.Group, &#39; Group, Electrode &#39;, input$var.Electrodes.1, &#39; (negative values upward; time windows displayed)&#39;)

plot_GroupAndElectrode = ggplot(df2, aes(x=time, y=-microvolts, color=condition)) +
  geom_rect(xmin=160, xmax=216, ymin=7.5, ymax=-8, color = &#39;grey75&#39;, fill=&#39;black&#39;, alpha=0, linetype=&#39;longdash&#39;) +
  geom_rect(xmin=270, xmax=370, ymin=7.5, ymax=-8, color = &#39;grey75&#39;, fill=&#39;black&#39;, alpha=0, linetype=&#39;longdash&#39;) +
  geom_rect(xmin=350, xmax=550, ymin=8, ymax=-7.5, color = &#39;grey75&#39;, fill=&#39;black&#39;, alpha=0, linetype=&#39;longdash&#39;) +
  geom_rect(xmin=500, xmax=750, ymin=7.5, ymax=-8, color = &#39;grey75&#39;, fill=&#39;black&#39;, alpha=0, linetype=&#39;longdash&#39;) +
  geom_line(size=1, alpha = 1) + scale_linetype_manual(values=colours) +
  scale_y_continuous(limits=c(-8.38, 8.3), breaks=seq(-8,8,by=1), expand = c(0,0.1)) +
  scale_x_continuous(limits=c(-208,808),breaks=seq(-200,800,by=100), expand = c(0.005,0), labels= c(&#39;-200&#39;,&#39;-100 ms&#39;,&#39;0&#39;,&#39;100 ms&#39;,&#39;200&#39;,&#39;300 ms&#39;,&#39;400&#39;,&#39;500 ms&#39;,&#39;600&#39;,&#39;700 ms&#39;,&#39;800&#39;)) +
  ggtitle(spec_title) + theme_bw() + geom_vline(xintercept=0) +
  annotate(geom=&#39;segment&#39;, y=seq(-8,8,1), yend=seq(-8,8,1), x=-4, xend=8, color=&#39;black&#39;) +
  annotate(geom=&#39;segment&#39;, y=-8.2, yend=-8.38, x=seq(-200,800,100), xend=seq(-200,800,100), color=&#39;black&#39;) +
  geom_segment(x = -200, y = 0, xend = 800, yend = 0, size=0.5, color=&#39;black&#39;) +
  theme(legend.position = c(0.100, 0.150), legend.background = element_rect(fill=&#39;#EEEEEE&#39;, size=0),
	axis.title=element_blank(), legend.key.width = unit(1.2,&#39;cm&#39;), legend.text=element_text(size=17),
	legend.title = element_text(size=17, face=&#39;bold&#39;), plot.title= element_text(size=20, hjust = 0.5, vjust=2),
	axis.text.y = element_blank(), axis.text.x = element_text(size = 14, vjust= 2.12, face=&#39;bold&#39;, color = &#39;grey32&#39;, family=&#39;sans&#39;),
	axis.ticks=element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), 
	panel.grid.minor = element_blank(), plot.margin = unit(c(0.1,0.1,0,0), &#39;cm&#39;)) +
  annotate(&#39;segment&#39;, x=160, xend=216, y=-8, yend=-8, colour = &#39;grey75&#39;, size = 1.5) +
  annotate(&#39;segment&#39;, x=270, xend=370, y=-8, yend=-8, colour = &#39;grey75&#39;, size = 1.5) +
  annotate(&#39;segment&#39;, x=350, xend=550, y=-7.5, yend=-7.5, colour = &#39;grey75&#39;, size = 1.5) +
  annotate(&#39;segment&#39;, x=500, xend=750, y=-8, yend=-8, colour = &#39;grey75&#39;, size = 1.5) +
  scale_fill_manual(name = &#39;Context / Target trial&#39;, values=colours) +
  scale_color_manual(name = &#39;Context / Target trial&#39;, values=colours) +
  guides(linetype=guide_legend(override.aes = list(size=1.2))) +
   guides(color=guide_legend(override.aes = list(size=2.5))) +
# Print y axis labels within plot area:
  annotate(&#39;text&#39;, label = expression(bold(&#39;\u2013&#39; * &#39;3 &#39; * &#39;\u03bc&#39; * &#39;V&#39;)), x = -29, y = 3, size = 4.5, color = &#39;grey32&#39;, family=&#39;sans&#39;) +
  annotate(&#39;text&#39;, label = expression(bold(&#39;+3 &#39; * &#39;\u03bc&#39; * &#39;V&#39;)), x = -29, y = -3, size = 4.5, color = &#39;grey32&#39;, family=&#39;sans&#39;) +
  annotate(&#39;text&#39;, label = expression(bold(&#39;\u2013&#39; * &#39;6 &#39; * &#39;\u03bc&#39; * &#39;V&#39;)), x = -29, y = 6, size = 4.5, color = &#39;grey32&#39;, family=&#39;sans&#39;)

print(plot_GroupAndElectrode)

output$downloadPlot.1 &amp;lt;- downloadHandler(
	filename &amp;lt;- function(file){
	paste0(input$var.Group, &#39; group, electrode &#39;, input$var.Electrodes.1, &#39;, &#39;, Sys.Date(), &#39;.png&#39;)},
   	content &amp;lt;- function(file){
      		png(file, units=&#39;in&#39;, width=13, height=5, res=900)
      		print(plot_GroupAndElectrode)
      		dev.off()},
	contentType = &#39;image/png&#39;)
  } )
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;# From ui.R script

downloadButton(&#39;downloadPlot.1&#39;, &#39;Download HD plot&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Rising to the challenge&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My experience with Shiny has been so good I&#39;ve been sharing &lt;a href=&#34;https://pablobernabeu.github.io/2017/01/01/the-case-for-data-dashboards.-first-steps-with-r-shiny/&#34;&gt;my experience&lt;/a&gt;. Yet, on my first crawling days, I spent an eternity stuck with this elephant in my room: &amp;ldquo;μ&amp;rdquo;. This &lt;em&gt;μ&lt;/em&gt; letter (micro-souvenir from hell, as I later knew it), was part of the labels of my plots. All I knew was that I could not deploy the app online, even while I could perfectly launch it locally in my laptop. So, I wondered what use was to deploy locally if I couldn&#39;t publish the app?! Eventually, I read about UTF-8 encoding in one forum. Bless them forums. All I had to do was use &amp;ldquo;Âμ&amp;rdquo; instead of the single &amp;ldquo;μ&amp;rdquo;. A better option I found later was: &lt;code&gt;expression(&amp;quot;\u03bc&amp;quot;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Beyond encoding issues, I had a tough time embedding images. You know, the &amp;lsquo;www&amp;rsquo; folder&amp;hellip; To be honest, I still haven&#39;t handled the &amp;lsquo;www&amp;rsquo; way&amp;ndash;but where there&#39;s a will there&#39;s a way. I managed to include my images by uploading them to a website and then entering their URL in &amp;ldquo;img(src&amp;rdquo;, avoiding the use of folder paths.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;img(src=&amp;quot;https://preview.ibb.co/n7qiYR/EEG_montage.png 1&amp;quot;, height=500, width=1000)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Long after I had built the app, I added another image&amp;ndash;the &lt;em&gt;favicon&lt;/em&gt; (the little icon on the browser tab).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tags$head(tags$link(rel=&amp;quot;shortcut icon&amp;quot;, href=&amp;quot;https://image.ibb.co/fXUwzb/favic.png&amp;quot;)),  # web favicon
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;div style = &#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Bernabeu, P., Willems, R. M., &amp;amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs [Data dashboard]. Retrieved from &lt;a href=&#34;https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment/&#34;&gt;https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Louisiana-Minnesota-Texas crisis across media and time: a big data exercise</title>
      <link>/2016/the-louisiana-minnesota-texas-crisis-across-media-and-time-a-big-data-exercise/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>/2016/the-louisiana-minnesota-texas-crisis-across-media-and-time-a-big-data-exercise/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;Racism has long been ingrained in human societies. Ancient Greek Aristotle already claimed that non-Greeks were slaves by nature, as they easily submitted to despotic government (Reilly, Kaufman, &amp;amp; Bodino, 2002). This study focuses on racism in the United States, which extends from the foundation of the country, when black people were generally born into slavery, and were at any rate regarded as an inferior people. US racism stands out globally for two reasons. First, the country has played a hegemonic part in the World since soon after its foundation. Second, the US is regarded as the most advanced society technology-wise, as it sets the minutes for the technology sector worldwide. In spite of these advantages, the country has long suffered the plague of widespread racism. Indeed, the abolition of slavery in the mid-nineteenth century did not grant equal citizen rights to the black population. Over time, the black population started to confront this situation. Especially the mid-nineteenth century saw large uprisings and a patent division of different societal sectors, as reflected in literary works such as Ellison’s ‘Invisible Man’ (1952). Inequality and confrontation about racism has extended to date, and the costs thereof have been large in terms of lives and otherwise (Feagin, 2004).&lt;/p&gt;
&lt;p&gt;With the era of global communication, what happens in the World’s most powerful country is quickly and largely spread overseas—so too with racism matters. The last major such event related to racism happened during the first weeks of July 2016. Within five days, two cases of dubious, lethal police intervention with black citizens were followed by the killing of five policemen by a black youth. The specific course of events was as follows. On July 5th, Alton Sterling was killed by police officers in Louisiana. Next day, Philando Castile was also killed by police. In this case, the presence of Castile’s girlfriend during the tragedy likely determined the following events, because she described the event to the media, underscoring how gratuitous the killing was. During the following hours, outrage escalated within the already-wary population of the US. Yet the crisis would not stop there. During one of the various demonstrations held across the country, a dozen policemen were shot by a sniper, leaving five of them dead. The attacker was a black youth linked to black militant groups which target the Establishment on the grounds of patent racial discrimination. We will refer to this concatenation of events as the Louisiana-Minnesota-Texas (LMT) crisis.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the welter of events in Louisiana, Minnesota and Texas, some journalists warned of the return of social divisions such as those from the mid-nineteenth century. Such divisions might lead some people to an incomplete perception of the situation, and thus hinder the achievement of any solutions. However, President Obama denied such divisions as he spoke at the funeral for the policemen killed. At the same time, he addressed each of the different groups in the problem, including Establishment institutions and black protesters, advising them all to exercise greater open-mindedness towards the other aspects and bands in the problem (&lt;a href=&#34;https://obamawhitehouse.archives.gov/the-press-office/2016/07/12/remarks-president-memorial-service-fallen-dallas-police-officers&#34;&gt;see statement&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;It must be noted that this small study is primarily a way for us to practise data analysis at a &lt;a href=&#34;https://www.vu.nl/en/programmes/short/summer-school/courses/big-data-in-society/index.aspx&#34;&gt;course&lt;/a&gt;. Neither the background nor the analyses make a realistic study of racism or the LMT crisis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;goals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Goals&lt;/h2&gt;
&lt;p&gt;We wanted to look at these developments from the scope of online data. For this purpose, we scraped online discussions on these developments from a variety of media sources, and within defined time frames in the crisis. We then probed for any noticeable fluctuations in the topics throughout the course of events, and also for any differences across the different media. In this analysis, we took an exploratory approach by means of topic modeling. We wanted to check, first, whether topic modeling would be sensitive and useful at all within such a compact time scale. Were it to allow us, we would analyze how the journalistic and the social media reflected any fluctuations based on the live developments in Louisiana, Minnesota and Texas. As such, the dependent variable (DV) in this study is the overall topic under discussion, which we measured via topic modeling. So, the language we analyze are messages related to the LMT crisis. Two factors are checked as potentially affecting the DV, namely Media and Time.&lt;/p&gt;
&lt;p&gt;The Media factor regarded the three different sources of information from which we retrieved LMC content. These sources were: (1) the New York Times (NYT), (2) public tweets related to the NYT, and (3) public comments on the NYT’s Facebook posts.&lt;/p&gt;
&lt;p&gt;The Time factor was based on the following periods. The first period, from 2 to 4 July, was selected as a baseline during which no remarkable events racism-wise happened. The second period, including 5 and 6 July, contains the days when the two black citizens were killed by policemen. The third period, from 7 to 11 July, contains the aftermath of the crisis overall.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hypotheses&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Hypotheses&lt;/h2&gt;
&lt;p&gt;We had several hypotheseses for our planned analyses. For the Media factor, we hypothesized a greater objectivity and formality overall for the NYT articles compared to the other two sources.&lt;/p&gt;
&lt;p&gt;We did not have any hypotheses about the Time factor, i.e., the nature of any potential topic changes. In fact, we had considerable reservations as to whether any fluctuations would present, given the fact that the latent feeling of such a crisis might stays negative, critical and fearful from the start, regardless of particular events.&lt;/p&gt;
&lt;p&gt;With respect to the interaction between the two factors, we hypothesized that the NYT articles would present the lowest degree of thematic variation, due to the fact that such journal pieces require time to investigate and write up—even if they are published online. Comparatively, popular comments on Twitter and Facebook would present more emotionality and subjectivity, and likely they would also present greater influence of immediate events. Furthermore, Twitter should be yet more immediate than Facebook.&lt;/p&gt;
&lt;p&gt;Last, with respect to the DV, we did not actually have any hypotheses about the nature of possible topic fluctuations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Methods&lt;/h2&gt;
&lt;p&gt;Online reactions to the LMT developments were scraped from various online sources. This content was constrained to language, bearing no extensions such as pictures or videos. In order to narrow the scope of the information, all scraped sources were related to The New York Times journal. The sources were, first, the NYT online edition (nytimes.com); second, public tweets related to NYT (&lt;span class=&#34;citation&#34;&gt;@nytimes&lt;/span&gt;); and, third, public comments posted on the NYT page (&lt;span class=&#34;citation&#34;&gt;@nytimes&lt;/span&gt;). Crucially, these sources are different in nature. Whereas the articles in the journal’s online edition broadly follow the standard article form of mainstream journals, Facebook comments on the page are aligned with the Facebook standards, that is, comparatively informal and outspoken. In accord, tweets referring to &lt;span class=&#34;citation&#34;&gt;@nytimes&lt;/span&gt; follow the Twitter conventions, characterized by the 140-character restriction, and the relative immediacy of their information (Oh &amp;amp; Syn, 2015; Wang, He &amp;amp; Zhao, 2014; Josephson &amp;amp; Miller, 2015).&lt;/p&gt;
&lt;p&gt;The method to scrape content related to the LMT crisis was through keywords. For the three media, the following keywords were entered, such that articles containing &lt;em&gt;any&lt;/em&gt; of those words would be returned: ‘black’ OR ‘racism’ OR ‘police’ OR ‘dallas’ OR ‘alton’ OR ‘sterling’ OR ‘philando’ OR ‘castile.’ Further particulars are provided in turn.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;New York Times&lt;/em&gt;. This scraping pipeline started from the official API site for NYT (&lt;a href=&#34;https://developer.nytimes.com&#34; class=&#34;uri&#34;&gt;https://developer.nytimes.com&lt;/a&gt;). Metadata was downloaded for 2,000 articles adjusting to the abovementioned keywords. This returned articles dating back to the start of the year. After preprocessing, 29 articles were returned for period 1; 53 for period 2; and 275 for period 3.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Twitter&lt;/em&gt;. This scraping was performed through Geoff Jentry’s R package ‘twitteR’ (&lt;a href=&#34;https://github.com/geoffjentry/twitteR&#34; class=&#34;uri&#34;&gt;https://github.com/geoffjentry/twitteR&lt;/a&gt;). Here tweets were selected based on the same keywords, in addition to ‘&lt;span class=&#34;citation&#34;&gt;@nytimes&lt;/span&gt;’. Due to the ten-day maximum range of Twitter’s API, no time range was entered. Retweets were removed. With the naked eye we realized that the tweets contained considerable information on Saudi events, we entered the word ‘saudi’ as a negative keyword. After preprocessing, 157 articles were returned for period 1; 489 for period 2; and 5025 for period 3.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Facebook&lt;/em&gt;. The ‘RFacebook’ R package, by Pablo Barbera (&lt;a href=&#34;https://github.com/pablobarbera/Rfacebook&#34; class=&#34;uri&#34;&gt;https://github.com/pablobarbera/Rfacebook&lt;/a&gt;), was used for this scraping. The Facebook API currently allows for the download of all or any posts from one page, with no time restrictions (broader-search functions seem to have been deprecated). We downloaded any comments on the pages’ posts which adjusted to our keywords. After preprocessing, 195 articles were returned for period 1; 1107 for period 2; and 8724 for period 3.&lt;/p&gt;
&lt;p&gt;Preprocessing was performed equally for all sources—as standard in topic modeling, by removing non-relevant (‘stop-words’) and non-linguistic elements. Removed items included the names of the media, as well as numbers, punctuation, links, and technical signs such as @.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5. Results&lt;/h2&gt;
&lt;p&gt;Our hypothesis about the Media factor was only partially confirmed. First, sentiment analysis showed that NYT posts were very tempered, with an average sentiment score near 0. In contrast, Twitter presented a rather negative sentiment (Thelwall, Buckley, &amp;amp; Paltoglou, 2011; Saif, He, Fernandez, &amp;amp; Alani, 2016). Yet, to our surprise, Facebook came out with a neutrality close to that of NYT articles, even if there was greater variance among the scores of the Facebook posts. These overall tendencies are illustrated in the plot below. Caution must recommended, however, when considering this sentiment analysis, as this technique is arguably fuzzy generally, and especially so with data under such a tight time frame. This is the case because sentiment analysis, as other big data techniques, capitalizes on the size of data. What it lacks on the precision aspect, compared to null-significance hypothesis testing, for instance, it compensates with the size of the samples, in which the noise is suppressed by thousands of cases. In this case, however, the sampling within only nine days of unusual circumstances calls for circumspection.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/RRisto/Summer-school-course-project/raw/master/BLM/SentientScoreDatasetBoxplot.png&#34; alt=&#34;↑ Sentiment comparison across sources&#34; /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Next, topic modeling was conducted on each source separately. The parameters for topic discovery were entered based on several attempts with different numbers of topics (K) and internal-coherence thresholds (alpha). Finally, topics were selected alike for every source, K = 3, alpha = .2. Below, the first ten words for each topic in each source are shown (note that columns are aligned rightward).&lt;/p&gt;
&lt;div id=&#34;new-york-times-articles&#34; class=&#34;section level6&#34;&gt;
&lt;h6&gt;&lt;em&gt;New York Times articles&lt;/em&gt;&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;##    Foreign policy Shooting Elections
## 1            said   police       new
## 2         percent     said       one
## 3            vote officers    people
## 4            year    black       can
## 5          brexit   dallas      like
## 6            will  officer      york
## 7           since shooting     trump
## 8        european      two      july
## 9         british     shot      even
## 10        britain     says      just&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;facebook-comments&#34; class=&#34;section level6&#34;&gt;
&lt;h6&gt;&lt;em&gt;Facebook comments&lt;/em&gt;&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;##      Police People Black lives matter
## 1    police people              black
## 2       gun   will              lives
## 3      cops    can             matter
## 4    people   like              white
## 5  officers    get             people
## 6      dont    one             racist
## 7      just   just                blm
## 8       man   need             police
## 9       get   dont             blacks
## 10  officer police              obama&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tweets&#34; class=&#34;section level6&#34;&gt;
&lt;h6&gt;&lt;em&gt;Tweets&lt;/em&gt;&lt;/h6&gt;
&lt;pre&gt;&lt;code&gt;##    Racism Dallas shooting Philando shooting
## 1   black          police            police
## 2  police          dallas            blacks
## 3   white        officers               new
## 4   lives          killed          shooting
## 5  people         protest          philando
## 6  racism        shooting             force
## 7     amp       shootings           castile
## 8  matter           alton               use
## 9    stop        sterling              says
## 10   cops           baton          violence&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To start, it may stand out that different topics appear across sources, all the while some are indeed shared. This is perfectly normal for topic modeling on different sources, even when the same topic is being studied. Indeed, it is very relevant for us to remark on the inclusion of foreign affairs and election matters within the NYT articles, but not within people’s tweets and Facebook comments. This makes sense for several reasons. To start, the space a journalist counts on in a NYT article is considerable, compared to tweets, and also compared to ruling conventions of Facebook posts (users may write further, but the average simply will not). Second, the breadth of relation in NYT articles likely responds to the expectations from renown journalists to enrich the news with a broader contextualization. Furthermore, this extension of topics might correspond to the tacit but doubtless alignment of journals to concrete political agendas. While people commenting on Twitter or Facebook are plausibly characterized by just the same virtues and vices, their online reactions could be driven by more emotion and immediacy of focus than those of mass media journalists.&lt;/p&gt;
&lt;p&gt;For greater visualization, we also provide some captions from the interactive LDAvis tool below. Please click on the figure titles to enjoy the full visualization.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://github.com/RRisto/Summer-school-course-project/raw/master/BLM/Untitled3.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;br&gt; &lt;a href=&#34;http://rristo.github.io/NYT/index.html&#34;&gt;↑ LDAvis visualization of NYT articles (click to explore in detail)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://github.com/RRisto/Summer-school-course-project/raw/master/BLM/Untitled1.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;br&gt; &lt;a href=&#34;http://rristo.github.io/Facebook/index.html#topic=0&amp;amp;lambda=1&amp;amp;term=&#34;&gt;↑ LDAvis visualization of Facebook comments (click to explore in detail)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://github.com/RRisto/Summer-school-course-project/raw/master/BLM/Untitled.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;br&gt; &lt;a href=&#34;http://rristo.github.io/Twitter/index.html#topic=0&amp;amp;lambda=1&amp;amp;term=&#34;&gt;↑ LDAvis visualization of tweets (click to explore in detail)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In order to specifically compare different content sources, we plotted the major language from two sources on the same plot, with an axis spanning from one source to the other, as shown below. The size of the words indicates the frequency of use, and the colour is essentially parallel with the axis, with specific different colours for different corpora, and darker hues for greater association.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://github.com/RRisto/Summer-school-course-project/raw/master/BLM/CorporaCompareFbvsNYT.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;↑ Facebook comments and NYT articles&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://github.com/RRisto/Summer-school-course-project/raw/master/BLM/CorporaCompareFbvsTw.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;↑ Facebook comments and tweets&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://github.com/RRisto/Summer-school-course-project/raw/master/BLM/CorporaCompareTwvsNYT.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;↑ Tweets and NYT articles&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We went on to analyze the overlap in topics across journals, in order to quantitatively check whether some topics were indeed shared across sources, even if in different positions (for instance, topic 1 in some source and topic 3 in some other). We did this by means of cosine similarity scores. This scores represent the degree of similarity of two sources on a continuous scale from 0 to 1, where 1 would mean identical. The plots illustrate this comparisons in turn.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/RRisto/Summer-school-course-project/raw/master/BLM/CosineSimFbVsNYT.png&#34; alt=&#34;Similarity between Facebook comments and NYT articles&#34; /&gt;
&lt;br&gt;
↑ Similarity between Facebook comments and NYT articles
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/RRisto/Summer-school-course-project/raw/master/BLM/CosineSimFbVsTw.png&#34; alt=&#34;Similarity between Facebook comments and tweets&#34; /&gt;
&lt;br&gt;
↑ Similarity between Facebook comments and tweets
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/RRisto/Summer-school-course-project/raw/master/BLM/CosineSimTwvsNYT.png&#34; alt=&#34;Similarity between tweets and NYT articles&#34; /&gt;
&lt;br&gt;
↑ Similarity between tweets and NYT articles
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Last, the interaction of Time and Media was analyzed. As expected, we found differences in the way topics fluctuated over time in the different sources, albeit in unexpected ways. NYT and tweets articles presented great variation, suggesting day-bound sensitivity to the developments. This was to be expected from Twitter, as it is famous for its immediacy. However, the immediacy of NYT articles was rather surprising, as they might have lagged behind due to the necessary investigation and editing for such kind of journalistic pieces. Unlike traditional paper-based NYT articles, this immediacy is now enabled by the publication online. Another unexpected finding was the relative stillness of Facebook posts over time. Since they are published at the minute, and nowadays mostly from mobile, we had thought they would present greater immediacy than NYT articles. We could hypothesize on this, but this would be best analyzed in further research. The plot below illustrates this interaction.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://github.com/RRisto/Summer-school-course-project/raw/master/BLM/TopicTime.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;↑ Topic fluctuations over time for the three content sources&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;6. Discussion&lt;/h2&gt;
&lt;p&gt;In this small-scale study, we analyzed the impact of a racism-related crisis in the American society online. This crisis started with the killing of two black citizens by policemen under dubious circumstances, which was followed by massive media attention and street demonstrations, and then continued to the fatal shooting of five policemen by a black militant (the crisis continued yet further after our analyses). The impact of this crisis was large, with a state funeral being organized for the killed policemen, and a presidential address warning of the direction of social tensions, and the need for greater empathy from all social sections involved.&lt;/p&gt;
&lt;p&gt;We scraped the divided the social reaction to these events from three online sources, namely, the NYT online edition, public comments on the NYT Facebook page, and finally NYT-related tweets. The method was based on keywords highly relevant to this crisis, namely: ‘black’ OR ‘racism’ OR ‘police’ OR ‘dallas’ OR ‘alton’ OR ‘sterling’ OR ‘philando’ OR ‘castile’. We analyzed the Media factor and the Time factor separately, and more interestingly we looked at the interaction between these two factors.&lt;/p&gt;
&lt;p&gt;As results, we found, first, that NYT articles were the most neutral, closely followed by Facebook posts. In contrast, tweets presented greater negativity overall. Next, we looked at topics within each time frame in each of the three sources. These topics differed across sources, even though there were also considerable overlaps. For instance, NYT articles and related tweets shared the content of their second topics, both of which revolved around ‘shooting.’ We went further to quantitatively measure any such overlaps or otherwise differences across sources. Cosine similarity—which ranges from 1, totally related, to 0, not related at all—confirmed our naked eye feeling. For instance, for the overlap between the aforementioned topics, there was a cosine similarity of .71. In contrast, a cosine of .03 came up for other comparisons, which also makes sense due to the intrinsic differences among these sources.&lt;/p&gt;
&lt;p&gt;Last, the interaction between Time and Source was qualitatively analyzed by means of a plot, and we found that Twitter and NYT articles were most sensitive to live developments in the crisis, whereas Facebook comments lagged behing in this immediacy. All of these findings were discussed within a framework of qualitative big data analysis.&lt;/p&gt;
&lt;p&gt;The data mass probed in these analyses could be described as medium-sized data in the big data field. This field is relatively recent, and the sucessful, seminal examples we count on tend to feature larger sizes of data. In particular, for time frames, it is rather uncommon to find such a tight scale as we excerpted. This fact complicates the drawing of assured conclusions from our findings, because we lack well-known precedents along these lines. While the social sciences have developed their tools for small samples, the tools of big data are currently designed for the larger amounts of data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;additional-materials&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Additional materials&lt;/h2&gt;
&lt;p&gt;All materials are made public on the &lt;a href=&#34;https://github.com/RRisto/Summer-school-course-project&#34;&gt;RRisto GitHub page&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div style=&#34;text-indent:-2em; margin-left:2em;&#34;&gt;
&lt;p&gt;Ellison, R. (1952). &lt;em&gt;Invisible Man&lt;/em&gt;. New York: Random House.&lt;/p&gt;
&lt;p&gt;Feagin, J. R. (2004). Documenting the Costs of Slavery, Segregation, and Contemporary Racism: Why Reparations Are in Order for African &lt;em&gt;Americans Harvard BlackLetter Law Journal, 20&lt;/em&gt;, 49-81.&lt;/p&gt;
&lt;p&gt;Josephson, S., &amp;amp; Miller, J. S. (2015). Just State the Facts on Twitter: Eye Tracking Shows That Readers May Ignore Questions Posted by News Organizations On Twitter But Not on Facebook. &lt;em&gt;Visual Communication Quarterly, 22&lt;/em&gt;(2), 94-105.&lt;/p&gt;
&lt;p&gt;Oh, S., &amp;amp; Syn, S. Y. (2015). Motivations for sharing information and social support in social media: A comparative analysis of Facebook, Twitter, Delicious, YouTube, and Flickr. &lt;em&gt;Journal Of The Association For Information Science And Technology, 66&lt;/em&gt;(10), 2045-2060.&lt;/p&gt;
&lt;p&gt;Reilly, K., Kaufman, S., &amp;amp; Bodino, A. (2003). &lt;em&gt;Racism: A Global Reader&lt;/em&gt;. London: M. E. Sharpe&lt;/p&gt;
&lt;p&gt;Saif, H., He, Y., Fernandez, M., &amp;amp; Alani, H. (2016). Contextual semantics for sentiment analysis of Twitter. &lt;em&gt;Information Processing And Management, 52&lt;/em&gt;(1), 5-19.&lt;/p&gt;
&lt;p&gt;Thelwall, M., Buckley, K., &amp;amp; Paltoglou, G. (2011). Sentiment in Twitter events. &lt;em&gt;Journal Of The American Society For Information Science And Technology, 62&lt;/em&gt;(2), 406-418.&lt;/p&gt;
&lt;p&gt;Wang, P., He, W., &amp;amp; Zhao, J. (2014). A Tale of Three Social Networks: User Activity Comparisons across Facebook, Twitter, and Foursquare. &lt;em&gt;IEEE Internet Computing, 18&lt;/em&gt;(2), 10-15.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;A later update: On July 17, 2016—days after the current analysis—, the LMT crisis was extended with the killing of two policemen in the same Louisiana city where Alton Sterling had been killed.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
