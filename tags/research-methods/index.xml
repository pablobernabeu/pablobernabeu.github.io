<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>research methods | Pablo Bernabeu</title>
    <link>https://pablobernabeu.github.io/tags/research-methods/</link>
      <atom:link href="https://pablobernabeu.github.io/tags/research-methods/index.xml" rel="self" type="application/rss+xml" />
    <description>research methods</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-uk</language><copyright>Pablo Bernabeu, 2015—2024. Licence: [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/). Email: pcbernabeu@gmail.com. No cookies operated by this website. Cookies only used by third-party systems such as [Disqus](https://help.disqus.com/en/articles/1717155-use-of-cookies).</copyright><lastBuildDate>Sun, 30 Jun 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://pablobernabeu.github.io/img/default_preview_image.png</url>
      <title>research methods</title>
      <link>https://pablobernabeu.github.io/tags/research-methods/</link>
    </image>
    
    <item>
      <title>R functions for checking and fixing vmrk files from BrainVision</title>
      <link>https://pablobernabeu.github.io/2024/r-functions-for-checking-and-fixing-vmrk-files-from-brainvision/</link>
      <pubDate>Sun, 30 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2024/r-functions-for-checking-and-fixing-vmrk-files-from-brainvision/</guid>
      <description>


&lt;p&gt;Electroencephalography (EEG) has become a cornerstone for understanding the intricate workings of the human brain in the field of neuroscience. However, EEG software and hardware come with their own set of constraints, particularly in the management of markers, also known as triggers. This article aims to shed light on these limitations and future prospects of marker management in EEG studies, while also introducing R functions that can help deal with vmrk files from BrainVision.&lt;/p&gt;
&lt;p&gt;Markers, serving as timestamps that indicate specific events during data collection, play a crucial role in EEG studies. These events could range from the onset of a stimulus to the participant’s response. However, one of the major constraints in current EEG systems is the limitation of markers to numbers between 1 and 255. This limitation is due to the fact that markers are typically stored as 8-bit unsigned integers in computer memory, which can only represent numbers in the range of 0 to 255. However, the number 0 is usually reserved for system use, leaving only the numbers 1 to 255 available for markers.&lt;/p&gt;
&lt;p&gt;This numerical constraint can pose significant challenges in the interpretation of markers, especially in complex experimental designs where a multitude of events need to be marked and differentiated. It necessitates careful documentation of each marker’s purpose prior to running the study. This means that researchers must meticulously map each number to a specific event or condition in their experiment, which can be a daunting task, especially for complex studies with numerous conditions and events.&lt;/p&gt;
&lt;p&gt;Looking towards the future, one might wonder if it will become possible to send markers with semantic information, instead of being constrained to numbers between 1 and 255. This would allow researchers to encode more detailed information in each marker, such as the type of stimulus presented or the specific condition being tested. Such a development could revolutionize the way we conduct and analyze EEG studies, offering greater flexibility in experimental design and more nuanced insights into brain activity.&lt;/p&gt;
&lt;p&gt;Below, we’ll demonstrate some functions from &lt;a href=&#34;https://github.com/pablobernabeu/EEG-tools-and-tips&#34;&gt;this GitHub repository&lt;/a&gt; that help inspect and fix vmrk files. We’ll work with the vmrk file shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Read the file into a vector of lines
readLines(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/EEG-tools-and-tips/main/example.vmrk&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Brain Vision Data Exchange Marker File, Version 1.0&amp;quot;                               
##  [2] &amp;quot;&amp;quot;                                                                                  
##  [3] &amp;quot;[Common Infos]&amp;quot;                                                                    
##  [4] &amp;quot;DataFile=3.eeg&amp;quot;                                                                    
##  [5] &amp;quot;Codepage=utf-8&amp;quot;                                                                    
##  [6] &amp;quot;&amp;quot;                                                                                  
##  [7] &amp;quot;[Marker Infos]&amp;quot;                                                                    
##  [8] &amp;quot;; Each entry: Mk&amp;lt;Marker number&amp;gt;=&amp;lt;Type&amp;gt;,&amp;lt;Description&amp;gt;,&amp;lt;Position in data points&amp;gt;,&amp;quot;   
##  [9] &amp;quot;; &amp;lt;Size in data points&amp;gt;, &amp;lt;Channel number (0 = marker is related to all channels)&amp;gt;,&amp;quot;
## [10] &amp;quot;; &amp;lt;Date (YYYYMMDDhhmmssuuuuuu)&amp;gt;&amp;quot;                                                   
## [11] &amp;quot;; Fields are delimited by commas, some fields might be omited (empty).&amp;quot;            
## [12] &amp;quot;; Commas in type or description text are coded as \&amp;quot;\\1\&amp;quot;.&amp;quot;                        
## [13] &amp;quot;Mk1=New Segment,,1,1,0,20240318111955090000&amp;quot;                                       
## [14] &amp;quot;Mk2=Stimulus,S254,20419,1,0&amp;quot;                                                       
## [15] &amp;quot;Mk3=Stimulus,S  5,22332,1,0&amp;quot;                                                       
## [16] &amp;quot;Mk4=Stimulus,S 42,23095,1,0&amp;quot;                                                       
## [17] &amp;quot;Mk5=Stimulus,S143,23100,1,0&amp;quot;                                                       
## [18] &amp;quot;Mk6=Stimulus,S  2,23106,1,0&amp;quot;                                                       
## [19] &amp;quot;Mk7=Stimulus,S102,23111,1,0&amp;quot;                                                       
## [20] &amp;quot;Mk8=Stimulus,S  6,25882,1,0&amp;quot;                                                       
## [21] &amp;quot;Mk9=Stimulus,S  5,28106,1,0&amp;quot;                                                       
## [22] &amp;quot;Mk10=Stimulus,S 50,29053,1,0&amp;quot;                                                      
## [23] &amp;quot;Mk11=Stimulus,S241,29058,1,0&amp;quot;                                                      
## [24] &amp;quot;Mk12=Stimulus,S  1,29063,1,0&amp;quot;                                                      
## [25] &amp;quot;Mk13=Stimulus,S101,29069,1,0&amp;quot;                                                      
## [26] &amp;quot;Mk14=Stimulus,S  6,31830,1,0&amp;quot;                                                      
## [27] &amp;quot;Mk15=Stimulus,S  5,34056,1,0&amp;quot;                                                      
## [28] &amp;quot;Mk16=Stimulus,S 49,35055,1,0&amp;quot;                                                      
## [29] &amp;quot;Mk17=Stimulus,S226,35060,1,0&amp;quot;                                                      
## [30] &amp;quot;Mk18=Stimulus,S  2,35066,1,0&amp;quot;                                                      
## [31] &amp;quot;Mk19=Stimulus,S103,35071,1,0&amp;quot;                                                      
## [32] &amp;quot;Mk20=Stimulus,S  6,37242,1,0&amp;quot;                                                      
## [33] &amp;quot;Mk21=Stimulus,S  5,39436,1,0&amp;quot;                                                      
## [34] &amp;quot;Mk22=Stimulus,S 43,40417,1,0&amp;quot;                                                      
## [35] &amp;quot;Mk23=Stimulus,S155,40423,1,0&amp;quot;                                                      
## [36] &amp;quot;Mk24=Stimulus,S  2,40429,1,0&amp;quot;                                                      
## [37] &amp;quot;Mk25=Stimulus,S103,40434,1,0&amp;quot;                                                      
## [38] &amp;quot;Mk26=Stimulus,S  6,42481,1,0&amp;quot;                                                      
## [39] &amp;quot;Mk27=Stimulus,S  5,44662,1,0&amp;quot;                                                      
## [40] &amp;quot;Mk28=Stimulus,S 40,45678,1,0&amp;quot;                                                      
## [41] &amp;quot;Mk29=Stimulus,S118,45683,1,0&amp;quot;                                                      
## [42] &amp;quot;Mk30=Stimulus,S  1,45688,1,0&amp;quot;                                                      
## [43] &amp;quot;Mk31=Stimulus,S103,45693,1,0&amp;quot;                                                      
## [44] &amp;quot;Mk32=Stimulus,S  6,47621,1,0&amp;quot;                                                      
## [45] &amp;quot;Mk33=Stimulus,S  5,49809,1,0&amp;quot;                                                      
## [46] &amp;quot;Mk34=Stimulus,S 50,50808,1,0&amp;quot;                                                      
## [47] &amp;quot;Mk35=Stimulus,S237,50813,1,0&amp;quot;                                                      
## [48] &amp;quot;Mk36=Stimulus,S  3,50818,1,0&amp;quot;                                                      
## [49] &amp;quot;Mk37=Stimulus,S101,50823,1,0&amp;quot;                                                      
## [50] &amp;quot;Mk38=Stimulus,S  6,53823,1,0&amp;quot;                                                      
## [51] &amp;quot;Mk39=Stimulus,S  5,56042,1,0&amp;quot;                                                      
## [52] &amp;quot;Mk40=Stimulus,S 40,57129,1,0&amp;quot;                                                      
## [53] &amp;quot;Mk41=Stimulus,S114,57134,1,0&amp;quot;                                                      
## [54] &amp;quot;Mk42=Stimulus,S  3,57140,1,0&amp;quot;                                                      
## [55] &amp;quot;Mk43=Stimulus,S103,57145,1,0&amp;quot;                                                      
## [56] &amp;quot;Mk44=Stimulus,S  6,59661,1,0&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;numbering-trials&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Numbering trials&lt;/h2&gt;
&lt;p&gt;The antediluvian limitation of markers often prevents us from storing the order of trials using markers. Thus, when we need to inspect or fix vmrk files, we must mentally divide the lines into trials. For instance, in our example vmrk file, all trials begin with the marker &lt;code&gt;S  5&lt;/code&gt;. The function below allows us to temporarily number trials by appending the number to the first marker of each trial. The parameters of the function allow us to select the &lt;code&gt;start_line&lt;/code&gt; to skip the metadata at the top of the vmrk file, as well as to select the number of &lt;code&gt;lines_per_trial&lt;/code&gt; and the &lt;code&gt;first_number&lt;/code&gt; to use in the first trial.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2FEEG-tools-and-tips%2Fblob%2Fmain%2Fnumber_trials.R%23L3-L24&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create temporary vmrk file with numbered trials

source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/EEG-tools-and-tips/main/number_trials.R&amp;#39;)

number_trials(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/EEG-tools-and-tips/main/example.vmrk&amp;#39;, 
              start_line = 15, lines_per_trial = 6, first_number = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Brain Vision Data Exchange Marker File, Version 1.0&amp;quot;                               
##  [2] &amp;quot;&amp;quot;                                                                                  
##  [3] &amp;quot;[Common Infos]&amp;quot;                                                                    
##  [4] &amp;quot;DataFile=3.eeg&amp;quot;                                                                    
##  [5] &amp;quot;Codepage=utf-8&amp;quot;                                                                    
##  [6] &amp;quot;&amp;quot;                                                                                  
##  [7] &amp;quot;[Marker Infos]&amp;quot;                                                                    
##  [8] &amp;quot;; Each entry: Mk&amp;lt;Marker number&amp;gt;=&amp;lt;Type&amp;gt;,&amp;lt;Description&amp;gt;,&amp;lt;Position in data points&amp;gt;,&amp;quot;   
##  [9] &amp;quot;; &amp;lt;Size in data points&amp;gt;, &amp;lt;Channel number (0 = marker is related to all channels)&amp;gt;,&amp;quot;
## [10] &amp;quot;; &amp;lt;Date (YYYYMMDDhhmmssuuuuuu)&amp;gt;&amp;quot;                                                   
## [11] &amp;quot;; Fields are delimited by commas, some fields might be omited (empty).&amp;quot;            
## [12] &amp;quot;; Commas in type or description text are coded as \&amp;quot;\\1\&amp;quot;.&amp;quot;                        
## [13] &amp;quot;Mk1=New Segment,,1,1,0,20240318111955090000&amp;quot;                                       
## [14] &amp;quot;Mk2=Stimulus,S254,20419,1,0&amp;quot;                                                       
## [15] &amp;quot;Mk3=Stimulus,S  5,22332,1,0  1&amp;quot;                                                    
## [16] &amp;quot;Mk4=Stimulus,S 42,23095,1,0&amp;quot;                                                       
## [17] &amp;quot;Mk5=Stimulus,S143,23100,1,0&amp;quot;                                                       
## [18] &amp;quot;Mk6=Stimulus,S  2,23106,1,0&amp;quot;                                                       
## [19] &amp;quot;Mk7=Stimulus,S102,23111,1,0&amp;quot;                                                       
## [20] &amp;quot;Mk8=Stimulus,S  6,25882,1,0&amp;quot;                                                       
## [21] &amp;quot;Mk9=Stimulus,S  5,28106,1,0  2&amp;quot;                                                    
## [22] &amp;quot;Mk10=Stimulus,S 50,29053,1,0&amp;quot;                                                      
## [23] &amp;quot;Mk11=Stimulus,S241,29058,1,0&amp;quot;                                                      
## [24] &amp;quot;Mk12=Stimulus,S  1,29063,1,0&amp;quot;                                                      
## [25] &amp;quot;Mk13=Stimulus,S101,29069,1,0&amp;quot;                                                      
## [26] &amp;quot;Mk14=Stimulus,S  6,31830,1,0&amp;quot;                                                      
## [27] &amp;quot;Mk15=Stimulus,S  5,34056,1,0  3&amp;quot;                                                   
## [28] &amp;quot;Mk16=Stimulus,S 49,35055,1,0&amp;quot;                                                      
## [29] &amp;quot;Mk17=Stimulus,S226,35060,1,0&amp;quot;                                                      
## [30] &amp;quot;Mk18=Stimulus,S  2,35066,1,0&amp;quot;                                                      
## [31] &amp;quot;Mk19=Stimulus,S103,35071,1,0&amp;quot;                                                      
## [32] &amp;quot;Mk20=Stimulus,S  6,37242,1,0&amp;quot;                                                      
## [33] &amp;quot;Mk21=Stimulus,S  5,39436,1,0  4&amp;quot;                                                   
## [34] &amp;quot;Mk22=Stimulus,S 43,40417,1,0&amp;quot;                                                      
## [35] &amp;quot;Mk23=Stimulus,S155,40423,1,0&amp;quot;                                                      
## [36] &amp;quot;Mk24=Stimulus,S  2,40429,1,0&amp;quot;                                                      
## [37] &amp;quot;Mk25=Stimulus,S103,40434,1,0&amp;quot;                                                      
## [38] &amp;quot;Mk26=Stimulus,S  6,42481,1,0&amp;quot;                                                      
## [39] &amp;quot;Mk27=Stimulus,S  5,44662,1,0  5&amp;quot;                                                   
## [40] &amp;quot;Mk28=Stimulus,S 40,45678,1,0&amp;quot;                                                      
## [41] &amp;quot;Mk29=Stimulus,S118,45683,1,0&amp;quot;                                                      
## [42] &amp;quot;Mk30=Stimulus,S  1,45688,1,0&amp;quot;                                                      
## [43] &amp;quot;Mk31=Stimulus,S103,45693,1,0&amp;quot;                                                      
## [44] &amp;quot;Mk32=Stimulus,S  6,47621,1,0&amp;quot;                                                      
## [45] &amp;quot;Mk33=Stimulus,S  5,49809,1,0  6&amp;quot;                                                   
## [46] &amp;quot;Mk34=Stimulus,S 50,50808,1,0&amp;quot;                                                      
## [47] &amp;quot;Mk35=Stimulus,S237,50813,1,0&amp;quot;                                                      
## [48] &amp;quot;Mk36=Stimulus,S  3,50818,1,0&amp;quot;                                                      
## [49] &amp;quot;Mk37=Stimulus,S101,50823,1,0&amp;quot;                                                      
## [50] &amp;quot;Mk38=Stimulus,S  6,53823,1,0&amp;quot;                                                      
## [51] &amp;quot;Mk39=Stimulus,S  5,56042,1,0  7&amp;quot;                                                   
## [52] &amp;quot;Mk40=Stimulus,S 40,57129,1,0&amp;quot;                                                      
## [53] &amp;quot;Mk41=Stimulus,S114,57134,1,0&amp;quot;                                                      
## [54] &amp;quot;Mk42=Stimulus,S  3,57140,1,0&amp;quot;                                                      
## [55] &amp;quot;Mk43=Stimulus,S103,57145,1,0&amp;quot;                                                      
## [56] &amp;quot;Mk44=Stimulus,S  6,59661,1,0&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;removing-trial-numbers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Removing trial numbers&lt;/h2&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2FEEG-tools-and-tips%2Fblob%2Fmain%2Fremove_trial_numbers.R%23L3-L20&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;Now, we will unnumber the trials. This can be especially useful after fixing any errors in the markers. For this demo, we’ll use an &lt;a href=&#34;https://github.com/pablobernabeu/EEG-tools-and-tips/blob/main/example_numbered_trials.vmrk&#34;&gt;example file with numbered trials&lt;/a&gt; that looks just like the output from &lt;code&gt;number_trials()&lt;/code&gt; shown above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Remove trial numbers from temporary vmrk file

source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/EEG-tools-and-tips/main/remove_trial_numbers.R&amp;#39;)

remove_trial_numbers(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/EEG-tools-and-tips/main/example_numbered_trials.vmrk&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Brain Vision Data Exchange Marker File, Version 1.0&amp;quot;                               
##  [2] &amp;quot;&amp;quot;                                                                                  
##  [3] &amp;quot;[Common Infos]&amp;quot;                                                                    
##  [4] &amp;quot;DataFile=3.eeg&amp;quot;                                                                    
##  [5] &amp;quot;Codepage=utf-8&amp;quot;                                                                    
##  [6] &amp;quot;&amp;quot;                                                                                  
##  [7] &amp;quot;[Marker Infos]&amp;quot;                                                                    
##  [8] &amp;quot;; Each entry: Mk&amp;lt;Marker number&amp;gt;=&amp;lt;Type&amp;gt;,&amp;lt;Description&amp;gt;,&amp;lt;Position in data points&amp;gt;,&amp;quot;   
##  [9] &amp;quot;; &amp;lt;Size in data points&amp;gt;, &amp;lt;Channel number (0 = marker is related to all channels)&amp;gt;,&amp;quot;
## [10] &amp;quot;; &amp;lt;Date (YYYYMMDDhhmmssuuuuuu)&amp;gt;&amp;quot;                                                   
## [11] &amp;quot;; Fields are delimited by commas, some fields might be omited (empty).&amp;quot;            
## [12] &amp;quot;; Commas in type or description text are coded as \&amp;quot;\\1\&amp;quot;.&amp;quot;                        
## [13] &amp;quot;Mk1=New Segment,,1,1,0,20240318111955090000&amp;quot;                                       
## [14] &amp;quot;Mk2=Stimulus,S254,20419,1,0&amp;quot;                                                       
## [15] &amp;quot;Mk3=Stimulus,S  5,22332,1,0&amp;quot;                                                       
## [16] &amp;quot;Mk4=Stimulus,S 42,23095,1,0&amp;quot;                                                       
## [17] &amp;quot;Mk5=Stimulus,S143,23100,1,0&amp;quot;                                                       
## [18] &amp;quot;Mk6=Stimulus,S  2,23106,1,0&amp;quot;                                                       
## [19] &amp;quot;Mk7=Stimulus,S102,23111,1,0&amp;quot;                                                       
## [20] &amp;quot;Mk8=Stimulus,S  6,25882,1,0&amp;quot;                                                       
## [21] &amp;quot;Mk9=Stimulus,S  5,28106,1,0&amp;quot;                                                       
## [22] &amp;quot;Mk10=Stimulus,S 50,29053,1,0&amp;quot;                                                      
## [23] &amp;quot;Mk11=Stimulus,S241,29058,1,0&amp;quot;                                                      
## [24] &amp;quot;Mk12=Stimulus,S  1,29063,1,0&amp;quot;                                                      
## [25] &amp;quot;Mk13=Stimulus,S101,29069,1,0&amp;quot;                                                      
## [26] &amp;quot;Mk14=Stimulus,S  6,31830,1,0&amp;quot;                                                      
## [27] &amp;quot;Mk15=Stimulus,S  5,34056,1,0&amp;quot;                                                      
## [28] &amp;quot;Mk16=Stimulus,S 49,35055,1,0&amp;quot;                                                      
## [29] &amp;quot;Mk17=Stimulus,S226,35060,1,0&amp;quot;                                                      
## [30] &amp;quot;Mk18=Stimulus,S  2,35066,1,0&amp;quot;                                                      
## [31] &amp;quot;Mk19=Stimulus,S103,35071,1,0&amp;quot;                                                      
## [32] &amp;quot;Mk20=Stimulus,S  6,37242,1,0&amp;quot;                                                      
## [33] &amp;quot;Mk21=Stimulus,S  5,39436,1,0&amp;quot;                                                      
## [34] &amp;quot;Mk22=Stimulus,S 43,40417,1,0&amp;quot;                                                      
## [35] &amp;quot;Mk23=Stimulus,S155,40423,1,0&amp;quot;                                                      
## [36] &amp;quot;Mk24=Stimulus,S  2,40429,1,0&amp;quot;                                                      
## [37] &amp;quot;Mk25=Stimulus,S103,40434,1,0&amp;quot;                                                      
## [38] &amp;quot;Mk26=Stimulus,S  6,42481,1,0&amp;quot;                                                      
## [39] &amp;quot;Mk27=Stimulus,S  5,44662,1,0&amp;quot;                                                      
## [40] &amp;quot;Mk28=Stimulus,S 40,45678,1,0&amp;quot;                                                      
## [41] &amp;quot;Mk29=Stimulus,S118,45683,1,0&amp;quot;                                                      
## [42] &amp;quot;Mk30=Stimulus,S  1,45688,1,0&amp;quot;                                                      
## [43] &amp;quot;Mk31=Stimulus,S103,45693,1,0&amp;quot;                                                      
## [44] &amp;quot;Mk32=Stimulus,S  6,47621,1,0&amp;quot;                                                      
## [45] &amp;quot;Mk33=Stimulus,S  5,49809,1,0&amp;quot;                                                      
## [46] &amp;quot;Mk34=Stimulus,S 50,50808,1,0&amp;quot;                                                      
## [47] &amp;quot;Mk35=Stimulus,S237,50813,1,0&amp;quot;                                                      
## [48] &amp;quot;Mk36=Stimulus,S  3,50818,1,0&amp;quot;                                                      
## [49] &amp;quot;Mk37=Stimulus,S101,50823,1,0&amp;quot;                                                      
## [50] &amp;quot;Mk38=Stimulus,S  6,53823,1,0&amp;quot;                                                      
## [51] &amp;quot;Mk39=Stimulus,S  5,56042,1,0&amp;quot;                                                      
## [52] &amp;quot;Mk40=Stimulus,S 40,57129,1,0&amp;quot;                                                      
## [53] &amp;quot;Mk41=Stimulus,S114,57134,1,0&amp;quot;                                                      
## [54] &amp;quot;Mk42=Stimulus,S  3,57140,1,0&amp;quot;                                                      
## [55] &amp;quot;Mk43=Stimulus,S103,57145,1,0&amp;quot;                                                      
## [56] &amp;quot;Mk44=Stimulus,S  6,59661,1,0&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;counting-markers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Counting markers&lt;/h2&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2FEEG-tools-and-tips%2Fblob%2Fmain%2Fcount_markers.R%23L3-L18&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/EEG-tools-and-tips/main/count_markers.R&amp;#39;)

count_markers(file = &amp;#39;https://raw.githubusercontent.com/pablobernabeu/EEG-tools-and-tips/main/example.vmrk&amp;#39;, 
              marker = &amp;#39;S  2&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Count of &amp;quot;S  2&amp;quot;: 3 instances&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;count_markers(file = &amp;#39;https://raw.githubusercontent.com/pablobernabeu/EEG-tools-and-tips/main/example.vmrk&amp;#39;, 
              marker = &amp;#39;S  3&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Count of &amp;quot;S  3&amp;quot;: 2 instances&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Making research materials Findable, Accessible, Interoperable and Reusable</title>
      <link>https://pablobernabeu.github.io/presentation/making-research-materials-findable-accessible-interoperable-reusable-fair/</link>
      <pubDate>Fri, 08 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/presentation/making-research-materials-findable-accessible-interoperable-reusable-fair/</guid>
      <description>


&lt;a href=&#39;https://mfr.osf.io/render?url=https://osf.io/h83yq/?direct%26mode=render%26action=download%26mode=render&#39;&gt;
&lt;button style = &#34;background-color: white; color: black; border: 2px solid #196F27; border-radius: 12px;&#34;&gt;
&lt;h3 style=&#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt;
&lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;i class=&#34;fas fa-mouse-pointer&#34;&gt;&lt;/i&gt;&lt;/span&gt;  Poster
&lt;/h3&gt;
&lt;/button&gt;
&lt;p&gt;&lt;/a&gt;  &lt;/p&gt;
&lt;style&gt;.embed-responsive{position:relative;height:100%;}.embed-responsive iframe{position:absolute;height:100%;}&lt;/style&gt;
&lt;script&gt;window.jQuery || document.write(&#39;&lt;script src=&#34;//code.jquery.com/jquery-1.11.2.min.js&#34;&gt;\x3C/script&gt;&#39;) &lt;/script&gt;
&lt;link href=&#34;https://mfr.osf.io/static/css/mfr.css&#34; media=&#34;all&#34; rel=&#34;stylesheet&#34;&gt;
&lt;div id=&#34;mfrIframe&#34; class=&#34;mfr mfr-file&#34;&gt;

&lt;/div&gt;
&lt;script src=&#34;https://mfr.osf.io/static/js/mfr.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;var mfrRender = new mfr.Render(&#34;mfrIframe&#34;, &#34;https://mfr.osf.io/render?url=https://osf.io/h83yq/?direct%26mode=render%26action=download%26mode=render&#34;);&lt;/script&gt;
&lt;div id=&#34;snippet-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Snippet 1&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;The use of code scripts facilitates the reproducibility, testability and expandability of materials.&lt;/em&gt;&lt;/p&gt;
&lt;div style=&#34;margin-top: 0; margin-bottom: 6%;&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;└── stimulus_preparation
    ├── Norway site, base stimuli.csv
    ├── Spain site, base stimuli.csv
    ├── base_images.R
    ├── R_functions
    │   ├── Session2_Pretraining_vocabulary.R
    │   ├── Session2_Training_gender_agreement.R
    │   ├── Session2_Test_gender_agreement.R
    │   ├── Session2_Experiment_gender_agreement.R
    ...
    ├── compile_all_stimuli.R&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;table-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Table 1&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;The minimal components of each language are contained in a base file.&lt;/em&gt;&lt;/p&gt;
&lt;div style=&#34;margin-top: 0; margin-bottom: 6%;&#34;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;verb_ID&lt;/th&gt;
&lt;th&gt;verb_type&lt;/th&gt;
&lt;th&gt;verb&lt;/th&gt;
&lt;th&gt;verb_contrast_ID&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;copula_be&lt;/td&gt;
&lt;td&gt;is&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;copula_be&lt;/td&gt;
&lt;td&gt;are&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;copula_look&lt;/td&gt;
&lt;td&gt;looks&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;copula_look&lt;/td&gt;
&lt;td&gt;look&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;transitive&lt;/td&gt;
&lt;td&gt;remembered&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;transitive&lt;/td&gt;
&lt;td&gt;forgot&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;transitive&lt;/td&gt;
&lt;td&gt;chose&lt;/td&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;transitive&lt;/td&gt;
&lt;td&gt;refused&lt;/td&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;snippet-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Snippet 2&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Tests were set throughout the workflow to control the frequency of some categories (R code).&lt;/em&gt;&lt;/p&gt;
&lt;div style=&#34;margin-top: 0; margin-bottom: 6%;&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;columns_to_check = c(&amp;#39;noun1_gender&amp;#39;, &amp;#39;number&amp;#39;, &amp;#39;person&amp;#39;, 
                     &amp;#39;verb&amp;#39;, &amp;#39;noun1&amp;#39;, &amp;#39;wrapup_noun&amp;#39;)

for(i in seq_along(columns_to_check)) {
  column = columns_to_check[i]
  number_of_unique_frequencies = 
    combinations %&amp;gt;% 
    filter(complete.cases(get(column)), get(column) != &amp;#39;&amp;#39;) %&amp;gt;% 
    group_by(get(column)) %&amp;gt;% tally() %&amp;gt;% select(n) %&amp;gt;% 
    n_distinct()
  if(number_of_unique_frequencies != 1) {
    warning(paste0(&amp;#39;Some elements in the column `&amp;#39;, column, 
                   &amp;#39;` appear more often than others.&amp;#39;))
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;snippet-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Snippet 3&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Seamless adjustment of parameters in each OpenSesame session (Python code).&lt;/em&gt;&lt;/p&gt;
&lt;div style=&#34;margin-top: 0; margin-bottom: 6%;&#34;&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;participant_parameters = 
  pd.read_csv(exp.get_file(&amp;#39;../parameters per participant/&amp;#39; + 
              var.study_site + 
              &amp;#39; site, parameters per participant.csv&amp;#39;))

var.resting_state_order = 
  participant_parameters.loc[
    participant_parameters[&amp;#39;participant&amp;#39;] ==
    var.subject_nr][&amp;#39;Session2_resting_state_order&amp;#39;].iloc[0]

var.language = 
  participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == 
                             var.subject_nr][&amp;#39;language&amp;#39;].iloc[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;snippet-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Snippet 4&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Sending serial-port triggers in OpenSesame to record ERPs (Python code).&lt;/em&gt;&lt;/p&gt;
&lt;div style=&#34;margin-top: 0; margin-bottom: 6%;&#34;&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Open the first serial port available
serialport = serial.Serial(serial.tools.list_ports.comports()[0].device)

# Send triggers to the port
def send_trigger(trigger):
    serialport.write(trigger.to_bytes(length = 1, byteorder = &amp;#39;big&amp;#39;))
    # 10 ms separation from next trigger (see BrainVision Recorder manual)
    time.sleep(0.01) 
    # reset port
    serialport.write(int(0).to_bytes(length = 1, byteorder = &amp;#39;big&amp;#39;)) 
    return;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;References&lt;/h4&gt;
&lt;p&gt;Barsalou, L. W. (2019). Establishing generalizable mechanisms. &lt;em&gt;Psychological Inquiry, 30&lt;/em&gt;(4), 220–230. &lt;a href=&#34;https://doi.org/10.1080/1047840X.2019.1693857&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1080/1047840X.2019.1693857&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cross, Z. R., Zou-Williams, L., Wilkinson, E. M., Schlesewsky, M., &amp;amp; Bornkessel-Schlesewsky, I. (2021). Mini Pinyin: A modified miniature language for studying language learning and incremental sentence processing. &lt;em&gt;Behavior Research Methods, 53(3)&lt;/em&gt;, 1218–1239. &lt;a href=&#34;https://doi.org/10.3758/s13428-020-01473-6&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13428-020-01473-6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;González Alonso, J., Alemán Bañón, J., DeLuca, V., Miller, D., Pereira Soares, S. M., Puig-Mayenco, E., Slaats, S., &amp;amp; Rothman, J. (2020). Event related potentials at initial exposure in third language acquisition: Implications from an artificial mini-grammar study. &lt;em&gt;Journal of Neurolinguistics, 56&lt;/em&gt;, 100939. &lt;a href=&#34;https://doi.org/10.1016/j.jneuroling.2020.100939&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.jneuroling.2020.100939&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Mitrofanova, N., Leivada, E., &amp;amp; Westergaard, M. (2023). Crosslinguistic influence in L3 acquisition: Evidence from artificial language learning. &lt;em&gt;Linguistic Approaches to Bilingualism, 13&lt;/em&gt;(5), 717-742. &lt;a href=&#34;https://doi.org/10.1075/lab.22063.mit&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1075/lab.22063.mit&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Morgan-Short, K., Finger, I., Grey, S., &amp;amp; Ullman, M. T. (2012). Second language processing shows increased native-like neural responses after months of no exposure. &lt;em&gt;PLOS ONE, 7&lt;/em&gt;(3), e32974. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0032974&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0032974&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pereira Soares, S. M., Kupisch, T., &amp;amp; Rothman, J. (2022). Testing potential transfer effects in heritage and adult L2 bilinguals acquiring a mini grammar as an additional language: An ERP approach. &lt;em&gt;Brain Sciences, 12&lt;/em&gt;(5), Article 5. &lt;a href=&#34;https://doi.org/10.3390/brainsci12050669&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3390/brainsci12050669&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. &lt;em&gt;Scientific Data, 3&lt;/em&gt;(1), Article 1. &lt;a href=&#34;https://doi.org/10.1038/sdata.2016.18&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1038/sdata.2016.18&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>rscopus_plus: An extension of the rscopus package</title>
      <link>https://pablobernabeu.github.io/2024/rscopus-plus-an-extension-of-the-rscopus-package/</link>
      <pubDate>Wed, 21 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2024/rscopus-plus-an-extension-of-the-rscopus-package/</guid>
      <description>


&lt;p&gt;Sometimes it’s useful to do a bibliometric analysis. To this end, the &lt;a href=&#34;https://github.com/pablobernabeu/rscopus_plus&#34;&gt;&lt;code&gt;rscopus_plus&lt;/code&gt;&lt;/a&gt; functions (Bernabeu, 2024) extend the R package &lt;a href=&#34;https://github.com/muschellij2/rscopus&#34;&gt;&lt;code&gt;rscopus&lt;/code&gt;&lt;/a&gt; (Muschelli, 2022) to administer the search quota and to enable specific searches and comparisons.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/rscopus_plus/blob/main/scopus_search_plus.R&#34;&gt;&lt;code&gt;scopus_search_plus&lt;/code&gt;&lt;/a&gt; runs &lt;code&gt;rscopus::scopus_search&lt;/code&gt; as many times as necessary based on the number of results and the search quota.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/rscopus_plus/blob/main/scopus_search_DOIs.R&#34;&gt;&lt;code&gt;scopus_search_DOIs&lt;/code&gt;&lt;/a&gt; gets DOIs from &lt;code&gt;scopus_search_plus&lt;/code&gt;, which can then be imported into a reference manager, such as Zotero, to create a list of references.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/rscopus_plus/blob/main/scopus_search_additional_DOIs.R&#34;&gt;&lt;code&gt;scopus_search_additional_DOIs&lt;/code&gt;&lt;/a&gt; searches for additional DOIs.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/rscopus_plus/blob/main/scopus_comparison.R&#34;&gt;&lt;code&gt;scopus_comparison&lt;/code&gt;&lt;/a&gt; compares counts of publications on various topics throughout a certain period. The comparison terms are shown in the legend and in the lines, and they all include the reference query.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/rscopus_plus/blob/main/plot_scopus_comparison.R&#34;&gt;&lt;code&gt;plot_scopus_comparison&lt;/code&gt;&lt;/a&gt; draws a line plot with the output from &lt;code&gt;scopus_comparison&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34; style=&#34;padding-bottom: 0; margin-top: 6%; margin-bottom: 6%;&#34;&gt;
&lt;p&gt;&lt;em&gt;Note.&lt;/em&gt; Before using any of the first four functions, the user must set their Scopus API key confidentially (see &lt;a href=&#34;https://cran.r-project.org/web/packages/rscopus/vignettes/api_key.html&#34;&gt;rscopus guidelines&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-of-use&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example of use&lt;/h2&gt;
&lt;p&gt;As an example (also available &lt;a href=&#34;https://github.com/pablobernabeu/L2_L3_EF/blob/main/biblio_analysis.R&#34;&gt;on GitHub&lt;/a&gt;), we’ll visualise the prevalence of three executive functions in the literatures on second and third language throughout the past two decades.&lt;/p&gt;
&lt;p&gt;First, we’ll use &lt;a href=&#34;https://github.com/pablobernabeu/rscopus_plus/blob/main/scopus_comparison.R&#34;&gt;&lt;code&gt;scopus_comparison()&lt;/code&gt;&lt;/a&gt; (fragment shown below).&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Frscopus_plus%2Fblob%2Fmain%2Fscopus_comparison.R%23L2-L23&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;Next, we’ll use &lt;a href=&#34;https://github.com/pablobernabeu/rscopus_plus/blob/main/plot_scopus_comparison.R&#34;&gt;&lt;code&gt;plot_scopus_comparison()&lt;/code&gt;&lt;/a&gt; (fragment shown below).&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Frscopus_plus%2Fblob%2Fmain%2Fplot_scopus_comparison.R%23L4-L14&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rscopus)
# set_api_key(&amp;#39;your_key_here&amp;#39;)  # (see https://cran.r-project.org/web/packages/rscopus/vignettes/api_key.html)

# I&amp;#39;ll read in mine from a file. If you do this, make sure not to share your file.
api_key = readLines(&amp;#39;scopus_key.txt&amp;#39;)
set_api_key(api_key)

library(dplyr)
library(patchwork)


# Load in Scopus search functions from https://github.com/pablobernabeu/rscopus_plus
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/rscopus_plus/main/scopus_comparison.R&amp;#39;)
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/rscopus_plus/main/plot_scopus_comparison.R&amp;#39;)


# General parameters
search_period = 1990:2023
quota = 20


# Prevalence of three executive functions in second language studies from 2000 to 2023. 

# In addition to &amp;quot;second language&amp;quot;, the reference query includes the terms 
# &amp;quot;learning&amp;quot; and &amp;quot;cognition&amp;quot; to make the scope of the search more relevant to 
# the topic of interest. 

reference_query = &amp;#39;&amp;quot;second language&amp;quot;&amp;#39;

comparison_terms = c( &amp;#39;&amp;quot;working memory&amp;quot;&amp;#39;, &amp;#39;inhibit*&amp;#39;, &amp;#39;&amp;quot;implicit learning&amp;quot;&amp;#39; )

N_comparison_terms = length(comparison_terms)

L2_EF = 
  scopus_comparison(reference_query, comparison_terms, 
                    search_period, quota, verbose = FALSE, 
                    reference_query_field_tag = &amp;#39;TITLE-ABS-KEY&amp;#39;)

saveRDS(L2_EF, &amp;#39;L2_EF.rds&amp;#39;)

L2_EF = readRDS(&amp;#39;L2_EF.rds&amp;#39;)  # it&amp;#39;s possible to load results directly

plot_L2_EF = 
  plot_scopus_comparison(L2_EF, 
                         pub_count_in_legend = FALSE, 
                         pub_count_in_lines = TRUE) +
  scale_color_manual(
    values = c( &amp;quot;[ref.] + &amp;#39;\&amp;quot;working memory\&amp;quot;&amp;#39;&amp;quot; = scales::hue_pal()(N_comparison_terms)[1],
                &amp;quot;[ref.] + &amp;#39;inhibit*&amp;#39;&amp;quot; = scales::hue_pal()(N_comparison_terms)[2], 
                &amp;quot;[ref.] + &amp;#39;\&amp;quot;implicit learning\&amp;quot;&amp;#39;&amp;quot; = scales::hue_pal()(N_comparison_terms)[3] )
  ) + 
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  # Prepare layout for the multi-plot combination
  theme(axis.text.x = element_blank(), axis.title.x = element_blank(), 
        legend.position = &amp;#39;none&amp;#39;, plot.margin = margin(0, 0, 15, 0))


# Prevalence of three executive functions in third language studies from 2000 to 2023. 

# In addition to &amp;quot;third language&amp;quot;, the reference query includes the terms &amp;quot;learning&amp;quot; 
# and &amp;quot;cognition&amp;quot; to make the scope of the search more relevant to the topic of 
# interest. 

reference_query = &amp;#39;&amp;quot;third language&amp;quot;&amp;#39;

# Other parameters identical to those used in the query above.

L3_EF = 
  scopus_comparison(reference_query, comparison_terms, 
                    search_period, quota, verbose = FALSE, 
                    reference_query_field_tag = &amp;#39;TITLE-ABS-KEY&amp;#39;)

saveRDS(L3_EF, &amp;#39;L3_EF.rds&amp;#39;)

L3_EF = readRDS(&amp;#39;L3_EF.rds&amp;#39;)  # it&amp;#39;s possible to load results directly

plot_L3_EF = 
  plot_scopus_comparison(L3_EF, 
                         pub_count_in_legend = FALSE, 
                         pub_count_in_lines = TRUE) +
  scale_color_manual(
    values = c( &amp;quot;[ref.] + &amp;#39;\&amp;quot;working memory\&amp;quot;&amp;#39;&amp;quot; = scales::hue_pal()(N_comparison_terms)[1],
                &amp;quot;[ref.] + &amp;#39;inhibit*&amp;#39;&amp;quot; = scales::hue_pal()(N_comparison_terms)[2], 
                &amp;quot;[ref.] + &amp;#39;\&amp;quot;implicit learning\&amp;quot;&amp;#39;&amp;quot; = scales::hue_pal()(N_comparison_terms)[3] )
  ) + 
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  # Prepare layout for the multi-plot combination
  theme(axis.text.x = element_text(margin = margin(7, 0, 0, 0, &amp;#39;pt&amp;#39;)),
        axis.title.x = element_text(margin = margin(8, 0, 0, 0, &amp;#39;pt&amp;#39;)),
        legend.position = &amp;#39;inside&amp;#39;, legend.position.inside = c(.82, .8))


# Combine plots

plot_L2_EF + plot_L3_EF + 
  plot_layout(ncol = 1, axes = &amp;#39;collect&amp;#39;) &amp;amp;
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(vjust = 0.5, size = 13), 
        plot.title = element_markdown(hjust = 0.5, size = 12),
        legend.text = element_text(size = 11,),
        legend.background = element_rect(color = &amp;#39;grey80&amp;#39;, fill = &amp;#39;grey99&amp;#39;), 
        legend.margin = margin(0, 5, 2, 0)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2024/rscopus-plus-an-extension-of-the-rscopus-package/index.en_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;720&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The total number of publications over the current period is shown between brackets after each query.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Bernabeu, P. (2024). &lt;em&gt;rscopus_plus&lt;/em&gt;. OSF. &lt;a href=&#34;https://doi.org/10.17605/OSF.IO/BUZQ6&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17605/OSF.IO/BUZQ6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Muschelli, J. (2022). &lt;em&gt;Package ’rscopus’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/rscopus/rscopus.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/rscopus/rscopus.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A session logbook for a longitudinal study using conditional formatting in Excel</title>
      <link>https://pablobernabeu.github.io/2023/a-session-logbook-for-a-longitudinal-study-using-conditional-formatting-in-excel/</link>
      <pubDate>Sat, 02 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/a-session-logbook-for-a-longitudinal-study-using-conditional-formatting-in-excel/</guid>
      <description>


&lt;p&gt;Longitudinal studies consist of several sessions, and often involve session session conductors. To facilitate the planning, registration and tracking of sessions, a session logbook becomes even more necessary than usual. To this end, an Excel workbook with conditional formatting can help automatise some formats and visualise the progress.&lt;/p&gt;
&lt;p&gt;Below is an example that is &lt;a href=&#34;https://1drv.ms/x/s!AouK9kQQrXKooGKVLbdBzz-DStmj?e=mbNZE4&#34;&gt;available on OneDrive&lt;/a&gt;. To fully access this workbook, it may be downloaded via &lt;kbd&gt;File&lt;/kbd&gt; &amp;gt; &lt;kbd&gt;Save as&lt;/kbd&gt; &amp;gt; &lt;kbd&gt;Download a copy&lt;/kbd&gt;. Alternatively, the workbook can be exported to one’s own account on OneDrive.&lt;/p&gt;
&lt;p&gt;The conditional formats include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;fill in blanks in columns A, C or D to remove the background&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;fill in blanks in F and G, or I and J, or L and M, etc, to highlight the entire session in green&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;fill in any notes columns to highlight them in red&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe width=&#34;700&#34; height=&#34;365&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; src=&#34;https://1drv.ms/x/c/a872ad1044f68a8b/UQSLivZEEK1yIICoYhAAAAAAABsHnSAl4C4iZLc?em=2&amp;amp;AllowTyping=True&amp;amp;ActiveCell=&amp;#39;in&amp;#39;!A1&amp;amp;Item=&amp;#39;in&amp;#39;!A1%3AW7&amp;amp;wdHideGridlines=True&amp;amp;wdDownloadButton=True&amp;amp;wdInConfigurator=True&amp;amp;wdInConfigurator=True&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;These conditional formats can be consulted (and edited) from the &lt;kbd&gt;Home&lt;/kbd&gt; tab by opening &lt;kbd&gt;Conditional Formatting&lt;/kbd&gt; &amp;gt; &lt;kbd&gt;Manage Rules&lt;/kbd&gt;. At the top, the option &lt;kbd&gt;This Worksheet&lt;/kbd&gt; should be selected to view all formulas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Motivating a preregistration (especially in experimental linguistics)</title>
      <link>https://pablobernabeu.github.io/2023/motivating-a-preregistration-especially-in-experimental-linguistics/</link>
      <pubDate>Fri, 24 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/motivating-a-preregistration-especially-in-experimental-linguistics/</guid>
      <description>


&lt;div id=&#34;the-disorder&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The disorder&lt;/h2&gt;
&lt;p&gt;Studies in the cognitive sciences often feature multiple experimental conditions and other independent variables. Once the study progresses, the analytical liberties associated with all the conditions and variables are compounded by the myriad possible steps in data processing and analysis. All these combinations lead to a &lt;em&gt;garden of forking paths&lt;/em&gt;, and the &lt;strong&gt;researcher degrees of freedom&lt;/strong&gt; soar to unexpected highs.&lt;/p&gt;
&lt;p&gt;The system of professional incentives in academia largely ignores the issue of researcher degrees of freedom. When CVs are assessed, quantity weighs more than quality of research. When studies are assessed, statistical significance weighs more than methodological rigour. Thus, the low replication rates in various fields, including linguistics (Kobrock &amp;amp; Roettger, 2023), are hardly surprising (cf. Barsalou, 2019).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-treatment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The treatment&lt;/h2&gt;
&lt;p&gt;The number of researcher degrees of freedom, and their unforeseen influence, can be reduced a priori by publishing a preregistration before data collection has begun (or before the analysis in the case of meta-analyses and secondary-data analyes). The preregistration takes a bit of time, which poses a challenge because funding systems often require &lt;em&gt;doing things&lt;/em&gt; as soon and as fast as possible, driven by a questionable notion of scientific productivity.&lt;/p&gt;
&lt;div id=&#34;how-to-gather-the-strength&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to gather the strength&lt;/h3&gt;
&lt;p&gt;The best argument to motivate a preregistration may be that does not incur any extra time. It only requires frontloading an important portion of the work. This effort will be rewarded when the study is published, as preregistered analyses are received with greater trust by peer-reviewers and other readers.&lt;/p&gt;
&lt;p&gt;If any contributors of a project can gather just enough time and interest to initiate a preregistration in time, the researchers can attempt to pocket this important asset for their study. They will reap the reward in time, and so will their field of research at large.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-to-do&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What to do&lt;/h3&gt;
&lt;p&gt;Initially, rather than detailed analytical steps, it is the broader theoretical aspects that should be laid out. An abstract and an introduction should be written, describing why the study has been designed in such a way, what analyses will be performed, and what hypotheses are afforded by the literature. Next, some methodological details regarding the materials and the analyses should be added.&lt;/p&gt;
&lt;p&gt;The degree of detail in the preregistration can be determined by the researchers alone. Yet, ceteris paribus, the greater the detail in a preregistration, the greater the trustworthiness of the analyses and the results. Multiple preregistration guidelines exist by now, including some field-specific ones.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solace&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Solace&lt;/h3&gt;
&lt;p&gt;Preregistration is not perfect, but is a lesser evil that reduces the misuse of statistical analysis in science (Mertzen et al., 2021; Roettger, 2023).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Even the strongest blizzards start with a single snowflake. (Sara Raasch)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Rules are made to be broken—at least as and when necessary. That is, deviations from the preregistration are possible, and indeed very frequent (Bakker et al., 2020; van den Akker et al., 2023).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Bakker, M., Veldkamp, C. L. S., Assen, M. A. L. M. van, Crompvoets, E. A. V., Ong, H. H., Nosek, B. A., Soderberg, C. K., Mellor, D., &amp;amp; Wicherts, J. M. (2020). Ensuring the quality and specificity of preregistrations. &lt;em&gt;PLOS Biology, 18&lt;/em&gt;(12), e3000937. &lt;a href=&#34;https://doi.org/10.1371/journal.pbio.3000937&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pbio.3000937&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barsalou, L. W. (2019). Establishing generalizable mechanisms. &lt;em&gt;Psychological Inquiry, 30&lt;/em&gt;(4), 220–230. &lt;a href=&#34;https://doi.org/10.1080/1047840X.2019.1693857&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1080/1047840X.2019.1693857&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kobrock, K., &amp;amp; Roettger, T. B. (2023). Assessing the replication landscape in experimental linguistics. &lt;em&gt;Glossa Psycholinguistics, 2&lt;/em&gt;(1). &lt;a href=&#34;https://doi.org/10.5070/G6011135&#34; class=&#34;uri&#34;&gt;https://doi.org/10.5070/G6011135&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Mertzen, D., Lago, S., &amp;amp; Vasishth, S. (2021). The benefits of preregistration for hypothesis-driven bilingualism research. &lt;em&gt;Bilingualism: Language and Cognition, 24&lt;/em&gt;(5), 807–812. &lt;a href=&#34;https://doi.org/10.1017/S1366728921000031&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1017/S1366728921000031&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Roettger, T. B. (2021). Preregistration in experimental linguistics: Applications, challenges, and limitations. &lt;em&gt;Linguistics, 59&lt;/em&gt;(5), 1227–1249. &lt;a href=&#34;https://doi.org/10.1515/ling-2019-0048&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1515/ling-2019-0048&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;van den Akker, O. R., van Assen, M. A. L. M., Enting, M., de Jonge, M., Ong, H. H., Rüffer, F., Schoenmakers, M., Stoevenbelt, A. H., Wicherts, J. M., &amp;amp; Bakker, M. (2023). Selective hypothesis reporting in psychology: Comparing preregistrations and corresponding publications. &lt;em&gt;Advances in Methods and Practices in Psychological Science, 6&lt;/em&gt;(3), 25152459231187988. &lt;a href=&#34;https://doi.org/10.1177/25152459231187988&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/25152459231187988&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Learning how to use Zotero</title>
      <link>https://pablobernabeu.github.io/2023/learning-how-to-use-zotero/</link>
      <pubDate>Fri, 10 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/learning-how-to-use-zotero/</guid>
      <description>


&lt;p&gt;Is it worth learning how to use a reference management system such as Zotero? Maybe.&lt;/p&gt;
&lt;p&gt;The hours you invest in learning how to use Zotero (approx. 10 hours) are likely to pay off, as they will save you a lot of time that you would otherwise spend formatting, revising and correcting references. In addition, this skill would become part of your skill set.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://library-guides.ucl.ac.uk/zotero/installing-zotero&#34;&gt;A great guide&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://library-guides.ucl.ac.uk/zotero/further-help&#34;&gt;Free, online webinars in which you could participate and ask questions&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://library-guides.ucl.ac.uk/zotero/using-zotero-with-word&#34;&gt;Zotero and Word&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://chrome.google.com/webstore/detail/zotero-connector/ekhagklcjbdpajgpjgmbionohlpdbjgc&#34;&gt;Zotero extension for the Chrome browser, to quickly add papers to your libraries&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>FAQs on mixed-effects models</title>
      <link>https://pablobernabeu.github.io/2023/faqs-on-mixed-effects-models/</link>
      <pubDate>Sat, 04 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/faqs-on-mixed-effects-models/</guid>
      <description>


&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;I am dealing with nested data, and I remember from an article by &lt;a href=&#34;https://doi.org/10.1016/S0022-5371(73)80014-3&#34;&gt;Clark (1973)&lt;/a&gt; that nested should be analysed using special models. I’ve looked into mixed-effects models, and I’ve reached a structure with random intercepts by subjects and by items. Is this fine?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;In early days, researchers would aggregate the data across these repeated measures to prevent the violation of the assumption of independence of observations, which is one of the most important assumptions in statistics. With the advent of mixed-effects models, researchers began accounting for these repeated measures using random intercepts and slopes. However, problems of convergence led many researchers to remove random slopes. This became widespread until, over the past few years, we have realised that random slopes are necessary to prevent an inflation of the Type I error due to the violation of the assumption of independence (&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;Brauer &amp;amp; Curtin, 2018&lt;/a&gt;; &lt;a href=&#34;http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf&#34;&gt;Singmann &amp;amp; Kellen, 2019&lt;/a&gt;). Please see Table 17 in Brauer and Curtin (2018). Due to the present reasons, the models in the current article are anti-conservative. To redress this problem, please consider the inclusion of random slopes by participant for all between-items variables [e.g., &lt;code&gt;(stimulus_condition | participant)&lt;/code&gt;], and random slopes by item for all between-participants variables [e.g., &lt;code&gt;(extraversion | item)&lt;/code&gt;]. Interaction terms should also have the corresponding slopes, except when the variables in the interaction vary within different units, that is, one between participants and one between items (&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;Brauer &amp;amp; Curtin, 2018&lt;/a&gt;). Each of the random intercepts and random slopes included in the model should be noted in the main text, for instance using footnotes in the results table (see &lt;a href=&#34;https://bookdown.org/pablobernabeu/language-sensorimotor-conceptual-processing-statistical-power/study-2.1-semantic-priming.html#semanticpriming-results&#34;&gt;example&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;I calculated the &lt;em&gt;p&lt;/em&gt; values by comparing minimally-different models using the &lt;code&gt;anova&lt;/code&gt; function. Is this fine?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.3758/s13428-016-0809-y&#34;&gt;Luke (2017)&lt;/a&gt; warns that the &lt;em&gt;p&lt;/em&gt; values calculated by model comparison—which are based on likelihood ratio tests—can be anti-conservative. Therefore, the Kenward-Roger and the Satterthwaite methods are recommended instead (both available in other packages, such as &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;lmerTest&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/afex/afex.pdf&#34;&gt;afex&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;The lme4 package only runs on one thread (CPU) but the computer has 8. Do you have any advice on making the model run using more of the threads? It’s taking a very long time. I’ve seen these two possible solutions online from 2018 (&lt;a href=&#34;https://stackoverflow.com/questions/48315268/how-can-i-make-r-using-more-than-1-core-8-available-on-a-ubuntu-rstudio-server&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://stat.ethz.ch/pipermail/r-sig-mixed-models/2018q3/027170.html&#34;&gt;here&lt;/a&gt;) but would like some advice if they have any or have attempted either of these solutions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;From the information I have seen in the past as well as right now, parallelising (g)lmer intentionally would be very involved. There is certainly interest in it, as your resources show (also see &lt;a href=&#34;https://github.com/lme4/lme4/issues?q=is%3Aissue+parallel&#34;&gt;here&lt;/a&gt;). However, the current information suggests to me that it is not possible.&lt;/p&gt;
&lt;p&gt;Interestingly, some isolated cases of unintentional parallelisation have been documented, and the developers of the &lt;a href=&#34;https://cran.r-project.org/web/packages/lme4/lme4.pdf&#34;&gt;lme4&lt;/a&gt; package were &lt;a href=&#34;&#34;&gt;surprised about them&lt;/a&gt; because they have not created this feature (see &lt;a href=&#34;https://github.com/lme4/lme4/issues/492&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://github.com/lme4/lme4/issues/627&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I think the best approach may be running your model(s) in a high-performance computing (HPC) cluster. Although this would not reduce the amount of time required for each model, it would have two advantages. First, your own computers wouldn’t be busy for days, and second, you could even run several models at the same time without exhausting your own computers. I still have access to the HPC at my previous university, and it would be fine for me to send your model(s) there if that would help you. Feel free to let me know. Otherwise I can see that your university has this facility too.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;We took your advice and ran the model on a supercomputer - it took roughly 2.5 days, which is what it took for the model to run on my iMac and a gaming laptop Vivienne has.&lt;/p&gt;
&lt;p&gt;The model, however, didn’t converge. We have read that you can use &lt;code&gt;allFit()&lt;/code&gt; to try the fit with all available optimizers. Do you have any experience using this? If you did, I wondered where this would sit in the code for the model? How and where do I add this in to check all available optimizers, please?&lt;/p&gt;
&lt;p&gt;I have attached my code in a txt file and the data in excel for you to see, in case it is of any use.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;The multi-optimizer check is indeed a way (albeit tentative) to probe into the convergence. Convergence has long been a fuzzy subject, as there are different standpoints depending on the degree of conservativeness that is sought after by the analysts.&lt;/p&gt;
&lt;p&gt;On Page 124 in my thesis (&lt;a href=&#34;https://osf.io/97u5c&#34; class=&#34;uri&#34;&gt;https://osf.io/97u5c&lt;/a&gt;), you can find this multi-optimizer check (also see this &lt;a href=&#34;https://pablobernabeu.github.io/2021/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit&#34;&gt;blog post&lt;/a&gt;). All the code is available on OSF. More generally, I discuss the issue of convergence throughout the thesis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;I have run the model with &lt;code&gt;optimizer=&#34;nloptwrap&#34;&lt;/code&gt; and &lt;code&gt;algorithm=&#34;NLOPT_LN_BOBYQA&#34;&lt;/code&gt; and received the following warning message (once the model ran) -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;In optwrap(optimizer, devfun, start, rho$lower, control = control, :
convergence code 5 from nloptwrap: NLOPT_MAXEVAL_REACHED: optimization stopped becasue maxeval (above) was reached.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does this mean that the model didn’t converge? I’m only asking because I wasn’t given a statement saying it didn’t converge, as it did with Nelder_Mead. It was stated (at the end of summary table)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Optimizer (Nelder_Mead) convergence code: 4 (failure to converge in 10000 evaluations)
failure to converge in 10000 evaluations&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;Please try &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/semanticpriming_lmerTest.R#L109&#34;&gt;increasing the max number of iterations&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;We increased the max number of iterations to 1e6 and then 1e7, and the model didn’t converge. But it has converged with &lt;code&gt;maxeval=1e8&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I wanted to ask please, do you know of any issues with the max iterations being this high and effecting the interpretability of the model? Or is it completely fine?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;There are no side-effects to increasing the number of iterations (see Remedy 6 in &lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;Brauer &amp;amp; Curtin, 2018&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>FAIR standards for the creation of research materials, with examples</title>
      <link>https://pablobernabeu.github.io/2023/fair-standards-for-the-creation-of-research-materials-with-examples/</link>
      <pubDate>Thu, 02 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/fair-standards-for-the-creation-of-research-materials-with-examples/</guid>
      <description>


&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34; style=&#34;padding-bottom: 0; margin-bottom: 6%;&#34;&gt;
&lt;p&gt;In the fast-paced world of scientific research, establishing minimum standards for the creation of research materials is essential. Whether it’s stimuli, custom software for data collection, or scripts for statistical analysis, the quality and transparency of these materials significantly impact the reproducibility and credibility of research. This blog post explores the importance of adhering to FAIR (Findable, Accessible, Interoperable, Reusable) principles, and offers practical examples for researchers, with a focus on the cognitive sciences.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Notwithstanding the need for speed in most scientific projects, what should be the &lt;strong&gt;minimum acceptable standards&lt;/strong&gt; in the &lt;strong&gt;creation of research materials&lt;/strong&gt; such as stimuli, custom software for data collection (e.g., experiment in jsPsych, OpenSesame or psychoPy), or scripts for statistical analysis?&lt;/p&gt;
&lt;p&gt;The answer to this question is contingent upon the field of research, the purpose and the duration of the project, and many other contextual factors. So, to narrow down the scope and come at a general answer, let’s suppose we asked a researcher in the cognitive sciences (e.g., a linguist, a psychologist or a neuroscientist) who values open science. Perhaps, such a researchers would be satisfied with a method for the creation of materials that &lt;strong&gt;allows the creators of the materials, as well as their collaborators and any other stakeholders (e.g., any fellow scientists working in the same field), to explore, understand, reproduce, modify, and reuse the materials following their completion and thereafter&lt;/strong&gt;. Let’s review some of the implements that can help fulfil these standards.&lt;/p&gt;
&lt;div id=&#34;fairness&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;FAIRness&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.go-fair.org/fair-principles&#34;&gt;FAIR Guiding Principles for scientific data management and stewardship&lt;/a&gt; exhaustively describe a protocol for making materials &lt;strong&gt;F&lt;/strong&gt;indable, &lt;strong&gt;A&lt;/strong&gt;ccessible, &lt;strong&gt;I&lt;/strong&gt;nteroperable and &lt;strong&gt;R&lt;/strong&gt;eusable. These terms cover the five allowances listed above, along with other important aspects.&lt;/p&gt;
&lt;p&gt;Let’s look at some instantiations of the FAIR principles.&lt;/p&gt;
&lt;div id=&#34;sharing-the-materials&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sharing the materials&lt;/h2&gt;
&lt;p&gt;A condition sine qua non is to share the materials publicly online, as far as possible. Repositories on servers such as OSF or GitHub are often sufficient to this end. Unfortunately, most studies in the cognitive sciences still do not share the complete materials.&lt;/p&gt;
&lt;p&gt;One of the reasons why sharing is so important is to prevent wrong assumptions by the audience that will consider the research. That is, when the materials of a study are not publicly shared online, the readers of the papers are left with two options: to assume that there were no errors or to assume that were some errors of an uncertain degree. The method followed in the creation of the materials should free the readers of this tribulation, by allowing them to consult the materials and their preparation in full, or as completely as possible.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reproducibility&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reproducibility&lt;/h2&gt;
&lt;p&gt;It is convenient to allow others, and our future selves, to reproduce the materials throughout their preparation and at any time thereafter. For this purpose, &lt;strong&gt;R&lt;/strong&gt; can be used to register in scripts as many as possible of the steps followed throughout the preparation of the materials. Far from being only a software for data analysis, R allows the preparation of texts, images, audios, etc. Humans err, by definition. That can be counted on. Conveniently, registering the steps followed during weeks or months of preparation allows us to offload part of the documentation efforts. It’s a way of video-recording, as it were, all the additions, subtractions, replacements, transformations and calculations performed with the raw materials, for the creation of the final materials.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generous-documentation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generous documentation&lt;/h2&gt;
&lt;p&gt;Under the curse of knowledge, the creators of research materials may believe that their materials are self-explanatory. Often they are more obscure than they think. To allow any other stakeholders, including their future selves, to exercise the five allowances listed above—i.e., explore, understand, reproduce, modify, and reuse the materials—, the preparation process and the end materials should be documented with enough detail. This can be done using README.txt files throughout the project. Using the &lt;code&gt;.txt&lt;/code&gt; format/extension is recommended because other formats, such as Microsoft Word, may not be (fully) available in some computers. To exemplify the format and the content of readme files, below is an excerpt from a longitudinal study on which I’ve been working.&lt;/p&gt;
&lt;textarea readonly style=&#39;border-color: lightgrey; overflow: auto; color: darkblue; font-size: 90%; min-width: 125%; height: 4600px; white-space: pre; overflow-wrap: normal; padding-right: 0.5em; padding-left: 1em;&#39;&gt;

-- Post-training test --

In Sessions 2, 3, 4 and 6, if the test is failed in the first attempt, the training and the test are 
repeated (following González Alonso et al., 2020). In such cases, the result is shown at the end 
of the second attempt. The session advances if the accuracy achieved in the second attempt exceeds 
80%, whereas the session stops if the accuracy is lower. In the latter situation, an &#39;End of session&#39; 
message is presented, flanked by two orange circles, and followed by an acknowledgement for the 
participant. Once the participant has read this screen, the experimenter quits the session by 
pressing &#39;ESC&#39; and then &#39;Q&#39;.


== Stimuli ==

The stimulus lists are described in the R functions that were used to create the stimuli, as well as
in the &#39;list&#39; column in the stimulus files.


== Participant-specific parameters for lab-based sessions ==

Each participant was assigned certain parameters in advance, including the mini-language, the order 
of the resting-state parts, and the stimulus lists. The code that was used to create this assignment 
is available in the &#39;stimulus_preparation&#39; folder. 

Due to the pre-assignment of the parameters, there is a fixed set of participant IDs that can be 
used in OpenSesame. These identification numbers range between 1 and 144. If an ID outside of this 
range is used, the OpenSesame session does not run.


== General procedure for lab-based sessions ==

At the beginning of lab-based Sessions 2, 3, 4 and 6, the experimenter will first signal the lab is 
busy using a light or a sign. Next, they will ascertain what participant and what session applies. 
This is done using the session logbook that is shared among all session conductors. This session 
logbook is instantly updated online using a cloud service, such as OneDrive. 

Next, the experimenter starts OpenSesame by opening the program directly (not by opening the 
session-specific file), and then opens the appropriate session within OpenSesame. This procedure 
helps prevent the opening of a standalone Python window, the closing of which would result in the 
closing of OpenSesame. Next, the experimenter opens BrainVision Recorder.

When the participant arrives in the lab, they are informed that they can use the toilet outside. 
The participant is also offered some water. 

Next, the size of their head is measured, and an appropriate cap is tried on the participant’s 
head. Next, the cap is placed on a dummy head, and the electrodes are attached to the cap. At 
that point, to prevent signal interference, the participant is kindly asked to either put their 
mobile devices (phone, tablet, smartwatch) in flight mode, or to leave them outside of the booth. 

Next, to protect the participant&#39;s clothes from any drops of gel, a towel is placed on their 
upper back, covering shoulders and upper torso. Both ends of the towel are clipped together at 
the front using two or three clothes pegs. Next, the cap is fitted on the participant&#39;s head. To 
prevent the cap from being pulled back during the session, the splitter box is attached to the 
towel on the participant&#39;s back, right below their head. Next, measures are taken to adjust the 
position of the cap evenly, first from the nasion to the inion, and then from the tip of an ear 
to the other ear. 

Next, the experimenter returns to OpenSesame and runs the session in full screen by clicking on 
the full green triangle at the top left. Then, a file explorer window opens, in which the 
experimenter must assign a subject number consistent with the session logbook, and must select 
the destination folder for the logfile. The destination folder is called &#39;logfiles&#39;. Any prompts 
to overwrite a logfile must normally be refused, or considered carefully, due to the risk of 
losing data from previous sessions.

In the first screen, the experimenter can disable some of the tasks. This option can be used if a 
session has ended abruptly, in which case the session can be resumed from a near checkpoint. In 
such a case, the experimenter must first note this incident in their logbook, and rename the log 
file that was produced on the first run, by appending &#39;_first_run&#39; to the name. This prevents 
overwriting the file on the second run. Next, they must open a new session, enter the same 
participant ID, and select the appropriate part from which to begin. This part must be the part 
immediately following the last part that was completed in full. For instance, if a session ended
abruptly during the experiment, the beginning selected on the second run would be the experiment. 
Once the session has finished completely, the first log file and the second log file must be 
safely merged into a single file, keeping only the fully completed tasks.

In the first instructional screen, participants are asked to refrain from asking any questions 
unless it is necessary, so that all participants can receive the same instructions.

At the beginning of the resting-state part in Session 2, and at the beginning of the Experiment 
part, instructions are presented on the screen that ask participants to stay as still as possible 
during the following task. The screen contains an orange-coloured square with the letters &#39;i.s.r&#39;, 
that remind the experimenter to check the impedance and the signal, and finally to begin recording 
the EEG signal. If the impedance of any electrodes is poor, the experimenter may enter the booth 
to lower the impedance of the electrodes affected. Otherwise, after validating the signal and the 
impedance, the experimenter can begin the recording in BrainVision, and press the letter &#39;C&#39; twice 
in the stimulus computer. At that point, a green circle will appear, along with instructions for 
the participant. 

Similarly, at the end of the eyes-closed resting-state measurement (which is five minutes long), 
the experimenter must intervene when they see the screen with the orange stripes, by knocking on 
the door to let the participant open their eyes. 

Furthermore, at the end of the resting-state part and at the end of the Experiment part, a screen
with a crossed-out R appears to remind the experimenter to stop recording the EEG. 

Notice that the above-mentioned stages, characterised by screens with orange stripes, require the 
experimenter&#39;s intervention. The experimenter must allow the participant to read any text on these 
screens. Next, the experimenter must press the letter &#39;C&#39; twice to let the session continue. This 
protocol provides the experimenter with control when necessary. The experimenter should be aware 
of the use of the letter &#39;C&#39; at these points, as the requirement is not signalled on the screen 
to prevent participants from pressing the letter themselves. 

During the experiment, it is important to monitor the EEG signal. If it ever becomes very noisy, 
the experimenter must wait until the next break and the participant to stop, so that the signal 
can be verified. If the noise in the signal is due to the participant&#39;s movement, they should be 
asked again to please stay as still possible. If the noise is due to an increase in the 
impedance of some electrodes, the impedance of those electrodes should be revised.

The experiment in Session 2 contains breaks every 40 trials, whereas the experiments in subsequent 
sessions contain breaks every 50 trials. During these breaks, the number of the current trial 
appears in grey on the bottom right corner of the screen.

If a session ends abruptly during the experiment, but there is not enough time to restart the 
session from the experiment, then the data should be uploaded to the repository. 


== Definition of items in OpenSesame (only for programming purposes, not for in-session use) ==

  -- Each major part of the session is contained in a sequence item that is named in capital 
     letters (e.g., &#39;PRETRAINING&#39;, &#39;TRAINING&#39;, &#39;TEST&#39;, &#39;EXPERIMENT&#39;).

  -- &#39;continue_space&#39;: allows proceeding to the following screen after pressing the space bar, 
     which should be done by the participant. In most cases, two presses are required, as 
     detailed on the screen.

  -- &#39;continue_c&#39;: allows proceeding to the following screen after pressing the letter &#39;C&#39;, 
     which should be done by the experimenter. In most cases, two presses are required, as 
     detailed on the screen.


== Variables in the OpenSesame log files ==

In the log files produced by OpenSesame, each part of the session (e.g., Test, Experiment) is 
identified in the variable &#39;session_part&#39;. The names of the response variables are &#39;response&#39;,
&#39;response_time&#39; and &#39;correct&#39;. Item-specific response variables follow the formats of 
&#39;response_[item_name]&#39;, &#39;response_time_[item_name]&#39; and &#39;correct_[item_name]&#39; 
(see https://osdoc.cogsci.nl/3.3/manual/variables/#response-variables).

The output is verbose and requires preprocessing of the data. For instance, the last response 
in each loop may appear twice in the output, due to the processing of the response. These 
duplicates can--and must--be cleaned up by discarding the rows that have the same trial number
as the preceding row.


== EEG triggers ==

Triggers are sent to the EEG recorder throughout the experiment. The system for sending 
triggers is set up in OpenSesame script within the inline script &#39;EEG_trigger_setup&#39;.

The key to the triggers is provided below.

  0: reset trigger port in BrainVision Recorder. This trigger is integrated in the 
     trigger-sending function.

  -- Resting-state EEG part --

    10: beginning of eyes-open resting-state EEG

    11: end of eyes-open resting-state EEG

    12: beginning of eyes-closed resting-state EEG

    13: end of eyes-closed resting-state EEG

  -- Experiment part --

    5: fixation mark

    -- ID of each target sentence (only applicable to target trials) --

        110--253: triggers ranging between 110 and 253, time-locked to the onset of the 
          word of interest in each trial.
&lt;/textarea&gt;
&lt;div id=&#34;comments-in-code-scripts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comments in code scripts&lt;/h3&gt;
&lt;p&gt;It is helpful for our future selves, for our collaborators, and for any other stakeholders associated with a project—which includes any fellow researchers worldwide—to include comments in code scripts. These comments should introduce the purpose of the script at the top, and the purpose of various components of the code. Some excerpts are shown below as examples.&lt;/p&gt;
&lt;script src=&#39;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Ffrequentist_bayesian_plot.R%23L3-L35&amp;style=a11y-dark&amp;type=code&amp;showFullPath=on&amp;showCopy=on&amp;showLineNumbers=on&amp;showFileMeta=on&#39;&gt;&lt;/script&gt;
&lt;script src=&#39;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Fdata%2Fsemanticpriming_data_preparation.R%23L29-L60&amp;style=a11y-dark&amp;type=code&amp;showFullPath=on&amp;showCopy=on&amp;showLineNumbers=on&amp;showFileMeta=on&#39;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;open-source-software&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Open-source software&lt;/h2&gt;
&lt;p&gt;Where possible, open-source software should be used. Open-source software is free, and hence more accessible. Open-source software can be classified in various dimensions, such as the size of the user base. The more users, the greater the support, because the core developers have more resources, and the users will often help each other in public forums such as Stack Exchange. For instance, a programming language such as R boasts millions of users worldwide who count on support in public forums and in R-specific forums such as the &lt;a href=&#34;https://community.rstudio.com&#34;&gt;Posit Community&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Other software are not as large. For instance, open-source software for psychological research (e.g., &lt;a href=&#34;https://osdoc.cogsci.nl/&#34;&gt;OpenSesame&lt;/a&gt;, &lt;a href=&#34;https://www.psychopy.org/&#34;&gt;psychoPy&lt;/a&gt;) are far smaller than R in terms of community. Yet, these software too can count on substantial support. For the more basic uses, most of the way has already been paved, and the existing documentation suffices. For more advanced uses, the smaller size of the community can become more obvious, as one needs to spend more time looking for solutions.&lt;/p&gt;
&lt;p&gt;Regardless of the size of the community, all else being equal, open-source software is the right choice to ensure access to one’s work for all (potential) stakeholders in the future. The other option, proprietary software, entails dependence on the services of a private company.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidiness-and-parsimony-in-computer-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidiness and parsimony in computer code&lt;/h2&gt;
&lt;p&gt;Code scripts should be as tidy and parsimonious as possible. For instance, to prevent overly long scripts that would impair the comprehension of the materials, it is useful to break down large projects into nested scripts, and &lt;code&gt;source&lt;/code&gt; (i.e., run) the smaller scripts in the larger scripts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compose all stimuli for Sessions 2, 3, 4 and 6

# Create participant-specific parameters
source(&amp;#39;stimulus_preparation/participant_parameters.R&amp;#39;)

# Frame base images
source(&amp;#39;stimulus_preparation/base_images.R&amp;#39;)

# Session 2
source(&amp;#39;stimulus_preparation/Session 2/Session2_compile_all_stimuli.R&amp;#39;)

# Session 3
source(&amp;#39;stimulus_preparation/Session 3/Session3_compile_all_stimuli.R&amp;#39;)

# Session 4
source(&amp;#39;stimulus_preparation/Session 4/Session4_compile_all_stimuli.R&amp;#39;)

# Session 6
source(&amp;#39;stimulus_preparation/Session 6/Session6_compile_all_stimuli.R&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tidiness-and-parsimony-in-project-directories&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidiness and parsimony in project directories&lt;/h2&gt;
&lt;p&gt;A directory tree is useful to display all the folders in a project. The tree can be produced in the RStudio ‘Terminal’ console using the following one-line command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;find . -type d | sed -e &amp;quot;s/[^-][^\/]*\//  |/g&amp;quot; -e &amp;quot;s/|\([^ ]\)/| - \1/&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output will look like the following (excerpt from &lt;a href=&#34;https://osf.io/gt5uf/wiki&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf/wiki&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.
  | - bayesian_priors
  |  | - plots
  | - semanticpriming
  |  | - analysis_with_visualsimilarity
  |  |  | - model_diagnostics
  |  |  |  | - results
  |  |  |  | - plots
  |  |  | - results
  |  |  | - plots
  |  |  | - correlations
  |  |  |  | - plots
  |  | - frequentist_bayesian_plots
  |  |  | - plots
  |  | - frequentist_analysis
  |  |  | - model_diagnostics
  |  |  |  | - results
  |  |  |  | - plots
  |  |  | - lexical_covariates_selection
  |  |  |  | - results
  |  |  |  | - plots
  |  |  | - results
  |  |  | - plots&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Adhering to best practices—including the FAIR principles—in the creation of research materials enhances transparency, accessibility and reproducibility in scientific research. These standards facilitate researchers’ work beyond the short term, and increase the reliability of scientific work, thus contributing to the best use of resources.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Preprocessing the Norwegian Web as Corpus (NoWaC) in R</title>
      <link>https://pablobernabeu.github.io/2023/preprocessing-the-norwegian-web-as-corpus-nowac-in-r/</link>
      <pubDate>Thu, 28 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/preprocessing-the-norwegian-web-as-corpus-nowac-in-r/</guid>
      <description>


&lt;div id=&#34;the-present-script-can-be-used-to-pre-process-data-from-a-frequency-list-of-the-norwegian-as-web-corpus-nowac-guevara-2010.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The present script can be used to pre-process data from a frequency list of the Norwegian as Web Corpus (NoWaC; Guevara, 2010).&lt;/h3&gt;
&lt;p&gt;Before using the script, the frequency list should be downloaded from &lt;a href=&#34;https://www.hf.uio.no/iln/english/about/organization/text-laboratory/projects/nowac/nowac-frequency.html&#34;&gt;this URL&lt;/a&gt;. The list is described as ‘frequency list sorted primary alphabetic and secondary by frequency within each character’, and &lt;a href=&#34;https://www.tekstlab.uio.no/nowac/download/nowac-1.1.lemma.frek.sort_alf_frek.txt.gz&#34;&gt;this is the direct URL&lt;/a&gt;. The download requires signing in to an institutional network. Last, the downloaded file should be unzipped.&lt;/p&gt;
&lt;p&gt;The script is shown below.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fpreprocessing-NoWaC-Corpus-in-R%2Fblob%2Fmain%2Fpreprocessing_NoWaC_Corpus.R%23L19-L74&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Guevara, E. R. (2010). NoWaC: A large web-based corpus for Norwegian. In &lt;em&gt;Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop&lt;/em&gt; (pp. 1-7). &lt;a href=&#34;https://aclanthology.org/W10-1501&#34; class=&#34;uri&#34;&gt;https://aclanthology.org/W10-1501&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rodina, Y., &amp;amp; Westergaard, M. (2015). Grammatical gender in Norwegian: Language acquisition and language change. &lt;em&gt;Journal of Germanic Linguistics, 27&lt;/em&gt;(2), 145–187. &lt;a href=&#34;https://doi.org/10.1017/S1470542714000245&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1017/S1470542714000245&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rodina, Y., &amp;amp; Westergaard, M. (2021). Grammatical gender and declension class in language change: A study of the loss of feminine gender in Norwegian. &lt;em&gt;Journal of Germanic Linguistics, 33&lt;/em&gt;(3), 235–263. &lt;a href=&#34;https://doi.org/10.1017/S1470542719000217&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1017/S1470542719000217&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ggplotting power curves from the &lsquo;simr&rsquo; package</title>
      <link>https://pablobernabeu.github.io/2023/ggplotting-power-curves-from-simr-package/</link>
      <pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/ggplotting-power-curves-from-simr-package/</guid>
      <description>


&lt;p&gt;The R package ‘simr’ has greatly facilitated power analysis for mixed-effects models using Monte Carlo simulation (which involves running hundreds or thousands of tests under slight variations of the data). The &lt;code&gt;powerCurve&lt;/code&gt; function is used to estimate the statistical power for various sample sizes in one go. Since the tests are run serially, they can take a VERY long time; approximately, the time it takes to run the model supplied once (say, a few hours) &lt;em&gt;times&lt;/em&gt; the number of simulations (&lt;code&gt;nsim&lt;/code&gt;, which should be higher than 200), and &lt;em&gt;times&lt;/em&gt; the number of different sample sizes examined. While there isn’t a built-in parallel method, the power curves for different sample sizes can be run separately, and the results can be progressively combined as each component finishes running (see &lt;a href=&#34;https://pablobernabeu.github.io/2021/parallelizing-simr-powercurve&#34;&gt;tutorial&lt;/a&gt;). The power curves produced by &lt;code&gt;simr&lt;/code&gt; are so good they deserve ‘ggplot2’ rendering. So, here’s a function for it.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2FpowercurvePlot%2Fblob%2Fmain%2FpowercurvePlot.R%23L3-L82&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;a-usage-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A usage example&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
library(simr)
library(ggplot2)

# Toy model with data from &amp;#39;simr&amp;#39; package
fit = lmer(y ~ x + (x | g), data = simdata)

# Extend sample size of `g`
fit_extended_g = extend(fit, along = &amp;#39;g&amp;#39;, n = 12)

fit_powercurve = 
  powerCurve(fit_extended_g, fixed(&amp;#39;x&amp;#39;), 
             along = &amp;#39;g&amp;#39;, breaks = c(4, 6, 8, 10, 12), 
             nsim = 50, seed = 123, progress = FALSE)

# Read in custom function to ggplot results from simr::powerCurve

source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/powercurvePlot/main/powercurvePlot.R&amp;#39;)

powercurvePlot(fit_powercurve, number_x_axis_levels = 6) +
  
  # Change some defaults
  
  xlab(&amp;quot;Number of levels in &amp;#39;g&amp;#39;&amp;quot;) +
  
  theme(plot.title = element_blank(),
        axis.title.x = element_text(size = 18), 
        axis.title.y = element_text(size = 18), 
        axis.text = element_text(size = 17))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2023/ggplotting-power-curves-from-simr-package/index.en_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to visually assess the convergence of a mixed-effects model by plotting various optimizers</title>
      <link>https://pablobernabeu.github.io/2023/how-to-visually-assess-the-convergence-of-a-mixed-effects-model-by-plotting-various-optimizers/</link>
      <pubDate>Sat, 24 Jun 2023 16:42:34 +0200</pubDate>
      <guid>https://pablobernabeu.github.io/2023/how-to-visually-assess-the-convergence-of-a-mixed-effects-model-by-plotting-various-optimizers/</guid>
      <description>


&lt;p&gt;To assess whether convergence warnings render the results invalid, or on the contrary, the results can be deemed valid in spite of the warnings, &lt;a href=&#34;https://cran.r-project.org/web/packages/lme4/lme4.pdf&#34;&gt;Bates et al. (2023)&lt;/a&gt; suggest refitting models affected by convergence warnings with a variety of optimizers. The authors argue that, if the different optimizers produce practically-equivalent results, the results are valid. The &lt;code&gt;allFit&lt;/code&gt; function from the ‘lme4’ package allows the refitting of models using a number of optimizers. To use the seven optimizers listed above, two extra packages must be installed: ‘dfoptim’ and ‘optimx’ (see lme4 manual). The output from &lt;code&gt;allFit&lt;/code&gt; contains several statistics on the fixed and the random effects fitted by each optimizer.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
#&amp;gt; Loading required package: Matrix
library(dfoptim)
library(optimx)

# Create data using code by Ben Bolker from 
# https://stackoverflow.com/a/38296264/7050882

set.seed(101)
spin = runif(600, 1, 24)
reg = runif(600, 1, 15)
ID = rep(c(&amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,&amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;7&amp;quot;, &amp;quot;8&amp;quot;, &amp;quot;9&amp;quot;, &amp;quot;10&amp;quot;))
day = rep(1:30, each = 10)
testdata &amp;lt;- data.frame(spin, reg, ID, day)
testdata$fatigue &amp;lt;- testdata$spin * testdata$reg/10 * rnorm(30, mean=3, sd=2)

# Model
fit = lmer(fatigue ~ spin * reg + (1|ID),
           data = testdata, REML = TRUE)

# Refit model using all available algorithms
multi_fit = allFit(fit)
#&amp;gt; bobyqa : [OK]
#&amp;gt; Nelder_Mead : [OK]
#&amp;gt; nlminbwrap : [OK]
#&amp;gt; nmkbw : [OK]
#&amp;gt; optimx.L-BFGS-B : [OK]
#&amp;gt; nloptwrap.NLOPT_LN_NELDERMEAD : [OK]
#&amp;gt; nloptwrap.NLOPT_LN_BOBYQA : [OK]

# Show results 
summary(multi_fit)$fixef
#&amp;gt;                               (Intercept)      spin       reg  spin:reg
#&amp;gt; bobyqa                          -2.975678 0.5926561 0.1437204 0.1834016
#&amp;gt; Nelder_Mead                     -2.975675 0.5926559 0.1437202 0.1834016
#&amp;gt; nlminbwrap                      -2.975677 0.5926560 0.1437203 0.1834016
#&amp;gt; nmkbw                           -2.975678 0.5926561 0.1437204 0.1834016
#&amp;gt; optimx.L-BFGS-B                 -2.975680 0.5926562 0.1437205 0.1834016
#&amp;gt; nloptwrap.NLOPT_LN_NELDERMEAD   -2.975666 0.5926552 0.1437196 0.1834017
#&amp;gt; nloptwrap.NLOPT_LN_BOBYQA       -2.975678 0.5926561 0.1437204 0.1834016

# Read in function from GitHub
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/plot.fixef.allFit/main/plot.fixef.allFit.R&amp;#39;)

plot.fixef.allFit(multi_fit, 
                  
                  select_predictors = c(&amp;#39;spin&amp;#39;, &amp;#39;reg&amp;#39;, &amp;#39;spin:reg&amp;#39;), 
                  
                  # Increase padding at top and bottom of Y axis
                  multiply_y_axis_limits = 1.3,
                  
                  y_title = &amp;#39;Fixed effect (*b*)&amp;#39;)
#&amp;gt; Loading required package: dplyr
#&amp;gt; 
#&amp;gt; Attaching package: &amp;#39;dplyr&amp;#39;
#&amp;gt; The following objects are masked from &amp;#39;package:stats&amp;#39;:
#&amp;gt; 
#&amp;gt;     filter, lag
#&amp;gt; The following objects are masked from &amp;#39;package:base&amp;#39;:
#&amp;gt; 
#&amp;gt;     intersect, setdiff, setequal, union
#&amp;gt; Loading required package: reshape2
#&amp;gt; Loading required package: stringr
#&amp;gt; Loading required package: scales
#&amp;gt; Loading required package: ggplot2
#&amp;gt; Loading required package: ggtext
#&amp;gt; Loading required package: patchwork&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/XYQDug2.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# Alternative using plot-specific Y axes and other modified settings

plot.fixef.allFit(multi_fit, 
                  
                  select_predictors = c(&amp;#39;spin&amp;#39;, &amp;#39;spin:reg&amp;#39;), 
                  
                  # Use plot-specific Y axis limits
                  shared_y_axis_limits = FALSE,
                  
                  decimal_places = 7, 
                  
                  # Move up Y axis title
                  y_title_hjust = 4.5,
                  
                  y_title = &amp;#39;Fixed effect (*b*)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/BYXJYxM.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;Created on 2023-06-26 with &lt;a href=&#34;https://reprex.tidyverse.org&#34;&gt;reprex v2.0.2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Assigning participant-specific parameters automatically in OpenSesame</title>
      <link>https://pablobernabeu.github.io/2023/assigning-participant-specific-parameters-automatically-in-opensesame/</link>
      <pubDate>Wed, 17 May 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/assigning-participant-specific-parameters-automatically-in-opensesame/</guid>
      <description>


&lt;p&gt;OpenSesame offers options to counterbalance properties of the stimulus across participants. However, in cases of more involved assignments of session parameters across participants, it becomes necessary to write a bit of Python code in an inline script, which should be placed at the top of the timeline. In such a script, the participant-specific parameters are loaded in from a csv file. Below is a minimal example of the csv file.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;17%&#34; /&gt;
&lt;col width=&#34;22%&#34; /&gt;
&lt;col width=&#34;20%&#34; /&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;24%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;participant&lt;/th&gt;
&lt;th&gt;language&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;training_list&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;test_list&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;experiment_list&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;7&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;8&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Below is the corresponding inline script. The code &lt;code&gt;.iloc[0]&lt;/code&gt; at the end of the lines is used to select a cell.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Assign participant-specific parameters. For each participant-specific factor in the 
# stimulus files, take the level corresponding to the index position specified in the 
# participant parameters file.

import csv  # handle csv file
import pandas as pd  # handle data frames

participant_parameters = pd.read_csv(exp.get_file(&amp;#39;stimuli/parameters per participant.csv&amp;#39;))

var.participant = var.subject_nr

var.language = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;language&amp;#39;].iloc[0]

var.training_list = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;training_list&amp;#39;].iloc[0]

var.test_list = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;test_list&amp;#39;].iloc[0]

var.experiment_list = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;experiment_list&amp;#39;].iloc[0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, the variables created in the script can be used in &lt;kbd&gt;Run if&lt;/kbd&gt; conditions (e.g., &lt;code&gt;[language] == &#39;Mini-English&#39;&lt;/code&gt;), as replacements inside file names (e.g., &lt;code&gt;[language] training, List [training_list].csv&lt;/code&gt;), and as input in sketchpads (e.g., &lt;code&gt;Current list: [training_list]&lt;/code&gt;).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Specifying version number in OSF download links</title>
      <link>https://pablobernabeu.github.io/2023/specifying-version-number-in-osf-download-links/</link>
      <pubDate>Tue, 16 May 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/specifying-version-number-in-osf-download-links/</guid>
      <description>


&lt;p&gt;In the preparation of projects, files are often downloaded from OSF. It is good to document the URL addresses that were used for the downloads. These URLs can be provided in a code script (&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/data/semanticpriming_data_preparation.R#L294-L295&#34;&gt;see example&lt;/a&gt;) or in a README file. Better yet, it’s possible to specify the version of each file in the URL. This specification helps reduce the possibility of inaccuracies later, should any files be modified afterwards.&lt;/p&gt;
&lt;p&gt;The versions of files can be consulted on the right-hand side of the file page on OSF, as shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/OSF%20revisions.png&#34; width=&#34;550&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, the appropriate version can be specified in the download link by appending &lt;code&gt;?version=&lt;/code&gt;&lt;strong&gt;X&lt;/strong&gt; at the end. For instance, the seventh version is specified in the link &lt;a href=&#34;https://osf.io/hx6tz/download?version=7&#34;&gt;https://osf.io/hx6tz/download&lt;code&gt;?version=7&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Covariates are necessary to validate the variables of interest and to prevent bogus theories</title>
      <link>https://pablobernabeu.github.io/2023/covariates-are-necessary-to-validate-the-variables-of-interest-and-to-prevent-bogus-theories/</link>
      <pubDate>Mon, 15 May 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/covariates-are-necessary-to-validate-the-variables-of-interest-and-to-prevent-bogus-theories/</guid>
      <description>


&lt;p&gt;The need for covariates—or &lt;em&gt;nuisance variables&lt;/em&gt;—in statistical analyses is twofold. The first reason is purely statistical and the second reason is academic.&lt;/p&gt;
&lt;p&gt;First, the use of covariates is often necessary when the variable(s) of interest in a study may be connected to, and affected by, some satellite variables (Bottini et al., 2022; Elze et al., 2017; Sassenhagen &amp;amp; Alday, 2016). This complex scenario is the most common one due to the multivariate, dynamic, interactive nature of the real world.&lt;/p&gt;
&lt;p&gt;Second, the use of covariates is often necessary to prevent the development of bogus, redundant theories. Academics are strongly rewarded for developing theories. As we know, wherever there are strong rewards, there are serious risks. An academic could—consciously or not—produce a theory that is too closely related to an existing theory. So closely related are these theories that the second version might not warrant a name of its own. In such a scenario, covariates are useful and indeed necessary to vet the unique nature of the second version. That is, the first and the second version must be tested in the same model, and the variables corresponding to the first version can be construed as &lt;em&gt;covariates&lt;/em&gt;. This allows both the developers of the theories and the readers to compare the effects corresponding to each version of the theory, and to assess the degree of separation between them.&lt;/p&gt;
&lt;p&gt;The perverted use of covariates (Stefan &amp;amp; Schönbrodt, 2023)—however frequent and harmful—stands completely orthogonal to the correct usage of covariates, in the same way that a stethoscope can be used for good or for bad purposes. It would be poorly informed and misleading to conflate the correct and the incorrect uses, or to reject the use of covariates altogether due to the incorrect uses.&lt;/p&gt;
&lt;p&gt;In conclusion, the effects of interest in correlational/observational studies can be subject to mediation and moderation by satellite variables. These variables cannot be manipulated in correlational/observational studies, but they can—and often should—be included as covariates in the statistical models, to ward off spurious results and to vet similar theories.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Bottini, R., Morucci, P., D’Urso, A., Collignon, O., &amp;amp; Crepaldi, D. (2022). The concreteness advantage in lexical decision does not depend on perceptual simulations. &lt;em&gt;Journal of Experimental Psychology: General, 151&lt;/em&gt;(3), 731–738. &lt;a href=&#34;https://doi.org/10.1037/xge0001090&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/xge0001090&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Elze, M. C., Gregson, J., Baber, U., Williamson, E., Sartori, S., Mehran, R., Nichols, M., Stone, G. W., &amp;amp; Pocock, S. J. (2017). Comparison of propensity score methods and covariate adjustment: Evaluation in 4 cardiovascular studies. &lt;em&gt;Journal of the American College of Cardiology, 69&lt;/em&gt;(3), 345-357. &lt;a href=&#34;https://doi.org/10.1016/j.jacc.2016.10.060&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.jacc.2016.10.060&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Sassenhagen, J., &amp;amp; Alday, P. M. (2016). A common misapplication of statistical inference: Nuisance control with null-hypothesis significance tests. &lt;em&gt;Brain and Language, 162&lt;/em&gt;, 42-45. &lt;a href=&#34;https://doi.org/10.1016/j.bandl.2016.08.001&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.bandl.2016.08.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Stefan, A. M., &amp;amp; Schönbrodt, F. D. (2023). Big little lies: A compendium and simulation of p-hacking strategies. &lt;em&gt;Royal Society Open Science, 10&lt;/em&gt;(2), 220346. &lt;a href=&#34;https://doi.org/10.1098/rsos.220346&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1098/rsos.220346&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Brief Clarifications, Open Questions: Commentary on Liu et al. (2018)</title>
      <link>https://pablobernabeu.github.io/2021/brief-clarifications-open-questions-commentary-on-liu-et-al-2018/</link>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2021/brief-clarifications-open-questions-commentary-on-liu-et-al-2018/</guid>
      <description>&lt;p&gt;Liu et al. (2018) present a study that implements the conceptual modality switch (CMS) paradigm, which has been used to investigate the modality-specific nature of conceptual representations (Pecher et al., 2003). Liu et al.&amp;lsquo;s experiment uses event-related potentials (ERPs; similarly, see Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013). In the design of the switch conditions, the experiment implements a corpus analysis to distinguish between purely-embodied modality switches and switches that are more liable to linguistic bootstrapping (also see Bernabeu et al., 2017; Louwerse &amp;amp; Connell, 2011). The procedure for stimulus selection was novel as well as novel; thus, it could prove useful in future studies, too. In addition, the application of bayesian statistics is an interesting and promising novelty in the present research area.&lt;/p&gt;
&lt;p&gt;In reviewing the literature, Liu (2018) and Liu et al. (2018) contend that previous studies may be strongly biased due to methodological decisions in the analysis of ERPs. These decisions particularly regard the latency&amp;mdash;i.e., time windows&amp;mdash;and the topographic regions of interest&amp;mdash;i.e., subsets of electrodes. Thus, Liu et al. identify a &amp;lsquo;highly inconsistent&amp;rsquo; (p. 6) landscape in the ERP components that have been ascribed to the CMS effect in previous studies. Similarly, Liu (p. 47) writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Several studies have looked for the ERP manifestations of modality switching costs (Bernabeu, Willems, &amp;amp; Louwerse, 2017; Collins, Pecher, Zeelenberg, &amp;amp; Coulson, 2011; Hald, Hocking, Vernon, Marshall, &amp;amp; Garnham, 2013; Hald, Marshall, Janssen, &amp;amp; Garnham, 2011). However, what they found was not a clear picture. Not only was a significant effect found in the time window for the N400 component, but also a so-called early N400-like effect around 300ms (Bernabeu et al., 2017; Hald et al., 2011), the N1-P2 complex around 200ms (Bernabeu et al., 2017; Hald et al., 2013, 2011), as well as the late positivity component (LPC) after 600ms (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;drastic-conclusions&#34;&gt;Drastic conclusions&lt;/h3&gt;
&lt;p&gt;Liu et al. (2018) conclude that a confirmatory research approach is not warranted, and adopt a semi-exploratory approach, creating time windows of 50 ms each, rather than windows linked to known ERP components (Swaab et al., 2012), and using bayesian statistics. Such a drastic conclusion appears to stem from the assumption that, if the CMS were robust enough, it would present in the same guise across studies. Such an assumption, however, may merit further examination, considering the multiplicity of known and unknown variables that may differ across experiments (Barsalou, 2019). This variability influences the &lt;em&gt;replication praxis&lt;/em&gt;, as it were. Indeed, Liu et al. themselves allude to one such variable (p. 7).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;These previous studies did not only examine the effect of modality switching but also other linguistic factors such as negated sentences, which could easily distort observed waveforms (Luck, 2005).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu et al.&amp;lsquo;s (2018) conclusions about the &amp;lsquo;inconsistent&amp;rsquo; results do not seem to duly weigh the differences across studies. None of the existing studies is a direct replication of another. The example quoted from Liu et al. above, regarding the presence of negated sentences, is one of the less important differences because one of the corresponding studies implemented the negation as a controlled, experimental condition, contrasting with affirmative sentences (Hald et al., 2013). Yet, other differences exist in virtually every aspect, from the types of modality switch used to the time-locking of ERPs. For instance, the studies vary in the implementation of the modality switches. Whereas Hald et al. (2011) distinguish between a switch and a non-switch condition, Bernabeu et al. (2017) analysed each type of switch separately&amp;mdash;i.e., auditory-to-visual, haptic-to-visual, visual-to-visual. Another difference across the studies is the onset point for ERPs. For instance, Bernabeu et al. time-locked ERPs to the point at which the modality switch is actually elicited in the CMS paradigm&amp;mdash;namely, the first word in target trials. In contrast, the other studies time-locked ERPs to the second word. In addition, these studies vary in their timelines&amp;mdash;i.e., presentation of words and inter-stimulus intervals&amp;mdash;, as well as in the words that were used as stimuli, in the preprocessing of ERPs, in the statistical analysis, and even in the language of testing in one case (all studies using English except Bernabeu et al., 2017, which used Dutch). Moreover, the studies differ in the sample size, ranging from ten finally-analysed participants (Hald et al, 2011) to 46 finally-analysed participants (Bernabeu et al., 2017). Last, the studies differ in the number of items per modality switch condition, ranging from 17 (in one of Liu et al.&amp;lsquo;s, 2018 conditions) to 40 per condition (Hald et al., 2011, 2013). Undoubtedly, seeing larger sample sizes and more stimulus items used in ERP studies is something to promote and celebrate. Last, it may be noted that Liu et al.&amp;lsquo;s stance on the inconsistency of previous results starkly contrasts with their use of a single study&amp;mdash;Hald et al. (2011)&amp;mdash;, with &lt;em&gt;N&lt;/em&gt; = 10, as the motivation for their sample size (Albers &amp;amp; Lakens, 2018).&lt;/p&gt;
&lt;h3 id=&#34;clarifications&#34;&gt;Clarifications&lt;/h3&gt;
&lt;p&gt;Liu et al. (2018) apply bayesian statistics reportedly to help reduce the bias that may exist in the present literature. However, the reviews by Liu (2018) and Liu et al. appear to gloss over some important aspects regarding previous studies. For instance, Liu writes (p. 53):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The inflation of the probability of Type I error leads to an over-confidence in the interpretation of the results. For example, in the studies on modality switching costs, different time windows were chosen to test the early effect of modality switching costs. While Bernabeu et al. (2017), Hald et al. (2011) and Hald et al. (2013) examined the segment of ERP waveform between 190ms and 300ms or 160ms and 215ms based on visual inspection and found significant effects, Collins et al. (2011) chose a prescribed time window between 100ms and 200ms before the analysis and did not find the effect.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu also refers to the issue of multiple tests related to the multiple time windows and electrodes we find in ERP studies (p. 59):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It is typical for ERP studies to conduct multiple comparisons (e.g., running the same ANOVA repeatedly on different subsets of data like different time windows and different groups of electrodes). This would massively increase Type I error if no post hoc correction is conducted. However, if Bonferroni or other correction is conducted, it will render the study over-conservative, thus increasing the chance of Type II error. In the present thesis, 90 electrodes will be analysed individually, with 20 time slices in each trial. That results in 1800 NHSTs for each critical variable. A correction of multiple comparison will require a critical level of 2.78 x 10ˆ-5 for each test for a family-wise critical level of .05 (and an uncorrected test will almost definitely lead to false positive results). This stringent criterion could conceivably render it meaningless any p-values we can obtain from a statistical package.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Arguably, the scenario presented by Liu (2018), in which a researcher could conduct a purely data-driven analysis of ERP data, is extreme. The field of psycholinguistics, in general, does not have a tradition of purely data-driven analysis. Instead, it blends a humanistic background with a scientific methodology. As a result, the hypotheses and methods tend to be largely driven by the available literature. For instance, Bernabeu et al. (2017, quoted below from p. 1632) based their time windows and regions of interest on the most relevant of the preceding studies (also see Bernabeu, 2017).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Electrodes were divided into an anterior and a posterior area (also done in Hald et al., 2011). Albeit a superficial division, we found it sufficient for the research question. Time windows were selected as in Hald et al., except for the last window, which was extended up to 750 ms post word onset, instead of 700 ms, because the characteristic component of that latency tends to extend until then, as we confirmed by visual inspection of these results.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The literature-based approach follows the advice from Luck and Gaspelin (2017, p. 149), who wrote: &amp;lsquo;a researcher who wants to avoid significant but bogus effects would be advised to focus on testing a priori predictions without using the observed data to guide the selection of time windows or electrode sites&amp;rsquo; (also see Armstrong, 2014; for a more conservative stance, see Luck &amp;amp; Gaspelin, 2017). In addition, notice that the extension of the last window by 50 ms was informed by Swaab et al. (2012), who report results by which the P600 component (the main component occurring after the N400 in word reading) extended up to 800 ms.&lt;/p&gt;
&lt;p&gt;Next, Liu et al. (2018, pp. 6&amp;ndash;7) write:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;However, the findings of these components have been highly inconsistent. The N400 effect alone was found in the posterior region in some cases (Bernabeu et al., 2017; Hald et al., 2013), while in anterior region in others (Collins et al., 2011, Hald et al. (2011)).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yet, Bernabeu et al. (2017, p. 1632) had written:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In certain parts over the time course, the effect appeared in both anterior and posterior areas&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu et al. (2018) continue (p. 7):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In some cases, it was found in the typical window around 400ms (Collins et al., 2011), while in others an earlier window from 270ms to 370ms (Bernabeu et al., 2017; Hald et al., 2011).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, Bernabeu et al. (2017, p. 1632) had written:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The ERP results revealed a CMS effect from Time Window 1 on, larger after 350 ms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Regarding the time-locking of ERPs, Liu (2018, p. 43) writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Because the properties were usually salient for the concepts, the switching costs might have already been incurred when participants were processing the concept word. Bernabeu et al. (2017), in their recent replication of previous ERP studies, reversed the order of concept and property and did not find an immediate effect from the property onset. In future studies, it is recommended to adopt the reverse order, control the concept words so that they do not automatically activate the properties before the words are shown, or analyse epochs after both the concept and property words.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above excerpt seems to reveal a misunderstanding of Bernabeu et al.&amp;lsquo;s (2017) method and results (we may assume that, by &amp;lsquo;immediate&amp;rsquo;, Liu (2018) is referring to the 200 ms point or afterwards, since that is about as immediate as it gets; see Amsel et al., 2014; Swaab et al., 2012; Van Dam et al., 2014). Bernabeu et al.&amp;lsquo;s abstract mentioned (p. 1629):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Event-Related Potentials (ERPs) were time-locked to the onset of the first word (property) in the target trials so as to measure the effect online and to avoid a within-trial confound. A switch effect was found, characterized by more negative ERP amplitudes for modality switches than no-switches. It proved significant in four typical time windows from 160 to 750 milliseconds post word onset, with greater strength in posterior brain regions, and after 350 milliseconds.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Indeed, time-locking ERPs to the beginning of the trial was one of the principal features of Bernabeu et al.&amp;lsquo;s (2017) experiment. Complementing that, the property words were placed in the first trial because they are more perceptually loaded than concepts (Lynott &amp;amp; Connell, 2013), thus better suiting the main basis of the CMS paradigm. We know that the semantic processing of a word often commences within the first 200 ms (Amsel et al., 2014; Van Dam et al., 2014). Considering the importance of the time course in the grounding of conceptual representations (Hauk, 2016), it seems important to measure the CMS from the moment that it is elicited&amp;mdash;namely, in all experiments, from the first word of the target trial (Bernabeu, 2017; Bernabeu et al., 2017), rather than letting several hundreds of milliseconds elapse. Nonetheless, from a methodological perspective, it would be interesting to compare the two approaches in a dedicated study. This would precisely reveal the speed at which modality-specific meaning becomes activated during conceptual processing.&lt;/p&gt;
&lt;h3 id=&#34;outstanding-issues-random-effects-and-correction-for-multiple-tests&#34;&gt;Outstanding issues: Random effects and correction for multiple tests&lt;/h3&gt;
&lt;p&gt;A methodological issue affecting the statistical analysis of all the studies hereby considered (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013; Liu et al., 2018) is the absence of some applicable random effects. Some of the studies did not apply any random effects (Collins et al., 2011; Hald et al., 2011, 2013). Even in psycholinguistics, use of linear mixed-effects models is still increasing (Meteyard &amp;amp; Davies, 2020; Yarkoni, 2020). Yet, in those studies that did apply random effects, the corresponding  structure was not as exhaustive as it should have been, as they lacked random slopes. In Bernabeu et al. (2017), a model selection approach was applied (Matuschek et al., 2017), whereby each random effect was tested and only kept in the model if it significantly improved the fit. In Liu et al. (2018), random slopes were deemed unfeasible due to computational constraints (for background, see Brauer &amp;amp; Curtin, 2017). Applying a complete random effects structure is important for a robust statistical analysis (Barr et al., 2013; Yarkoni, 2020).&lt;/p&gt;
&lt;p&gt;Another issue is that of multiple tests. Where a small number of levels is used (e.g., time windows, topographic regions of interest) and these are informed by the literature, the advice has often been ambiguous as to whether a correction should be applied (e.g., Armstrong, 2014; for a more conservative stance, see Luck &amp;amp; Gaspelin, 2017). Indeed, no correction was applied in any of the four studies that used frequentist statistics (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013).&lt;/p&gt;
&lt;h4 id=&#34;reanalysis-of-bernabeu-et-al-2017&#34;&gt;Reanalysis of Bernabeu et al. (2017)&lt;/h4&gt;
&lt;p&gt;The results of Bernabeu et al. (2017) were reanalysed after publication using a more complete random effects structure that incorporated by-participant random slopes for the modality-switch factor&amp;mdash;i.e., &lt;code&gt;(condition | participant)&lt;/code&gt;. The results were also corrected for multiple tests using the Holm-Bonferroni correction (Holm, 1979). For this purpose, the lowest p-value in the four time windows was multiplied by 4, the next p-value by 3, the next by 2, and the highest p-value was left as it unmodified. In this stepwise correction, if a nonsignificant p-value was reached, all the subsequent p-values became nonsignificant (see &lt;a href=&#34;https://osf.io/unvfs/&#34;&gt;analysis script&lt;/a&gt;). The &lt;a href=&#34;https://osf.io/qhe5s/&#34;&gt;results&lt;/a&gt; differed from the original, slopes-free models (available &lt;a href=&#34;https://osf.io/sx3nw/&#34;&gt;script&lt;/a&gt; and &lt;a href=&#34;https://osf.io/4v38d/&#34;&gt;results&lt;/a&gt;) in that the main effect of the modality switch factor became nonsignificant in the second time window (160&amp;ndash;216 ms) and in the fourth one (500&amp;ndash;750 ms), while remaining significant in the third time window (350&amp;ndash;550 ms). Incidentally, note that main effects may not be directly interpretable as the same same variables are present interactions (Kam &amp;amp; Franzese, 2007). Yet, the interactions of modality switch with participant group (quick/slow) and scalp location (anterior/posterior) retained the same significance.&lt;/p&gt;
&lt;h3 id=&#34;open-questions&#34;&gt;Open questions&lt;/h3&gt;
&lt;p&gt;Liu (2018) and Liu et al. (2018) raise interesting and important questions. Firstly, future research may be conducted to investigate what determines the variability of ERP results&amp;mdash;in terms of ERP components, time windows and topographic regions of interest. This research could include a comparison with other measurements, such as response times, to test whether ERPs are less reliable&amp;mdash;i.e., more variable across studies&amp;mdash;than response times. Similarly, future research may investigate whether the ERP literature is more biased than literature employing other measures, such as response times. In addition, future research could investigate whether moving to exploratory, bayesian research designs is a necessary or sufficient condition to reduce bias in research and improve the precision of experimental measurements. Current alternatives to such an approach include direct (or conceptual) replications designed to achieve a higher power than previous studies (e.g., Chen et al., 2019). Arguably, policies determining funding decisions would need to change if we are to fully acknowledge the importance of &lt;em&gt;direct&lt;/em&gt; replication (Howe &amp;amp; Perfors, 2018; Kunert, 2016; Simons, 2014; Zwaan et al., 2018). Last, future research may investigate whether &amp;lsquo;clear picture&amp;rsquo; results are realistic, desirable or necessary, and whether unclear-picture results should be eschewed; or whether, on the contrary, clear-picture results may largely be the product of publication bias&amp;mdash;that is, the pressure to hide or misreport those aspects of a study that could challenge its acceptance by peer-reviewers or any other academics.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;div class = &#39;hanging-indent&#39;&gt;
&lt;p&gt;Albers, C., &amp;amp; Lakens, D. (2018). When power analyses based on pilot data are biased: Inaccurate effect size estimators and follow-up bias. &lt;em&gt;Journal of Experimental Social Psychology, 74&lt;/em&gt;, 187–195. &lt;a href=&#34;https://doi.org/10.1016/j.jesp.2017.09.004&#34;&gt;https://doi.org/10.1016/j.jesp.2017.09.004&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Amsel, B. D., Urbach, T. P., &amp;amp; Kutas, M. (2014). Empirically grounding grounded cognition: the case of color. &lt;em&gt;Neuroimage, 99&lt;/em&gt;, 149-157. &lt;a href=&#34;https://doi.org/10.1016/j.neuroimage.2014.05.025&#34;&gt;https://doi.org/10.1016/j.neuroimage.2014.05.025&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Armstrong, R. A. (2014). When to use the Bonferroni correction. &lt;em&gt;Ophthalmic and Physiological Optics, 34&lt;/em&gt;(5), 502-508. &lt;a href=&#34;https://doi.org/10.1111/opo.12131&#34;&gt;https://doi.org/10.1111/opo.12131&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barr, D. J., Levy, R., Scheepers, C., &amp;amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. &lt;em&gt;Journal of Memory and Language, 68&lt;/em&gt;, 255–278. &lt;a href=&#34;http://dx.doi.org/10.1016/j.jml.2012.11.001&#34;&gt;http://dx.doi.org/10.1016/j.jml.2012.11.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barsalou, L. W. (2019). Establishing generalizable mechanisms. &lt;em&gt;Psychological Inquiry, 30&lt;/em&gt;(4), 220-230. &lt;a href=&#34;https://doi.org/10.1080/1047840X.2019.1693857&#34;&gt;https://doi.org/10.1080/1047840X.2019.1693857&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P. (2017). &lt;em&gt;Modality switches occur early and extend late in conceptual processing: evidence from ERPs&lt;/em&gt; [Master&#39;s thesis]. School of Humanities, Tilburg University. &lt;a href=&#34;https://psyarxiv.com/5gjvk&#34;&gt;https://psyarxiv.com/5gjvk&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P., Willems, R. M., &amp;amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, &amp;amp; E. J. Davelaar (Eds.), &lt;em&gt;Proceedings of the 39th Annual Conference of the Cognitive Science Society&lt;/em&gt; (pp. 1629-1634). Austin, TX: Cognitive Science Society. &lt;a href=&#34;https://doi.org/10.31234/osf.io/a5pcz&#34;&gt;https://doi.org/10.31234/osf.io/a5pcz&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chen, S., Szabelska, A., Chartier, C. R., Kekecs, Z., Lynott, D., Bernabeu, P., … Schmidt, K. (2018). &lt;em&gt;Investigating object orientation effects across 14 languages&lt;/em&gt;. PsyArXiv. &lt;a href=&#34;https://doi.org/10.31234/osf.io/t2pjv/&#34;&gt;https://doi.org/10.31234/osf.io/t2pjv/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Collins, J., Pecher, D., Zeelenberg, R., &amp;amp; Coulson, S. (2011). Modality switching in a property verification task: an ERP study of what happens when candles flicker after high heels click. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2011.00010&#34;&gt;https://doi.org/10.3389/fpsyg.2011.00010&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hald, L. A., Hocking, I., Vernon, D., Marshall, J.-A., &amp;amp; Garnham, A. (2013). Exploring modality switching effects in negated sentences: further evidence for grounded representations. &lt;em&gt;Frontiers in Psychology, 4&lt;/em&gt;, 93. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2013.00093&#34;&gt;https://doi.org/10.3389/fpsyg.2013.00093&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hald, L. A., Marshall, J.-A., Janssen, D. P., &amp;amp; Garnham, A. (2011). Switching modalities in a sentence verification task: ERP evidence for embodied language processing. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2011.00045&#34;&gt;https://doi.org/10.3389/fpsyg.2011.00045&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hauk, O. (2016). Only time will tell–why temporal information is essential for our neuroscientific understanding of semantics. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;(4), 1072-1079. &lt;a href=&#34;https://doi.org/10.3758/s13423-015-0873-9&#34;&gt;https://doi.org/10.3758/s13423-015-0873-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Holm, S. (1979). A simple sequentially rejective multiple test procedure. &lt;em&gt;Scandinavian Journal of Statistics, 6&lt;/em&gt;, 65-70. &lt;a href=&#34;http://www.jstor.org/stable/4615733&#34;&gt;http://www.jstor.org/stable/4615733&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Howe, P. D., &amp;amp; Perfors, A. (2018). An argument for how (and why) to incentivise replication. &lt;em&gt;Behavioral and Brain Sciences, 41&lt;/em&gt;, e135-e135. &lt;a href=&#34;http://dx.doi.org/10.1017/S0140525X18000705&#34;&gt;http://dx.doi.org/10.1017/S0140525X18000705&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kam, C. D., &amp;amp; Franzese, R. J. (2007). &lt;em&gt;Modeling and interpreting interactive hypotheses in regression analysis&lt;/em&gt;. Ann Arbor, MI: University of Michigan Press.&lt;/p&gt;
&lt;p&gt;Kunert, R. (2016). Internal conceptual replications do not increase independent replication success. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;(5), 1631-1638. &lt;a href=&#34;https://doi.org/10.3758/s13423-016-1030-9&#34;&gt;https://doi.org/10.3758/s13423-016-1030-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, P. (2018). &lt;em&gt;Embodied-linguistic conceptual representations during metaphor processing&lt;/em&gt;. Doctoral thesis, Lancaster University, UK. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/489&#34;&gt;https://doi.org/10.17635/lancaster/thesis/489&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, P., Lynott, D., &amp;amp; Connell, L. (2018). Continuous neural activations of simulation-linguistic representations in modality switching costs. In P. Liu, &lt;em&gt;Embodied-linguistic conceptual representations during metaphor processing&lt;/em&gt;. Doctoral thesis, Lancaster University, UK. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/489&#34;&gt;https://doi.org/10.17635/lancaster/thesis/489&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Luck, S. J. (2005). Ten simple rules for designing ERP experiments. In T. C. Handy (Ed.), &lt;em&gt;Event-related potentials: A methods handbook&lt;/em&gt;.
MIT Press.&lt;/p&gt;
&lt;p&gt;Luck, S. J., &amp;amp; Gaspelin, N. (2017). How to get statistically significant effects in any ERP experiment (and why you shouldn&#39;t). &lt;em&gt;Psychophysiology, 54&lt;/em&gt;(1), 146-157. &lt;a href=&#34;https://doi.org/10.1111/psyp.12639&#34;&gt;https://doi.org/10.1111/psyp.12639&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., &amp;amp; Bates, D. (2017). Balancing type 1 error and power in linear mixed models. &lt;em&gt;Journal of Memory and Language, 94&lt;/em&gt;, 305–315. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2017.01.001&#34;&gt;https://doi.org/10.1016/j.jml.2017.01.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meteyard, L., &amp;amp; Davies, R. A. (2020). Best practice guidance for linear mixed-effects models in psychological science. &lt;em&gt;Journal of Memory and Language, 112&lt;/em&gt;, 104092. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2020.104092&#34;&gt;https://doi.org/10.1016/j.jml.2020.104092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pecher, D., Zeelenberg, R., &amp;amp; Barsalou, L. W. (2003). Verifying different-modality properties for concepts produces switching costs. &lt;em&gt;Psychological Science, 14&lt;/em&gt;, 2, 119-24. &lt;a href=&#34;https://doi.org/10.1111/1467-9280.t01-1-01429&#34;&gt;https://doi.org/10.1111/1467-9280.t01-1-01429&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Simons, D. J. (2014). The value of direct replication. &lt;em&gt;Perspectives on Psychological Science, 9&lt;/em&gt;(1), 76–80. &lt;a href=&#34;https://doi.org/10.1177/1745691613514755&#34;&gt;https://doi.org/10.1177/1745691613514755&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Swaab, T. Y., Ledoux, K., Camblin, C. C., &amp;amp; Boudewyn, M. A. (2012). Language-related ERP components. In S. J. Luck &amp;amp; E. S. Kappenman (Eds.), &lt;em&gt;Oxford handbook of event-related potential components&lt;/em&gt; (pp. 397–440). Oxford University Press. &lt;a href=&#34;https://doi.org/10.1093/oxfordhb/9780195374148.013.0197&#34;&gt;https://doi.org/10.1093/oxfordhb/9780195374148.013.0197&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Van Dam, W. O., Brazil, I. A., Bekkering, H., &amp;amp; Rueschemeyer, S.-A. (2014). Flexibility in embodied language processing: context effects in lexical access. &lt;em&gt;Topics in Cognitive Science, 6&lt;/em&gt;(3), 407–424. &lt;a href=&#34;https://doi.org/10.1111/tops.12100&#34;&gt;https://doi.org/10.1111/tops.12100&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yarkoni, T. (2020). The generalizability crisis. &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt;, 1-37. &lt;a href=&#34;https://doi.org/10.1017/S0140525X20001685&#34;&gt;https://doi.org/10.1017/S0140525X20001685&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zwaan, R., Etz, A., Lucas, R., &amp;amp; Donnellan, M. (2018). Making replication mainstream. &lt;em&gt;Behavioral and Brain Sciences, 41&lt;/em&gt;, E120. &lt;a href=&#34;https://doi.org/10.1017/S0140525X17001972&#34;&gt;https://doi.org/10.1017/S0140525X17001972&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Web application for the simulation of experimental data</title>
      <link>https://pablobernabeu.github.io/applications-and-dashboards/experimental-data-simulation/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/applications-and-dashboards/experimental-data-simulation/</guid>
      <description>


&lt;div style=&#34;font-size: 25px; color: #614064; padding-top: 15px; padding-bottom: 10px;&#34;&gt;
&lt;i class=&#34;fas fa-chalkboard-teacher fa-xs&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt; &lt;i class=&#34;fas fa-university fa-xs&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;  Purposes
&lt;/div&gt;
&lt;p&gt;This open-source, R-based web application is suitable for educational and research purposes in experimental and quantitative sciences. It allows the &lt;strong&gt;creation of varied data sets with specified structures, such as between-group and within-participant variables, that can be categorical or continuous.&lt;/strong&gt; These parameters can be set throughout the various tabs (sections) from the top menu. In the last tab, the data set can be downloaded. The benefits of this application include time-saving and flexibility in the control of parameters.&lt;/p&gt;
&lt;div id=&#34;guidelines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guidelines&lt;/h3&gt;
&lt;p&gt;General guidelines include the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;In the names of variables&lt;/strong&gt;, it’s recommended only to use alphanumeric characters and underscore signs. The latter can be used to separate characters or words (e.g., &lt;em&gt;variable_name&lt;/em&gt;). Different names should be used for each variable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;In the levels of categorical variables&lt;/strong&gt;, alphanumeric, special characters and spaces are allowed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;In numeric fields&lt;/strong&gt; (e.g., ‘Mean’, ‘Standard deviation’, ‘Relative probability [0, 1]’), only numbers and decimal points are allowed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;As the data set increases&lt;/strong&gt;, so does the processing time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More specific guidelines are available in each section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;globe_with_meridians-the-web-application-can-be-launched-here.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;🌐  The web application can be &lt;a href=&#34;https://pablo-bernabeu.shinyapps.io/experimental-data-simulation/&#34;&gt;launched here&lt;/a&gt;.&lt;/h3&gt;
&lt;div style=&#34;padding-top:8px; padding-bottom:2px; margin-bottom:-20px; color:#665F5F;&#34;&gt;
Screenshot of the &lt;em&gt;Dependent&lt;/em&gt; tab (&lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/raw/master/Screenshot.png&#34;&gt;view larger&lt;/a&gt;)
&lt;/div&gt;
&lt;p&gt;&lt;img style=&#34;max-width: 800px; display: block; margin-left: auto; margin-right: auto; padding-bottom: 15px;&#34; src=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/raw/master/Screenshot.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;p&gt;Bernabeu, P., &amp;amp; Lynott, D. (2020). &lt;em&gt;Web application for the simulation of experimental data&lt;/em&gt; (Version 1.4). &lt;a href=&#34;https://github.com/pablobernabeu/Experiment-simulation-app/&#34;&gt;https://github.com/pablobernabeu/Experiment-simulation-app/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Code&lt;/h3&gt;
&lt;p&gt;This web application was developed in &lt;a href=&#34;https://www.r-project.org/about.html&#34;&gt;R&lt;/a&gt; (R Core Team, 2020). The code is &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/blob/master/index.Rmd&#34;&gt;available on Github&lt;/a&gt;, where contributions may be made. The initial code for this application was influenced by Section 5.7 (&lt;a href=&#34;https://crumplab.github.io/programmingforpsych/simulating-and-analyzing-data-in-r.html#simulating-data-for-multi-factor-designs&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Simulating data for multi-factor designs&lt;/em&gt;&lt;/a&gt;) in Crump (2017). The R packages used include ‘dplyr’ (Wickham, François, Henry, &amp;amp; Müller, 2018), ‘DT’ (Xie, 2020), ‘flexdashboard’ (Iannone, Allaire, &amp;amp; Borges, 2020), ‘shiny’ (Chang, Cheng, Allaire, Xie, &amp;amp; McPherson, 2020) and ‘stringr’ (Wickham, 2019).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;options-for-development-and-local-use-of-the-app&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Options for development and local use of the app&lt;/h3&gt;
&lt;div id=&#34;option-a-using-local-rrstudio-or-rstudio-cloud-project-or-binder-rstudio-environment&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Option A) Using local R/RStudio or &lt;a href=&#34;https://rstudio.cloud/project/1739958&#34;&gt;RStudio Cloud project&lt;/a&gt; or &lt;a href=&#34;https://mybinder.org/v2/gh/pablobernabeu/Experimental-data-simulation/master?urlpath=rstudio&#34;&gt;Binder RStudio environment&lt;/a&gt;&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;[Step only necessary in R/RStudio] Install the packages in the versions used in the &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/releases&#34;&gt;latest release of this application&lt;/a&gt;, by running:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;#39;devtools&amp;#39;)
library(devtools)
install_version(&amp;#39;dplyr&amp;#39;, &amp;#39;1.0.2&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;DT&amp;#39;, &amp;#39;0.15&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;flexdashboard&amp;#39;, &amp;#39;0.5.2&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;htmltools&amp;#39;, &amp;#39;0.5.0&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;knitr&amp;#39;, &amp;#39;1.30&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;ngram&amp;#39;, &amp;#39;3.0.4&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;purrr&amp;#39;, &amp;#39;0.3.4&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;shiny&amp;#39;, &amp;#39;1.5.0&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;stringr&amp;#39;, &amp;#39;1.4.0&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;tidyr&amp;#39;, &amp;#39;1.1.2&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open the &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/blob/master/index.Rmd&#34;&gt;index.Rmd&lt;/a&gt; script.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the application by clicking on &lt;kbd&gt;▶️ Run document&lt;/kbd&gt; at the top left, or by running &lt;code&gt;rmarkdown::run(&#39;index.Rmd&#39;)&lt;/code&gt; in the console.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Click on &lt;kbd&gt;Open in Browser&lt;/kbd&gt; at the top left.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;option-b-using-dockerfile-see-instructions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Option B) Using Dockerfile (&lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation#option-b-using-dockerfile-vsochs-pull-request&#34;&gt;see instructions&lt;/a&gt;)&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgements&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Acknowledgements&lt;/h3&gt;
&lt;p&gt;Thank you to RStudio for the free hosting server used by this application, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;shinyapps.io&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div style=&#34;text-indent:-1.5em; margin-left:1.5em;&#34;&gt;
&lt;p&gt;Chang, W., Cheng, J., Allaire, J., Xie, Y., &amp;amp; McPherson, J. (2020). shiny: Web Application Framework for R. R package version 1.4.0. Available at &lt;a href=&#34;http://CRAN.R-project.org/package=shiny&#34;&gt;http://CRAN.R-project.org/package=shiny&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Crump, M. J. C. (2017). Programming for Psychologists: Data Creation and Analysis (Version 1.1). &lt;a href=&#34;https://crumplab.github.io/programmingforpsych/&#34;&gt;https://crumplab.github.io/programmingforpsych/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Iannone, R., Allaire, J. J., &amp;amp; Borges, B. (2020). Flexdashboard: R Markdown Format for Flexible Dashboards. &lt;a href=&#34;http://rmarkdown.rstudio.com/flexdashboard&#34;&gt;http://rmarkdown.rstudio.com/flexdashboard&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;R Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Wickham, H. (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0. &lt;a href=&#34;https://CRAN.R-project.org/package=stringr&#34;&gt;https://CRAN.R-project.org/package=stringr&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Wickham, H., François, R., Henry, L., &amp;amp; Müller, K. (2018). dplyr: A Grammar of Data Manipulation. R package version 0.7.6. &lt;a href=&#34;https://CRAN.R-project.org/package=dplyr&#34;&gt;https://CRAN.R-project.org/package=dplyr&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Xie, Y. (2020). DT: A Wrapper of the JavaScript Library “DataTables”. R package version 0.14. Available at &lt;a href=&#34;https://CRAN.R-project.org/package=DT&#34;&gt;https://CRAN.R-project.org/package=DT&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;contact&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Contact&lt;/h3&gt;
&lt;p&gt;To submit any questions or feedback, please post &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/issues&#34;&gt;an issue&lt;/a&gt;, or email Pablo Bernabeu at &lt;a href=&#34;mailto:pcbernabeu@gmail.com&#34;&gt;pcbernabeu@gmail.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Modality exclusivity norms for 747 properties and concepts in Dutch: A replication of English</title>
      <link>https://pablobernabeu.github.io/2016/modality-exclusivity-norms-for-747-properties-and-concepts-in-dutch-a-replication-of-english/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2016/modality-exclusivity-norms-for-747-properties-and-concepts-in-dutch-a-replication-of-english/</guid>
      <description>


</description>
    </item>
    
  </channel>
</rss>
