<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>research methods | Pablo Bernabeu</title>
    <link>https://pablobernabeu.github.io/tags/research-methods/</link>
      <atom:link href="https://pablobernabeu.github.io/tags/research-methods/index.xml" rel="self" type="application/rss+xml" />
    <description>research methods</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-uk</language><copyright>Pablo Bernabeu, 2015—2024. Licence: [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/). Email: pcbernabeu@gmail.com. No cookies operated by this website. Cookies only used by third-party systems such as [Disqus](https://help.disqus.com/en/articles/1717155-use-of-cookies).</copyright><lastBuildDate>Fri, 08 Mar 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://pablobernabeu.github.io/img/default_preview_image.png</url>
      <title>research methods</title>
      <link>https://pablobernabeu.github.io/tags/research-methods/</link>
    </image>
    
    <item>
      <title>Making research materials Findable, Accessible, Interoperable and Reusable (FAIR)</title>
      <link>https://pablobernabeu.github.io/talk/making-research-materials-findable-accessible-interoperable-reusable-fair/</link>
      <pubDate>Fri, 08 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/talk/making-research-materials-findable-accessible-interoperable-reusable-fair/</guid>
      <description>


&lt;hr&gt;
&lt;p&gt;This poster is available to researchers upon request while the study is in progress. Afterwards, it will be made public.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Cross, Z. R., Zou-Williams, L., Wilkinson, E. M., Schlesewsky, M., &amp;amp; Bornkessel-Schlesewsky, I. (2021). Mini Pinyin: A modified miniature language for studying language learning and incremental sentence processing. &lt;em&gt;Behavior Research Methods, 53(3)&lt;/em&gt;, 1218–1239. &lt;a href=&#34;https://doi.org/10.3758/s13428-020-01473-6&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13428-020-01473-6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;González Alonso, J., Alemán Bañón, J., DeLuca, V., Miller, D., Pereira Soares, S. M., Puig-Mayenco, E., Slaats, S., &amp;amp; Rothman, J. (2020). Event related potentials at initial exposure in third language acquisition: Implications from an artificial mini-grammar study. &lt;em&gt;Journal of Neurolinguistics, 56&lt;/em&gt;, 100939. &lt;a href=&#34;https://doi.org/10.1016/j.jneuroling.2020.100939&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.jneuroling.2020.100939&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Morgan-Short, K., Finger, I., Grey, S., &amp;amp; Ullman, M. T. (2012). Second language processing shows increased native-like neural responses after months of no exposure. &lt;em&gt;PLOS ONE, 7&lt;/em&gt;(3), e32974. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0032974&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0032974&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pereira Soares, S. M., Kupisch, T., &amp;amp; Rothman, J. (2022). Testing potential transfer effects in heritage and adult L2 bilinguals acquiring a mini grammar as an additional language: An ERP approach. &lt;em&gt;Brain Sciences, 12&lt;/em&gt;(5), Article 5. &lt;a href=&#34;https://doi.org/10.3390/brainsci12050669&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3390/brainsci12050669&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. &lt;em&gt;Scientific Data, 3&lt;/em&gt;(1), Article 1. &lt;a href=&#34;https://doi.org/10.1038/sdata.2016.18&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1038/sdata.2016.18&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A session logbook for a longitudinal study using conditional formatting in Excel</title>
      <link>https://pablobernabeu.github.io/2023/a-session-logbook-for-a-longitudinal-study-using-conditional-formatting-in-excel/</link>
      <pubDate>Sat, 02 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/a-session-logbook-for-a-longitudinal-study-using-conditional-formatting-in-excel/</guid>
      <description>


&lt;p&gt;Longitudinal studies consist of several sessions, and often involve session session conductors. To facilitate the planning, registration and tracking of sessions, a session logbook becomes even more necessary than usual. To this end, an Excel workbook with conditional formatting can help automatise some formats and visualise the progress.&lt;/p&gt;
&lt;p&gt;Below is an example that is &lt;a href=&#34;https://1drv.ms/x/s!AouK9kQQrXKooGKVLbdBzz-DStmj?e=mbNZE4&#34;&gt;available on OneDrive&lt;/a&gt;. To fully access this workbook, it may be downloaded via &lt;kbd&gt;File&lt;/kbd&gt; &amp;gt; &lt;kbd&gt;Save as&lt;/kbd&gt; &amp;gt; &lt;kbd&gt;Download a copy&lt;/kbd&gt;. Alternatively, the workbook can be exported to one’s own account on OneDrive.&lt;/p&gt;
&lt;p&gt;The conditional formats include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;fill in blanks in columns A, C or D to remove the background&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;fill in blanks in F and G, or I and J, or L and M, etc, to highlight the entire session in green&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;fill in any notes columns to highlight them in red&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe src=&#34;https://onedrive.live.com/embed?resid=A872AD1044F68A8B%214194&amp;amp;authkey=!ABsHnSAl4C4iZLc&amp;amp;em=2&#34; width=&#34;100%&#34; height=&#34;500px&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;margin-top: 1.5%; margin-bottom: 4%;&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;These conditional formats can be consulted (and edited) from the &lt;kbd&gt;Home&lt;/kbd&gt; tab by opening &lt;kbd&gt;Conditional Formatting&lt;/kbd&gt; &amp;gt; &lt;kbd&gt;Manage Rules&lt;/kbd&gt;. At the top, the option &lt;kbd&gt;This Worksheet&lt;/kbd&gt; should be selected to view all formulas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Motivating a preregistration (especially in experimental linguistics)</title>
      <link>https://pablobernabeu.github.io/2023/motivating-a-preregistration-especially-in-experimental-linguistics/</link>
      <pubDate>Fri, 24 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/motivating-a-preregistration-especially-in-experimental-linguistics/</guid>
      <description>


&lt;div id=&#34;the-disorder&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The disorder&lt;/h2&gt;
&lt;p&gt;Studies in the cognitive sciences often feature multiple experimental conditions and other independent variables. Once the study progresses, the analytical liberties associated with all the conditions and variables are compounded by the myriad possible steps in data processing and analysis. All these combinations lead to a &lt;em&gt;garden of forking paths&lt;/em&gt;, and the &lt;strong&gt;researcher degrees of freedom&lt;/strong&gt; soar to unexpected highs.&lt;/p&gt;
&lt;p&gt;The system of professional incentives in academia largely ignores the issue of researcher degrees of freedom. When CVs are assessed, quantity weighs more than quality of research. When studies are assessed, statistical significance weighs more than methodological rigour. Thus, the low replication rates in various fields, including linguistics (Kobrock &amp;amp; Roettger, 2023), are hardly surprising (cf. Barsalou, 2019).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-treatment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The treatment&lt;/h2&gt;
&lt;p&gt;The number of researcher degrees of freedom, and their unforeseen influence, can be reduced a priori by publishing a preregistration before data collection has begun (or before the analysis in the case of meta-analyses and secondary-data analyes). The preregistration takes a bit of time, which poses a challenge because funding systems often require &lt;em&gt;doing things&lt;/em&gt; as soon and as fast as possible, driven by a questionable notion of scientific productivity.&lt;/p&gt;
&lt;div id=&#34;how-to-gather-the-strength&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to gather the strength&lt;/h3&gt;
&lt;p&gt;The best argument to motivate a preregistration may be that does not incur any extra time. It only requires frontloading an important portion of the work. This effort will be rewarded when the study is published, as preregistered analyses are received with greater trust by peer-reviewers and other readers.&lt;/p&gt;
&lt;p&gt;If any contributors of a project can gather just enough time and interest to initiate a preregistration in time, the researchers can attempt to pocket this important asset for their study. They will reap the reward in time, and so will their field of research at large.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-to-do&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What to do&lt;/h3&gt;
&lt;p&gt;Initially, rather than detailed analytical steps, it is the broader theoretical aspects that should be laid out. An abstract and an introduction should be written, describing why the study has been designed in such a way, what analyses will be performed, and what hypotheses are afforded by the literature. Next, some methodological details regarding the materials and the analyses should be added.&lt;/p&gt;
&lt;p&gt;The degree of detail in the preregistration can be determined by the researchers alone. Yet, ceteris paribus, the greater the detail in a preregistration, the greater the trustworthiness of the analyses and the results. Multiple preregistration guidelines exist by now, including some field-specific ones.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solace&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Solace&lt;/h3&gt;
&lt;p&gt;Preregistration is not perfect, but is a lesser evil that reduces the misuse of statistical analysis in science (Mertzen et al., 2021; Roettger, 2023).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Even the strongest blizzards start with a single snowflake. (Sara Raasch)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Rules are made to be broken—at least as and when necessary. That is, deviations from the preregistration are possible, and indeed very frequent (Bakker et al., 2020; van den Akker et al., 2023).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Bakker, M., Veldkamp, C. L. S., Assen, M. A. L. M. van, Crompvoets, E. A. V., Ong, H. H., Nosek, B. A., Soderberg, C. K., Mellor, D., &amp;amp; Wicherts, J. M. (2020). Ensuring the quality and specificity of preregistrations. &lt;em&gt;PLOS Biology, 18&lt;/em&gt;(12), e3000937. &lt;a href=&#34;https://doi.org/10.1371/journal.pbio.3000937&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pbio.3000937&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barsalou, L. W. (2019). Establishing generalizable mechanisms. &lt;em&gt;Psychological Inquiry, 30&lt;/em&gt;(4), 220–230. &lt;a href=&#34;https://doi.org/10.1080/1047840X.2019.1693857&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1080/1047840X.2019.1693857&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kobrock, K., &amp;amp; Roettger, T. B. (2023). Assessing the replication landscape in experimental linguistics. &lt;em&gt;Glossa Psycholinguistics, 2&lt;/em&gt;(1). &lt;a href=&#34;https://doi.org/10.5070/G6011135&#34; class=&#34;uri&#34;&gt;https://doi.org/10.5070/G6011135&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Mertzen, D., Lago, S., &amp;amp; Vasishth, S. (2021). The benefits of preregistration for hypothesis-driven bilingualism research. &lt;em&gt;Bilingualism: Language and Cognition, 24&lt;/em&gt;(5), 807–812. &lt;a href=&#34;https://doi.org/10.1017/S1366728921000031&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1017/S1366728921000031&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Roettger, T. B. (2021). Preregistration in experimental linguistics: Applications, challenges, and limitations. &lt;em&gt;Linguistics, 59&lt;/em&gt;(5), 1227–1249. &lt;a href=&#34;https://doi.org/10.1515/ling-2019-0048&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1515/ling-2019-0048&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;van den Akker, O. R., van Assen, M. A. L. M., Enting, M., de Jonge, M., Ong, H. H., Rüffer, F., Schoenmakers, M., Stoevenbelt, A. H., Wicherts, J. M., &amp;amp; Bakker, M. (2023). Selective hypothesis reporting in psychology: Comparing preregistrations and corresponding publications. &lt;em&gt;Advances in Methods and Practices in Psychological Science, 6&lt;/em&gt;(3), 25152459231187988. &lt;a href=&#34;https://doi.org/10.1177/25152459231187988&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/25152459231187988&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Learning how to use Zotero</title>
      <link>https://pablobernabeu.github.io/2023/learning-how-to-use-zotero/</link>
      <pubDate>Fri, 10 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/learning-how-to-use-zotero/</guid>
      <description>


&lt;p&gt;Is it worth learning how to use a reference management system such as Zotero? Maybe.&lt;/p&gt;
&lt;p&gt;The hours you invest in learning how to use Zotero (approx. 10 hours) are likely to pay off, as they will save you a lot of time that you would otherwise spend formatting, revising and correcting references. In addition, this skill would become part of your skill set.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://library-guides.ucl.ac.uk/zotero/installing-zotero&#34;&gt;A great guide&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://library-guides.ucl.ac.uk/zotero/further-help&#34;&gt;Free, online webinars in which you could participate and ask questions&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://library-guides.ucl.ac.uk/zotero/using-zotero-with-word&#34;&gt;Zotero and Word&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://chrome.google.com/webstore/detail/zotero-connector/ekhagklcjbdpajgpjgmbionohlpdbjgc&#34;&gt;Zotero extension for the Chrome browser, to quickly add papers to your libraries&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>FAQs on mixed-effects models</title>
      <link>https://pablobernabeu.github.io/2023/faqs-on-mixed-effects-models/</link>
      <pubDate>Sat, 04 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/faqs-on-mixed-effects-models/</guid>
      <description>


&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;I am dealing with nested data, and I remember from an article by &lt;a href=&#34;https://doi.org/10.1016/S0022-5371(73)80014-3&#34;&gt;Clark (1973)&lt;/a&gt; that nested should be analysed using special models. I’ve looked into mixed-effects models, and I’ve reached a structure with random intercepts by subjects and by items. Is this fine?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;In early days, researchers would aggregate the data across these repeated measures to prevent the violation of the assumption of independence of observations, which is one of the most important assumptions in statistics. With the advent of mixed-effects models, researchers began accounting for these repeated measures using random intercepts and slopes. However, problems of convergence led many researchers to remove random slopes. This became widespread until, over the past few years, we have realised that random slopes are necessary to prevent an inflation of the Type I error due to the violation of the assumption of independence (&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;Brauer &amp;amp; Curtin, 2018&lt;/a&gt;; &lt;a href=&#34;http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf&#34;&gt;Singmann &amp;amp; Kellen, 2019&lt;/a&gt;). Please see Table 17 in Brauer and Curtin (2018). Due to the present reasons, the models in the current article are anti-conservative. To redress this problem, please consider the inclusion of random slopes by participant for all between-items variables [e.g., &lt;code&gt;(stimulus_condition | participant)&lt;/code&gt;], and random slopes by item for all between-participants variables [e.g., &lt;code&gt;(extraversion | item)&lt;/code&gt;]. Interaction terms should also have the corresponding slopes, except when the variables in the interaction vary within different units, that is, one between participants and one between items (&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;Brauer &amp;amp; Curtin, 2018&lt;/a&gt;). Each of the random intercepts and random slopes included in the model should be noted in the main text, for instance using footnotes in the results table (see &lt;a href=&#34;https://bookdown.org/pablobernabeu/language-sensorimotor-conceptual-processing-statistical-power/study-2.1-semantic-priming.html#semanticpriming-results&#34;&gt;example&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;I calculated the &lt;em&gt;p&lt;/em&gt; values by comparing minimally-different models using the &lt;code&gt;anova&lt;/code&gt; function. Is this fine?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.3758/s13428-016-0809-y&#34;&gt;Luke (2017)&lt;/a&gt; warns that the &lt;em&gt;p&lt;/em&gt; values calculated by model comparison—which are based on likelihood ratio tests—can be anti-conservative. Therefore, the Kenward-Roger and the Satterthwaite methods are recommended instead (both available in other packages, such as &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;lmerTest&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/afex/afex.pdf&#34;&gt;afex&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;The lme4 package only runs on one thread (CPU) but the computer has 8. Do you have any advice on making the model run using more of the threads? It’s taking a very long time. I’ve seen these two possible solutions online from 2018 (&lt;a href=&#34;https://stackoverflow.com/questions/48315268/how-can-i-make-r-using-more-than-1-core-8-available-on-a-ubuntu-rstudio-server&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://stat.ethz.ch/pipermail/r-sig-mixed-models/2018q3/027170.html&#34;&gt;here&lt;/a&gt;) but would like some advice if they have any or have attempted either of these solutions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;From the information I have seen in the past as well as right now, parallelising (g)lmer intentionally would be very involved. There is certainly interest in it, as your resources show (also see &lt;a href=&#34;https://github.com/lme4/lme4/issues?q=is%3Aissue+parallel&#34;&gt;here&lt;/a&gt;). However, the current information suggests to me that it is not possible.&lt;/p&gt;
&lt;p&gt;Interestingly, some isolated cases of unintentional parallelisation have been documented, and the developers of the &lt;a href=&#34;https://cran.r-project.org/web/packages/lme4/lme4.pdf&#34;&gt;lme4&lt;/a&gt; package were &lt;a href=&#34;&#34;&gt;surprised about them&lt;/a&gt; because they have not created this feature (see &lt;a href=&#34;https://github.com/lme4/lme4/issues/492&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://github.com/lme4/lme4/issues/627&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I think the best approach may be running your model(s) in a high-performance computing (HPC) cluster. Although this would not reduce the amount of time required for each model, it would have two advantages. First, your own computers wouldn’t be busy for days, and second, you could even run several models at the same time without exhausting your own computers. I still have access to the HPC at my previous university, and it would be fine for me to send your model(s) there if that would help you. Feel free to let me know. Otherwise I can see that your university has this facility too.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;We took your advice and ran the model on a supercomputer - it took roughly 2.5 days, which is what it took for the model to run on my iMac and a gaming laptop Vivienne has.&lt;/p&gt;
&lt;p&gt;The model, however, didn’t converge. We have read that you can use &lt;code&gt;allFit()&lt;/code&gt; to try the fit with all available optimizers. Do you have any experience using this? If you did, I wondered where this would sit in the code for the model? How and where do I add this in to check all available optimizers, please?&lt;/p&gt;
&lt;p&gt;I have attached my code in a txt file and the data in excel for you to see, in case it is of any use.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;The multi-optimizer check is indeed a way (albeit tentative) to probe into the convergence. Convergence has long been a fuzzy subject, as there are different standpoints depending on the degree of conservativeness that is sought after by the analysts.&lt;/p&gt;
&lt;p&gt;On Page 124 in my thesis (&lt;a href=&#34;https://osf.io/97u5c&#34; class=&#34;uri&#34;&gt;https://osf.io/97u5c&lt;/a&gt;), you can find this multi-optimizer check (also see this &lt;a href=&#34;https://pablobernabeu.github.io/2021/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit&#34;&gt;blog post&lt;/a&gt;). All the code is available on OSF. More generally, I discuss the issue of convergence throughout the thesis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;I have run the model with &lt;code&gt;optimizer=&#34;nloptwrap&#34;&lt;/code&gt; and &lt;code&gt;algorithm=&#34;NLOPT_LN_BOBYQA&#34;&lt;/code&gt; and received the following warning message (once the model ran) -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;In optwrap(optimizer, devfun, start, rho$lower, control = control, :
convergence code 5 from nloptwrap: NLOPT_MAXEVAL_REACHED: optimization stopped becasue maxeval (above) was reached.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does this mean that the model didn’t converge? I’m only asking because I wasn’t given a statement saying it didn’t converge, as it did with Nelder_Mead. It was stated (at the end of summary table)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Optimizer (Nelder_Mead) convergence code: 4 (failure to converge in 10000 evaluations)
failure to converge in 10000 evaluations&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;Please try &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/semanticpriming_lmerTest.R#L109&#34;&gt;increasing the max number of iterations&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;We increased the max number of iterations to 1e6 and then 1e7, and the model didn’t converge. But it has converged with &lt;code&gt;maxeval=1e8&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I wanted to ask please, do you know of any issues with the max iterations being this high and effecting the interpretability of the model? Or is it completely fine?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 45px;&#34;&gt;
&lt;p&gt;There are no side-effects to increasing the number of iterations (see Remedy 6 in &lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;Brauer &amp;amp; Curtin, 2018&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>FAIR standards for the creation of research materials, with examples</title>
      <link>https://pablobernabeu.github.io/2023/fair-standards-for-the-creation-of-research-materials-with-examples/</link>
      <pubDate>Thu, 02 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/fair-standards-for-the-creation-of-research-materials-with-examples/</guid>
      <description>


&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34; style=&#34;padding-bottom: 0; margin-bottom: 6%;&#34;&gt;
&lt;p&gt;In the fast-paced world of scientific research, establishing minimum standards for the creation of research materials is essential. Whether it’s stimuli, custom software for data collection, or scripts for statistical analysis, the quality and transparency of these materials significantly impact the reproducibility and credibility of research. This blog post explores the importance of adhering to FAIR (Findable, Accessible, Interoperable, Reusable) principles, and offers practical examples for researchers, with a focus on the cognitive sciences.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Notwithstanding the need for speed in most scientific projects, what should be the &lt;strong&gt;minimum acceptable standards&lt;/strong&gt; in the &lt;strong&gt;creation of research materials&lt;/strong&gt; such as stimuli, custom software for data collection (e.g., experiment in jsPsych, OpenSesame or psychoPy), or scripts for statistical analysis?&lt;/p&gt;
&lt;p&gt;The answer to this question is contingent upon the field of research, the purpose and the duration of the project, and many other contextual factors. So, to narrow down the scope and come at a general answer, let’s suppose we asked a researcher in the cognitive sciences (e.g., a linguist, a psychologist or a neuroscientist) who values open science. Perhaps, such a researchers would be satisfied with a method for the creation of materials that &lt;strong&gt;allows the creators of the materials, as well as their collaborators and any other stakeholders (e.g., any fellow scientists working in the same field), to explore, understand, reproduce, modify, and reuse the materials following their completion and thereafter&lt;/strong&gt;. Let’s review some of the implements that can help fulfil these standards.&lt;/p&gt;
&lt;div id=&#34;fairness&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;FAIRness&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.go-fair.org/fair-principles&#34;&gt;FAIR Guiding Principles for scientific data management and stewardship&lt;/a&gt; exhaustively describe a protocol for making materials &lt;strong&gt;F&lt;/strong&gt;indable, &lt;strong&gt;A&lt;/strong&gt;ccessible, &lt;strong&gt;I&lt;/strong&gt;nteroperable and &lt;strong&gt;R&lt;/strong&gt;eusable. These terms cover the five allowances listed above, along with other important aspects.&lt;/p&gt;
&lt;p&gt;Let’s look at some instantiations of the FAIR principles.&lt;/p&gt;
&lt;div id=&#34;sharing-the-materials&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sharing the materials&lt;/h2&gt;
&lt;p&gt;A condition sine qua non is to share the materials publicly online, as far as possible. Repositories on servers such as OSF or GitHub are often sufficient to this end. Unfortunately, most studies in the cognitive sciences still do not share the complete materials.&lt;/p&gt;
&lt;p&gt;One of the reasons why sharing is so important is to prevent wrong assumptions by the audience that will consider the research. That is, when the materials of a study are not publicly shared online, the readers of the papers are left with two options: to assume that there were no errors or to assume that were some errors of an uncertain degree. The method followed in the creation of the materials should free the readers of this tribulation, by allowing them to consult the materials and their preparation in full, or as completely as possible.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reproducibility&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reproducibility&lt;/h2&gt;
&lt;p&gt;It is convenient to allow others, and our future selves, to reproduce the materials throughout their preparation and at any time thereafter. For this purpose, &lt;strong&gt;R&lt;/strong&gt; can be used to register in scripts as many as possible of the steps followed throughout the preparation of the materials. Far from being only a software for data analysis, R allows the preparation of texts, images, audios, etc. Humans err, by definition. That can be counted on. Conveniently, registering the steps followed during weeks or months of preparation allows us to offload part of the documentation efforts. It’s a way of video-recording, as it were, all the additions, subtractions, replacements, transformations and calculations performed with the raw materials, for the creation of the final materials.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generous-documentation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generous documentation&lt;/h2&gt;
&lt;p&gt;Under the curse of knowledge, the creators of research materials may believe that their materials are self-explanatory. Often they are more obscure than they think. To allow any other stakeholders, including their future selves, to exercise the five allowances listed above—i.e., explore, understand, reproduce, modify, and reuse the materials—, the preparation process and the end materials should be documented with enough detail. This can be done using README.txt files throughout the project. Using the &lt;code&gt;.txt&lt;/code&gt; format/extension is recommended because other formats, such as Microsoft Word, may not be (fully) available in some computers. To exemplify the format and the content of readme files, below is an excerpt from a longitudinal study on which I’ve been working.&lt;/p&gt;
&lt;textarea readonly style=&#39;border-color: lightgrey; overflow: auto; color: darkblue; font-size: 90%; min-width: 125%; height: 4600px; white-space: pre; overflow-wrap: normal; padding-right: 0.5em; padding-left: 1em;&#39;&gt;

-- Post-training test --

In Sessions 2, 3, 4 and 6, if the test is failed in the first attempt, the training and the test are 
repeated (following González Alonso et al., 2020). In such cases, the result is shown at the end 
of the second attempt. The session advances if the accuracy achieved in the second attempt exceeds 
80%, whereas the session stops if the accuracy is lower. In the latter situation, an &#39;End of session&#39; 
message is presented, flanked by two orange circles, and followed by an acknowledgement for the 
participant. Once the participant has read this screen, the experimenter quits the session by 
pressing &#39;ESC&#39; and then &#39;Q&#39;.


== Stimuli ==

The stimulus lists are described in the R functions that were used to create the stimuli, as well as
in the &#39;list&#39; column in the stimulus files.


== Participant-specific parameters for lab-based sessions ==

Each participant was assigned certain parameters in advance, including the mini-language, the order 
of the resting-state parts, and the stimulus lists. The code that was used to create this assignment 
is available in the &#39;stimulus_preparation&#39; folder. 

Due to the pre-assignment of the parameters, there is a fixed set of participant IDs that can be 
used in OpenSesame. These identification numbers range between 1 and 144. If an ID outside of this 
range is used, the OpenSesame session does not run.


== General procedure for lab-based sessions ==

At the beginning of Sessions 2, 3, 4 and 6, the experimenter starts OpenSesame by opening the 
program directly (not by opening the session-specific file), and then opens the appropriate session 
within OpenSesame. This procedure helps prevent the opening of a standalone Python window, the 
closing of which would result in the closing of OpenSesame. Next, the experimenter opens BrainVision 
Recorder.

Next, the experimenter fits the participant with the EEG cap, which they will wear throughout the 
session. To prevent them from being pulled down, please attach the splitter box neatly to the 
towel on the participant&#39;s back. 

Next, the experimenter returns to OpenSesame and runs the session in full screen by clicking on 
the full green triangle at the top left. Next, the experimenter selects a folder to store the 
logfile. It is important to select the folder corresponding to each session to avoid overwriting 
existing logfiles. Any prompts to overwrite a logfile must always be refused.

In the first screen, the experimenter can disable some of the tasks. This option can be used if a 
session has ended abruptly, in which case the session can be resumed from a near checkpoint. In 
such a case, the experimenter must first note this incident in their logbook, and rename the log 
file that was produced on the first run, by appending &#39;_first_run&#39; to the name. This prevents 
overwriting the file on the second run. Next, they must open a new session, enter the same 
participant ID, and select the appropriate part from which to begin. This part must be the part 
immediately following the last part that was completed in full. For instance, if a session ended
abruptly during the experiment, the beginning selected on the second run would be the experiment. 
Once the session has finished completely, the first log file and the second log file must be 
safely merged into a single file, keeping only the fully completed tasks.

In the first instructional screen, participants are asked to refrain from asking any questions 
unless it is necessary, so that all participants can receive the same instructions.

At the beginning of the Resting-state part (present in Sessions 2 and 4) and at the beginning of 
the Experiment part, instructions are presented on the screen that ask participants to stay as 
still as possible during the following task. The screen contains an orange-coloured square with 
the letters &#39;i.s.r&#39;, that remind the experimenter to check the impedance and the signal, and 
finally to begin recording the EEG signal. If the impedance of any electrodes is poor, the 
experimenter may enter the booth to lower the impedance of the electrodes affected. Otherwise, 
after validating the signal and the impedance, the experimenter can begin the recording in 
BrainVision, and press the letter &#39;C&#39; twice in the stimulus computer. At that point, a green 
circle will appear, along with instructions for the participant. 

Similarly, at the end of the Resting-state part and at the end of the Experiment part, a screen
with a crossed-out R appears to remind the experimenter to stop recording the EEG. 

Notice that, at some important stages during the sessions, the letter &#39;C&#39; must be pressed twice 
by the experimenter to let the session continue. This protocol provides the experimenter with 
control when necessary. These moments are signalled by a &#39;wait a moment&#39; notice for the 
participant, and by two orange-coloured stripes on the screen. The experimenter should be aware 
of the use of the letter &#39;C&#39; at these points, as the requirement is not signalled on the screen 
to prevent participants from pressing the letter themselves. 

During the experiment, it is important to monitor the EEG signal. If it ever becomes very noisy, 
the experiment must be paused by pressing the ESC key, and the problem must be resolved. If the 
noise is due to movement by the participant, they should be asked again to please stay as still 
possible. If the noise is due to an increase in the impedance of some electrodes, the impedance 
of those electrodes should be revised.

The Experiment part in each session contains a break every 40 trials. During these breaks, the 
number of the current trial appears in grey on the bottom right corner of the screen.


== Definition of items in OpenSesame (only for programming purposes, not for in-session use) ==

  -- Each major part of the session is contained in a sequence item that is named in capital 
     letters (e.g., &#39;PRETRAINING&#39;, &#39;TRAINING&#39;, &#39;TEST&#39;, &#39;EXPERIMENT&#39;).

  -- &#39;continue_space&#39;: allows proceeding to the following screen after pressing the space bar, 
     which should be done by the participant. In most cases, two presses are required, as 
     detailed on the screen.

  -- &#39;continue_c&#39;: allows proceeding to the following screen after pressing the letter &#39;C&#39;, 
     which should be done by the experimenter. In most cases, two presses are required, as 
     detailed on the screen.


== Variables in the OpenSesame log files ==

In the log files produced by OpenSesame, each part of the session (e.g., Test, Experiment) is 
identified in the variable &#39;session_part&#39;. The names of the response variables are &#39;response&#39;,
&#39;response_time&#39; and &#39;correct&#39;. Item-specific response variables follow the formats of 
&#39;response_[item_name]&#39;, &#39;response_time_[item_name]&#39; and &#39;correct_[item_name]&#39; 
(see https://osdoc.cogsci.nl/3.3/manual/variables/#response-variables).

The output is verbose and requires preprocessing of the data. For instance, the last response 
in each loop may appear twice in the output, due to the processing of the response. These 
duplicates can--and must--be cleaned up by discarding the rows that have the same trial number
as the preceding row.


== EEG triggers ==

Triggers are sent to the EEG recorder throughout the experiment. The system for sending 
triggers is set up in OpenSesame script within the inline script &#39;EEG_trigger_setup&#39;.

The key to the triggers is provided below.

  0: reset trigger port in BrainVision Recorder. This trigger is integrated in the 
     trigger-sending function.

  -- Resting-state EEG part --

    10: beginning of eyes-open resting-state EEG

    11: end of eyes-open resting-state EEG

    12: beginning of eyes-closed resting-state EEG

    13: end of eyes-closed resting-state EEG

  -- Experiment part --

    5: fixation mark

    -- ID of each target sentence (only applicable to target trials) --

        110--253: triggers ranging between 110 and 253, time-locked to the onset of the 
          word of interest in each trial.
&lt;/textarea&gt;
&lt;div id=&#34;comments-in-code-scripts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comments in code scripts&lt;/h3&gt;
&lt;p&gt;It is helpful for our future selves, for our collaborators, and for any other stakeholders associated with a project—which includes any fellow researchers worldwide—to include comments in code scripts. These comments should introduce the purpose of the script at the top, and the purpose of various components of the code. Some excerpts are shown below as examples.&lt;/p&gt;
&lt;script src=&#39;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Ffrequentist_bayesian_plot.R%23L3-L35&amp;style=a11y-dark&amp;type=code&amp;showFullPath=on&amp;showCopy=on&amp;showLineNumbers=on&amp;showFileMeta=on&#39;&gt;&lt;/script&gt;
&lt;script src=&#39;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Fdata%2Fsemanticpriming_data_preparation.R%23L29-L60&amp;style=a11y-dark&amp;type=code&amp;showFullPath=on&amp;showCopy=on&amp;showLineNumbers=on&amp;showFileMeta=on&#39;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;open-source-software&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Open-source software&lt;/h2&gt;
&lt;p&gt;Where possible, open-source software should be used. Open-source software is free, and hence more accessible. Open-source software can be classified in various dimensions, such as the size of the user base. The more users, the greater the support, because the core developers have more resources, and the users will often help each other in public forums such as Stack Exchange. For instance, a programming language such as R boasts millions of users worldwide who count on support in public forums and in R-specific forums such as the &lt;a href=&#34;https://community.rstudio.com&#34;&gt;Posit Community&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Other software are not as large. For instance, open-source software for psychological research (e.g., &lt;a href=&#34;https://osdoc.cogsci.nl/&#34;&gt;OpenSesame&lt;/a&gt;, &lt;a href=&#34;https://www.psychopy.org/&#34;&gt;psychoPy&lt;/a&gt;) are far smaller than R in terms of community. Yet, these software too can count on substantial support. For the more basic uses, most of the way has already been paved, and the existing documentation suffices. For more advanced uses, the smaller size of the community can become more obvious, as one needs to spend more time looking for solutions.&lt;/p&gt;
&lt;p&gt;Regardless of the size of the community, all else being equal, open-source software is the right choice to ensure access to one’s work for all (potential) stakeholders in the future. The other option, proprietary software, entails dependence on the services of a private company.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidiness-and-parsimony-in-computer-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidiness and parsimony in computer code&lt;/h2&gt;
&lt;p&gt;Code scripts should be as tidy and parsimonious as possible. For instance, to prevent overly long scripts that would impair the comprehension of the materials, it is useful to break down large projects into nested scripts, and &lt;code&gt;source&lt;/code&gt; (i.e., run) the smaller scripts in the larger scripts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compose all stimuli for Sessions 2, 3, 4 and 6

# Create participant-specific parameters
source(&amp;#39;stimulus_preparation/participant_parameters.R&amp;#39;)

# Frame base images
source(&amp;#39;stimulus_preparation/base_images.R&amp;#39;)

# Session 2
source(&amp;#39;stimulus_preparation/Session 2/Session2_compile_all_stimuli.R&amp;#39;)

# Session 3
source(&amp;#39;stimulus_preparation/Session 3/Session3_compile_all_stimuli.R&amp;#39;)

# Session 4
source(&amp;#39;stimulus_preparation/Session 4/Session4_compile_all_stimuli.R&amp;#39;)

# Session 6
source(&amp;#39;stimulus_preparation/Session 6/Session6_compile_all_stimuli.R&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tidiness-and-parsimony-in-project-directories&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidiness and parsimony in project directories&lt;/h2&gt;
&lt;p&gt;A directory tree is useful to display all the folders in a project. The tree can be produced in the RStudio ‘Terminal’ console using the following one-line command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;find . -type d | sed -e &amp;quot;s/[^-][^\/]*\//  |/g&amp;quot; -e &amp;quot;s/|\([^ ]\)/| - \1/&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output will look like the following (excerpt from &lt;a href=&#34;https://osf.io/gt5uf/wiki&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf/wiki&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.
  | - bayesian_priors
  |  | - plots
  | - semanticpriming
  |  | - analysis_with_visualsimilarity
  |  |  | - model_diagnostics
  |  |  |  | - results
  |  |  |  | - plots
  |  |  | - results
  |  |  | - plots
  |  |  | - correlations
  |  |  |  | - plots
  |  | - frequentist_bayesian_plots
  |  |  | - plots
  |  | - frequentist_analysis
  |  |  | - model_diagnostics
  |  |  |  | - results
  |  |  |  | - plots
  |  |  | - lexical_covariates_selection
  |  |  |  | - results
  |  |  |  | - plots
  |  |  | - results
  |  |  | - plots&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Adhering to best practices—including the FAIR principles—in the creation of research materials enhances transparency, accessibility and reproducibility in scientific research. These standards facilitate researchers’ work beyond the short term, and increase the reliability of scientific work, thus contributing to the best use of resources.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Preprocessing the Norwegian Web as Corpus (NoWaC) in R</title>
      <link>https://pablobernabeu.github.io/2023/preprocessing-the-norwegian-web-as-corpus-nowac-in-r/</link>
      <pubDate>Thu, 28 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/preprocessing-the-norwegian-web-as-corpus-nowac-in-r/</guid>
      <description>


&lt;div id=&#34;the-present-script-can-be-used-to-pre-process-data-from-a-frequency-list-of-the-norwegian-as-web-corpus-nowac-guevara-2010.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The present script can be used to pre-process data from a frequency list of the Norwegian as Web Corpus (NoWaC; Guevara, 2010).&lt;/h3&gt;
&lt;p&gt;Before using the script, the frequency list should be downloaded from &lt;a href=&#34;https://www.hf.uio.no/iln/english/about/organization/text-laboratory/projects/nowac/nowac-frequency.html&#34;&gt;this URL&lt;/a&gt;. The list is described as ‘frequency list sorted primary alphabetic and secondary by frequency within each character’, and &lt;a href=&#34;https://www.tekstlab.uio.no/nowac/download/nowac-1.1.lemma.frek.sort_alf_frek.txt.gz&#34;&gt;this is the direct URL&lt;/a&gt;. The download requires signing in to an institutional network. Last, the downloaded file should be unzipped.&lt;/p&gt;
&lt;p&gt;The script is shown below.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fpreprocessing-NoWaC-Corpus-in-R%2Fblob%2Fmain%2Fpreprocessing_NoWaC_Corpus.R%23L19-L74&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Guevara, E. R. (2010). NoWaC: A large web-based corpus for Norwegian. In &lt;em&gt;Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop&lt;/em&gt; (pp. 1-7). &lt;a href=&#34;https://aclanthology.org/W10-1501&#34; class=&#34;uri&#34;&gt;https://aclanthology.org/W10-1501&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rodina, Y., &amp;amp; Westergaard, M. (2015). Grammatical gender in Norwegian: Language acquisition and language change. &lt;em&gt;Journal of Germanic Linguistics, 27&lt;/em&gt;(2), 145–187. &lt;a href=&#34;https://doi.org/10.1017/S1470542714000245&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1017/S1470542714000245&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rodina, Y., &amp;amp; Westergaard, M. (2021). Grammatical gender and declension class in language change: A study of the loss of feminine gender in Norwegian. &lt;em&gt;Journal of Germanic Linguistics, 33&lt;/em&gt;(3), 235–263. &lt;a href=&#34;https://doi.org/10.1017/S1470542719000217&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1017/S1470542719000217&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Who&#39;s through with convergence warnings? A list of papers that used &#39;allFit&#39; to check &#39;coefficients&#39; across &#39;optimizers&#39;</title>
      <link>https://pablobernabeu.github.io/2023/who-s-through-with-convergence-warnings-a-list-of-papers-that-used-allfit-to-check-coefficients-across-optimizers/</link>
      <pubDate>Sat, 08 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/who-s-through-with-convergence-warnings-a-list-of-papers-that-used-allfit-to-check-coefficients-across-optimizers/</guid>
      <description>


</description>
    </item>
    
    <item>
      <title>ggplotting power curves from the &#39;simr&#39; package</title>
      <link>https://pablobernabeu.github.io/2023/ggplotting-power-curves-from-simr-package/</link>
      <pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/ggplotting-power-curves-from-simr-package/</guid>
      <description>


&lt;p&gt;The R package ‘simr’ has greatly facilitated power analysis for mixed-effects models using Monte Carlo simulation (which involves running hundreds or thousands of tests under slight variations of the data). The &lt;code&gt;powerCurve&lt;/code&gt; function is used to estimate the statistical power for various sample sizes in one go. Since the tests are run serially, they can take a VERY long time; approximately, the time it takes to run the model supplied once (say, a few hours) &lt;em&gt;times&lt;/em&gt; the number of simulations (&lt;code&gt;nsim&lt;/code&gt;, which should be higher than 200), and &lt;em&gt;times&lt;/em&gt; the number of different sample sizes examined. While there isn’t a built-in parallel method, the power curves for different sample sizes can be run separately, and the results can be progressively combined as each component finishes running (see &lt;a href=&#34;https://pablobernabeu.github.io/2021/parallelizing-simr-powercurve&#34;&gt;tutorial&lt;/a&gt;). The power curves produced by &lt;code&gt;simr&lt;/code&gt; are so good they deserve ‘ggplot2’ rendering. So, here’s a function for it.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2FpowercurvePlot%2Fblob%2Fmain%2FpowercurvePlot.R%23L3-L82&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;a-usage-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A usage example&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
library(simr)
library(ggplot2)

# Toy model with data from &amp;#39;simr&amp;#39; package
fit = lmer(y ~ x + (x | g), data = simdata)

# Extend sample size of `g`
fit_extended_g = extend(fit, along = &amp;#39;g&amp;#39;, n = 12)

fit_powercurve = 
  powerCurve(fit_extended_g, fixed(&amp;#39;x&amp;#39;), 
             along = &amp;#39;g&amp;#39;, breaks = c(4, 6, 8, 10, 12), 
             nsim = 50, seed = 123, progress = FALSE)

# Read in custom function to ggplot results from simr::powerCurve

source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/powercurvePlot/main/powercurvePlot.R&amp;#39;)

powercurvePlot(fit_powercurve, number_x_axis_levels = 6) +
  
  # Change some defaults
  
  xlab(&amp;quot;Number of levels in &amp;#39;g&amp;#39;&amp;quot;) +
  
  theme(plot.title = element_blank(),
        axis.title.x = element_text(size = 18), 
        axis.title.y = element_text(size = 18), 
        axis.text = element_text(size = 17))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2023/ggplotting-power-curves-from-simr-package/index.en_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to visually assess the convergence of a mixed-effects model by plotting various optimizers</title>
      <link>https://pablobernabeu.github.io/2023/how-to-visually-assess-the-convergence-of-a-mixed-effects-model-by-plotting-various-optimizers/</link>
      <pubDate>Sat, 24 Jun 2023 16:42:34 +0200</pubDate>
      <guid>https://pablobernabeu.github.io/2023/how-to-visually-assess-the-convergence-of-a-mixed-effects-model-by-plotting-various-optimizers/</guid>
      <description>


&lt;p&gt;To assess whether convergence warnings render the results invalid, or on the contrary, the results can be deemed valid in spite of the warnings, &lt;a href=&#34;https://cran.r-project.org/web/packages/lme4/lme4.pdf&#34;&gt;Bates et al. (2023)&lt;/a&gt; suggest refitting models affected by convergence warnings with a variety of optimizers. The authors argue that, if the different optimizers produce practically-equivalent results, the results are valid. The &lt;code&gt;allFit&lt;/code&gt; function from the ‘lme4’ package allows the refitting of models using a number of optimizers. To use the seven optimizers listed above, two extra packages must be installed: ‘dfoptim’ and ‘optimx’ (see lme4 manual). The output from &lt;code&gt;allFit&lt;/code&gt; contains several statistics on the fixed and the random effects fitted by each optimizer.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
#&amp;gt; Loading required package: Matrix
library(dfoptim)
library(optimx)

# Create data using code by Ben Bolker from 
# https://stackoverflow.com/a/38296264/7050882

set.seed(101)
spin = runif(600, 1, 24)
reg = runif(600, 1, 15)
ID = rep(c(&amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,&amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;7&amp;quot;, &amp;quot;8&amp;quot;, &amp;quot;9&amp;quot;, &amp;quot;10&amp;quot;))
day = rep(1:30, each = 10)
testdata &amp;lt;- data.frame(spin, reg, ID, day)
testdata$fatigue &amp;lt;- testdata$spin * testdata$reg/10 * rnorm(30, mean=3, sd=2)

# Model
fit = lmer(fatigue ~ spin * reg + (1|ID),
           data = testdata, REML = TRUE)

# Refit model using all available algorithms
multi_fit = allFit(fit)
#&amp;gt; bobyqa : [OK]
#&amp;gt; Nelder_Mead : [OK]
#&amp;gt; nlminbwrap : [OK]
#&amp;gt; nmkbw : [OK]
#&amp;gt; optimx.L-BFGS-B : [OK]
#&amp;gt; nloptwrap.NLOPT_LN_NELDERMEAD : [OK]
#&amp;gt; nloptwrap.NLOPT_LN_BOBYQA : [OK]

# Show results 
summary(multi_fit)$fixef
#&amp;gt;                               (Intercept)      spin       reg  spin:reg
#&amp;gt; bobyqa                          -2.975678 0.5926561 0.1437204 0.1834016
#&amp;gt; Nelder_Mead                     -2.975675 0.5926559 0.1437202 0.1834016
#&amp;gt; nlminbwrap                      -2.975677 0.5926560 0.1437203 0.1834016
#&amp;gt; nmkbw                           -2.975678 0.5926561 0.1437204 0.1834016
#&amp;gt; optimx.L-BFGS-B                 -2.975680 0.5926562 0.1437205 0.1834016
#&amp;gt; nloptwrap.NLOPT_LN_NELDERMEAD   -2.975666 0.5926552 0.1437196 0.1834017
#&amp;gt; nloptwrap.NLOPT_LN_BOBYQA       -2.975678 0.5926561 0.1437204 0.1834016

# Read in function from GitHub
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/plot.fixef.allFit/main/plot.fixef.allFit.R&amp;#39;)

plot.fixef.allFit(multi_fit, 
                  
                  select_predictors = c(&amp;#39;spin&amp;#39;, &amp;#39;reg&amp;#39;, &amp;#39;spin:reg&amp;#39;), 
                  
                  # Increase padding at top and bottom of Y axis
                  multiply_y_axis_limits = 1.3,
                  
                  y_title = &amp;#39;Fixed effect (*b*)&amp;#39;)
#&amp;gt; Loading required package: dplyr
#&amp;gt; 
#&amp;gt; Attaching package: &amp;#39;dplyr&amp;#39;
#&amp;gt; The following objects are masked from &amp;#39;package:stats&amp;#39;:
#&amp;gt; 
#&amp;gt;     filter, lag
#&amp;gt; The following objects are masked from &amp;#39;package:base&amp;#39;:
#&amp;gt; 
#&amp;gt;     intersect, setdiff, setequal, union
#&amp;gt; Loading required package: reshape2
#&amp;gt; Loading required package: stringr
#&amp;gt; Loading required package: scales
#&amp;gt; Loading required package: ggplot2
#&amp;gt; Loading required package: ggtext
#&amp;gt; Loading required package: patchwork&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/XYQDug2.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# Alternative using plot-specific Y axes and other modified settings

plot.fixef.allFit(multi_fit, 
                  
                  select_predictors = c(&amp;#39;spin&amp;#39;, &amp;#39;spin:reg&amp;#39;), 
                  
                  # Use plot-specific Y axis limits
                  shared_y_axis_limits = FALSE,
                  
                  decimal_places = 7, 
                  
                  # Move up Y axis title
                  y_title_hjust = 4.5,
                  
                  y_title = &amp;#39;Fixed effect (*b*)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/BYXJYxM.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;Created on 2023-06-26 with &lt;a href=&#34;https://reprex.tidyverse.org&#34;&gt;reprex v2.0.2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Assigning participant-specific parameters automatically in OpenSesame</title>
      <link>https://pablobernabeu.github.io/2023/assigning-participant-specific-parameters-automatically-in-opensesame/</link>
      <pubDate>Wed, 17 May 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/assigning-participant-specific-parameters-automatically-in-opensesame/</guid>
      <description>


&lt;p&gt;OpenSesame offers options to counterbalance properties of the stimulus across participants. However, in cases of more involved assignments of session parameters across participants, it becomes necessary to write a bit of Python code in an inline script, which should be placed at the top of the timeline. In such a script, the participant-specific parameters are loaded in from a csv file. Below is a minimal example of the csv file.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;17%&#34; /&gt;
&lt;col width=&#34;22%&#34; /&gt;
&lt;col width=&#34;20%&#34; /&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;24%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;participant&lt;/th&gt;
&lt;th&gt;language&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;training_list&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;test_list&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;experiment_list&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;7&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;8&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Below is the corresponding inline script. The code &lt;code&gt;.iloc[0]&lt;/code&gt; at the end of the lines is used to select a cell.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;
# Assign participant-specific parameters. For each participant-specific factor in the 
# stimulus files, take the level corresponding to the index position specified in the 
# participant parameters file.

import csv  # handle csv file
import pandas as pd  # handle data frames

participant_parameters = pd.read_csv(exp.get_file(&amp;#39;stimuli/parameters per participant.csv&amp;#39;))

var.participant = var.subject_nr

var.language = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;language&amp;#39;].iloc[0]

var.training_list = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;training_list&amp;#39;].iloc[0]

var.test_list = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;test_list&amp;#39;].iloc[0]

var.experiment_list = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;experiment_list&amp;#39;].iloc[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Finally, the variables created in the script can be used in &lt;kbd&gt;Run if&lt;/kbd&gt; conditions (e.g., &lt;code&gt;[language] == &#39;Mini-English&#39;&lt;/code&gt;), as replacements inside file names (e.g., &lt;code&gt;[language] training, List [training_list].csv&lt;/code&gt;), and as input in sketchpads (e.g., &lt;code&gt;Current list: [training_list]&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Specifying version number in OSF download links</title>
      <link>https://pablobernabeu.github.io/2023/specifying-version-number-in-osf-download-links/</link>
      <pubDate>Tue, 16 May 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/specifying-version-number-in-osf-download-links/</guid>
      <description>


&lt;p&gt;In the preparation of projects, files are often downloaded from OSF. It is good to document the URL addresses that were used for the downloads. These URLs can be provided in a code script (&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/data/semanticpriming_data_preparation.R#L294-L295&#34;&gt;see example&lt;/a&gt;) or in a README file. Better yet, it’s possible to specify the version of each file in the URL. This specification helps reduce the possibility of inaccuracies later, should any files be modified afterwards.&lt;/p&gt;
&lt;p&gt;The versions of files can be consulted on the right-hand side of the file page on OSF, as shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/OSF%20revisions.png&#34; width=&#34;550&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, the appropriate version can be specified in the download link by appending &lt;code&gt;?version=&lt;/code&gt;&lt;strong&gt;X&lt;/strong&gt; at the end. For instance, the seventh version is specified in the link &lt;a href=&#34;https://osf.io/hx6tz/download?version=7&#34;&gt;https://osf.io/hx6tz/download&lt;code&gt;?version=7&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Covariates are necessary to validate the variables of interest and to prevent bogus theories</title>
      <link>https://pablobernabeu.github.io/2023/covariates-are-necessary-to-validate-the-variables-of-interest-and-to-prevent-bogus-theories/</link>
      <pubDate>Mon, 15 May 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/covariates-are-necessary-to-validate-the-variables-of-interest-and-to-prevent-bogus-theories/</guid>
      <description>


&lt;p&gt;The need for covariates—or &lt;em&gt;nuisance variables&lt;/em&gt;—in statistical analyses is twofold. The first reason is purely statistical and the second reason is academic.&lt;/p&gt;
&lt;p&gt;First, the use of covariates is often necessary when the variable(s) of interest in a study may be connected to, and affected by, some satellite variables (Bottini et al., 2022; Elze et al., 2017; Sassenhagen &amp;amp; Alday, 2016). This complex scenario is the most common one due to the multivariate, dynamic, interactive nature of the real world.&lt;/p&gt;
&lt;p&gt;Second, the use of covariates is often necessary to prevent the development of bogus, redundant theories. Academics are strongly rewarded for developing theories. As we know, wherever there are strong rewards, there are serious risks. An academic could—consciously or not—produce a theory that is too closely related to an existing theory. So closely related are these theories that the second version might not warrant a name of its own. In such a scenario, covariates are useful and indeed necessary to vet the unique nature of the second version. That is, the first and the second version must be tested in the same model, and the variables corresponding to the first version can be construed as &lt;em&gt;covariates&lt;/em&gt;. This allows both the developers of the theories and the readers to compare the effects corresponding to each version of the theory, and to assess the degree of separation between them.&lt;/p&gt;
&lt;p&gt;The perverted use of covariates (Stefan &amp;amp; Schönbrodt, 2023)—however frequent and harmful—stands completely orthogonal to the correct usage of covariates, in the same way that a stethoscope can be used for good or for bad purposes. It would be poorly informed and misleading to conflate the correct and the incorrect uses, or to reject the use of covariates altogether due to the incorrect uses.&lt;/p&gt;
&lt;p&gt;In conclusion, the effects of interest in correlational/observational studies can be subject to mediation and moderation by satellite variables. These variables cannot be manipulated in correlational/observational studies, but they can—and often should—be included as covariates in the statistical models, to ward off spurious results and to vet similar theories.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Bottini, R., Morucci, P., D’Urso, A., Collignon, O., &amp;amp; Crepaldi, D. (2022). The concreteness advantage in lexical decision does not depend on perceptual simulations. &lt;em&gt;Journal of Experimental Psychology: General, 151&lt;/em&gt;(3), 731–738. &lt;a href=&#34;https://doi.org/10.1037/xge0001090&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/xge0001090&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Elze, M. C., Gregson, J., Baber, U., Williamson, E., Sartori, S., Mehran, R., Nichols, M., Stone, G. W., &amp;amp; Pocock, S. J. (2017). Comparison of propensity score methods and covariate adjustment: Evaluation in 4 cardiovascular studies. &lt;em&gt;Journal of the American College of Cardiology, 69&lt;/em&gt;(3), 345-357.&lt;/p&gt;
&lt;p&gt;Sassenhagen, J., &amp;amp; Alday, P. M. (2016). A common misapplication of statistical inference: Nuisance control with null-hypothesis significance tests. &lt;em&gt;Brain and Language, 162&lt;/em&gt;, 42-45. &lt;a href=&#34;https://doi.org/10.1016/j.bandl.2016.08.001&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.bandl.2016.08.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Stefan, A. M., &amp;amp; Schönbrodt, F. D. (2023). Big little lies: A compendium and simulation of p-hacking strategies. &lt;em&gt;Royal Society Open Science, 10&lt;/em&gt;(2), 220346. &lt;a href=&#34;https://doi.org/10.1098/rsos.220346&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1098/rsos.220346&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Brief Clarifications, Open Questions: Commentary on Liu et al. (2018)</title>
      <link>https://pablobernabeu.github.io/2021/brief-clarifications-open-questions-commentary-on-liu-et-al-2018/</link>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2021/brief-clarifications-open-questions-commentary-on-liu-et-al-2018/</guid>
      <description>&lt;p&gt;Liu et al. (2018) present a study that implements the conceptual modality switch (CMS) paradigm, which has been used to investigate the modality-specific nature of conceptual representations (Pecher et al., 2003). Liu et al.&amp;lsquo;s experiment uses event-related potentials (ERPs; similarly, see Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013). In the design of the switch conditions, the experiment implements a corpus analysis to distinguish between purely-embodied modality switches and switches that are more liable to linguistic bootstrapping (also see Bernabeu et al., 2017; Louwerse &amp;amp; Connell, 2011). The procedure for stimulus selection was novel as well as novel; thus, it could prove useful in future studies, too. In addition, the application of bayesian statistics is an interesting and promising novelty in the present research area.&lt;/p&gt;
&lt;p&gt;In reviewing the literature, Liu (2018) and Liu et al. (2018) contend that previous studies may be strongly biased due to methodological decisions in the analysis of ERPs. These decisions particularly regard the latency&amp;mdash;i.e., time windows&amp;mdash;and the topographic regions of interest&amp;mdash;i.e., subsets of electrodes. Thus, Liu et al. identify a &amp;lsquo;highly inconsistent&amp;rsquo; (p. 6) landscape in the ERP components that have been ascribed to the CMS effect in previous studies. Similarly, Liu (p. 47) writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Several studies have looked for the ERP manifestations of modality switching costs (Bernabeu, Willems, &amp;amp; Louwerse, 2017; Collins, Pecher, Zeelenberg, &amp;amp; Coulson, 2011; Hald, Hocking, Vernon, Marshall, &amp;amp; Garnham, 2013; Hald, Marshall, Janssen, &amp;amp; Garnham, 2011). However, what they found was not a clear picture. Not only was a significant effect found in the time window for the N400 component, but also a so-called early N400-like effect around 300ms (Bernabeu et al., 2017; Hald et al., 2011), the N1-P2 complex around 200ms (Bernabeu et al., 2017; Hald et al., 2013, 2011), as well as the late positivity component (LPC) after 600ms (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;drastic-conclusions&#34;&gt;Drastic conclusions&lt;/h3&gt;
&lt;p&gt;Liu et al. (2018) conclude that a confirmatory research approach is not warranted, and adopt a semi-exploratory approach, creating time windows of 50 ms each, rather than windows linked to known ERP components (Swaab et al., 2012), and using bayesian statistics. Such a drastic conclusion appears to stem from the assumption that, if the CMS were robust enough, it would present in the same guise across studies. Such an assumption, however, may merit further examination, considering the multiplicity of known and unknown variables that may differ across experiments (Barsalou, 2019). This variability influences the &lt;em&gt;replication praxis&lt;/em&gt;, as it were. Indeed, Liu et al. themselves allude to one such variable (p. 7).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;These previous studies did not only examine the effect of modality switching but also other linguistic factors such as negated sentences, which could easily distort observed waveforms (Luck, 2005).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu et al.&amp;lsquo;s (2018) conclusions about the &amp;lsquo;inconsistent&amp;rsquo; results do not seem to duly weigh the differences across studies. None of the existing studies is a direct replication of another. The example quoted from Liu et al. above, regarding the presence of negated sentences, is one of the less important differences because one of the corresponding studies implemented the negation as a controlled, experimental condition, contrasting with affirmative sentences (Hald et al., 2013). Yet, other differences exist in virtually every aspect, from the types of modality switch used to the time-locking of ERPs. For instance, the studies vary in the implementation of the modality switches. Whereas Hald et al. (2011) distinguish between a switch and a non-switch condition, Bernabeu et al. (2017) analysed each type of switch separately&amp;mdash;i.e., auditory-to-visual, haptic-to-visual, visual-to-visual. Another difference across the studies is the onset point for ERPs. For instance, Bernabeu et al. time-locked ERPs to the point at which the modality switch is actually elicited in the CMS paradigm&amp;mdash;namely, the first word in target trials. In contrast, the other studies time-locked ERPs to the second word. In addition, these studies vary in their timelines&amp;mdash;i.e., presentation of words and inter-stimulus intervals&amp;mdash;, as well as in the words that were used as stimuli, in the preprocessing of ERPs, in the statistical analysis, and even in the language of testing in one case (all studies using English except Bernabeu et al., 2017, which used Dutch). Moreover, the studies differ in the sample size, ranging from ten finally-analysed participants (Hald et al, 2011) to 46 finally-analysed participants (Bernabeu et al., 2017). Last, the studies differ in the number of items per modality switch condition, ranging from 17 (in one of Liu et al.&amp;lsquo;s, 2018 conditions) to 40 per condition (Hald et al., 2011, 2013). Undoubtedly, seeing larger sample sizes and more stimulus items used in ERP studies is something to promote and celebrate. Last, it may be noted that Liu et al.&amp;lsquo;s stance on the inconsistency of previous results starkly contrasts with their use of a single study&amp;mdash;Hald et al. (2011)&amp;mdash;, with &lt;em&gt;N&lt;/em&gt; = 10, as the motivation for their sample size (Albers &amp;amp; Lakens, 2018).&lt;/p&gt;
&lt;h3 id=&#34;clarifications&#34;&gt;Clarifications&lt;/h3&gt;
&lt;p&gt;Liu et al. (2018) apply bayesian statistics reportedly to help reduce the bias that may exist in the present literature. However, the reviews by Liu (2018) and Liu et al. appear to gloss over some important aspects regarding previous studies. For instance, Liu writes (p. 53):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The inflation of the probability of Type I error leads to an over-confidence in the interpretation of the results. For example, in the studies on modality switching costs, different time windows were chosen to test the early effect of modality switching costs. While Bernabeu et al. (2017), Hald et al. (2011) and Hald et al. (2013) examined the segment of ERP waveform between 190ms and 300ms or 160ms and 215ms based on visual inspection and found significant effects, Collins et al. (2011) chose a prescribed time window between 100ms and 200ms before the analysis and did not find the effect.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu also refers to the issue of multiple tests related to the multiple time windows and electrodes we find in ERP studies (p. 59):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It is typical for ERP studies to conduct multiple comparisons (e.g., running the same ANOVA repeatedly on different subsets of data like different time windows and different groups of electrodes). This would massively increase Type I error if no post hoc correction is conducted. However, if Bonferroni or other correction is conducted, it will render the study over-conservative, thus increasing the chance of Type II error. In the present thesis, 90 electrodes will be analysed individually, with 20 time slices in each trial. That results in 1800 NHSTs for each critical variable. A correction of multiple comparison will require a critical level of 2.78 x 10ˆ-5 for each test for a family-wise critical level of .05 (and an uncorrected test will almost definitely lead to false positive results). This stringent criterion could conceivably render it meaningless any p-values we can obtain from a statistical package.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Arguably, the scenario presented by Liu (2018), in which a researcher could conduct a purely data-driven analysis of ERP data, is extreme. The field of psycholinguistics, in general, does not have a tradition of purely data-driven analysis. Instead, it blends a humanistic background with a scientific methodology. As a result, the hypotheses and methods tend to be largely driven by the available literature. For instance, Bernabeu et al. (2017, quoted below from p. 1632) based their time windows and regions of interest on the most relevant of the preceding studies (also see Bernabeu, 2017).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Electrodes were divided into an anterior and a posterior area (also done in Hald et al., 2011). Albeit a superficial division, we found it sufficient for the research question. Time windows were selected as in Hald et al., except for the last window, which was extended up to 750 ms post word onset, instead of 700 ms, because the characteristic component of that latency tends to extend until then, as we confirmed by visual inspection of these results.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The literature-based approach follows the advice from Luck and Gaspelin (2017, p. 149), who wrote: &amp;lsquo;a researcher who wants to avoid significant but bogus effects would be advised to focus on testing a priori predictions without using the observed data to guide the selection of time windows or electrode sites&amp;rsquo; (also see Armstrong, 2014; for a more conservative stance, see Luck &amp;amp; Gaspelin, 2017). In addition, notice that the extension of the last window by 50 ms was informed by Swaab et al. (2012), who report results by which the P600 component (the main component occurring after the N400 in word reading) extended up to 800 ms.&lt;/p&gt;
&lt;p&gt;Next, Liu et al. (2018, pp. 6&amp;ndash;7) write:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;However, the findings of these components have been highly inconsistent. The N400 effect alone was found in the posterior region in some cases (Bernabeu et al., 2017; Hald et al., 2013), while in anterior region in others (Collins et al., 2011, Hald et al. (2011)).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yet, Bernabeu et al. (2017, p. 1632) had written:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In certain parts over the time course, the effect appeared in both anterior and posterior areas&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu et al. (2018) continue (p. 7):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In some cases, it was found in the typical window around 400ms (Collins et al., 2011), while in others an earlier window from 270ms to 370ms (Bernabeu et al., 2017; Hald et al., 2011).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, Bernabeu et al. (2017, p. 1632) had written:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The ERP results revealed a CMS effect from Time Window 1 on, larger after 350 ms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Regarding the time-locking of ERPs, Liu (2018, p. 43) writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Because the properties were usually salient for the concepts, the switching costs might have already been incurred when participants were processing the concept word. Bernabeu et al. (2017), in their recent replication of previous ERP studies, reversed the order of concept and property and did not find an immediate effect from the property onset. In future studies, it is recommended to adopt the reverse order, control the concept words so that they do not automatically activate the properties before the words are shown, or analyse epochs after both the concept and property words.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above excerpt seems to reveal a misunderstanding of Bernabeu et al.&amp;lsquo;s (2017) method and results (we may assume that, by &amp;lsquo;immediate&amp;rsquo;, Liu (2018) is referring to the 200 ms point or afterwards, since that is about as immediate as it gets; see Amsel et al., 2014; Swaab et al., 2012; Van Dam et al., 2014). Bernabeu et al.&amp;lsquo;s abstract mentioned (p. 1629):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Event-Related Potentials (ERPs) were time-locked to the onset of the first word (property) in the target trials so as to measure the effect online and to avoid a within-trial confound. A switch effect was found, characterized by more negative ERP amplitudes for modality switches than no-switches. It proved significant in four typical time windows from 160 to 750 milliseconds post word onset, with greater strength in posterior brain regions, and after 350 milliseconds.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Indeed, time-locking ERPs to the beginning of the trial was one of the principal features of Bernabeu et al.&amp;lsquo;s (2017) experiment. Complementing that, the property words were placed in the first trial because they are more perceptually loaded than concepts (Lynott &amp;amp; Connell, 2013), thus better suiting the main basis of the CMS paradigm. We know that the semantic processing of a word often commences within the first 200 ms (Amsel et al., 2014; Van Dam et al., 2014). Considering the importance of the time course in the grounding of conceptual representations (Hauk, 2016), it seems important to measure the CMS from the moment that it is elicited&amp;mdash;namely, in all experiments, from the first word of the target trial (Bernabeu, 2017; Bernabeu et al., 2017), rather than letting several hundreds of milliseconds elapse. Nonetheless, from a methodological perspective, it would be interesting to compare the two approaches in a dedicated study. This would precisely reveal the speed at which modality-specific meaning becomes activated during conceptual processing.&lt;/p&gt;
&lt;h3 id=&#34;outstanding-issues-random-effects-and-correction-for-multiple-tests&#34;&gt;Outstanding issues: Random effects and correction for multiple tests&lt;/h3&gt;
&lt;p&gt;A methodological issue affecting the statistical analysis of all the studies hereby considered (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013; Liu et al., 2018) is the absence of some applicable random effects. Some of the studies did not apply any random effects (Collins et al., 2011; Hald et al., 2011, 2013). Even in psycholinguistics, use of linear mixed-effects models is still increasing (Meteyard &amp;amp; Davies, 2020; Yarkoni, 2020). Yet, in those studies that did apply random effects, the corresponding  structure was not as exhaustive as it should have been, as they lacked random slopes. In Bernabeu et al. (2017), a model selection approach was applied (Matuschek et al., 2017), whereby each random effect was tested and only kept in the model if it significantly improved the fit. In Liu et al. (2018), random slopes were deemed unfeasible due to computational constraints (for background, see Brauer &amp;amp; Curtin, 2017). Applying a complete random effects structure is important for a robust statistical analysis (Barr et al., 2013; Yarkoni, 2020).&lt;/p&gt;
&lt;p&gt;Another issue is that of multiple tests. Where a small number of levels is used (e.g., time windows, topographic regions of interest) and these are informed by the literature, the advice has often been ambiguous as to whether a correction should be applied (e.g., Armstrong, 2014; for a more conservative stance, see Luck &amp;amp; Gaspelin, 2017). Indeed, no correction was applied in any of the four studies that used frequentist statistics (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013).&lt;/p&gt;
&lt;h4 id=&#34;reanalysis-of-bernabeu-et-al-2017&#34;&gt;Reanalysis of Bernabeu et al. (2017)&lt;/h4&gt;
&lt;p&gt;The results of Bernabeu et al. (2017) were reanalysed after publication using a more complete random effects structure that incorporated by-participant random slopes for the modality-switch factor&amp;mdash;i.e., &lt;code&gt;(condition | participant)&lt;/code&gt;. The results were also corrected for multiple tests using the Holm-Bonferroni correction (Holm, 1979). For this purpose, the lowest p-value in the four time windows was multiplied by 4, the next p-value by 3, the next by 2, and the highest p-value was left as it unmodified. In this stepwise correction, if a nonsignificant p-value was reached, all the subsequent p-values became nonsignificant (see &lt;a href=&#34;https://osf.io/unvfs/&#34;&gt;analysis script&lt;/a&gt;). The &lt;a href=&#34;https://osf.io/qhe5s/&#34;&gt;results&lt;/a&gt; differed from the original, slopes-free models (available &lt;a href=&#34;https://osf.io/sx3nw/&#34;&gt;script&lt;/a&gt; and &lt;a href=&#34;https://osf.io/4v38d/&#34;&gt;results&lt;/a&gt;) in that the main effect of the modality switch factor became nonsignificant in the second time window (160&amp;ndash;216 ms) and in the fourth one (500&amp;ndash;750 ms), while remaining significant in the third time window (350&amp;ndash;550 ms). Incidentally, note that main effects may not be directly interpretable as the same same variables are present interactions (Kam &amp;amp; Franzese, 2007). Yet, the interactions of modality switch with participant group (quick/slow) and scalp location (anterior/posterior) retained the same significance.&lt;/p&gt;
&lt;h3 id=&#34;open-questions&#34;&gt;Open questions&lt;/h3&gt;
&lt;p&gt;Liu (2018) and Liu et al. (2018) raise interesting and important questions. Firstly, future research may be conducted to investigate what determines the variability of ERP results&amp;mdash;in terms of ERP components, time windows and topographic regions of interest. This research could include a comparison with other measurements, such as response times, to test whether ERPs are less reliable&amp;mdash;i.e., more variable across studies&amp;mdash;than response times. Similarly, future research may investigate whether the ERP literature is more biased than literature employing other measures, such as response times. In addition, future research could investigate whether moving to exploratory, bayesian research designs is a necessary or sufficient condition to reduce bias in research and improve the precision of experimental measurements. Current alternatives to such an approach include direct (or conceptual) replications designed to achieve a higher power than previous studies (e.g., Chen et al., 2019). Arguably, policies determining funding decisions would need to change if we are to fully acknowledge the importance of &lt;em&gt;direct&lt;/em&gt; replication (Howe &amp;amp; Perfors, 2018; Kunert, 2016; Simons, 2014; Zwaan et al., 2018). Last, future research may investigate whether &amp;lsquo;clear picture&amp;rsquo; results are realistic, desirable or necessary, and whether unclear-picture results should be eschewed; or whether, on the contrary, clear-picture results may largely be the product of publication bias&amp;mdash;that is, the pressure to hide or misreport those aspects of a study that could challenge its acceptance by peer-reviewers or any other academics.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;div class = &#39;hanging-indent&#39;&gt;
&lt;p&gt;Albers, C., &amp;amp; Lakens, D. (2018). When power analyses based on pilot data are biased: Inaccurate effect size estimators and follow-up bias. &lt;em&gt;Journal of Experimental Social Psychology, 74&lt;/em&gt;, 187–195. &lt;a href=&#34;https://doi.org/10.1016/j.jesp.2017.09.004&#34;&gt;https://doi.org/10.1016/j.jesp.2017.09.004&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Amsel, B. D., Urbach, T. P., &amp;amp; Kutas, M. (2014). Empirically grounding grounded cognition: the case of color. &lt;em&gt;Neuroimage, 99&lt;/em&gt;, 149-157. &lt;a href=&#34;https://doi.org/10.1016/j.neuroimage.2014.05.025&#34;&gt;https://doi.org/10.1016/j.neuroimage.2014.05.025&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Armstrong, R. A. (2014). When to use the Bonferroni correction. &lt;em&gt;Ophthalmic and Physiological Optics, 34&lt;/em&gt;(5), 502-508. &lt;a href=&#34;https://doi.org/10.1111/opo.12131&#34;&gt;https://doi.org/10.1111/opo.12131&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barr, D. J., Levy, R., Scheepers, C., &amp;amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. &lt;em&gt;Journal of Memory and Language, 68&lt;/em&gt;, 255–278. &lt;a href=&#34;http://dx.doi.org/10.1016/j.jml.2012.11.001&#34;&gt;http://dx.doi.org/10.1016/j.jml.2012.11.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barsalou, L. W. (2019). Establishing generalizable mechanisms. &lt;em&gt;Psychological Inquiry, 30&lt;/em&gt;(4), 220-230. &lt;a href=&#34;https://doi.org/10.1080/1047840X.2019.1693857&#34;&gt;https://doi.org/10.1080/1047840X.2019.1693857&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P. (2017). &lt;em&gt;Modality switches occur early and extend late in conceptual processing: evidence from ERPs&lt;/em&gt; [Master&#39;s thesis]. School of Humanities, Tilburg University. &lt;a href=&#34;https://psyarxiv.com/5gjvk&#34;&gt;https://psyarxiv.com/5gjvk&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P., Willems, R. M., &amp;amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, &amp;amp; E. J. Davelaar (Eds.), &lt;em&gt;Proceedings of the 39th Annual Conference of the Cognitive Science Society&lt;/em&gt; (pp. 1629-1634). Austin, TX: Cognitive Science Society. &lt;a href=&#34;https://doi.org/10.31234/osf.io/a5pcz&#34;&gt;https://doi.org/10.31234/osf.io/a5pcz&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chen, S., Szabelska, A., Chartier, C. R., Kekecs, Z., Lynott, D., Bernabeu, P., … Schmidt, K. (2018). &lt;em&gt;Investigating object orientation effects across 14 languages&lt;/em&gt;. PsyArXiv. &lt;a href=&#34;https://doi.org/10.31234/osf.io/t2pjv/&#34;&gt;https://doi.org/10.31234/osf.io/t2pjv/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Collins, J., Pecher, D., Zeelenberg, R., &amp;amp; Coulson, S. (2011). Modality switching in a property verification task: an ERP study of what happens when candles flicker after high heels click. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2011.00010&#34;&gt;https://doi.org/10.3389/fpsyg.2011.00010&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hald, L. A., Hocking, I., Vernon, D., Marshall, J.-A., &amp;amp; Garnham, A. (2013). Exploring modality switching effects in negated sentences: further evidence for grounded representations. &lt;em&gt;Frontiers in Psychology, 4&lt;/em&gt;, 93. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2013.00093&#34;&gt;https://doi.org/10.3389/fpsyg.2013.00093&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hald, L. A., Marshall, J.-A., Janssen, D. P., &amp;amp; Garnham, A. (2011). Switching modalities in a sentence verification task: ERP evidence for embodied language processing. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2011.00045&#34;&gt;https://doi.org/10.3389/fpsyg.2011.00045&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hauk, O. (2016). Only time will tell–why temporal information is essential for our neuroscientific understanding of semantics. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;(4), 1072-1079. &lt;a href=&#34;https://doi.org/10.3758/s13423-015-0873-9&#34;&gt;https://doi.org/10.3758/s13423-015-0873-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Holm, S. (1979). A simple sequentially rejective multiple test procedure. &lt;em&gt;Scandinavian Journal of Statistics, 6&lt;/em&gt;, 65-70. &lt;a href=&#34;http://www.jstor.org/stable/4615733&#34;&gt;http://www.jstor.org/stable/4615733&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Howe, P. D., &amp;amp; Perfors, A. (2018). An argument for how (and why) to incentivise replication. &lt;em&gt;Behavioral and Brain Sciences, 41&lt;/em&gt;, e135-e135. &lt;a href=&#34;http://dx.doi.org/10.1017/S0140525X18000705&#34;&gt;http://dx.doi.org/10.1017/S0140525X18000705&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kam, C. D., &amp;amp; Franzese, R. J. (2007). &lt;em&gt;Modeling and interpreting interactive hypotheses in regression analysis&lt;/em&gt;. Ann Arbor, MI: University of Michigan Press.&lt;/p&gt;
&lt;p&gt;Kunert, R. (2016). Internal conceptual replications do not increase independent replication success. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;(5), 1631-1638. &lt;a href=&#34;https://doi.org/10.3758/s13423-016-1030-9&#34;&gt;https://doi.org/10.3758/s13423-016-1030-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, P. (2018). &lt;em&gt;Embodied-linguistic conceptual representations during metaphor processing&lt;/em&gt;. Doctoral thesis, Lancaster University, UK. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/489&#34;&gt;https://doi.org/10.17635/lancaster/thesis/489&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, P., Lynott, D., &amp;amp; Connell, L. (2018). Continuous neural activations of simulation-linguistic representations in modality switching costs. In P. Liu, &lt;em&gt;Embodied-linguistic conceptual representations during metaphor processing&lt;/em&gt;. Doctoral thesis, Lancaster University, UK. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/489&#34;&gt;https://doi.org/10.17635/lancaster/thesis/489&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Luck, S. J. (2005). Ten simple rules for designing ERP experiments. In T. C. Handy (Ed.), &lt;em&gt;Event-related potentials: A methods handbook&lt;/em&gt;.
MIT Press.&lt;/p&gt;
&lt;p&gt;Luck, S. J., &amp;amp; Gaspelin, N. (2017). How to get statistically significant effects in any ERP experiment (and why you shouldn&#39;t). &lt;em&gt;Psychophysiology, 54&lt;/em&gt;(1), 146-157. &lt;a href=&#34;https://doi.org/10.1111/psyp.12639&#34;&gt;https://doi.org/10.1111/psyp.12639&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., &amp;amp; Bates, D. (2017). Balancing type 1 error and power in linear mixed models. &lt;em&gt;Journal of Memory and Language, 94&lt;/em&gt;, 305–315. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2017.01.001&#34;&gt;https://doi.org/10.1016/j.jml.2017.01.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meteyard, L., &amp;amp; Davies, R. A. (2020). Best practice guidance for linear mixed-effects models in psychological science. &lt;em&gt;Journal of Memory and Language, 112&lt;/em&gt;, 104092. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2020.104092&#34;&gt;https://doi.org/10.1016/j.jml.2020.104092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pecher, D., Zeelenberg, R., &amp;amp; Barsalou, L. W. (2003). Verifying different-modality properties for concepts produces switching costs. &lt;em&gt;Psychological Science, 14&lt;/em&gt;, 2, 119-24. &lt;a href=&#34;https://doi.org/10.1111/1467-9280.t01-1-01429&#34;&gt;https://doi.org/10.1111/1467-9280.t01-1-01429&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Simons, D. J. (2014). The value of direct replication. &lt;em&gt;Perspectives on Psychological Science, 9&lt;/em&gt;(1), 76–80. &lt;a href=&#34;https://doi.org/10.1177/1745691613514755&#34;&gt;https://doi.org/10.1177/1745691613514755&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Swaab, T. Y., Ledoux, K., Camblin, C. C., &amp;amp; Boudewyn, M. A. (2012). Language-related ERP components. In S. J. Luck &amp;amp; E. S. Kappenman (Eds.), &lt;em&gt;Oxford handbook of event-related potential components&lt;/em&gt; (pp. 397–440). Oxford University Press. &lt;a href=&#34;https://doi.org/10.1093/oxfordhb/9780195374148.013.0197&#34;&gt;https://doi.org/10.1093/oxfordhb/9780195374148.013.0197&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Van Dam, W. O., Brazil, I. A., Bekkering, H., &amp;amp; Rueschemeyer, S.-A. (2014). Flexibility in embodied language processing: context effects in lexical access. &lt;em&gt;Topics in Cognitive Science, 6&lt;/em&gt;(3), 407–424. &lt;a href=&#34;https://doi.org/10.1111/tops.12100&#34;&gt;https://doi.org/10.1111/tops.12100&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yarkoni, T. (2020). The generalizability crisis. &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt;, 1-37. &lt;a href=&#34;https://doi.org/10.1017/S0140525X20001685&#34;&gt;https://doi.org/10.1017/S0140525X20001685&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zwaan, R., Etz, A., Lucas, R., &amp;amp; Donnellan, M. (2018). Making replication mainstream. &lt;em&gt;Behavioral and Brain Sciences, 41&lt;/em&gt;, E120. &lt;a href=&#34;https://doi.org/10.1017/S0140525X17001972&#34;&gt;https://doi.org/10.1017/S0140525X17001972&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Web application for the simulation of experimental data</title>
      <link>https://pablobernabeu.github.io/applications-and-dashboards/experimental-data-simulation/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/applications-and-dashboards/experimental-data-simulation/</guid>
      <description>


&lt;div style=&#34;font-size: 25px; color: #614064; padding-top: 15px; padding-bottom: 10px;&#34;&gt;
&lt;i class=&#34;fas fa-chalkboard-teacher fa-xs&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt; &lt;i class=&#34;fas fa-university fa-xs&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;  Purposes
&lt;/div&gt;
&lt;p&gt;This open-source, R-based web application is suitable for educational and research purposes in experimental and quantitative sciences. It allows the &lt;strong&gt;creation of varied data sets with specified structures, such as between-group and within-participant variables, that can be categorical or continuous.&lt;/strong&gt; These parameters can be set throughout the various tabs (sections) from the top menu. In the last tab, the data set can be downloaded. The benefits of this application include time-saving and flexibility in the control of parameters.&lt;/p&gt;
&lt;div id=&#34;guidelines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guidelines&lt;/h3&gt;
&lt;p&gt;General guidelines include the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;In the names of variables&lt;/strong&gt;, it’s recommended only to use alphanumeric characters and underscore signs. The latter can be used to separate characters or words (e.g., &lt;em&gt;variable_name&lt;/em&gt;). Different names should be used for each variable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;In the levels of categorical variables&lt;/strong&gt;, alphanumeric, special characters and spaces are allowed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;In numeric fields&lt;/strong&gt; (e.g., ‘Mean’, ‘Standard deviation’, ‘Relative probability [0, 1]’), only numbers and decimal points are allowed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;As the data set increases&lt;/strong&gt;, so does the processing time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More specific guidelines are available in each section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;globe_with_meridians-the-web-application-can-be-launched-here.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;🌐  The web application can be &lt;a href=&#34;https://pablo-bernabeu.shinyapps.io/experimental-data-simulation/&#34;&gt;launched here&lt;/a&gt;.&lt;/h3&gt;
&lt;div style=&#34;padding-top:8px; padding-bottom:2px; margin-bottom:-20px; color:#665F5F;&#34;&gt;
Screenshot of the &lt;em&gt;Dependent&lt;/em&gt; tab (&lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/raw/master/Screenshot.png&#34;&gt;view larger&lt;/a&gt;)
&lt;/div&gt;
&lt;p&gt;&lt;img style=&#34;max-width: 800px; display: block; margin-left: auto; margin-right: auto; padding-bottom: 15px;&#34; src=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/raw/master/Screenshot.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;p&gt;Bernabeu, P., &amp;amp; Lynott, D. (2020). &lt;em&gt;Web application for the simulation of experimental data&lt;/em&gt; (Version 1.4). &lt;a href=&#34;https://github.com/pablobernabeu/Experiment-simulation-app/&#34;&gt;https://github.com/pablobernabeu/Experiment-simulation-app/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Code&lt;/h3&gt;
&lt;p&gt;This web application was developed in &lt;a href=&#34;https://www.r-project.org/about.html&#34;&gt;R&lt;/a&gt; (R Core Team, 2020). The code is &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/blob/master/index.Rmd&#34;&gt;available on Github&lt;/a&gt;, where contributions may be made. The initial code for this application was influenced by Section 5.7 (&lt;a href=&#34;https://crumplab.github.io/programmingforpsych/simulating-and-analyzing-data-in-r.html#simulating-data-for-multi-factor-designs&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Simulating data for multi-factor designs&lt;/em&gt;&lt;/a&gt;) in Crump (2017). The R packages used include ‘dplyr’ (Wickham, François, Henry, &amp;amp; Müller, 2018), ‘DT’ (Xie, 2020), ‘flexdashboard’ (Iannone, Allaire, &amp;amp; Borges, 2020), ‘shiny’ (Chang, Cheng, Allaire, Xie, &amp;amp; McPherson, 2020) and ‘stringr’ (Wickham, 2019).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;options-for-development-and-local-use-of-the-app&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Options for development and local use of the app&lt;/h3&gt;
&lt;div id=&#34;option-a-using-local-rrstudio-or-rstudio-cloud-project-or-binder-rstudio-environment&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Option A) Using local R/RStudio or &lt;a href=&#34;https://rstudio.cloud/project/1739958&#34;&gt;RStudio Cloud project&lt;/a&gt; or &lt;a href=&#34;https://mybinder.org/v2/gh/pablobernabeu/Experimental-data-simulation/master?urlpath=rstudio&#34;&gt;Binder RStudio environment&lt;/a&gt;&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;[Step only necessary in R/RStudio] Install the packages in the versions used in the &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/releases&#34;&gt;latest release of this application&lt;/a&gt;, by running:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;#39;devtools&amp;#39;)
library(devtools)
install_version(&amp;#39;dplyr&amp;#39;, &amp;#39;1.0.2&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;DT&amp;#39;, &amp;#39;0.15&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;flexdashboard&amp;#39;, &amp;#39;0.5.2&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;htmltools&amp;#39;, &amp;#39;0.5.0&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;knitr&amp;#39;, &amp;#39;1.30&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;ngram&amp;#39;, &amp;#39;3.0.4&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;purrr&amp;#39;, &amp;#39;0.3.4&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;shiny&amp;#39;, &amp;#39;1.5.0&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;stringr&amp;#39;, &amp;#39;1.4.0&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)
install_version(&amp;#39;tidyr&amp;#39;, &amp;#39;1.1.2&amp;#39;, &amp;#39;http://cran.us.r-project.org&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open the &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/blob/master/index.Rmd&#34;&gt;index.Rmd&lt;/a&gt; script.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the application by clicking on &lt;kbd&gt;▶️ Run document&lt;/kbd&gt; at the top left, or by running &lt;code&gt;rmarkdown::run(&#39;index.Rmd&#39;)&lt;/code&gt; in the console.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Click on &lt;kbd&gt;Open in Browser&lt;/kbd&gt; at the top left.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;option-b-using-dockerfile-see-instructions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Option B) Using Dockerfile (&lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation#option-b-using-dockerfile-vsochs-pull-request&#34;&gt;see instructions&lt;/a&gt;)&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgements&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Acknowledgements&lt;/h3&gt;
&lt;p&gt;Thank you to RStudio for the free hosting server used by this application, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;shinyapps.io&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div style=&#34;text-indent:-1.5em; margin-left:1.5em;&#34;&gt;
&lt;p&gt;Chang, W., Cheng, J., Allaire, J., Xie, Y., &amp;amp; McPherson, J. (2020). shiny: Web Application Framework for R. R package version 1.4.0. Available at &lt;a href=&#34;http://CRAN.R-project.org/package=shiny&#34;&gt;http://CRAN.R-project.org/package=shiny&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Crump, M. J. C. (2017). Programming for Psychologists: Data Creation and Analysis (Version 1.1). &lt;a href=&#34;https://crumplab.github.io/programmingforpsych/&#34;&gt;https://crumplab.github.io/programmingforpsych/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Iannone, R., Allaire, J. J., &amp;amp; Borges, B. (2020). Flexdashboard: R Markdown Format for Flexible Dashboards. &lt;a href=&#34;http://rmarkdown.rstudio.com/flexdashboard&#34;&gt;http://rmarkdown.rstudio.com/flexdashboard&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;R Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL &lt;a href=&#34;https://www.R-project.org/&#34;&gt;https://www.R-project.org/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Wickham, H. (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0. &lt;a href=&#34;https://CRAN.R-project.org/package=stringr&#34;&gt;https://CRAN.R-project.org/package=stringr&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Wickham, H., François, R., Henry, L., &amp;amp; Müller, K. (2018). dplyr: A Grammar of Data Manipulation. R package version 0.7.6. &lt;a href=&#34;https://CRAN.R-project.org/package=dplyr&#34;&gt;https://CRAN.R-project.org/package=dplyr&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Xie, Y. (2020). DT: A Wrapper of the JavaScript Library “DataTables”. R package version 0.14. Available at &lt;a href=&#34;https://CRAN.R-project.org/package=DT&#34;&gt;https://CRAN.R-project.org/package=DT&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;contact&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Contact&lt;/h3&gt;
&lt;p&gt;To submit any questions or feedback, please post &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation/issues&#34;&gt;an issue&lt;/a&gt;, or email Pablo Bernabeu at &lt;a href=&#34;mailto:pcbernabeu@gmail.com&#34;&gt;pcbernabeu@gmail.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Modality exclusivity norms for 747 properties and concepts in Dutch: A replication of English</title>
      <link>https://pablobernabeu.github.io/2016/modality-exclusivity-norms-for-747-properties-and-concepts-in-dutch-a-replication-of-english/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2016/modality-exclusivity-norms-for-747-properties-and-concepts-in-dutch-a-replication-of-english/</guid>
      <description>


</description>
    </item>
    
  </channel>
</rss>
