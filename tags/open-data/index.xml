<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>open data | Pablo Bernabeu</title>
    <link>https://pablobernabeu.github.io/tags/open-data/</link>
      <atom:link href="https://pablobernabeu.github.io/tags/open-data/index.xml" rel="self" type="application/rss+xml" />
    <description>open data</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-uk</language><copyright>Pablo Bernabeu, 2015—2026. Licence: [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/). Email: pcbernabeu@gmail.com. Cookies only used by third-party systems such as [Disqus](https://help.disqus.com/en/articles/1717155-use-of-cookies).</copyright><lastBuildDate>Thu, 18 Dec 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://pablobernabeu.github.io/img/default_preview_image.png</url>
      <title>open data</title>
      <link>https://pablobernabeu.github.io/tags/open-data/</link>
    </image>
    
    <item>
      <title>Beyond the paper: Why open science is the new research standard</title>
      <link>https://pablobernabeu.github.io/2025/motivating-open-science-practices/</link>
      <pubDate>Thu, 18 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2025/motivating-open-science-practices/</guid>
      <description>&lt;p&gt;Scientific research is undergoing a fundamental transformation. What began in the early 2010s as a niche advocacy movement has evolved into a global standard for excellence. We are moving away from a &amp;lsquo;trust me&amp;rsquo; culture toward a &amp;lsquo;show me&amp;rsquo; culture, where transparency is the primary currency of credibility.&lt;/p&gt;
&lt;p&gt;The following recent publications from Nature and The Conversation underscore why this momentum is now irreversible:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nature.com/articles/s41563-023-01790-z&#34;&gt;Trust but verify&lt;/a&gt; – &lt;em&gt;Nature Materials&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nature.com/articles/s41467-024-54614-2&#34;&gt;Reproducibility and transparency: What’s going on and how can we help&lt;/a&gt; – Nature Communications&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://theconversation.com/reproducibility-may-be-the-key-idea-students-need-to-balance-trust-in-evidence-with-healthy-skepticism-251771&#34;&gt;Reproducibility may be the key idea students need to balance trust in evidence with healthy skepticism&lt;/a&gt; – The Conversation&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-mechanics-of-transparency&#34;&gt;The Mechanics of Transparency&lt;/h2&gt;
&lt;p&gt;Open science is more than a philosophy; it is a practical toolkit. At the heart of our methodology are two pillars: preregistration (the public archiving of the theoretical background, the research methods to be used and hypotheses posited before data collection) and the open sharing of raw data and materials.&lt;/p&gt;
&lt;p&gt;This approach creates a &amp;lsquo;win-win-win-win&amp;rsquo; scenario for everyone involved in the research ecosystem.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For Funders: Maximising Research ROI&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Open science ensures that a funder’s investment continues to yield dividends long after the final report is filed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Lasting Visibility: By sharing data under the &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode.en&#34;&gt;Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License (abbreviated as CC BY-NC-ND 4.0)&lt;/a&gt;&amp;mdash;the gold standard for protecting intellectual credit while preventing unauthorised commercial use&amp;mdash;funders demonstrate a commitment to cutting-edge corporate social responsibility (e.g., see &lt;a href=&#34;https://osf.io/search?filter_rights=%5B%7B%22label%22:%22CC-By%20Attribution-NonCommercial-NoDerivatives%204.0%20International%22,%22value%22:%22https:%2F%2Fcreativecommons.org%2Flicenses%2Fby-nc-nd%2F4.0%2Flegalcode%22,%22cardSearchResultCount%22:19302%7D%5D&#34;&gt;OSF projects using this licence&lt;/a&gt;). There are lots of possible licences with varying permissions and restrictions. For instance, &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/legalcode.en&#34;&gt;CC BY 4.0&lt;/a&gt; allows all use provided an acknowledgment of the authorship.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Public Recognition: Funders can be prominently featured in OSF repositories using direct links that help document the funder&#39;s role in high-quality, reproducible research for a global audience (see an &lt;a href=&#34;https://osf.io/fajne&#34;&gt;example repository&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;For the Project: Prestige through Rigour&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By adopting these protocols, a project distinguishes itself from the &amp;lsquo;black box&amp;rsquo; approach of traditional studies. It signals to peer reviewers and international collaborators that the study is a gold-standard project, positioned at the vanguard of modern academic practice.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;For Researchers: Building a Living Portfolio&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For the research team, especially early-career researchers (ECRs), open science is a strategic career move. Sharing data on the Open Science Framework (OSF) creates a public, citable portfolio of technical expertise, facilitating recognition and collaboration far beyond a standard CV.&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;For the Community: The Multiplier Effect&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The true power of open science lies in its ability to empower others. By providing a unique, high-quality dataset and analysis framework, we allow the global scientific community to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Independently verify our findings, strengthening the overall body of knowledge.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Repurpose data for new research questions, multiplying the impact of the original funding.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;a-commitment-to-integrity&#34;&gt;A Commitment to Integrity&lt;/h2&gt;
&lt;p&gt;Open science is not an &amp;lsquo;optional extra&amp;rsquo; or a bureaucratic hurdle; it is a commitment to the idea that the best science is done in the light. By prioritising transparency, we ensure that our findings are not just published, but permanent, verifiable and truly impactful. As we move forward, these practices will define which projects lead the conversation and which are left behind in the archives.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;The goal is simple:&lt;/strong&gt; To produce research that isn&#39;t just &amp;lsquo;right for now,&amp;rsquo; but remains robust and reusable for the future.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Making research materials Findable, Accessible, Interoperable and Reusable</title>
      <link>https://pablobernabeu.github.io/presentation/making-research-materials-findable-accessible-interoperable-reusable-fair/</link>
      <pubDate>Thu, 05 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/presentation/making-research-materials-findable-accessible-interoperable-reusable-fair/</guid>
      <description>


&lt;a href=&#39;https://osf.io/h83yq&#39;&gt;
&lt;button style = &#34;background-color: white; color: black; border: 2px solid #196F27; border-radius: 12px;&#34;&gt;
&lt;h3 style=&#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt;
&lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;/span&gt; Poster
&lt;/h3&gt;
&lt;/button&gt;
&lt;p&gt;&lt;/a&gt;  &lt;/p&gt;
&lt;div class=&#34;document-viewer-container&#34; style=&#34;height: 80vh; min-height: 400px;&#34;&gt;
&lt;iframe src=&#34;https://cdn.jsdelivr.net/gh/pablobernabeu/LESS-Project@main/presentations/Bernabeu%20et%20al%20FAIR%20at%20HILS%202024.pdf&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;border: none&#34; title=&#34;Document Viewer&#34; loading=&#34;lazy&#34;&gt;
&lt;p&gt;
Your browser does not support embedded PDFs. &lt;a href=&#34;https://cdn.jsdelivr.net/gh/pablobernabeu/LESS-Project@main/presentations/Bernabeu%20et%20al%20FAIR%20at%20HILS%202024.pdf&#34; target=&#34;_blank&#34;&gt;Download the PDF&lt;/a&gt; instead.
&lt;/p&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;div style=&#34;margin-top: 8%;&#34;&gt;

&lt;/div&gt;
&lt;div id=&#34;snippet-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Snippet 1&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;The use of code scripts facilitates the reproducibility, testability and expandability of materials.&lt;/em&gt;&lt;/p&gt;
&lt;div style=&#34;margin-top: 0; margin-bottom: 6%;&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;└── stimulus_preparation
    ├── Norway site, base stimuli.csv
    ├── Spain site, base stimuli.csv
    ├── base_images.R
    ├── R_functions
    │   ├── Session2_Pretraining_vocabulary.R
    │   ├── Session2_Training_gender_agreement.R
    │   ├── Session2_Test_gender_agreement.R
    │   ├── Session2_Experiment_gender_agreement.R
    ...
    ├── compile_all_stimuli.R&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;table-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Table 1&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;The minimal components of each language are contained in a base file.&lt;/em&gt;&lt;/p&gt;
&lt;div style=&#34;margin-top: 0; margin-bottom: 6%;&#34;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;verb_ID&lt;/th&gt;
&lt;th&gt;verb_type&lt;/th&gt;
&lt;th&gt;verb&lt;/th&gt;
&lt;th&gt;verb_contrast_ID&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;copula_be&lt;/td&gt;
&lt;td&gt;is&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;copula_be&lt;/td&gt;
&lt;td&gt;are&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;copula_look&lt;/td&gt;
&lt;td&gt;looks&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;copula_look&lt;/td&gt;
&lt;td&gt;look&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;transitive&lt;/td&gt;
&lt;td&gt;remembered&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;transitive&lt;/td&gt;
&lt;td&gt;forgot&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;transitive&lt;/td&gt;
&lt;td&gt;chose&lt;/td&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;transitive&lt;/td&gt;
&lt;td&gt;refused&lt;/td&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;snippet-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Snippet 2&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Tests were set throughout the workflow to control the frequency of some categories (R code).&lt;/em&gt;&lt;/p&gt;
&lt;div style=&#34;margin-top: 0; margin-bottom: 6%;&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;columns_to_check = c(&amp;#39;noun1_gender&amp;#39;, &amp;#39;number&amp;#39;, &amp;#39;person&amp;#39;, 
                     &amp;#39;verb&amp;#39;, &amp;#39;noun1&amp;#39;, &amp;#39;wrapup_noun&amp;#39;)

for(i in seq_along(columns_to_check)) {
  column = columns_to_check[i]
  number_of_unique_frequencies = 
    combinations %&amp;gt;% 
    filter(complete.cases(get(column)), get(column) != &amp;#39;&amp;#39;) %&amp;gt;% 
    group_by(get(column)) %&amp;gt;% tally() %&amp;gt;% select(n) %&amp;gt;% 
    n_distinct()
  if(number_of_unique_frequencies != 1) {
    warning(paste0(&amp;#39;Some elements in the column `&amp;#39;, column, 
                   &amp;#39;` appear more often than others.&amp;#39;))
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;snippet-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Snippet 3&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Seamless adjustment of parameters in each OpenSesame session (Python code).&lt;/em&gt;&lt;/p&gt;
&lt;div style=&#34;margin-top: 0; margin-bottom: 6%;&#34;&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;participant_parameters = 
  pd.read_csv(exp.get_file(&amp;#39;../parameters per participant/&amp;#39; + 
              var.study_site + 
              &amp;#39; site, parameters per participant.csv&amp;#39;))

var.resting_state_order = 
  participant_parameters.loc[
    participant_parameters[&amp;#39;participant&amp;#39;] ==
    var.subject_nr][&amp;#39;Session2_resting_state_order&amp;#39;].iloc[0]

var.language = 
  participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == 
                             var.subject_nr][&amp;#39;language&amp;#39;].iloc[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;snippet-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Snippet 4&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Sending serial-port triggers in OpenSesame to record ERPs (Python code).&lt;/em&gt;&lt;/p&gt;
&lt;div style=&#34;margin-top: 0; margin-bottom: 6%;&#34;&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Open the first serial port available
serialport = serial.Serial(serial.tools.list_ports.comports()[0].device)

# Send triggers to the port
def send_trigger(trigger):
    serialport.write(trigger.to_bytes(length = 1, byteorder = &amp;#39;big&amp;#39;))
    # 10 ms separation from next trigger (see BrainVision Recorder manual)
    time.sleep(0.01) 
    # reset port
    serialport.write(int(0).to_bytes(length = 1, byteorder = &amp;#39;big&amp;#39;)) 
    return;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;References&lt;/h4&gt;
&lt;p&gt;Barsalou, L. W. (2019). Establishing generalizable mechanisms. &lt;em&gt;Psychological Inquiry, 30&lt;/em&gt;(4), 220–230. &lt;a href=&#34;https://doi.org/10.1080/1047840X.2019.1693857&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1080/1047840X.2019.1693857&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cross, Z. R., Zou-Williams, L., Wilkinson, E. M., Schlesewsky, M., &amp;amp; Bornkessel-Schlesewsky, I. (2021). Mini Pinyin: A modified miniature language for studying language learning and incremental sentence processing. &lt;em&gt;Behavior Research Methods, 53(3)&lt;/em&gt;, 1218–1239. &lt;a href=&#34;https://doi.org/10.3758/s13428-020-01473-6&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13428-020-01473-6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;González Alonso, J., Alemán Bañón, J., DeLuca, V., Miller, D., Pereira Soares, S. M., Puig-Mayenco, E., Slaats, S., &amp;amp; Rothman, J. (2020). Event related potentials at initial exposure in third language acquisition: Implications from an artificial mini-grammar study. &lt;em&gt;Journal of Neurolinguistics, 56&lt;/em&gt;, 100939. &lt;a href=&#34;https://doi.org/10.1016/j.jneuroling.2020.100939&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.jneuroling.2020.100939&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Mitrofanova, N., Leivada, E., &amp;amp; Westergaard, M. (2023). Crosslinguistic influence in L3 acquisition: Evidence from artificial language learning. &lt;em&gt;Linguistic Approaches to Bilingualism, 13&lt;/em&gt;(5), 717-742. &lt;a href=&#34;https://doi.org/10.1075/lab.22063.mit&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1075/lab.22063.mit&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Morgan-Short, K., Finger, I., Grey, S., &amp;amp; Ullman, M. T. (2012). Second language processing shows increased native-like neural responses after months of no exposure. &lt;em&gt;PLOS ONE, 7&lt;/em&gt;(3), e32974. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0032974&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0032974&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pereira Soares, S. M., Kupisch, T., &amp;amp; Rothman, J. (2022). Testing potential transfer effects in heritage and adult L2 bilinguals acquiring a mini grammar as an additional language: An ERP approach. &lt;em&gt;Brain Sciences, 12&lt;/em&gt;(5), Article 5. &lt;a href=&#34;https://doi.org/10.3390/brainsci12050669&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3390/brainsci12050669&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. &lt;em&gt;Scientific Data, 3&lt;/em&gt;(1), Article 1. &lt;a href=&#34;https://doi.org/10.1038/sdata.2016.18&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1038/sdata.2016.18&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Towards reproducibility and maximally-open data</title>
      <link>https://pablobernabeu.github.io/presentation/towards-reproducibility-and-maximally-open-data/</link>
      <pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/presentation/towards-reproducibility-and-maximally-open-data/</guid>
      <description>&lt;iframe src=&#34;https://slideshare.net/slideshow/embed_code/key/btTmVtxioR1Ru0&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;a href=&#34;https://slideshare.net/PabloBernabeu/towards-reproducibility-and-maximallyopen-data-248393658&#34; title=&#34;Towards reproducibility and maximally-open data&#34; target=&#34;_blank&#34;&gt;Slideshare&lt;/a&gt;&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data dashboard: Butterfly species richness in Los Angeles</title>
      <link>https://pablobernabeu.github.io/applications-and-dashboards/butterfly-species-richness-in-la/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/applications-and-dashboards/butterfly-species-richness-in-la/</guid>
      <description>&lt;a href=&#39;https://pablobernabeu.github.io/dashboards/Butterfly-species-richness-in-LA&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #196F27; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt;
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;/span&gt; Dashboard
      &lt;/h3&gt;&lt;/button&gt;
      &lt;/a&gt;
&lt;br&gt;
&lt;br&gt;
&lt;p&gt;This dashboard presents open data&lt;/a&gt; (&lt;a href=&#39;https://github.com/jcoliver/bioscan/blob/master/data/iNaturalist-clean-reduced.csv&#39;&gt;iNaturalist&lt;/a&gt; and &lt;a href=&#39;https://github.com/jcoliver/bioscan/blob/master/data/BioScanDataComplete.csv&#39;&gt;BioScan&lt;/a&gt;) from &lt;a href=&#34;https://doi.org/10.3390/insects9040186&#34;&gt;Prudic et al. (2018)&lt;/a&gt;. In their study, Prudic et al. compared citizen science with traditional methods in the measurement of butterfly populations.&lt;/p&gt;
&lt;p&gt;I developed this dashboard after reproducing the &lt;a href=&#34;https://github.com/jcoliver/bioscan&#34;&gt;analyses of the original study&lt;/a&gt; in a &lt;a href=&#34;https://github.com/reprohack/reprohack-hq/blob/master/README.md&#34;&gt;Reprohack session&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My coding tasks included transforming the data to a long format,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# There are pseudovariables, that is, observations entered as variables. 
# Since most R processes need the tidy format, convert below 
# (see https://r4ds.had.co.nz/tidy-data.html). The specific numbers 
# found through traps and crowdsourcing methods are preserved.

BioScan = BioScan %&amp;gt;% pivot_longer(
    cols = Anthocharis_sara:Vanessa_cardui, names_to = &amp;quot;Species&amp;quot;,
    values_to = &amp;quot;Number&amp;quot;, values_drop_na = TRUE
  )

# Compare
#str(BioScan)
#str(dat)
# 928 rows now; the result of 29 pseudo-variables being transposed 
# into rows, interacting with 32 previous rows, i.e., 29 * 32 = 928.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;merging three data sets,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# The iNaturalist data set presents a challenge that differs from the 
# pseudovariables found above. The number of animals of each species 
# must be computed from repeated entries, per site.

iNaturalist = merge(iNaturalist, 
                    iNaturalist %&amp;gt;% 
                      count(species, site, name = &#39;Number&#39;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and, as ever, wrangling with the format of the dashboard pages to preserve the format of a table.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Species details {style=&amp;quot;background-color: #FCFCFC;&amp;quot;}
=======================================================================

Column {style=&amp;quot;data-width:100%; position:static; height:1000px;&amp;quot;}
-----------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;div class = &#39;hanging-indent&#39;&gt;
&lt;p&gt;Prudic, K. L., Oliver, J. C., Brown, B. V., &amp;amp; Long, E. C. (2018). Comparisons of citizen science data-gathering approaches to evaluate urban butterfly diversity. &lt;em&gt;Insects, 9&lt;/em&gt;(4), 186. &lt;a href=&#34;https://doi.org/10.3390/insects9040186&#34;&gt;https://doi.org/10.3390/insects9040186&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data is present: Workshops and datathons</title>
      <link>https://pablobernabeu.github.io/2020/data-is-present-workshops-and-datathons/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2020/data-is-present-workshops-and-datathons/</guid>
      <description>


&lt;div id=&#34;enhanced-data-presentation-using-reproducible-documents-and-dashboards&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Enhanced data presentation using reproducible documents and dashboards&lt;/h2&gt;
&lt;div id=&#34;calendar&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calendar&lt;/h3&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;9%&#34; /&gt;
&lt;col width=&#34;23%&#34; /&gt;
&lt;col width=&#34;39%&#34; /&gt;
&lt;col width=&#34;27%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Date&lt;/th&gt;
&lt;th&gt;Title&lt;/th&gt;
&lt;th&gt;Event and location&lt;/th&gt;
&lt;th&gt;Registration&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;26 Nov 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pablobernabeu.github.io/presentation/2020-11-26-mixed-effects-models-in-r-and-a-new-tool-for-data-simulation&#34;&gt;Mixed-effects models in R, and a new tool for data simulation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;New Tricks Seminars, Dept. Psychology, Lancaster University [online]&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;8 Oct 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pablobernabeu.github.io/presentation/2020-10-08-reproducibilidad-en-torno-a-una-aplicacion-web/&#34;&gt;Reproducibilidad en torno a una aplicación web&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Reprohack en español, LatinR Conference 2020 [online]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.eventbrite.com.ar/e/reprohack-en-espanol-latinr-2020-tickets-121741832097&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;13 Aug 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/pablobernabeu/CarpentryCon-2020-workshop-Open-Data-Reproducibility&#34;&gt;Open data and reproducibility: R Markdown, data dashboards and Binder v2.1&lt;/a&gt; (co-led with Florencia D’Andrea)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://2020.carpentrycon.org/schedule/&#34;&gt;CarpentryCon@Home&lt;/a&gt;, The Carpentries [online]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://2020.carpentrycon.org/schedule/&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;26 July 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/pablobernabeu/UKCLC2020-workshop-Open-data-and-reproducibility&#34;&gt;Open data and reproducibility: R Markdown, data dashboards and Binder&lt;/a&gt; (co-led with Eirini Zormpa)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.ukclc2020.com/pre-conference&#34;&gt;UK Cognitive Linguistics Conference&lt;/a&gt;, University of Birmingham [online]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.ukclc2020.com/registration&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;6 May 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pablobernabeu.github.io/PDF/LU-May-Markdown-workshop-programme.pdf&#34;&gt;R Markdown&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Lancaster University [online]&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Event cancelled&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://newcastle2020.satrdays.org/&#34;&gt;Open data and reproducibility 2.0&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://newcastle2020.satrdays.org/&#34;&gt;SatRday Newcastle upon Tyne&lt;/a&gt;, Newcastle University&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://newcastle2020.satrdays.org/&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div style=&#34;margin-top: 5%;&#34;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;This project offers free activities to learn and practise reproducible data presentation. &lt;a href=&#34;https://www.software.ac.uk/about/fellows/pablo-bernabeu&#34;&gt;Pablo Bernabeu&lt;/a&gt; organises these events in the context of a &lt;a href=&#34;https://www.software.ac.uk/programmes-and-events/fellowship-programme&#34;&gt;Software Sustainability Institute Fellowship&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;open-source-software&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Open-source software&lt;/h4&gt;
&lt;p&gt;Programming languages such as &lt;a href=&#34;https://www.r-project.org/about.html&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt; offer free, powerful resources for data processing, visualisation and analysis. Experience in these programs is highly valued in data-intensive disciplines.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;open-data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Open data&lt;/h4&gt;
&lt;p&gt;Original data has become a &lt;a href=&#34;https://www.nature.com/articles/d41586-019-01506-x&#34;&gt;public good in many research fields&lt;/a&gt; thanks to cultural and technological advances. On the internet, we can find innumerable data sets from sources such as scientific journals and repositories (e.g., &lt;a href=&#34;https://osf.io&#34;&gt;OSF&lt;/a&gt;), local and national governments (e.g., &lt;a href=&#34;https://data.london.gov.uk/&#34;&gt;London&lt;/a&gt;, UK [&lt;a href=&#34;https://www.ukdataservice.ac.uk/&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://data.gov.uk/&#34;&gt;2&lt;/a&gt;]), non-governmental organisations (e.g., &lt;a href=&#34;https://data.world/datasets/ngo&#34;&gt;data.world&lt;/a&gt;), etc. Researchers inside and outside academia nowadays share a lot of their data under attribution licences (e.g., &lt;a href=&#34;https://creativecommons.org/&#34;&gt;Creative Commons&lt;/a&gt;, the UK &lt;a href=&#34;http://www.nationalarchives.gov.uk/doc/open-government-licence/version/1/&#34;&gt;Open Government Licence&lt;/a&gt;, etc.). This allows any external analysts to access these raw data, create (additional) visualisations and analyses, and share these. In society, making data more accessible can &lt;a href=&#34;https://digitalcommons.law.yale.edu/cgi/viewcontent.cgi?article=1140&amp;amp;context=yhrdlj&#34;&gt;demonstrably benefit citizens&lt;/a&gt; (despite &lt;a href=&#34;https://firstmonday.org/ojs/index.php/fm/article/view/3316/2764#author&#34;&gt;limitations&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;activities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Activities&lt;/h2&gt;
&lt;p&gt;Activities comprise free &lt;strong&gt;workshops&lt;/strong&gt; and &lt;a href=&#34;#datathons-creating-reproducible-documents-and-dashboards&#34;&gt;&lt;strong&gt;datathons&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;workshops&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Workshops&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.r-project.org&#34;&gt;R is a programming language&lt;/a&gt; greatly equipped for the creation of reproducible documents and dashboards. Four workshops are offered that cover a suite of interrelated tools—R, R Markdown, data dashboards and Binder environments—, all underlain by reproducible workflows and open-source software.&lt;/p&gt;
&lt;p&gt;Each workshop includes &lt;strong&gt;taught and practical sections&lt;/strong&gt;. The practice provides a chance for participants to experience and address common issues with the code. The level of taught sections is largely tailored to participants; similarly, practice sections are individually adaptable by means of easier and tougher tasks. The duration is also flexible, and some of the workshops can be combined into the same session.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://rstudio.com/&#34;&gt;RStudio&lt;/a&gt; interface is used in all workshops. Multi-levelled, real code examples are used. Throughout the workshops, and especially in the practice sections, individual questions will be encouraged.&lt;/p&gt;
&lt;div id=&#34;workshop-1-introduction-to-r&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 1: Introduction to R&lt;/h4&gt;
&lt;p&gt;This workshop can serve as an introduction to R or a revision. It demonstrates what can be done in R, and provides resources for individual training. Since the duration is limited, online courses are also recommended (&lt;a href=&#34;https://www.coursera.org/courses?query=r&#34;&gt;see examples&lt;/a&gt; and &lt;a href=&#34;https://learner.coursera.help/hc/en-us/articles/209819033-Apply-for-Financial-Aid-or-a-Scholarship&#34;&gt;fee waivers for full content&lt;/a&gt;).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://adv-r.had.co.nz/Data-structures.html&#34;&gt;Data structures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.sthda.com/english/wiki/installing-and-using-r-packages&#34;&gt;Packages&lt;/a&gt;: general-purpose examples (e.g., &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt;) and more specific ones (e.g., for &lt;a href=&#34;https://cran.r-project.org/web/packages/lsr/lsr.pdf&#34;&gt;statistics&lt;/a&gt; or &lt;a href=&#34;https://cran.r-project.org/web/packages/GEOmap/GEOmap.pdf&#34;&gt;geography&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/rio/vignettes/rio.html&#34;&gt;Loading and writing data, in native and foreign formats&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Tidy&lt;/em&gt; format&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Wide&lt;/em&gt; versus &lt;em&gt;long&lt;/em&gt; format. For most processes in R, &lt;a href=&#34;https://r4ds.had.co.nz/tidy-data.html&#34;&gt;data needs to be in a tidy, long format&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://doi.org/10.1371/journal.pbio.3000202.g001&#39;&gt;&lt;img width = &#39;45%&#39; src = &#39;https://journals.plos.org/plosbiology/article/file?id=10.1371/journal.pbio.3000202.g001&amp;type=large&#39; alt = &#39;Illustration of wide and tidy data formats, from Postma and Goedhart (2019)&#39; style = &#39;margin-top:-0.1px;&#39; /&gt;&lt;/a&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:80%; color:darkgrey; text-align:center; margin-top:-24px; margin-bottom:20px;&#34;&gt;
Illustration from &lt;a href=&#34;https://doi.org/10.1371/journal.pbio.3000202.g001&#34;&gt;Postma and Goedhart (2019)&lt;/a&gt;.
&lt;/p&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://psyteachr.github.io/msc-data-skills/joins.html#joins&#34;&gt;Combining data sets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cookbook-r.com/Manipulating_data/Summarizing_data/&#34;&gt;Data summaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://philmcaleer.github.io/ug2-practical/visualisation-through-ggplot2.html&#34;&gt;Plots with &lt;code&gt;ggplot2::ggplot()&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://plot.ly/ggplot2/&#34;&gt;Interactive plots with &lt;code&gt;plotly::ggplotly()&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learningstatisticswithr-bookdown.netlify.com/part-v-statistical-tools.html&#34;&gt;Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer_and_Curtin_LMEMs-2017-Psych_Methods.pdf&#34;&gt;Linear mixed-effects models&lt;/a&gt; (see also &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0749596X20300061?dgcid=coauthor#b0670&#34;&gt;a review of practices&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://swcarpentry.github.io/r-novice-inflammation/02-func-R/&#34;&gt;How functions work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Debugging&lt;/em&gt;. Code errors are known as bugs. They can tiresome, but also interesting sometimes! :sweat_smile: Some tips for the first many years of experience include: reading and investigating error messages, in both source and console windows; controlling letter case and typos; closing parentheses and inverted commas; ensuring to have the necessary packages installed and loaded; following the format required by each function. To debug, break up code into subcomponents and test each of those to find out the source of the error. Once we act on that, the best outcome is seeing the code work, but sometimes different errors overlap, in which case we may see one error disappearing before another one appears. Debugging soon leads to proficient information seeking. The search process often begins on an internet search engine and extends to user communities, package documentation, tutorials, blogs… (see &lt;a href=&#34;https://youtu.be/Nj9J5iCSMB0?t=2687&#34;&gt;video explanation&lt;/a&gt;). &lt;a href=&#34;https://adv-r.hadley.nz/debugging.html&#34;&gt;Advanced debugging tools are also available&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Vast availability of free resources on the internet, from &lt;a href=&#34;https://www.coursera.org/courses?query=r%20programming&#34;&gt;Coursera&lt;/a&gt; and other MOOC sites, &lt;a href=&#34;https://education.rstudio.com/&#34;&gt;RStudio&lt;/a&gt;, &lt;a href=&#34;https://psyteachr.github.io/&#34;&gt;University of Glasgow&lt;/a&gt;, &lt;a href=&#34;http://swcarpentry.github.io/r-novice-inflammation/&#34;&gt;Carpentries&lt;/a&gt;, etc.&lt;/li&gt;
&lt;li&gt;Community: &lt;a href=&#34;https://stackoverflow.com/&#34;&gt;StackOverflow&lt;/a&gt;, &lt;a href=&#34;https://community.rstudio.com/&#34;&gt;RStudio Community&lt;/a&gt;, &lt;a href=&#34;https://github.com&#34;&gt;Github issues&lt;/a&gt; (e.g., for R packages), etc. Using and contributing back.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rstudio.cloud/&#34;&gt;RStudio Cloud&lt;/a&gt;: a personal RStudio environment on the internet&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;workshop-2-r-markdown-documents&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 2: R Markdown documents&lt;/h4&gt;
&lt;p&gt;Set your input and output in stone using &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;R Markdown&lt;/a&gt;. The analysis reports may be enriched with website features (HTML/CSS) and published as HTML, PDF or Word documents. Moreover, with R packages such as &lt;code&gt;bookdown&lt;/code&gt;, &lt;code&gt;bookdownplus&lt;/code&gt;, &lt;code&gt;blogdown&lt;/code&gt; and &lt;code&gt;flexdashboard&lt;/code&gt;, documents can be formatted as &lt;a href=&#34;https://awesome-blogdown.com/&#34;&gt;websites&lt;/a&gt;, &lt;a href=&#34;https://bookdown.org/&#34;&gt;digital papers and books&lt;/a&gt; and &lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/&#34;&gt;data dashboards&lt;/a&gt;. Other useful packages include &lt;code&gt;rmarkdown&lt;/code&gt;, &lt;code&gt;knitr&lt;/code&gt;, &lt;code&gt;DT&lt;/code&gt;, &lt;code&gt;kableExtra&lt;/code&gt; and &lt;code&gt;ggplot2&lt;/code&gt;. Further background: &lt;a href=&#34;https://www.youtube.com/watch?v=Nj9J5iCSMB0&#34;&gt;presentation by Michael Frank&lt;/a&gt;, &lt;a href=&#34;https://www.eddjberry.com/talks/reproducible-writing-with-rmarkdown.html#1&#34;&gt;slides by Ed Berry&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As well as facilitating the reproducibility of analyses and results to third parties, R Markdown is helpful &lt;em&gt;during the creation&lt;/em&gt; of a report. In particular, it reduces the chances of errors and the number of repetitive tasks. For instance, any part of the data can be inputted in the text directly from the source, rather than manually copying it (e.g., &lt;code&gt;`r mean(dat[dat$location==&#39;Havana&#39;, &#39;measure&#39;])`&lt;/code&gt; (&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/r-code.html&#34;&gt;expand&lt;/a&gt;). Thus, if and when the analysis needs to be changed or updated, the report can be automatically updated at the click of a button. In another area, the captions for figures and tables can be automatised using &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook/cross-ref.html&#34;&gt;cross-reference labels&lt;/a&gt; (e.g., Table &lt;code&gt;\@ref(tab:mtcars)&lt;/code&gt;). This secures the match between the text and the captions of figures and tables, and it automatically updates the numbering whenever and wherever a new figure or table is introduced.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://bookdownplus.netlify.com/portfolio/&#39;&gt;&lt;img width = &#39;50%&#39; src = &#39;https://github.com/pablobernabeu/bookdownplus/blob/master/inst2/copernicus/showcase/copernicus2.png?raw=true&#39; alt = &#39;Example of paper created with bookdownplus (image retrieved from R bookdownplus package)&#39;/&gt;&lt;/a&gt;
&lt;p align=&#34;center&#34; style=&#34;text-align:center; margin-top:-30px; margin-bottom:30px;&#34;&gt;
Image from bookdownplus package (&lt;a href=&#34;https://bookdownplus.netlify.com/portfolio/&#34; class=&#34;uri&#34;&gt;https://bookdownplus.netlify.com/portfolio/&lt;/a&gt;).
&lt;/p&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;workshop-3-introduction-to-data-dashboards&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 3: Introduction to data dashboards&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01782/full&#34;&gt;Data dashboards are web applications used to visualise data&lt;/a&gt; in detail through tables and plots. They assist in explaining and accounting for our data processing and analysis. They don’t require any coding from the end user. While most dashboards and web applications present existing data, a few of them serve the purpose of creating or simulating new data (see &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation&#34;&gt;example&lt;/a&gt;).&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/&#39;&gt; &lt;img width = &#39;90%&#39; src = &#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/8.png&#39; alt = &#39;Illustration of the usage of dashboards alongside data repositories&#39; /&gt; &lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;These all-reproducible dashboards are published as websites, and thus, they can include hyperlinks and downloadable files. Some of the R packages used are &lt;code&gt;knitr&lt;/code&gt;, &lt;code&gt;DT&lt;/code&gt;, &lt;code&gt;kableExtra&lt;/code&gt;, &lt;code&gt;reactable&lt;/code&gt;, &lt;code&gt;ggplot2&lt;/code&gt;, &lt;code&gt;plotly&lt;/code&gt;, &lt;code&gt;rmarkdown&lt;/code&gt;, &lt;code&gt;flexdashboard&lt;/code&gt; and &lt;code&gt;shiny&lt;/code&gt;. The aim of this workshop is to practise creating different forms of dashboards—&lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/&#34;&gt;Flexdashboard&lt;/a&gt; and &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;Shiny&lt;/a&gt;—the latter of which offers &lt;a href=&#34;https://mastering-shiny.org/&#34;&gt;greater features&lt;/a&gt;, and to practise also with the hosting platforms fitting each type—such as personal websites, &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;, &lt;a href=&#34;https://mybinder.org/&#34;&gt;Binder&lt;/a&gt;, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;Shinyapps&lt;/a&gt; and &lt;a href=&#34;https://rstudio.com/products/shiny/shiny-server/&#34;&gt;custom servers&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A great thing about dashboards is that they may be made very simple, but they can also be taken to the next level using some HTML, CSS or Javascript code (on top of the back-end code present in the R packages used), which is addressed in the next workshop.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;workshop-4-binder-environments-and-improving-data-dashboards&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 4: Binder environments and improving data dashboards&lt;/h4&gt;
&lt;div id=&#34;binder&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Binder&lt;/h5&gt;
&lt;p&gt;&lt;a href=&#34;https://mybinder.org/&#34;&gt;Binder&lt;/a&gt; is a tool to facilitate public access to software environments—for instance, by publishing an RStudio environment on the internet. Binder can also host Shiny apps. It is generously free &lt;a href=&#34;https://discourse.jupyter.org/t/mybinder-org-cost-updates/2426&#34;&gt;for users&lt;/a&gt;. After looking at the &lt;a href=&#34;https://github.com/binder-examples/r&#34;&gt;nuts and bolts of a deployment&lt;/a&gt;, participants will be able to deploy their own Binder environments and check the result by the end of the workshop. For this purpose, it’s recommended to have data and R code ready, ideally in a GitHub repository.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;improving-data-dashboards&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Improving data dashboards&lt;/h5&gt;
&lt;p&gt;We will practise how to improve the functionality of dashboards using some HTML, CSS and Javascript code, which is &lt;a href=&#34;https://www.w3schools.com/whatis/&#34;&gt;the basis of websites&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!-- Javascript function to enable a hovering tooltip --&amp;gt;
&amp;lt;script&amp;gt;
$(document).ready(function(){
$(&amp;#39;[data-toggle=&amp;quot;tooltip1&amp;quot;]&amp;#39;).tooltip();
});
&amp;lt;/script&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://shiny.rstudio.com/gallery/&#39;&gt; &lt;img align = &#39;center&#39; width = &#39;60%&#39; src = &#39;https://raw.githubusercontent.com/pablobernabeu/data-is-present/master/dashboard%20gif.gif&#39; alt = &#39;Examples of data dashboards&#39; /&gt; &lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trade-offs-among-dashboards&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Trade-offs among dashboards&lt;/h5&gt;
&lt;p&gt;Next, we will practise with three dashboard types—&lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/using.html&#34;&gt;Flexdashboard&lt;/a&gt;, &lt;a href=&#34;https://shiny.rstudio.com/tutorial/&#34;&gt;Shiny&lt;/a&gt; and &lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/shiny.html&#34;&gt;Flexdashboard-Shiny&lt;/a&gt;—and with the suitable hosting platforms. Firstly, the strength of Flexdashboard (&lt;a href=&#34;http://rpubs.com/pcbernabeu/Dutch-modality-exclusivity-norms&#34;&gt;example&lt;/a&gt;) is its basis on R Markdown, yielding an unmatched user interface (&lt;em&gt;front-end&lt;/em&gt;). Secondly, the strength of Shiny (&lt;a href=&#34;https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment/&#34;&gt;example&lt;/a&gt;) is the input reactivity (&lt;em&gt;back-end&lt;/em&gt;) it offers, allowing users to download sections of data they select, in various formats. Last, Flexdashboard-Shiny (&lt;a href=&#34;https://pablobernabeu.shinyapps.io/dutch-modality-exclusivity-norms/&#34;&gt;example&lt;/a&gt;) combines the best of both worlds.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ &lt;b&gt; Flexdashboard &lt;/b&gt; ★
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ ★ &lt;b&gt; Shiny &lt;/b&gt; ★ ★
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ ★ ★ &lt;b&gt; Flexdashboard-Shiny &lt;/b&gt; ★ ★ ★
&lt;/p&gt;
&lt;p&gt;Flexdashboard types are rendered as an HTML document—simple websites—, and can therefore be easily published on personal sites or &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;. This is convenient because no special hosting is required. In contrast, Shiny and Flexdashboard-Shiny types offer greater features, but require Shiny servers. Fortunately, the shinyapps.io server is available for free, up to some &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;usage limit&lt;/a&gt;. This server can host any of the three dashboards mentioned here. Another good option is presented by Binder environments, which can host the Shiny-type dashboards with no (explicit) limit. Yet, the Flexdashboard-Shiny type cannot be hosted in this server (&lt;a href=&#34;https://github.com/jupyter/repo2docker/issues/799&#34;&gt;as of January 2020, at least&lt;/a&gt;). Consequently, greater functionality may come at a cost for dashboards that have any considerable traffic, whereas dashboards with low traffic may do well on shinyapps.io. Knowing these trade-offs can help navigate &lt;a href=&#34;https://support.rstudio.com/hc/en-us/articles/217592947-What-are-the-limits-of-the-shinyapps-io-Free-plan-&#34;&gt;usage limits&lt;/a&gt;, save on web hosting fees, and increase the availability of our dashboards online, as we can offer fall-back versions on different platforms, as in the example below:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;… &lt;em&gt;&lt;a href=&#34;https://pablobernabeu.shinyapps.io/dutch-modality-exclusivity-norms/&#34;&gt;preferred-dashboard&lt;/a&gt; (in case of downtime, please visit this &lt;a href=&#34;http://rpubs.com/pcbernabeu/Dutch-modality-exclusivity-norms&#34;&gt;alternative&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Transforming dashboards into the different versions can be as easy as enabling or disabling some features, especially input reactivity. For instance, if we want to downgrade a Flexdashboard-Shiny to a Flexdashboard, to publish it outside of a Shiny server (see &lt;a href=&#34;https://github.com/pablobernabeu/Modality-exclusivity-norms-Bernabeu-et-al/blob/master/Dutch-modality-exclusivity-norms-RPubs.Rmd&#34;&gt;example&lt;/a&gt;), we must delete &lt;code&gt;runtime:shiny&lt;/code&gt; from the header, and disable reactive features, as below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
``` r
# Number of words selected on sidebar
# reactive(cat(paste0(&amp;#39;Words selected below: &amp;#39;, nrow(selected_props()))))
```&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;free-accounts-and-tips&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Free accounts and tips&lt;/h5&gt;
&lt;p&gt;Hosting sites have specific terms of use. For instance, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;shinyapps.io&lt;/a&gt; has a free starter license with limited use. Free apps can handle a large but limited amount of data, and up to five apps may be created. Beyond this, RStudio offers a wide range of subscriptions starting at $9/month.&lt;/p&gt;
&lt;p&gt;Memory and traffic limits of the free shinyapps.io account can sometimes present problems when heavy data data sets are used, or there are many visits to the app. The memory overload issue is often flagged as &lt;code&gt;Shiny cannot use on-disk bookmarking&lt;/code&gt;, whereas excessive traffic may see the app not loading. Fortunately, usage limits need not always require a paid subscription or a &lt;a href=&#34;https://www.r-bloggers.com/alternative-approaches-to-scaling-shiny-with-rstudio-shiny-server-shinyproxy-or-custom-architecture/&#34;&gt;custom server&lt;/a&gt;, thanks to the following workarounds:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;develop app locally as far as possible, and only deploy to shinyapps.io only at the last stage;&lt;/li&gt;
&lt;li&gt;prune data set, leaving only the necessary data;&lt;/li&gt;
&lt;li&gt;if necessary, unlink data by splitting it into different sets, reducing computational demands;&lt;/li&gt;
&lt;li&gt;if necessary, use various apps (five are allowed in each free shinyapps.io account);&lt;/li&gt;
&lt;li&gt;if necessary, link from the app to a PDF with visualisations requiring heavy, interlinked data. High-resolution plots can be rendered into a PDF document in a snap, using code such as below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;pdf(&amp;#39;List of plots per page&amp;#39;, width = 13, height = 5)
print(plot1)
print(plot2)
# ...
print(plot150)
dev.off()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conveniently, all text in a PDF—even in plots—is indexed, so it can be searched [ Ctrl+f / Cmd+f / 🔍 ] (see &lt;a href=&#34;https://osf.io/2tpxn/&#34;&gt;example&lt;/a&gt;). Furthermore, you may also &lt;a href=&#34;http://www.ilovepdf.com/&#34;&gt;merge the rendered PDF with any other documents&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;prerequisites-and-suggestions-for-participation-in-each-workshop&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Prerequisites and suggestions for participation in each workshop&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Necessary:&lt;/em&gt; laptop or computer with &lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;https://rstudio.com/products/rstudio/download/&#34;&gt;RStudio&lt;/a&gt; installed, or access to &lt;a href=&#34;https://rstudio.cloud/&#34;&gt;RStudio Cloud&lt;/a&gt;; familiarity with the content of the preceding workshops through the web links herein.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Suggested:&lt;/em&gt; having your own data and R code ready (on a Github repository if participating in Workshop 4); participation in some of the preceding workshops.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;datathons-creating-reproducible-documents-and-dashboards&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Datathons: creating reproducible documents and dashboards&lt;/h3&gt;
&lt;p&gt;In these coding meetups, participants collaborate to create reproducible documents or dashboards using the data and software they prefer (see &lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/tree/master/examples-documents-dashboards&#34;&gt;examples&lt;/a&gt;). Since the work can be split across different people and sections, some nice products may be achieved within a session. Any programming languages may be used.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Data used:&lt;/strong&gt; academic or non-academic data of your own or from open-access sources such as &lt;a href=&#34;https://osf.io&#34;&gt;OSF&lt;/a&gt;, scientific journals, governments, international institutions, NGOs, etc.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inspired by the great &lt;a href=&#34;https://reprohack.github.io/reprohack-hq/&#34;&gt;Reprohacks&lt;/a&gt;, content suggestions are encouraged. That is, if you’d like to have a reproducible document or dashboard created for a certain, open-access data set, please let us know, and some participants may take it on. Suggestions may be posted as &lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/issues&#34;&gt;issues&lt;/a&gt; or emailed to &lt;a href=&#34;mailto:p.bernabeu@lancaster.ac.uk&#34; class=&#34;email&#34;&gt;p.bernabeu@lancaster.ac.uk&lt;/a&gt;.
&lt;br&gt;&lt;/br&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Purposes&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;collaborating to visualise data in novel ways using reproducible documents or interactive dashboards. For this purpose, participants sometimes draw on additional data to look at a bigger picture;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;reflecting on the process by reviewing the techniques applied and challenges encountered.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt; A key aspect of datathons is the creation of output. Documents and dashboards are (co-)authored by the participants who work on them, who can then publish them on their websites, or on &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;, &lt;a href=&#34;https://mybinder.org/&#34;&gt;Binder&lt;/a&gt;, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;Shinyapps&lt;/a&gt; or &lt;a href=&#34;https://rstudio.com/products/shiny/shiny-server/&#34;&gt;custom servers&lt;/a&gt;. Time constraints notwithstanding, a lot of this output may be very enticing for further development by the same participants, or even by other people if the code is shared online. Just like with data, an attribution licence can be attached to the code.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;prerequisites-and-suggestions-for-participation-in-datathons&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Prerequisites and suggestions for participation in datathons&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Necessary:&lt;/em&gt; basic knowledge of reproducible documents or dashboards.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Suggested:&lt;/em&gt; familiarity with the development of reproducible documents or dashboards; an idea about the data you’d like to work with and the kind of document or dashboard you want to create.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;contact&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Contact&lt;/h2&gt;
&lt;p&gt;Please submit any queries or requests by &lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/issues&#34;&gt;posting an issue&lt;/a&gt; or emailing &lt;a href=&#34;mailto:p.bernabeu@lancaster.ac.uk&#34; class=&#34;email&#34;&gt;p.bernabeu@lancaster.ac.uk&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dutch modality exclusivity norms for 336 properties and 411 concepts</title>
      <link>https://pablobernabeu.github.io/publication/dutch-modality-exclusivity-norms-for-336-properties-and-411-concepts/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/publication/dutch-modality-exclusivity-norms-for-336-properties-and-411-concepts/</guid>
      <description>&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;div class = &#39;hanging-indent&#39;&gt;
&lt;p&gt;Bernabeu, P. (2018). &lt;em&gt;Dutch modality exclusivity norms for 336 properties and 411 concepts&lt;/em&gt;. PsyArXiv. &lt;a href=&#34;https://doi.org/10.31234/osf.io/s2c5h&#34;&gt;https://doi.org/10.31234/osf.io/s2c5h&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;related-podcast&#34;&gt;Related podcast&lt;/h3&gt;
&lt;p&gt;&lt;i class=&#34;fa-solid fa-wand-magic-sparkles&#34; style=&#39;color:darkgrey;&#39;&gt;&lt;/i&gt; &lt;span style=&#39;color:darkgrey; font-style:italic; font-size:85%;&#39;&gt;Created using Google Gemini and NotebookLM.&lt;/span&gt;&lt;/p&gt;
&lt;iframe allow=&#34;autoplay *; encrypted-media *; fullscreen *; clipboard-write&#34; frameborder=&#34;0&#34; height=&#34;175&#34; style=&#34;width:100%;max-width:660px;overflow:hidden;border-radius:10px;&#34; sandbox=&#34;allow-forms allow-popups allow-same-origin allow-scripts allow-storage-access-by-user-activation allow-top-navigation-by-user-activation&#34; src=&#34;https://embed.podcasts.apple.com/us/podcast/behind-the-curtains-methods-used-to-investigate/id1837010092?i=1000725594999&#34;&gt;&lt;/iframe&gt;
&lt;h3 id=&#34;related-references&#34;&gt;Related references&lt;/h3&gt;
&lt;div class = &#39;related-references&#39;&gt;
&lt;div class = &#39;hanging-indent&#39;&gt;
&lt;p&gt;Anderson, A. J., Binder, J. R., Fernandino, L., Humphries, C. J., Conant, L. L., Aguilar, M., Wang, X., Doko, D., &amp;amp; Raizada, R. D. S. (2016). Predicting Neural Activity Patterns Associated with Sentences Using a Neurobiologically Motivated Model of Semantic Representation. &lt;em&gt;Cerebral Cortex&lt;/em&gt;, cercor;bhw240v1. &lt;a href=&#34;https://doi.org/10.1093/cercor/bhw240&#34;&gt;https://doi.org/10.1093/cercor/bhw240&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Anderson, A. J., Binder, J. R., Fernandino, L., Humphries, C. J., Conant, L. L., Raizada, R. D. S., Lin, F., &amp;amp; Lalor, E. C. (2019). An Integrated Neural Decoder of Linguistic and Experiential Meaning. &lt;em&gt;The Journal of Neuroscience&lt;/em&gt;, &lt;em&gt;39&lt;/em&gt;(45), 8969&amp;ndash;8987. &lt;a href=&#34;https://doi.org/10.1523/JNEUROSCI.2575-18.2019&#34;&gt;https://doi.org/10.1523/JNEUROSCI.2575-18.2019&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Anderson, A. J., &amp;amp; Lin, F. (2019). How pattern information analyses of semantic brain activity elicited in language comprehension could contribute to the early identification of Alzheimer&#39;s Disease. &lt;em&gt;NeuroImage: Clinical&lt;/em&gt;, &lt;em&gt;22&lt;/em&gt;, 101788. &lt;a href=&#34;https://doi.org/10.1016/j.nicl.2019.101788&#34;&gt;https://doi.org/10.1016/j.nicl.2019.101788&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bagli, M. (2023). How to Point with Language: English Source-Based Language to Describe Taste Qualities. &lt;em&gt;Lublin Studies in Modern Languages and Literature&lt;/em&gt;, &lt;em&gt;42&lt;/em&gt;(2), 31&amp;ndash;46. &lt;a href=&#34;https://doi.org/10.17951/lsmll.2023.47.2.31-46&#34;&gt;https://doi.org/10.17951/lsmll.2023.47.2.31-46&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Banks, B., Borghi, A. M., Fargier, R., Fini, C., Jonauskaite, D., Mazzuca, C., Montalti, M., Villani, C., &amp;amp; Woodin, G. (2023). Consensus Paper: Current Perspectives on Abstract Concepts and Future Research Directions. &lt;em&gt;Journal of Cognition&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;(1), 62. &lt;a href=&#34;https://doi.org/10.5334/joc.238&#34;&gt;https://doi.org/10.5334/joc.238&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bolognesi, M., Burgers, C., &amp;amp; Caselli, T. (2020). On abstraction: Decoupling conceptual concreteness and categorical specificity. &lt;em&gt;Cognitive Processing&lt;/em&gt;, &lt;em&gt;21&lt;/em&gt;(3), 365&amp;ndash;381. &lt;a href=&#34;https://doi.org/10.1007/s10339-020-00965-9&#34;&gt;https://doi.org/10.1007/s10339-020-00965-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Borghi, A. M., Mazzuca, C., Gervasi, A. M., Mannella, F., &amp;amp; Tummolini, L. (2023). Grounded cognition can be multimodal all the way down. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, 1&amp;ndash;5. &lt;a href=&#34;https://doi.org/10.1080/23273798.2023.2210238&#34;&gt;https://doi.org/10.1080/23273798.2023.2210238&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bottini, R., Morucci, P., D&#39;Urso, A., Collignon, O., &amp;amp; Crepaldi, D. (2022). The concreteness advantage in lexical decision does not depend on perceptual simulations. &lt;em&gt;Journal of Experimental Psychology: General&lt;/em&gt;, &lt;em&gt;151&lt;/em&gt;(3), 731&amp;ndash;738. &lt;a href=&#34;https://doi.org/10.1037/xge0001090&#34;&gt;https://doi.org/10.1037/xge0001090&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bruffaerts, R., De Deyne, S., Meersmans, K., Liuzzi, A. G., Storms, G., &amp;amp; Vandenberghe, R. (2019). Redefining the resolution of semantic knowledge in the brain: Advances made by the introduction of models of semantics in neuroimaging. &lt;em&gt;Neuroscience &amp;amp; Biobehavioral Reviews&lt;/em&gt;, &lt;em&gt;103&lt;/em&gt;, 3&amp;ndash;13. &lt;a href=&#34;https://doi.org/10.1016/j.neubiorev.2019.05.015&#34;&gt;https://doi.org/10.1016/j.neubiorev.2019.05.015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Caballero, R., &amp;amp; Paradis, C. (2020). Soundscapes in English and Spanish: A corpus investigation of verb constructions. &lt;em&gt;Language and Cognition&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(4), 705&amp;ndash;728. &lt;a href=&#34;https://doi.org/10.1017/langcog.2020.19&#34;&gt;https://doi.org/10.1017/langcog.2020.19&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Caballero, R., &amp;amp; Paradis, C. (2023). Sharing Perceptual Experiences through Language. &lt;em&gt;Journal of Intelligence&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt;(7), 129. &lt;a href=&#34;https://doi.org/10.3390/jintelligence11070129&#34;&gt;https://doi.org/10.3390/jintelligence11070129&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Calzavarini, F. (2023). Rethinking modality-specificity in the cognitive neuroscience of concrete word meaning: A position paper. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, 1&amp;ndash;23. &lt;a href=&#34;https://doi.org/10.1080/23273798.2023.2173789&#34;&gt;https://doi.org/10.1080/23273798.2023.2173789&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Carney, J. (2020). Thinking avant la lettre: A Review of 4E Cognition. &lt;em&gt;Evolutionary Studies in Imaginative Culture&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(1), 77&amp;ndash;90. &lt;a href=&#34;https://doi.org/10.26613/esic.4.1.172&#34;&gt;https://doi.org/10.26613/esic.4.1.172&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Charmhun Jo, Sun-A Kim, &amp;amp; Chu-Ren Huang. (2022). Linguistic synesthesia in Korean: A compound word-based study of cross-modal directionality. &lt;em&gt;Linguistic Research&lt;/em&gt;, &lt;em&gt;39&lt;/em&gt;(2), 275&amp;ndash;296. &lt;a href=&#34;https://doi.org/10.17250/KHISLI.39.2.202206.002&#34;&gt;https://doi.org/10.17250/KHISLI.39.2.202206.002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chedid, G., Brambati, S. M., Bedetti, C., Rey, A. E., Wilson, M. A., &amp;amp; Vallet, G. T. (2019). Visual and auditory perceptual strength norms for 3,596 French nouns and their relationship with other psycholinguistic variables. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;51&lt;/em&gt;(5), 2094&amp;ndash;2105. &lt;a href=&#34;https://doi.org/10.3758/s13428-019-01254-w&#34;&gt;https://doi.org/10.3758/s13428-019-01254-w&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chedid, G., Wilson, M. A., Bedetti, C., Rey, A. E., Vallet, G. T., &amp;amp; Brambati, S. M. (2019). Norms of conceptual familiarity for 3,596 French nouns and their contribution in lexical decision. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;51&lt;/em&gt;(5), 2238&amp;ndash;2247. &lt;a href=&#34;https://doi.org/10.3758/s13428-018-1106-8&#34;&gt;https://doi.org/10.3758/s13428-018-1106-8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chen, I.-H., Zhao, Q., Long, Y., Lu, Q., &amp;amp; Huang, C.-R. (2019). Mandarin Chinese modality exclusivity norms. &lt;em&gt;PLOS ONE&lt;/em&gt;, &lt;em&gt;14&lt;/em&gt;(2), e0211336. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0211336&#34;&gt;https://doi.org/10.1371/journal.pone.0211336&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Connell, L. (2019). What have labels ever done for us? The linguistic shortcut in conceptual processing. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;34&lt;/em&gt;(10), 1308&amp;ndash;1318. &lt;a href=&#34;https://doi.org/10.1080/23273798.2018.1471512&#34;&gt;https://doi.org/10.1080/23273798.2018.1471512&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Connell, L., Lynott, D., &amp;amp; Banks, B. (2018). Interoception: The forgotten modality in perceptual grounding of abstract and concrete concepts. &lt;em&gt;Philosophical Transactions of the Royal Society B: Biological Sciences&lt;/em&gt;, &lt;em&gt;373&lt;/em&gt;(1752), 20170143. &lt;a href=&#34;https://doi.org/10.1098/rstb.2017.0143&#34;&gt;https://doi.org/10.1098/rstb.2017.0143&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Corciulo, S., Bioglio, L., Basile, V., Patti, V., &amp;amp; Damiano, R. (2023). The DEEP Sensorium: A multidimensional approach to sensory domain labelling. &lt;em&gt;Companion Proceedings of the ACM Web Conference 2023&lt;/em&gt;, 661&amp;ndash;668. &lt;a href=&#34;https://doi.org/10.1145/3543873.3587631&#34;&gt;https://doi.org/10.1145/3543873.3587631&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Davis, C. P., &amp;amp; Yee, E. (2021). Building semantic memory from embodied and distributional language experience. &lt;em&gt;WIREs Cognitive Science&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(5), e1555. &lt;a href=&#34;https://doi.org/10.1002/wcs.1555&#34;&gt;https://doi.org/10.1002/wcs.1555&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;De Deyne, S., Navarro, D. J., Perfors, A., Brysbaert, M., &amp;amp; Storms, G. (2019). The &amp;ldquo;Small World of Words&amp;rdquo; English word association norms for over 12,000 cue words. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;51&lt;/em&gt;(3), 987&amp;ndash;1006. &lt;a href=&#34;https://doi.org/10.3758/s13428-018-1115-7&#34;&gt;https://doi.org/10.3758/s13428-018-1115-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dellantonio, S., &amp;amp; Pastore, L. (2017). The &amp;lsquo;Proprioceptive&amp;rsquo; Component of Abstract Concepts. In S. Dellantonio &amp;amp; L. Pastore, &lt;em&gt;Internal Perception&lt;/em&gt; (Vol. 40, pp. 297&amp;ndash;357). Springer Berlin Heidelberg. &lt;a href=&#34;https://doi.org/10.1007/978-3-662-55763-1_6&#34;&gt;https://doi.org/10.1007/978-3-662-55763-1_6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Díez-Álamo, A. M., Díez, E., Alonso, M. Á., Vargas, C. A., &amp;amp; Fernandez, A. (2018). Normative ratings for perceptual and motor attributes of 750 object concepts in Spanish. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;50&lt;/em&gt;(4), 1632&amp;ndash;1644. &lt;a href=&#34;https://doi.org/10.3758/s13428-017-0970-y&#34;&gt;https://doi.org/10.3758/s13428-017-0970-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Díez-Álamo, A. M., Díez, E., Wojcik, D. Z., Alonso, M. A., &amp;amp; Fernandez, A. (2019). Sensory experience ratings for 5,500 Spanish words. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;51&lt;/em&gt;(3), 1205&amp;ndash;1215. &lt;a href=&#34;https://doi.org/10.3758/s13428-018-1057-0&#34;&gt;https://doi.org/10.3758/s13428-018-1057-0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dove, G. (2021). The Challenges of Abstract Concepts. In M. D. Robinson &amp;amp; L. E. Thomas (Eds.), &lt;em&gt;Handbook of Embodied Psychology&lt;/em&gt; (pp. 171&amp;ndash;195). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-78471-3_8&#34;&gt;https://doi.org/10.1007/978-3-030-78471-3_8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dymarska, A., Connell, L., &amp;amp; Banks, B. (2023). More is not necessarily better: How different aspects of sensorimotor experience affect recognition memory for words. &lt;em&gt;Journal of Experimental Psychology: Learning, Memory, and Cognition&lt;/em&gt;, &lt;em&gt;49&lt;/em&gt;(10), 1572&amp;ndash;1587. &lt;a href=&#34;https://doi.org/10.1037/xlm0001265&#34;&gt;https://doi.org/10.1037/xlm0001265&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fischer, M. H., &amp;amp; Shaki, S. (2018). Number concepts: Abstract and embodied. &lt;em&gt;Philosophical Transactions of the Royal Society B: Biological Sciences&lt;/em&gt;, &lt;em&gt;373&lt;/em&gt;(1752), 20170125. &lt;a href=&#34;https://doi.org/10.1098/rstb.2017.0125&#34;&gt;https://doi.org/10.1098/rstb.2017.0125&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fishman, A. (2022). The picture looks like my music sounds: Directional preferences in synesthetic metaphors in the absence of lexical factors. &lt;em&gt;Language and Cognition&lt;/em&gt;, &lt;em&gt;14&lt;/em&gt;(2), 208&amp;ndash;227. &lt;a href=&#34;https://doi.org/10.1017/langcog.2022.2&#34;&gt;https://doi.org/10.1017/langcog.2022.2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Gangemi, A. (2020). Closing the Loop between knowledge patterns in cognition and the Semantic Web. &lt;em&gt;Semantic Web&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt;(1), 139&amp;ndash;151. &lt;a href=&#34;https://doi.org/10.3233/SW-190383&#34;&gt;https://doi.org/10.3233/SW-190383&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ghandhari, M., Fini, C., Da Rold, F., &amp;amp; Borghi, A. M. (2020). Different kinds of embodied language: A comparison between Italian and Persian languages. &lt;em&gt;Brain and Cognition&lt;/em&gt;, &lt;em&gt;142&lt;/em&gt;, 105581. &lt;a href=&#34;https://doi.org/10.1016/j.bandc.2020.105581&#34;&gt;https://doi.org/10.1016/j.bandc.2020.105581&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Gijssels, T., &amp;amp; Casasanto, D. (2020). Hand-use norms for Dutch and English manual action verbs: Implicit measures from a pantomime task. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;52&lt;/em&gt;(4), 1744&amp;ndash;1767. &lt;a href=&#34;https://doi.org/10.3758/s13428-020-01347-x&#34;&gt;https://doi.org/10.3758/s13428-020-01347-x&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Harpaintner, M., Sim, E.-J., Trumpp, N. M., Ulrich, M., &amp;amp; Kiefer, M. (2020). The grounding of abstract concepts in the motor and visual system: An fMRI study. &lt;em&gt;Cortex&lt;/em&gt;, &lt;em&gt;124&lt;/em&gt;, 1&amp;ndash;22. &lt;a href=&#34;https://doi.org/10.1016/j.cortex.2019.10.014&#34;&gt;https://doi.org/10.1016/j.cortex.2019.10.014&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Harpaintner, M., Trumpp, N. M., &amp;amp; Kiefer, M. (2018). The Semantic Content of Abstract Concepts: A Property Listing Study of 296 Abstract Words. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;, 1748. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2018.01748&#34;&gt;https://doi.org/10.3389/fpsyg.2018.01748&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Harpaintner, M., Trumpp, N. M., &amp;amp; Kiefer, M. (2022). Time course of brain activity during the processing of motor- and vision-related abstract concepts: Flexibility and task dependency. &lt;em&gt;Psychological Research&lt;/em&gt;, &lt;em&gt;86&lt;/em&gt;(8), 2560&amp;ndash;2582. &lt;a href=&#34;https://doi.org/10.1007/s00426-020-01374-5&#34;&gt;https://doi.org/10.1007/s00426-020-01374-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hartman, J., &amp;amp; Paradis, C. (2023). The language of sound: Events and meaning multitasking of words. &lt;em&gt;Cognitive Linguistics&lt;/em&gt;, &lt;em&gt;34&lt;/em&gt;(3&amp;ndash;4), 445&amp;ndash;477. &lt;a href=&#34;https://doi.org/10.1515/cog-2022-0006&#34;&gt;https://doi.org/10.1515/cog-2022-0006&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hörberg, T., Larsson, M., &amp;amp; Olofsson, J. K. (2022). The Semantic Organization of the English Odor Vocabulary. &lt;em&gt;Cognitive Science&lt;/em&gt;, &lt;em&gt;46&lt;/em&gt;(11), e13205. &lt;a href=&#34;https://doi.org/10.1111/cogs.13205&#34;&gt;https://doi.org/10.1111/cogs.13205&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Huang, C.-R., &amp;amp; Xiong, J. (2019). Linguistic synaesthesia in Chinese. In C.-R. Huang, Z. Jing-Schmidt, &amp;amp; B. Meisterernst (Eds.), &lt;em&gt;The Routledge Handbook of Chinese Applied Linguistics&lt;/em&gt; (1st ed., pp. 294&amp;ndash;312). Routledge. &lt;a href=&#34;https://doi.org/10.4324/9781315625157-20&#34;&gt;https://doi.org/10.4324/9781315625157-20&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Iatropoulos, G., Herman, P., Lansner, A., Karlgren, J., Larsson, M., &amp;amp; Olofsson, J. K. (2018). The language of smell: Connecting linguistic and psychophysical properties of odor descriptors. &lt;em&gt;Cognition&lt;/em&gt;, &lt;em&gt;178&lt;/em&gt;, 37&amp;ndash;49. &lt;a href=&#34;https://doi.org/10.1016/j.cognition.2018.05.007&#34;&gt;https://doi.org/10.1016/j.cognition.2018.05.007&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Jacobs, A. M., &amp;amp; Kinder, A. (2017). &lt;em&gt;&amp;ldquo;The Brain Is the Prisoner of Thought&amp;rdquo;&lt;/em&gt;: A Machine-Learning Assisted Quantitative Narrative Analysis of Literary Metaphors for Use in Neurocognitive Poetics. &lt;em&gt;Metaphor and Symbol&lt;/em&gt;, &lt;em&gt;32&lt;/em&gt;(3), 139&amp;ndash;160. &lt;a href=&#34;https://doi.org/10.1080/10926488.2017.1338015&#34;&gt;https://doi.org/10.1080/10926488.2017.1338015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Jo, C. (2022). Linguistic Synesthesia in Korean: Universality and Variation. &lt;em&gt;SAGE Open&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(3), 215824402211178. &lt;a href=&#34;https://doi.org/10.1177/21582440221117804&#34;&gt;https://doi.org/10.1177/21582440221117804&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Johns, B. T. (2022). Accounting for item-level variance in recognition memory: Comparing word frequency and contextual diversity. &lt;em&gt;Memory &amp;amp; Cognition&lt;/em&gt;, &lt;em&gt;50&lt;/em&gt;(5), 1013&amp;ndash;1032. &lt;a href=&#34;https://doi.org/10.3758/s13421-021-01249-z&#34;&gt;https://doi.org/10.3758/s13421-021-01249-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Jones, L. L., Wurm, L. H., Calcaterra, R. D., &amp;amp; Ofen, N. (2017). Integrative Priming of Compositional and Locative Relations. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2017.00359&#34;&gt;https://doi.org/10.3389/fpsyg.2017.00359&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Julich-Warpakowski, N., &amp;amp; Pérez Sobrino, P. (2023). Introduction: Current challenges in metaphor research. &lt;em&gt;Metaphor and the Social World&lt;/em&gt;, &lt;em&gt;13&lt;/em&gt;(1), 1&amp;ndash;15. &lt;a href=&#34;https://doi.org/10.1075/msw.00026.jul&#34;&gt;https://doi.org/10.1075/msw.00026.jul&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kaiser, E. (2021). Consequences of Sensory Modality for Perspective-Taking: Comparing Visual, Olfactory and Gustatory Perception. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;, 701486. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2021.701486&#34;&gt;https://doi.org/10.3389/fpsyg.2021.701486&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kernot, D., Bossomaier, T., &amp;amp; Bradbury, R. (2017). Novel Text Analysis for Investigating Personality: Identifying the Dark Lady in Shakespeare&#39;s Sonnets. &lt;em&gt;Journal of Quantitative Linguistics&lt;/em&gt;, &lt;em&gt;24&lt;/em&gt;(4), 255&amp;ndash;272. &lt;a href=&#34;https://doi.org/10.1080/09296174.2017.1304049&#34;&gt;https://doi.org/10.1080/09296174.2017.1304049&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kernot, D., Bossomaier, T., &amp;amp; Bradbury, R. (2018). Using Shakespeare&#39;s Sotto Voce to Determine True Identity From Text. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;, 289. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2018.00289&#34;&gt;https://doi.org/10.3389/fpsyg.2018.00289&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kernot, D., Bossomaier, T., &amp;amp; Bradbury, R. (2019). The Stylometric Impacts of Ageing and Life Events on Identity. &lt;em&gt;Journal of Quantitative Linguistics&lt;/em&gt;, &lt;em&gt;26&lt;/em&gt;(1), 1&amp;ndash;21. &lt;a href=&#34;https://doi.org/10.1080/09296174.2017.1405719&#34;&gt;https://doi.org/10.1080/09296174.2017.1405719&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Khatin-Zadeh, O., Hu, J., Banaruee, H., &amp;amp; Marmolejo-Ramos, F. (2023). How emotions are metaphorically embodied: Measuring hand and head action strengths of typical emotional states. &lt;em&gt;Cognition and Emotion&lt;/em&gt;, &lt;em&gt;37&lt;/em&gt;(3), 486&amp;ndash;498. &lt;a href=&#34;https://doi.org/10.1080/02699931.2023.2181314&#34;&gt;https://doi.org/10.1080/02699931.2023.2181314&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kiefer, M., Pielke, L., &amp;amp; Trumpp, N. M. (2022). Differential temporo-spatial pattern of electrical brain activity during the processing of abstract concepts related to mental states and verbal associations. &lt;em&gt;NeuroImage&lt;/em&gt;, &lt;em&gt;252&lt;/em&gt;, 119036. &lt;a href=&#34;https://doi.org/10.1016/j.neuroimage.2022.119036&#34;&gt;https://doi.org/10.1016/j.neuroimage.2022.119036&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kim, M.-K., Müller, H. M., &amp;amp; Weiss, S. (2021). What you &amp;ldquo;mean&amp;rdquo; is not what I &amp;ldquo;mean&amp;rdquo;: Categorization of verbs by Germans and Koreans using the semantic differential. &lt;em&gt;Lingua&lt;/em&gt;, &lt;em&gt;252&lt;/em&gt;, 103012. &lt;a href=&#34;https://doi.org/10.1016/j.lingua.2020.103012&#34;&gt;https://doi.org/10.1016/j.lingua.2020.103012&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Koblet, O., &amp;amp; Purves, R. S. (2020). From online texts to Landscape Character Assessment: Collecting and analysing first-person landscape perception computationally. &lt;em&gt;Landscape and Urban Planning&lt;/em&gt;, &lt;em&gt;197&lt;/em&gt;, 103757. &lt;a href=&#34;https://doi.org/10.1016/j.landurbplan.2020.103757&#34;&gt;https://doi.org/10.1016/j.landurbplan.2020.103757&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Körner, A., Castillo, M., Drijvers, L., Fischer, M. H., Günther, F., Marelli, M., Platonova, O., Rinaldi, L., Shaki, S., Trujillo, J. P., Tsaregorodtseva, O., &amp;amp; Glenberg, A. M. (2023). Embodied Processing at Six Linguistic Granularity Levels: A Consensus Paper. &lt;em&gt;Journal of Cognition&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;(1), 60. &lt;a href=&#34;https://doi.org/10.5334/joc.231&#34;&gt;https://doi.org/10.5334/joc.231&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Krishna, P. P., Arulmozi, S., &amp;amp; Mishra, R. K. (2022). &amp;ldquo;Do You See and Hear More? A Study on Telugu Perception Verbs.&amp;rdquo; &lt;em&gt;Journal of Psycholinguistic Research&lt;/em&gt;, &lt;em&gt;51&lt;/em&gt;(3), 473&amp;ndash;484. &lt;a href=&#34;https://doi.org/10.1007/s10936-021-09827-7&#34;&gt;https://doi.org/10.1007/s10936-021-09827-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kumcu, A. (2021). Linguistic Synesthesia in Turkish: A Corpus-based Study of Crossmodal Directionality. &lt;em&gt;Metaphor and Symbol&lt;/em&gt;, &lt;em&gt;36&lt;/em&gt;(4), 241&amp;ndash;255. &lt;a href=&#34;https://doi.org/10.1080/10926488.2021.1921557&#34;&gt;https://doi.org/10.1080/10926488.2021.1921557&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lau, S. H., Huang, Y., Ferreira, V. S., &amp;amp; Vul, E. (2019). Perceptual features predict word frequency asymmetry across modalities. &lt;em&gt;Attention, Perception, &amp;amp; Psychophysics&lt;/em&gt;, &lt;em&gt;81&lt;/em&gt;(4), 1076&amp;ndash;1087. &lt;a href=&#34;https://doi.org/10.3758/s13414-019-01682-y&#34;&gt;https://doi.org/10.3758/s13414-019-01682-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lee, J., &amp;amp; Shin, J.-A. (2023). The cross-linguistic comparison of perceptual strength norms for Korean, English and L2 English. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;14&lt;/em&gt;, 1188909. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2023.1188909&#34;&gt;https://doi.org/10.3389/fpsyg.2023.1188909&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Li, M., Lu, Q., Long, Y., &amp;amp; Gui, L. (2017). Inferring Affective Meanings of Words from Word Embedding. &lt;em&gt;IEEE Transactions on Affective Computing&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;(4), 443&amp;ndash;456. &lt;a href=&#34;https://doi.org/10.1109/TAFFC.2017.2723012&#34;&gt;https://doi.org/10.1109/TAFFC.2017.2723012&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Littlemore, J., Sobrino, P. P., Houghton, D., Shi, J., &amp;amp; Winter, B. (2018). What makes a good metaphor? A cross-cultural study of computer-generated metaphor appreciation. &lt;em&gt;Metaphor and Symbol&lt;/em&gt;, &lt;em&gt;33&lt;/em&gt;(2), 101&amp;ndash;122. &lt;a href=&#34;https://doi.org/10.1080/10926488.2018.1434944&#34;&gt;https://doi.org/10.1080/10926488.2018.1434944&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, W., Bansal, D., Daruna, A., &amp;amp; Chernova, S. (2023). Learning instance-level N-ary semantic knowledge at scale for robots operating in everyday environments. &lt;em&gt;Autonomous Robots&lt;/em&gt;, &lt;em&gt;47&lt;/em&gt;(5), 529&amp;ndash;547. &lt;a href=&#34;https://doi.org/10.1007/s10514-023-10099-4&#34;&gt;https://doi.org/10.1007/s10514-023-10099-4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, W., Bansal, D., Daruna, A., &amp;amp; Chernova, S. (2021, July 12). Learning Instance-Level N-Ary Semantic Knowledge At Scale For Robots Operating in Everyday Environments. &lt;em&gt;Robotics: Science and Systems XVII&lt;/em&gt;. Robotics: Science and Systems 2021. &lt;a href=&#34;https://doi.org/10.15607/RSS.2021.XVII.035&#34;&gt;https://doi.org/10.15607/RSS.2021.XVII.035&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Long, Y., Xiang, R., Lu, Q., Huang, C.-R., &amp;amp; Li, M. (2021). Improving Attention Model Based on Cognition Grounded Data for Sentiment Analysis. &lt;em&gt;IEEE Transactions on Affective Computing&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(4), 900&amp;ndash;912. &lt;a href=&#34;https://doi.org/10.1109/TAFFC.2019.2903056&#34;&gt;https://doi.org/10.1109/TAFFC.2019.2903056&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lynott, D., Connell, L., Brysbaert, M., Brand, J., &amp;amp; Carney, J. (2020). The Lancaster Sensorimotor Norms: Multidimensional measures of perceptual and action strength for 40,000 English words. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;52&lt;/em&gt;(3), 1271&amp;ndash;1291. &lt;a href=&#34;https://doi.org/10.3758/s13428-019-01316-z&#34;&gt;https://doi.org/10.3758/s13428-019-01316-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Mahmood, A., &amp;amp; Yeganegi, S. (2023). Lexical sophistication and crowdfunding outcomes. &lt;em&gt;Venture Capital&lt;/em&gt;, 1&amp;ndash;32. &lt;a href=&#34;https://doi.org/10.1080/13691066.2023.2265565&#34;&gt;https://doi.org/10.1080/13691066.2023.2265565&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Marson, F., Paoletti, P., Naor-Ziv, R., Carducci, F., &amp;amp; Ben-Soussan, T. D. (2023). Embodied empathy and abstract concepts&amp;rsquo; concreteness: Evidence from contemplative practices. In &lt;em&gt;Progress in Brain Research&lt;/em&gt; (Vol. 277, pp. 181&amp;ndash;209). Elsevier. &lt;a href=&#34;https://doi.org/10.1016/bs.pbr.2022.12.005&#34;&gt;https://doi.org/10.1016/bs.pbr.2022.12.005&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Márton, Z. C., Türker, S., Rink, C., Brucker, M., Kriegel, S., Bodenmüller, T., &amp;amp; Riedel, S. (2018). Improving object orientation estimates by considering multiple viewpoints: Orientation histograms of symmetries and measurement models for view selection. &lt;em&gt;Autonomous Robots&lt;/em&gt;, &lt;em&gt;42&lt;/em&gt;(2), 423&amp;ndash;442. &lt;a href=&#34;https://doi.org/10.1007/s10514-017-9633-1&#34;&gt;https://doi.org/10.1007/s10514-017-9633-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Miceli, A., Wauthia, E., Kandana Arachchige, K., Lefebvre, L., Ris, L., &amp;amp; Simoes Loureiro, I. (2023). Perceptual strength influences lexical decision in Alzheimer&#39;s disease. &lt;em&gt;Journal of Neurolinguistics&lt;/em&gt;, &lt;em&gt;68&lt;/em&gt;, 101144. &lt;a href=&#34;https://doi.org/10.1016/j.jneuroling.2023.101144&#34;&gt;https://doi.org/10.1016/j.jneuroling.2023.101144&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Miceli, A., Wauthia, E., Lefebvre, L., Ris, L., &amp;amp; Simoes Loureiro, I. (2021). Perceptual and Interoceptive Strength Norms for 270 French Words. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;, 667271. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2021.667271&#34;&gt;https://doi.org/10.3389/fpsyg.2021.667271&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Miceli, A., Wauthia, E., Lefebvre, L., Vallet, G. T., Ris, L., &amp;amp; Loureiro, I. S. (2022). Differences related to aging in sensorimotor knowledge: Investigation of perceptual strength and body object interaction. &lt;em&gt;Archives of Gerontology and Geriatrics&lt;/em&gt;, &lt;em&gt;102&lt;/em&gt;, 104715. &lt;a href=&#34;https://doi.org/10.1016/j.archger.2022.104715&#34;&gt;https://doi.org/10.1016/j.archger.2022.104715&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Miklashevsky, A. (2018). Perceptual Experience Norms for 506 Russian Nouns: Modality Rating, Spatial Localization, Manipulability, Imageability and Other Variables. &lt;em&gt;Journal of Psycholinguistic Research&lt;/em&gt;, &lt;em&gt;47&lt;/em&gt;(3), 641&amp;ndash;661. &lt;a href=&#34;https://doi.org/10.1007/s10936-017-9548-1&#34;&gt;https://doi.org/10.1007/s10936-017-9548-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Morucci, P., Bottini, R., &amp;amp; Crepaldi, D. (2019). Augmented Modality Exclusivity Norms for Concrete and Abstract Italian Property Words. &lt;em&gt;Journal of Cognition&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;(1), 42. &lt;a href=&#34;https://doi.org/10.5334/joc.88&#34;&gt;https://doi.org/10.5334/joc.88&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Okuno, H. Y., &amp;amp; Guedes, G. (2020). Automatic XML creation for Multisensorial Books. &lt;em&gt;2020 XV Conferencia Latinoamericana de Tecnologias de Aprendizaje (LACLO)&lt;/em&gt;, 1&amp;ndash;6. &lt;a href=&#34;https://doi.org/10.1109/LACLO50806.2020.9381139&#34;&gt;https://doi.org/10.1109/LACLO50806.2020.9381139&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pathak, A., Velasco, C., Petit, O., &amp;amp; Calvert, G. A. (2019). Going to great lengths in the pursuit of luxury: How longer brand names can enhance the luxury perception of a brand. &lt;em&gt;Psychology &amp;amp; Marketing&lt;/em&gt;, &lt;em&gt;36&lt;/em&gt;(10), 951&amp;ndash;963. &lt;a href=&#34;https://doi.org/10.1002/mar.21247&#34;&gt;https://doi.org/10.1002/mar.21247&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pérez-Sánchez, M. Á., Stadthagen-Gonzalez, H., Guasch, M., Hinojosa, J. A., Fraga, I., Marín, J., &amp;amp; Ferré, P. (2021). EmoPro &amp;ndash; Emotional prototypicality for 1286 Spanish words: Relationships with affective and psycholinguistic variables. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;53&lt;/em&gt;(5), 1857&amp;ndash;1875. &lt;a href=&#34;https://doi.org/10.3758/s13428-020-01519-9&#34;&gt;https://doi.org/10.3758/s13428-020-01519-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Perlman, M., Little, H., Thompson, B., &amp;amp; Thompson, R. L. (2018). Iconicity in Signed and Spoken Vocabulary: A Comparison Between American Sign Language, British Sign Language, English, and Spanish. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;, 1433. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2018.01433&#34;&gt;https://doi.org/10.3389/fpsyg.2018.01433&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pexman, P. M., Muraki, E., Sidhu, D. M., Siakaluk, P. D., &amp;amp; Yap, M. J. (2019). Quantifying sensorimotor experience: Body&amp;ndash;object interaction ratings for more than 9,000 English words. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;51&lt;/em&gt;(2), 453&amp;ndash;466. &lt;a href=&#34;https://doi.org/10.3758/s13428-018-1171-z&#34;&gt;https://doi.org/10.3758/s13428-018-1171-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Plekhanov Russian University of Economics, Simonenko, M. A., Kazaryan, S. Y., &amp;amp; Plekhanov Russian University of Economics. (2023). Synaesthetic metaphor and its reproduction in Russian-to-English translation: A frame-based study. &lt;em&gt;RESEARCH RESULT Theoretical and Applied Linguistics&lt;/em&gt;, &lt;em&gt;9&lt;/em&gt;(3). &lt;a href=&#34;https://doi.org/10.18413/2313-8912-2023-9-3-0-2&#34;&gt;https://doi.org/10.18413/2313-8912-2023-9-3-0-2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pollock, L. (2018). Statistical and methodological problems with concreteness and other semantic variables: A list memory experiment case study. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;50&lt;/em&gt;(3), 1198&amp;ndash;1216. &lt;a href=&#34;https://doi.org/10.3758/s13428-017-0938-y&#34;&gt;https://doi.org/10.3758/s13428-017-0938-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Popović Stijačić, M., &amp;amp; Filipović Đurđević, D. (2022). Perceptual richness of words and its role in free and cued recall. &lt;em&gt;Primenjena Psihologija&lt;/em&gt;, &lt;em&gt;15&lt;/em&gt;(3), 355&amp;ndash;381. &lt;a href=&#34;https://doi.org/10.19090/pp.v15i3.2400&#34;&gt;https://doi.org/10.19090/pp.v15i3.2400&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Purves, R. S., Striedl, P., Kong, I., &amp;amp; Majid, A. (2023). Conceptualizing Landscapes Through Language: The Role of Native Language and Expertise in the Representation of Waterbody Related Terms. &lt;em&gt;Topics in Cognitive Science&lt;/em&gt;, &lt;em&gt;15&lt;/em&gt;(3), 560&amp;ndash;583. &lt;a href=&#34;https://doi.org/10.1111/tops.12652&#34;&gt;https://doi.org/10.1111/tops.12652&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Raj, R., Hörberg, T., Lindroos, R., Larsson, M., Herman, P., Laukka, E. J., &amp;amp; Olofsson, J. K. (2023). Odor identification errors reveal cognitive aspects of age-associated smell loss. &lt;em&gt;Cognition&lt;/em&gt;, &lt;em&gt;236&lt;/em&gt;, 105445. &lt;a href=&#34;https://doi.org/10.1016/j.cognition.2023.105445&#34;&gt;https://doi.org/10.1016/j.cognition.2023.105445&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Repetto, C., Rodella, C., Conca, F., Santi, G. C., &amp;amp; Catricalà, E. (2022). The Italian Sensorimotor Norms: Perception and action strength measures for 959 words. &lt;em&gt;Behavior Research Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3758/s13428-022-02004-1&#34;&gt;https://doi.org/10.3758/s13428-022-02004-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rey, A. E., Riou, B., Vallet, G. T., &amp;amp; Versace, R. (2017). The automatic visual simulation of words: A memory reactivated mask slows down conceptual access. &lt;em&gt;Canadian Journal of Experimental Psychology / Revue Canadienne de Psychologie Expérimentale&lt;/em&gt;, &lt;em&gt;71&lt;/em&gt;(1), 14&amp;ndash;22. &lt;a href=&#34;https://doi.org/10.1037/cep0000100&#34;&gt;https://doi.org/10.1037/cep0000100&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Reymore, L. (2022). Characterizing prototypical musical instrument timbres with timbre trait profiles. &lt;em&gt;Musicae Scientiae&lt;/em&gt;, &lt;em&gt;26&lt;/em&gt;(3), 648&amp;ndash;674. &lt;a href=&#34;https://doi.org/10.1177/10298649211001523&#34;&gt;https://doi.org/10.1177/10298649211001523&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;San Roque, L., Kendrick, K. H., Norcliffe, E., &amp;amp; Majid, A. (2018). Universal meaning extensions of perception verbs are grounded in interaction. &lt;em&gt;Cognitive Linguistics&lt;/em&gt;, &lt;em&gt;29&lt;/em&gt;(3), 371&amp;ndash;406. &lt;a href=&#34;https://doi.org/10.1515/cog-2017-0034&#34;&gt;https://doi.org/10.1515/cog-2017-0034&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Scerrati, E., Lugli, L., Nicoletti, R., &amp;amp; Borghi, A. M. (2017). The Multilevel Modality-Switch Effect: What Happens When We See the Bees Buzzing and Hear the Diamonds Glistening. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, &lt;em&gt;24&lt;/em&gt;(3), 798&amp;ndash;803. &lt;a href=&#34;https://doi.org/10.3758/s13423-016-1150-2&#34;&gt;https://doi.org/10.3758/s13423-016-1150-2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Schulte Im Walde, S., &amp;amp; Frassinelli, D. (2022). Distributional Measures of Semantic Abstraction. &lt;em&gt;Frontiers in Artificial Intelligence&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;, 796756. &lt;a href=&#34;https://doi.org/10.3389/frai.2021.796756&#34;&gt;https://doi.org/10.3389/frai.2021.796756&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Sidhu, D. M., &amp;amp; Pexman, P. M. (2018). Lonely sensational icons: Semantic neighbourhood density, sensory experience and iconicity. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;33&lt;/em&gt;(1), 25&amp;ndash;31. &lt;a href=&#34;https://doi.org/10.1080/23273798.2017.1358379&#34;&gt;https://doi.org/10.1080/23273798.2017.1358379&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Speed, L. J., &amp;amp; Brybaert, M. (2022). Dutch sensory modality norms. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;54&lt;/em&gt;(3), 1306&amp;ndash;1318. &lt;a href=&#34;https://doi.org/10.3758/s13428-021-01656-9&#34;&gt;https://doi.org/10.3758/s13428-021-01656-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Speed, L. J., &amp;amp; Majid, A. (2017). Dutch modality exclusivity norms: Simulating perceptual modality in space. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;49&lt;/em&gt;(6), 2204&amp;ndash;2218. &lt;a href=&#34;https://doi.org/10.3758/s13428-017-0852-3&#34;&gt;https://doi.org/10.3758/s13428-017-0852-3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Speed, L. J., &amp;amp; Majid, A. (2018). An Exception to Mental Simulation: No Evidence for Embodied Odor Language. &lt;em&gt;Cognitive Science&lt;/em&gt;, &lt;em&gt;42&lt;/em&gt;(4), 1146&amp;ndash;1178. &lt;a href=&#34;https://doi.org/10.1111/cogs.12593&#34;&gt;https://doi.org/10.1111/cogs.12593&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Speed, L. J., &amp;amp; Majid, A. (2020). Grounding language in the neglected senses of touch, taste, and smell. &lt;em&gt;Cognitive Neuropsychology&lt;/em&gt;, &lt;em&gt;37&lt;/em&gt;(5&amp;ndash;6), 363&amp;ndash;392. &lt;a href=&#34;https://doi.org/10.1080/02643294.2019.1623188&#34;&gt;https://doi.org/10.1080/02643294.2019.1623188&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Speed, L. J., Papies, E. K., &amp;amp; Majid, A. (2023). Mental simulation across sensory modalities predicts attractiveness of food concepts. &lt;em&gt;Journal of Experimental Psychology: Applied&lt;/em&gt;, &lt;em&gt;29&lt;/em&gt;(3), 557&amp;ndash;571. &lt;a href=&#34;https://doi.org/10.1037/xap0000461&#34;&gt;https://doi.org/10.1037/xap0000461&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Strik Lievers, F., &amp;amp; Winter, B. (2018). Sensory language across lexical categories. &lt;em&gt;Lingua&lt;/em&gt;, &lt;em&gt;204&lt;/em&gt;, 45&amp;ndash;61. &lt;a href=&#34;https://doi.org/10.1016/j.lingua.2017.11.002&#34;&gt;https://doi.org/10.1016/j.lingua.2017.11.002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Su, C., Wang, X., Wang, Z., &amp;amp; Chen, Y. (2019). A model of synesthetic metaphor interpretation based on cross-modality similarity. &lt;em&gt;Computer Speech &amp;amp; Language&lt;/em&gt;, &lt;em&gt;58&lt;/em&gt;, 1&amp;ndash;16. &lt;a href=&#34;https://doi.org/10.1016/j.csl.2019.03.003&#34;&gt;https://doi.org/10.1016/j.csl.2019.03.003&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tatiya, G., Hosseini, R., Hughes, M. C., &amp;amp; Sinapov, J. (2019). Sensorimotor Cross-Behavior Knowledge Transfer for Grounded Category Recognition. &lt;em&gt;2019 Joint IEEE 9th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)&lt;/em&gt;, 1&amp;ndash;6. &lt;a href=&#34;https://doi.org/10.1109/DEVLRN.2019.8850715&#34;&gt;https://doi.org/10.1109/DEVLRN.2019.8850715&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tatiya, G., Hosseini, R., Hughes, M. C., &amp;amp; Sinapov, J. (2020). A Framework for Sensorimotor Cross-Perception and Cross-Behavior Knowledge Transfer for Object Categorization. &lt;em&gt;Frontiers in Robotics and AI&lt;/em&gt;, &lt;em&gt;7&lt;/em&gt;, 522141. &lt;a href=&#34;https://doi.org/10.3389/frobt.2020.522141&#34;&gt;https://doi.org/10.3389/frobt.2020.522141&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tatiya, G., &amp;amp; Sinapov, J. (2019). Deep Multi-Sensory Object Category Recognition Using Interactive Behavioral Exploration. &lt;em&gt;2019 International Conference on Robotics and Automation (ICRA)&lt;/em&gt;, 7872&amp;ndash;7878. &lt;a href=&#34;https://doi.org/10.1109/ICRA.2019.8794095&#34;&gt;https://doi.org/10.1109/ICRA.2019.8794095&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Teodorescu, H.-N., &amp;amp; Bolea, S. C. (2019). Text Sectioning based on Stylometric Distances. &lt;em&gt;2019 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)&lt;/em&gt;, 1&amp;ndash;6. &lt;a href=&#34;https://doi.org/10.1109/SPED.2019.8906616&#34;&gt;https://doi.org/10.1109/SPED.2019.8906616&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thomason, J., Padmakumar, A., Sinapov, J., Walker, N., Jiang, Y., Yedidsion, H., Hart, J., Stone, P., &amp;amp; Mooney, R. (2020). Jointly Improving Parsing and Perception for Natural Language Commands through Human-Robot Dialog. &lt;em&gt;Journal of Artificial Intelligence Research&lt;/em&gt;, &lt;em&gt;67&lt;/em&gt;, 327&amp;ndash;374. &lt;a href=&#34;https://doi.org/10.1613/jair.1.11485&#34;&gt;https://doi.org/10.1613/jair.1.11485&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tjuka, A., Forkel, R., &amp;amp; List, J.-M. (2021). Linking norms, ratings, and relations of words and concepts across multiple language varieties. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;54&lt;/em&gt;(2), 864&amp;ndash;884. &lt;a href=&#34;https://doi.org/10.3758/s13428-021-01650-1&#34;&gt;https://doi.org/10.3758/s13428-021-01650-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tomsk State University, Rezanova, Z. I., Nekrasova, E. D., Tomsk State University, Miklashevsky, А. А., &amp;amp; Tomsk State University. (2018). INVESTIGATION OF PSYCHO-LINGUISTIC AND COGNITIVE ASPECTS OF LANGUAGE CONTACTING IN THE PROJECT &amp;ldquo;LINGUISTIC AND ETHNOCULTURAL DIVERSITY OF SOUTHERN SIBERIA IN SYNCHRONY AND DIAHRONY: INTERACTION OF LANGUAGES AND CULTURES.&amp;rdquo; &lt;em&gt;Rusin&lt;/em&gt;, &lt;em&gt;52&lt;/em&gt;, 107&amp;ndash;117. &lt;a href=&#34;https://doi.org/10.17223/18572685/52/8&#34;&gt;https://doi.org/10.17223/18572685/52/8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tomsk State University, Vladimirova, V. E., Rezanova, Z. I., Tomsk State University, Korshunova, I. S., &amp;amp; Tomsk State University. (2022). Ethno-linguistic contact as reflected in language cognition: Does bilingualism affect subjective assessments of perceptual semantics?*. &lt;em&gt;Rusin&lt;/em&gt;, &lt;em&gt;70&lt;/em&gt;, 214&amp;ndash;231. &lt;a href=&#34;https://doi.org/10.17223/18572685/70/12&#34;&gt;https://doi.org/10.17223/18572685/70/12&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Troche, J., Crutch, S. J., &amp;amp; Reilly, J. (2017). Defining a Conceptual Topography of Word Concreteness: Clustering Properties of Emotion, Sensation, and Magnitude among 750 English Words. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;, 1787. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2017.01787&#34;&gt;https://doi.org/10.3389/fpsyg.2017.01787&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Van De Weijer, J., Bianchi, I., &amp;amp; Paradis, C. (2023). Sensory modality profiles of antonyms. &lt;em&gt;Language and Cognition&lt;/em&gt;, 1&amp;ndash;15. &lt;a href=&#34;https://doi.org/10.1017/langcog.2023.20&#34;&gt;https://doi.org/10.1017/langcog.2023.20&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Vergallito, A., Petilli, M. A., &amp;amp; Marelli, M. (2020). Perceptual modality norms for 1,121 Italian words: A comparison with concreteness and imageability scores and an analysis of their impact in word processing tasks. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;52&lt;/em&gt;(4), 1599&amp;ndash;1616. &lt;a href=&#34;https://doi.org/10.3758/s13428-019-01337-8&#34;&gt;https://doi.org/10.3758/s13428-019-01337-8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Verheyen, S., De Deyne, S., Linsen, S., &amp;amp; Storms, G. (2020). Lexicosemantic, affective, and distributional norms for 1,000 Dutch adjectives. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;52&lt;/em&gt;(3), 1108&amp;ndash;1121. &lt;a href=&#34;https://doi.org/10.3758/s13428-019-01303-4&#34;&gt;https://doi.org/10.3758/s13428-019-01303-4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Vigliocco, G., Zhang, Y., Del Maschio, N., Todd, R., &amp;amp; Tuomainen, J. (2020). Electrophysiological signatures of English onomatopoeia. &lt;em&gt;Language and Cognition&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(1), 15&amp;ndash;35. &lt;a href=&#34;https://doi.org/10.1017/langcog.2019.38&#34;&gt;https://doi.org/10.1017/langcog.2019.38&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Villani, C., D&#39;Ascenzo, S., Borghi, A. M., Roversi, C., Benassi, M., &amp;amp; Lugli, L. (2022). Is justice grounded? How expertise shapes conceptual representation of institutional concepts. &lt;em&gt;Psychological Research&lt;/em&gt;, &lt;em&gt;86&lt;/em&gt;(8), 2434&amp;ndash;2450. &lt;a href=&#34;https://doi.org/10.1007/s00426-021-01492-8&#34;&gt;https://doi.org/10.1007/s00426-021-01492-8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Villani, C., Lugli, L., Liuzza, M. T., &amp;amp; Borghi, A. M. (2019). Varieties of abstract concepts and their multiple dimensions. &lt;em&gt;Language and Cognition&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt;(3), 403&amp;ndash;430. &lt;a href=&#34;https://doi.org/10.1017/langcog.2019.23&#34;&gt;https://doi.org/10.1017/langcog.2019.23&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wan, M., Su, Q., Ahrens, K., &amp;amp; Huang, C.-R. (2023). Perceptional and actional enrichment for metaphor detection with sensorimotor norms. &lt;em&gt;Natural Language Engineering&lt;/em&gt;, 1&amp;ndash;29. &lt;a href=&#34;https://doi.org/10.1017/S135132492300044X&#34;&gt;https://doi.org/10.1017/S135132492300044X&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wang, X., Su, C., &amp;amp; Chen, Y. (2019). A Method of Abstractness Ratings for Chinese Concepts. In A. Lotfi, H. Bouchachia, A. Gegov, C. Langensiepen, &amp;amp; M. McGinnity (Eds.), &lt;em&gt;Advances in Computational Intelligence Systems&lt;/em&gt; (Vol. 840, pp. 217&amp;ndash;226). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-319-97982-3_18&#34;&gt;https://doi.org/10.1007/978-3-319-97982-3_18&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wang, Y., &amp;amp; Zeng, Y. (2022a). Multisensory Concept Learning Framework Based on Spiking Neural Networks. &lt;em&gt;Frontiers in Systems Neuroscience&lt;/em&gt;, &lt;em&gt;16&lt;/em&gt;, 845177. &lt;a href=&#34;https://doi.org/10.3389/fnsys.2022.845177&#34;&gt;https://doi.org/10.3389/fnsys.2022.845177&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wang, Y., &amp;amp; Zeng, Y. (2022b). Statistical Analysis of Multisensory and Text-Derived Representations on Concept Learning. &lt;em&gt;Frontiers in Computational Neuroscience&lt;/em&gt;, &lt;em&gt;16&lt;/em&gt;, 861265. &lt;a href=&#34;https://doi.org/10.3389/fncom.2022.861265&#34;&gt;https://doi.org/10.3389/fncom.2022.861265&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wicke, P., &amp;amp; Bolognesi, M. (2020). Emoji-based semantic representations for abstract and concrete concepts. &lt;em&gt;Cognitive Processing&lt;/em&gt;, &lt;em&gt;21&lt;/em&gt;(4), 615&amp;ndash;635. &lt;a href=&#34;https://doi.org/10.1007/s10339-020-00971-x&#34;&gt;https://doi.org/10.1007/s10339-020-00971-x&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B. (2019). &lt;em&gt;Statistics for Linguists: An Introduction Using R&lt;/em&gt; (1st ed.). Routledge. &lt;a href=&#34;https://doi.org/10.4324/9781315165547&#34;&gt;https://doi.org/10.4324/9781315165547&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B. (2022). Mapping the landscape of exploratory and confirmatory data analysis in linguistics. In D. Tay &amp;amp; M. X. Pan (Eds.), &lt;em&gt;Data Analytics in Cognitive Linguistics&lt;/em&gt; (pp. 13&amp;ndash;48). De Gruyter. &lt;a href=&#34;https://doi.org/10.1515/9783110687279-002&#34;&gt;https://doi.org/10.1515/9783110687279-002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B. (2023). Abstract concepts and emotion: Cross-linguistic evidence and arguments against affective embodiment. &lt;em&gt;Philosophical Transactions of the Royal Society B: Biological Sciences&lt;/em&gt;, &lt;em&gt;378&lt;/em&gt;(1870), 20210368. &lt;a href=&#34;https://doi.org/10.1098/rstb.2021.0368&#34;&gt;https://doi.org/10.1098/rstb.2021.0368&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B., Lupyan, G., Perry, L. K., Dingemanse, M., &amp;amp; Perlman, M. (2023). Iconicity ratings for 14,000+ English words. &lt;em&gt;Behavior Research Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3758/s13428-023-02112-6&#34;&gt;https://doi.org/10.3758/s13428-023-02112-6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B., &amp;amp; Perlman, M. (2021). Size sound symbolism in the English lexicon. &lt;em&gt;Glossa: A Journal of General Linguistics&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;(1). &lt;a href=&#34;https://doi.org/10.5334/gjgl.1646&#34;&gt;https://doi.org/10.5334/gjgl.1646&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B., Perlman, M., &amp;amp; Majid, A. (2018). Vision dominates in perceptual language: English sensory vocabulary is optimized for usage. &lt;em&gt;Cognition&lt;/em&gt;, &lt;em&gt;179&lt;/em&gt;, 213&amp;ndash;220. &lt;a href=&#34;https://doi.org/10.1016/j.cognition.2018.05.008&#34;&gt;https://doi.org/10.1016/j.cognition.2018.05.008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B., Perlman, M., Perry, L. K., &amp;amp; Lupyan, G. (2017). Which words are most iconic?: Iconicity in English sensory words. &lt;em&gt;Interaction Studies. Social Behaviour and Communication in Biological and Artificial Systems&lt;/em&gt;, &lt;em&gt;18&lt;/em&gt;(3), 443&amp;ndash;464. &lt;a href=&#34;https://doi.org/10.1075/is.18.3.07win&#34;&gt;https://doi.org/10.1075/is.18.3.07win&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B., Sóskuthy, M., Perlman, M., &amp;amp; Dingemanse, M. (2022). Trilled /r/ is associated with roughness, linking sound and touch across spoken languages. &lt;em&gt;Scientific Reports&lt;/em&gt;, &lt;em&gt;12&lt;/em&gt;(1), 1035. &lt;a href=&#34;https://doi.org/10.1038/s41598-021-04311-7&#34;&gt;https://doi.org/10.1038/s41598-021-04311-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Winter, B., &amp;amp; Strik-Lievers, F. (2023). Semantic distance predicts metaphoricity and creativity judgments in synesthetic metaphors. &lt;em&gt;Metaphor and the Social World&lt;/em&gt;, &lt;em&gt;13&lt;/em&gt;(1), 59&amp;ndash;80. &lt;a href=&#34;https://doi.org/10.1075/msw.00029.win&#34;&gt;https://doi.org/10.1075/msw.00029.win&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Wu, C., &amp;amp; Mu, X. (2023). Sensory experience ratings (SERs) for 1,130 Chinese words: Relationships with other semantic and lexical psycholinguistic variables. &lt;em&gt;Linguistics Vanguard&lt;/em&gt;, &lt;em&gt;0&lt;/em&gt;(0). &lt;a href=&#34;https://doi.org/10.1515/lingvan-2022-0083&#34;&gt;https://doi.org/10.1515/lingvan-2022-0083&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Xiong, J., &amp;amp; Huang, C.-R. (2018). Somewhere in COLDNESS Lies Nibbāna: Lexical Manifestations of COLDNESS. In J.-F. Hong, Q. Su, &amp;amp; J.-S. Wu (Eds.), &lt;em&gt;Chinese Lexical Semantics&lt;/em&gt; (Vol. 11173, pp. 70&amp;ndash;81). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-04015-4_6&#34;&gt;https://doi.org/10.1007/978-3-030-04015-4_6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yin Zhong, &amp;amp; Chu-Ren Huang. (2020). Sweetness or Mouthfeel: A corpus-based study of the conceptualization of taste. &lt;em&gt;Linguistic Research&lt;/em&gt;, &lt;em&gt;37&lt;/em&gt;(3), 359&amp;ndash;387. &lt;a href=&#34;https://doi.org/10.17250/KHISLI.37.3.202012.001&#34;&gt;https://doi.org/10.17250/KHISLI.37.3.202012.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zeng, Y., Zhao, D., Zhao, F., Shen, G., Dong, Y., Lu, E., Zhang, Q., Sun, Y., Liang, Q., Zhao, Y., Zhao, Z., Fang, H., Wang, Y., Li, Y., Liu, X., Du, C., Kong, Q., Ruan, Z., &amp;amp; Bi, W. (2023). BrainCog: A spiking neural network based, brain-inspired cognitive intelligence engine for brain-inspired AI and brain simulation. &lt;em&gt;Patterns&lt;/em&gt;, &lt;em&gt;4&lt;/em&gt;(8), 100789. &lt;a href=&#34;https://doi.org/10.1016/j.patter.2023.100789&#34;&gt;https://doi.org/10.1016/j.patter.2023.100789&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhang, X., Amiri, S., Sinapov, J., Thomason, J., Stone, P., &amp;amp; Zhang, S. (2023). Multimodal embodied attribute learning by robots for object-centric action policies. &lt;em&gt;Autonomous Robots&lt;/em&gt;, &lt;em&gt;47&lt;/em&gt;(5), 505&amp;ndash;528. &lt;a href=&#34;https://doi.org/10.1007/s10514-023-10098-5&#34;&gt;https://doi.org/10.1007/s10514-023-10098-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhang, X., Sinapov, J., &amp;amp; Zhang, S. (2021, July 12). Planning Multimodal Exploratory Actions for Online Robot Attribute Learning. &lt;em&gt;Robotics: Science and Systems XVII&lt;/em&gt;. Robotics: Science and Systems 2021. &lt;a href=&#34;https://doi.org/10.15607/RSS.2021.XVII.005&#34;&gt;https://doi.org/10.15607/RSS.2021.XVII.005&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q. (2020a). From Linguistic Synaesthesia to Conceptual Metaphor Theory. In Q. Zhao, &lt;em&gt;Embodied Conceptualization or Neural Realization&lt;/em&gt; (Vol. 10, pp. 115&amp;ndash;128). Springer Singapore. &lt;a href=&#34;https://doi.org/10.1007/978-981-32-9315-1_7&#34;&gt;https://doi.org/10.1007/978-981-32-9315-1_7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q. (2020b). Methodology: A Corpus-Driven Approach. In Q. Zhao, &lt;em&gt;Embodied Conceptualization or Neural Realization&lt;/em&gt; (Vol. 10, pp. 19&amp;ndash;34). Springer Singapore. &lt;a href=&#34;https://doi.org/10.1007/978-981-32-9315-1_2&#34;&gt;https://doi.org/10.1007/978-981-32-9315-1_2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q., Ahrens, K., &amp;amp; Huang, C.-R. (2022). Linguistic synesthesia is metaphorical: A lexical-conceptual account. &lt;em&gt;Cognitive Linguistics&lt;/em&gt;, &lt;em&gt;33&lt;/em&gt;(3), 553&amp;ndash;583. &lt;a href=&#34;https://doi.org/10.1515/cog-2021-0098&#34;&gt;https://doi.org/10.1515/cog-2021-0098&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q., Huang, C.-R., &amp;amp; Ahrens, K. (2019). Directionality of linguistic synesthesia in Mandarin: A corpus-based study. &lt;em&gt;Lingua&lt;/em&gt;, &lt;em&gt;232&lt;/em&gt;, 102744. &lt;a href=&#34;https://doi.org/10.1016/j.lingua.2019.102744&#34;&gt;https://doi.org/10.1016/j.lingua.2019.102744&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q., &amp;amp; Long, Y. (2022). A Diachronic Study on Linguistic Synesthesia in Chinese. In M. Dong, Y. Gu, &amp;amp; J.-F. Hong (Eds.), &lt;em&gt;Chinese Lexical Semantics&lt;/em&gt; (Vol. 13250, pp. 84&amp;ndash;94). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-031-06547-7_6&#34;&gt;https://doi.org/10.1007/978-3-031-06547-7_6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhao, Q., Long, Y., &amp;amp; Huang, C.-R. (2020). Linguistic Synaesthesia of Mandarin Sensory Adjectives: Corpus-Based and Experimental Approaches. In J.-F. Hong, Y. Zhang, &amp;amp; P. Liu (Eds.), &lt;em&gt;Chinese Lexical Semantics&lt;/em&gt; (Vol. 11831, pp. 139&amp;ndash;146). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-38189-9_14&#34;&gt;https://doi.org/10.1007/978-3-030-38189-9_14&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhong, Y., Ahrens, K., &amp;amp; Huang, C.-R. (2023). Entity, event, and sensory modalities: An onto-cognitive account of sensory nouns. &lt;em&gt;Humanities and Social Sciences Communications&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 255. &lt;a href=&#34;https://doi.org/10.1057/s41599-023-01677-z&#34;&gt;https://doi.org/10.1057/s41599-023-01677-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhong, Y., Huang, C.-R., &amp;amp; Dong, S. (2022). Bodily sensation and embodiment: A corpus-based study of gustatory vocabulary in Mandarin Chinese. &lt;em&gt;Journal of Chinese Linguistics&lt;/em&gt;, &lt;em&gt;50&lt;/em&gt;(1), 196&amp;ndash;230. &lt;a href=&#34;https://doi.org/10.1353/jcl.2022.0008&#34;&gt;https://doi.org/10.1353/jcl.2022.0008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhong, Y., Wan, M., Ahrens, K., &amp;amp; Huang, C.-R. (2022). Sensorimotor norms for Chinese nouns and their relationship with orthographic and semantic variables. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;37&lt;/em&gt;(8), 1000&amp;ndash;1022. &lt;a href=&#34;https://doi.org/10.1080/23273798.2022.2035416&#34;&gt;https://doi.org/10.1080/23273798.2022.2035416&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhu, S., Wang, X., &amp;amp; Liu, P. (2021). Who Killed Sanmao and Virginia Woolf? A Comparative Study of Writers with Suicidal Attempt Based on a Quantitative Linguistic Method. In M. Liu, C. Kit, &amp;amp; Q. Su (Eds.), &lt;em&gt;Chinese Lexical Semantics&lt;/em&gt; (Vol. 12278, pp. 408&amp;ndash;420). Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-81197-6_34&#34;&gt;https://doi.org/10.1007/978-3-030-81197-6_34&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs</title>
      <link>https://pablobernabeu.github.io/2017/modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing-evidence-from-erps/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2017/modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing-evidence-from-erps/</guid>
      <description>&lt;p&gt;Research has extensively investigated whether conceptual processing is modality-specific—that is, whether meaning is processed to a large extent on the basis of perceptual and motor affordances (Barsalou, 2016). This possibility challenges long-established theories. It suggests a strong link between physical experience and language which is not borne out of the paradigmatic arbitrariness of words (see Lockwood, Dingemanse, &amp;amp; Hagoort, 2016). Modality-specificity also clashes with models of language that have no link to sensory and motor systems (Barsalou, 2016).&lt;/p&gt;
&lt;a href=&#39;https://pablobernabeu.github.io/publication/bernabeu-etal-2017/&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #4CAF50; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt; 
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;/span&gt; Conference paper &lt;/h3&gt;&lt;/button&gt;&lt;/a&gt; &amp;nbsp; 
&lt;a href=&#39;https://pablobernabeu.github.io/publication/bernabeu-2017-mphil-thesis/&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #196F27; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt; 
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;/span&gt; Master&#39;s thesis &lt;/h3&gt;&lt;/button&gt;&lt;/a&gt; &amp;nbsp; 
&lt;br&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/post/Conceptual_modality_switch_effect_measured_at_first_word&#34;&gt;Early discussion on ResearchGate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://osf.io/97unm/&#34;&gt;Data and code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pablobernabeu.github.io/applications-and-dashboards/bernabeu-etal-2017-modalityswitch/&#34;&gt;Data dashboard&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the Conceptual Modality Switch (CMS) paradigm, participants perform a property verification task, deciding whether certain property words can reasonably describe concept words. Covertly, the conceptual modality of consecutive trials is manipulated in order to produce specific switches in conceptual modality. For instance, after the trial &lt;em&gt;Soundless Answer&lt;/em&gt;, which is primarily auditory, the following trial may match in modality—&lt;em&gt;Loud Welcome&lt;/em&gt;—or mismatch—&lt;em&gt;Fine Selection&lt;/em&gt; (visual).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;designoverview.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Modality switches incur processing costs, as reflected in brain signals linked to semantic violation, and in longer response times (RTs) (Scerrati, Lugli, Nicoletti, &amp;amp; Borghi, 2016). This effect suggests that perceptual features of concepts are accessed during conceptual processing. More recently, however, the CMS effect was reanalysed using a non-perceptual alternative. Louwerse and Connell (2011) found that language statistics (the co-occurrence of words in a language) were able to approximately predict visual/haptic, olfactory/gustatory, and auditory modalities, but not the subtler differences between visual and haptic and between olfactory and gustatory, which seemed to be reserved for perceptual simulations. Moreover, faster response times (RTs) were best explained by language statistics, whereas slower RTs were best explained by perceptual simulations.&lt;/p&gt;
&lt;p&gt;The time course of word processing is important. Research suggests that word processing spans one second, during which different processes—semantic and post-semantic—gradually accumulate (Hauk, 2016). The later an effect, the more reasons to question it. Yet, having an early emergence does not either make an effect lexicosemantic, as the meaning encoded could have gone through working memory before activating the actual system of interest, e.g., sensorimotor (Mahon &amp;amp; Caramazza, 2008). Research also suggests that modal systems may contribute to conceptual processing early on—within 200 ms (Vukovic, Feurra, Shpektor, Myachykov, &amp;amp; Shtyrov, 2017). Thus, measuring effects online may prove valuable.&lt;/p&gt;
&lt;h2 id=&#34;experiment&#34;&gt;Experiment&lt;/h2&gt;
&lt;p&gt;Bernabeu, Willems and Louwerse (2017) investigated whether CMS reflects a functionally relevant process of simulation or instead arises only after basic conceptual processing has been attained. We also examined whether different processing systems, amodal and modal, may compatibly operate.&lt;/p&gt;
&lt;p&gt;We measured CMS online by time-locking Event-Related brain Potentials (ERPs) to the onset of the first word in the target trials, in order to assess how strongly CMS may be influenced by post-semantic processes. Previous research would predict an increase in the CMS effect over time because earlier processing is relatively amodal (Louwerse &amp;amp; Hutchinson, 2012).&lt;/p&gt;
&lt;p&gt;We tested the compatibility of amodal and modal processing by drawing on Louwerse and Connell’s (2011) findings. In this conceptual replication, we split participants into a Quick and a Slow group based on RT. Maintaining CMS as a within-subjects factor, we predicted that the larger modality switches (e.g., auditory to visual) would be picked up equally by both groups, whereas the subtler switches (e.g., haptic to visual) would be picked up only—or more clearly—by the Slow group.&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;The stimuli were normed (Bernabeu, Louwerse, &amp;amp; Willems, in prep.). Three CMS conditions were created—Auditory-to-visual, Haptic-to-visual, Visual-to-visual—, each with 36 target trials. The property verification task was pretested valid (&lt;em&gt;N&lt;/em&gt; = 19).&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;All participants but one responded correctly to over half of the trials, with an overall accuracy of 63%.&lt;/p&gt;
&lt;p&gt;ERPs showed a CMS effect from time window 1 on, larger after 350 ms. It appeared with both switch conditions, and was characterized by a more negative amplitude for the switch conditions compared to the no-switch condition. It was generally stronger in the posterior brain regions, and in the Slow group. The results are illustrated in the figure below, which includes 95% Confidence Intervals and time windows.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;stackERPs.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#39;50%&#39; src=&#39;results.jpg&#39; /&gt;&lt;/p&gt;
&lt;p&gt;The analysis was done with Linear Mixed Effects models. Final models presented good fits, with R&lt;sup&gt;2&lt;/sup&gt; ranging from .748 to .862. First, the CMS effect in time window 1 was confirmed significant. Such an early emergence is unprecedented in the CMS literature, and it may have been enabled by the time-locking of ERPs to the first word in target trials. In this time window, the only process not lexicosemantic is possibly working memory (Hauk, 2016), and therefore this early emergence adds support to the possibility that CMS was directly caused by perceptual simulation.&lt;/p&gt;
&lt;p&gt;Whereas in time window 1, the effect was circumscribed to an interaction with Brain Area, by Time Window 2, a main effect of CMS emerged. In Windows 3 and 4, the only experimental effect was CMS.&lt;/p&gt;
&lt;p&gt;Bonferroni-corrected, planned ANOVA contrasts into CMS conditions revealed that the no-switch condition differed significantly from the switch conditions. By contrast, the switch conditions (Haptic-to-visual and Auditory-to-visual) hardly differed from each other, underscoring the CMS effect.&lt;/p&gt;
&lt;p&gt;Although the interaction of Group and CMS was only significant in Time Windows 1 and 2, Windows 2 to 4 presented a pattern fitting our predictions (Louwerse &amp;amp; Connell, 2011). While the Slow group picked up the switches across all modalities similarly, the Quick group picked up the Auditory-to-visual switch more clearly than the Haptic-to-visual switch.&lt;/p&gt;
&lt;h3 id=&#34;statistical-analysis&#34;&gt;Statistical analysis&lt;/h3&gt;
&lt;p&gt;The statistical analysis is &lt;a href=&#34;https://osf.io/sx3nw&#34;&gt;available on OSF&lt;/a&gt; (to view the plots, please &lt;a href=&#34;https://osf.io/download/sx3nw&#34;&gt;download the document&lt;/a&gt;).&lt;/p&gt;
&lt;iframe src=&#34;https://mfr.osf.io/render?url=https%3A%2F%2Fosf.io%2Fdownload%2Fsx3nw%2F%3Fdirect%26mode%3Drender&#34;
        width=&#34;100%&#34;
        scrolling=&#34;yes&#34;
        height=&#34;677px&#34;
        marginheight=&#34;0&#34;
        frameborder=&#34;0&#34;
        allowfullscreen
        webkitallowfullscreen
&gt;
&lt;/iframe&gt;
&lt;h2 id=&#34;discussion&#34;&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Results broadly suggest that cognition may operate on qualitatively different systems for the same task. In conceptual processing, one of these systems appears to be modality-independent, potentially based on linguistic co-occurrences, whereas another system is modality-specific, linked to physical experience.&lt;/p&gt;
&lt;p&gt;A conference poster with further analyses is &lt;a href=&#34;https://osf.io/dj52n&#34;&gt;also available&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;div class = &#39;hanging-indent&#39;&gt;
&lt;p&gt;Barsalou, L. W. (2016). On staying grounded and avoiding quixotic dead ends. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Bernabeu, P., Louwerse, M. M., &amp;amp; Willems, R. M. (in prep.). Modality exclusivity norms for 747 properties and concepts in Dutch: a replication of English. Retrieved from &lt;a href=&#34;https://osf.io/brkjw/&#34;&gt;https://osf.io/brkjw/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P., Willems, R. M., &amp;amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, &amp;amp; E. J. Davelaar (Eds.), &lt;em&gt;Proceedings of the 39th Annual Conference of the Cognitive Science Society&lt;/em&gt; (pp. 1629-1634). Austin, TX: Cognitive Science Society. &lt;a href=&#34;https://doi.org/10.31234/osf.io/a5pcz&#34;&gt;https://doi.org/10.31234/osf.io/a5pcz&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hauk, O. (2016). Only time will tell—Why temporal information is essential for our neuroscientific understanding of semantics. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;, 4, 1072-1079.&lt;/p&gt;
&lt;p&gt;Lockwood, G., Hagoort, P., &amp;amp; Dingemanse, M. (2016). How iconicity helps people learn new words: neural correlates and individual differences in sound-symbolic bootstrapping. &lt;em&gt;Collabra, 2&lt;/em&gt;, 1, 7.&lt;/p&gt;
&lt;p&gt;Louwerse, M., &amp;amp; Connell, L. (2011). A taste of words: linguistic context and perceptual simulation predict the modality of words. &lt;em&gt;Cognitive Science, 35&lt;/em&gt;, 2, 381-98.&lt;/p&gt;
&lt;p&gt;Louwerse, M., &amp;amp; Hutchinson, S. (2012). Neurological evidence linguistic processes precede perceptual simulation in conceptual processing. &lt;em&gt;Frontiers in Psychology, 3&lt;/em&gt;, 385.&lt;/p&gt;
&lt;p&gt;Mahon, B. Z., &amp;amp; Caramazza, A. (2008). A critical look at the Embodied Cognition Hypothesis and a new proposal for grounding conceptual content. &lt;em&gt;Journal of Physiology - Paris, 102&lt;/em&gt;, 59-70.&lt;/p&gt;
&lt;p&gt;Scerrati, E., Lugli, L., Nicoletti, R., &amp;amp; Borghi, A. M. (2016). The Multilevel Modality-Switch Effect: What Happens When We See the Bees Buzzing and Hear the Diamonds Glistening. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, doi:10.3758/s13423-016-1150-2.&lt;/p&gt;
&lt;p&gt;Vukovic, V., Feurra, M., Shpektor, A., Myachykov, A., &amp;amp; Shtyrov, Y. (2017). Primary motor cortex functionally contributes to language comprehension: An online rTMS study. &lt;em&gt;Neuropsychologia, 96&lt;/em&gt;, 222-229.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The case for data dashboards: First steps in R Shiny</title>
      <link>https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/</guid>
      <description>


&lt;div style=&#34;font-size:110%;&#34;&gt;
&lt;b&gt; Dashboards for data visualisation, such as &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;R Shiny&lt;/a&gt; and &lt;a href=&#34;https://www.tableau.com/&#34;&gt;Tableau&lt;/a&gt;, allow the interactive exploration of data by means of drop-down lists and checkboxes, with no coding required from the final users. These web applications can be useful for both the data analyst and the public at large. &lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;margin-top: 4%;&#34;&gt;

&lt;/div&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;25%&#34; src=&#34;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/1.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Visualisation apps run on internet browsers. This allows for three options: private viewing (useful during analysis), selective sharing (used within work groups), or internet publication. Among the available platforms, &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;R Shiny&lt;/a&gt; and &lt;a href=&#34;https://www.tableau.com/&#34;&gt;Tableau&lt;/a&gt; stand out due to being relatively accessible to new users. Apps serve a broad variety of purposes (see &lt;a href=&#34;https://shiny.rstudio.com/gallery/&#34;&gt;this gallery&lt;/a&gt; and &lt;a href=&#34;https://www.tableau.com/products/desktop&#34;&gt;this one&lt;/a&gt;). In science and beyond, these apps allow us to go &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01782/full&#34;&gt;the extra mile in sharing data&lt;/a&gt;. Alongside files and code shared in repositories, we can present the data in a website, in the form of plots or tables. This facilitates the public exploration of each section of the data (groups, participants, trials…) &lt;a href=&#34;https://www.nature.com/articles/d41586-019-01506-x&#34;&gt;to anyone interested, and allows researchers to account for their proceeding&lt;/a&gt; in the analysis.&lt;/p&gt;
&lt;p&gt;&lt;img width = &#39;70%&#39; src=&#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/2.png&#39; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#39;60%&#39; src=&#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/3.png&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Publishers and journals highly encourage authors to make the most of their data by facilitating its easy exploration by the readership–even though they don’t normally offer the possibility of hosting any web applications yet.&lt;/p&gt;
&lt;p&gt;Apps can also prove valuable to those analysing the data. For instance, my app helped me to identify the extent of noise in a section of the data. Instead of running through a heavy score of code, the drop-down lists of the app let me seamlessly surf through the different sections.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At a certain point, I found a data section that was consistently noisier than the rest, and eventually I had to discard it from further statistical analyses. Yet, instead of removing that from the app, I maintained it with a note attached. This particular trait in the data was rather salient.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/5.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Beyond such a salient feature in the data, a visualisation app may also help to spot subtler patterns such as third variables or individual differences.&lt;/p&gt;
&lt;p&gt;There are several platforms for creating apps (e.g., Tableau, D3.js, and R Shiny). I focus on R Shiny here for three reasons: it is affordable to use, fairly accessible to new users, and well suited for science as it is based on the R language (see for instance &lt;a href=&#34;https://doi.org/10.1080/10691898.2018.1436999&#34;&gt;this article&lt;/a&gt;).&lt;/p&gt;
&lt;div id=&#34;how-to-shiny&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;How to Shiny&lt;/h4&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#39;30%&#39; src=&#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/6.png&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Shiny apps draw on any standard R code that you may already have. This is most commonly plots or tables, but other stuff such as images or Markdown texts are valid too. This is a nice thing to keep in mind when having to create a new app. Part of the job may already be done! The app is distributed among five different areas.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data file(s): These are any data files you’re using (e.g., with csv or rds extensions).&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;a.-server.r-script&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;1a. &lt;code&gt;server.R&lt;/code&gt; script&lt;/h5&gt;
&lt;p&gt;The &lt;code&gt;server&lt;/code&gt; script contains the central processes: plots, tables, etc. Code that existed independently of the app app may be brought into this script by slightly adapting it. At the top, call the &lt;code&gt;shiny&lt;/code&gt; library and any others used (e.g., ‘ggplot2’), and also read in the data. The snippet below shows the beginning of an example &lt;code&gt;server.R&lt;/code&gt; script.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
# server

library(shiny)
library(ggplot2)

EEG.ParticipantAndElectrode = readRDS(&amp;#39;EEG.ParticipantAndElectrode.rds&amp;#39;)
EEG.ParticipantAndBrainArea = readRDS(&amp;#39;EEG.ParticipantAndBrainArea.rds&amp;#39;)
EEG.GroupAndElectrode = readRDS(&amp;#39;EEG.GroupAndElectrode.rds&amp;#39;)
EEG.OLDGroupAndElectrode = readRDS(&amp;#39;EEG.OLDGroupAndElectrode.rds&amp;#39;)


server =

shinyServer(

  function(input, output) {

# plot_GroupAndElectrode:
    output$plot_GroupAndElectrode &amp;lt;- renderPlot({

dfelectrode &amp;lt;- aggregate(microvolts ~ electrode*time*condition, 
EEG.GroupAndElectrode[EEG.GroupAndElectrode$RT.based_Groups==input$var.Group,], mean)

df2 &amp;lt;- subset(dfelectrode, electrode == input$var.Electrodes.1)

df2$condition= as.factor(df2$condition)
df2$condition &amp;lt;- gsub(&amp;#39;visual2visual&amp;#39;, &amp;#39; Visual / Visual&amp;#39;, df2$condition)
df2$condition &amp;lt;- gsub(&amp;#39;haptic2visual&amp;#39;, &amp;#39; Haptic / Visual&amp;#39;, df2$condition)
df2$condition &amp;lt;- gsub(&amp;#39;auditory2visual&amp;#39;, &amp;#39; Auditory / Visual&amp;#39;, df2$condition)

df2$time &amp;lt;- as.integer(as.character(df2$time))
colours &amp;lt;- c(&amp;#39;firebrick1&amp;#39;, &amp;#39;dodgerblue&amp;#39;, &amp;#39;forestgreen&amp;#39;)
# green:visual2visual, blue:haptic2visual, red:auditory2visual

spec_title = paste0(&amp;#39;ERP waveforms for &amp;#39;, input$var.Group, &amp;#39; Group, Electrode &amp;#39;, input$var.Electrodes.1, &amp;#39; (negative values upward; time windows displayed)&amp;#39;)

plot_GroupAndElectrode = ggplot(df2, aes(x=time, y=-microvolts, color=condition)) +
  geom_rect(xmin=160, xmax=216, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=270, xmax=370, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=350, xmax=550, ymin=8, ymax=-7.5, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=500, xmax=750, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_line(size=1, alpha = 1) + scale_linetype_manual(values=colours) +
  scale_y_continuous(limits=c(-8.38, 8.3), breaks=seq(-8,8,by=1), expand = c(0,0.1)) +
  scale_x_continuous(limits=c(-208,808),breaks=seq(-200,800,by=100), expand = c(0.005,0), labels= c(&amp;#39;-200&amp;#39;,&amp;#39;-100 ms&amp;#39;,&amp;#39;0&amp;#39;,&amp;#39;100 ms&amp;#39;,&amp;#39;200&amp;#39;,&amp;#39;300 ms&amp;#39;,&amp;#39;400&amp;#39;,&amp;#39;500 ms&amp;#39;,&amp;#39;600&amp;#39;,&amp;#39;700 ms&amp;#39;,&amp;#39;800&amp;#39;)) +
  ggtitle(spec_title) + theme_bw() + geom_vline(xintercept=0) +
  annotate(geom=&amp;#39;segment&amp;#39;, y=seq(-8,8,1), yend=seq(-8,8,1), x=-4, xend=8, color=&amp;#39;black&amp;#39;) +
  annotate(geom=&amp;#39;segment&amp;#39;, y=-8.2, yend=-8.38, x=seq(-200,800,100), xend=seq(-200,800,100), color=&amp;#39;black&amp;#39;) +
  geom_segment(x = -200, y = 0, xend = 800, yend = 0, size=0.5, color=&amp;#39;black&amp;#39;) +
  theme(legend.position = c(0.100, 0.150), legend.background = element_rect(fill=&amp;#39;#EEEEEE&amp;#39;, size=0),
	axis.title=element_blank(), legend.key.width = unit(1.2,&amp;#39;cm&amp;#39;), legend.text=element_text(size=17),
	legend.title = element_text(size=17, face=&amp;#39;bold&amp;#39;), plot.title= element_text(size=20, hjust = 0.5, vjust=2),
	axis.text.y = element_blank(), axis.text.x = element_text(size = 14, vjust= 2.12, face=&amp;#39;bold&amp;#39;, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;),
	axis.ticks=element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), 
	panel.grid.minor = element_blank(), plot.margin = unit(c(0.1,0.1,0,0), &amp;#39;cm&amp;#39;)) +
  annotate(&amp;#39;segment&amp;#39;, x=160, xend=216, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=270, xend=370, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=350, xend=550, y=-7.5, yend=-7.5, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=500, xend=750, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  scale_fill_manual(name = &amp;#39;Context / Target trial&amp;#39;, values=colours) +
  scale_color_manual(name = &amp;#39;Context / Target trial&amp;#39;, values=colours) +
  guides(linetype=guide_legend(override.aes = list(size=1.2))) +
   guides(color=guide_legend(override.aes = list(size=2.5))) +
# Print y axis labels within plot area:
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;\u2013&amp;#39; * &amp;#39;3 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = 3, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;) +
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;+3 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = -3, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;) +
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;\u2013&amp;#39; * &amp;#39;6 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = 6, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;)

print(plot_GroupAndElectrode)

output$downloadPlot.1 &amp;lt;- downloadHandler(
	filename &amp;lt;- function(file){
	paste0(input$var.Group, &amp;#39; group, electrode &amp;#39;, input$var.Electrodes.1, &amp;#39;, &amp;#39;, Sys.Date(), &amp;#39;.png&amp;#39;)},
   	content &amp;lt;- function(file){
      		png(file, units=&amp;#39;in&amp;#39;, width=13, height=5, res=900)
      		print(plot_GroupAndElectrode)
      		dev.off()},
	contentType = &amp;#39;image/png&amp;#39;)
  } )

# ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://osf.io/uj8z4/&#34;&gt;— Whole script&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b.-ui.r-script&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;1b. &lt;code&gt;ui.R&lt;/code&gt; script&lt;/h5&gt;
&lt;p&gt;The &lt;code&gt;ui&lt;/code&gt; script defines the user interface. For instance, a factor column in the data that has multiple categories may be neatly displayed with a drop-down list on the side bar of the website. The interface may present a central plot before by a legend key below. The snippet below shows the beginning of an example &lt;code&gt;ui.R&lt;/code&gt; script.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
# UI

library(shiny)
library(ggplot2)

EEG.GroupAndElectrode = readRDS(&amp;#39;EEG.GroupAndElectrode.rds&amp;#39;)
EEG.ParticipantAndBrainArea = readRDS(&amp;#39;EEG.ParticipantAndBrainArea.rds&amp;#39;)
EEG.ParticipantAndElectrode = readRDS(&amp;#39;EEG.ParticipantAndElectrode.rds&amp;#39;)
EEG.OLDGroupAndElectrode = readRDS(&amp;#39;EEG.OLDGroupAndElectrode.rds&amp;#39;)


ui =

shinyUI(

   fluidPage(

    tags$head(tags$link(rel=&amp;#39;shortcut icon&amp;#39;, href=&amp;#39;https://image.ibb.co/fXUwzb/favic.png&amp;#39;)),  # web favicon
    tags$meta(charset=&amp;#39;UTF-8&amp;#39;),
    tags$meta(name=&amp;#39;description&amp;#39;, content=&amp;#39;This R Shiny visualisation dashboard presents data from a psycholinguistic ERP experiment (Bernabeu et al., 2017).&amp;#39;),
    tags$meta(name=&amp;#39;keywords&amp;#39;, content=&amp;#39;R, Shiny, ggplot2, visualisation, data, psycholinguistics, conceptual processing, modality switch, embodied cognition&amp;#39;),
    tags$meta(name=&amp;#39;viewport&amp;#39;, content=&amp;#39;width=device-width, initial-scale=1.0&amp;#39;),

    titlePanel(h3(strong(&amp;#39;Waveforms in detail from an ERP experiment on the Conceptual Modality Switch&amp;#39;), a(&amp;#39;(Bernabeu et al., 2017)&amp;#39;,
    href=&amp;#39;https://figshare.com/articles/EEG_study_on_conceptual_modality-switching_Bernabeu_et_al_in_prep_/4210863&amp;#39;, target=&amp;#39;_blank&amp;#39;,
    style = &amp;#39;color:#3E454E; text-decoration:underline; font-weight:normal&amp;#39;), 	align = &amp;#39;center&amp;#39;, style = &amp;#39;color:black&amp;#39;),

    windowTitle = &amp;#39;Visualization of ERP waveforms from experiment on Conceptual Modality Switch (Bernabeu et al., 2017)&amp;#39;),


    sidebarLayout(
	sidebarPanel(width = 2,


# Condition 1 for reactivity between tabs and sidebars

   conditionalPanel(
	condition = &amp;#39;input.tabvals == 1&amp;#39;,

	h5(a(strong(&amp;#39;See paper, statistics, all data.&amp;#39;), &amp;#39;Plots by group and brain area shown in paper.&amp;#39;,
	href=&amp;#39;https://figshare.com/articles/EEG_study_on_conceptual_modality-switching_Bernabeu_et_al_in_prep_/4210863&amp;#39;,
	target=&amp;#39;_blank&amp;#39;), align = &amp;#39;center&amp;#39;),
br(),

	selectInput(&amp;#39;var.Group&amp;#39;, label = &amp;#39;Group&amp;#39;, choices = list(&amp;#39;Quick&amp;#39;,&amp;#39;Slow&amp;#39;), selected = &amp;#39;Quick&amp;#39;),
	h6(&amp;#39;Quick G.: 23 participants&amp;#39;),
	h6(&amp;#39;Slow G.: 23 participants&amp;#39;),
br(),

	selectInput(&amp;#39;var.Electrodes.1&amp;#39;, label = h5(strong(&amp;#39;Electrode&amp;#39;), br(), &amp;#39;(see montage below)&amp;#39;),
                  choices = list(&amp;#39;1&amp;#39;,&amp;#39;2&amp;#39;,&amp;#39;3&amp;#39;,&amp;#39;4&amp;#39;,&amp;#39;5&amp;#39;,&amp;#39;6&amp;#39;,&amp;#39;7&amp;#39;,&amp;#39;8&amp;#39;,&amp;#39;9&amp;#39;,&amp;#39;10&amp;#39;,
			&amp;#39;11&amp;#39;,&amp;#39;12&amp;#39;,&amp;#39;13&amp;#39;,&amp;#39;14&amp;#39;,&amp;#39;15&amp;#39;,&amp;#39;16&amp;#39;,&amp;#39;17&amp;#39;,&amp;#39;18&amp;#39;,&amp;#39;19&amp;#39;,&amp;#39;20&amp;#39;,&amp;#39;21&amp;#39;,
			&amp;#39;22&amp;#39;,&amp;#39;23&amp;#39;,&amp;#39;24&amp;#39;,&amp;#39;25&amp;#39;,&amp;#39;26&amp;#39;,&amp;#39;27&amp;#39;,&amp;#39;28&amp;#39;,&amp;#39;29&amp;#39;,&amp;#39;30&amp;#39;,&amp;#39;31&amp;#39;,&amp;#39;33&amp;#39;,
			&amp;#39;34&amp;#39;,&amp;#39;35&amp;#39;,&amp;#39;36&amp;#39;,&amp;#39;37&amp;#39;,&amp;#39;38&amp;#39;,&amp;#39;39&amp;#39;,&amp;#39;40&amp;#39;,&amp;#39;41&amp;#39;,&amp;#39;42&amp;#39;,&amp;#39;43&amp;#39;,&amp;#39;44&amp;#39;,
			&amp;#39;45&amp;#39;,&amp;#39;46&amp;#39;,&amp;#39;47&amp;#39;,&amp;#39;48&amp;#39;,&amp;#39;49&amp;#39;,&amp;#39;50&amp;#39;,&amp;#39;51&amp;#39;,&amp;#39;52&amp;#39;,&amp;#39;53&amp;#39;,&amp;#39;54&amp;#39;,&amp;#39;55&amp;#39;,
			&amp;#39;56&amp;#39;,&amp;#39;57&amp;#39;,&amp;#39;58&amp;#39;,&amp;#39;59&amp;#39;,&amp;#39;60&amp;#39;), selected = &amp;#39;30&amp;#39; ),
br(), br(),

	h6(&amp;#39;Source code:&amp;#39;),
	h6(strong(&amp;#39;-  &amp;#39;), a(&amp;#39;server.R&amp;#39;, href=&amp;#39;https://osf.io/uj8z4/&amp;#39;, target=&amp;#39;_blank&amp;#39;, style = &amp;#39;text-decoration: underline;&amp;#39;)),
	h6(strong(&amp;#39;-  &amp;#39;), a(&amp;#39;ui.R&amp;#39;, href=&amp;#39;https://osf.io/8bwcx/&amp;#39;, target=&amp;#39;_blank&amp;#39;, style = &amp;#39;text-decoration: underline;&amp;#39;)),
br(),
	h6(a(&amp;#39;CC-By 4.0 License&amp;#39;, href=&amp;#39;https://osf.io/97unm/&amp;#39;, target=&amp;#39;_blank&amp;#39;), align = &amp;#39;center&amp;#39;, style = &amp;#39;text-decoration: underline;&amp;#39;),

br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(),
br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(),

	h5(a(strong(&amp;#39;See paper, statistics, all data.&amp;#39;),
	href=&amp;#39;https://figshare.com/articles/EEG_study_on_conceptual_modality-switching_Bernabeu_et_al_in_prep_/4210863&amp;#39;,
	target=&amp;#39;_blank&amp;#39;), align = &amp;#39;center&amp;#39;),
br(), br(), br(), br(), br(), br(), br(), br()
),

# ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://osf.io/8bwcx&#34;&gt;— Whole script&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;deployment-and-logs&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2. Deployment and logs&lt;/h5&gt;
&lt;p&gt;This script contains the commands for deploying the app on- or off-line, and for checking the session logs in case of any errors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;automatically-created-folder&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3. Automatically created folder&lt;/h5&gt;
&lt;p&gt;When the app is first deployed on the internet, a subfolder is automatically created with the name ‘rsconnect’. This folder contains a text file which can be used to modify the URL and the title of the webpage.&lt;/p&gt;
&lt;p&gt;Steps to create a Shiny app from scratch:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://shiny.rstudio.com/articles/shinyapps.html&#34;&gt;&lt;strong&gt;1. Tutorials (link).&lt;/strong&gt; Being open-source software, excellent directions are available through a Google search.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/7.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://shiny.rstudio.com/articles/shinyapps.html&#34;&gt;The core ideas are:&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As mentioned above, create a &lt;code&gt;ui.R&lt;/code&gt; script for the code containing the user interface, and create a &lt;code&gt;server.R&lt;/code&gt; script for the code containing the main content (your plots / tables, etc).&lt;/p&gt;
&lt;p&gt;At the top of both ui.R and server.R scripts, enter the command &lt;code&gt;library(shiny)&lt;/code&gt; and also load any other libraries you’re using (e.g., &lt;code&gt;ggplot2&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Test your app by deploying it locally, before launching online. For this purpose, first save the &lt;code&gt;ui&lt;/code&gt; and &lt;code&gt;server&lt;/code&gt; parts independently, as in:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
ui =

 shinyUI(

   fluidPage(

# ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then deploy locally by running:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;shinyApp(ui, server)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Managing to run the app locally is a great first step before launching online (which may sometimes prove a bit trickier).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://shiny.rstudio.com/articles/shinyapps.html&#34;&gt;&lt;strong&gt;2. User token (link).&lt;/strong&gt; Sign up and read in your private key—just to be done once in a computer.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Go for it.&lt;/strong&gt; After locally testing and saving the two main scripts (&lt;code&gt;ui.R&lt;/code&gt; and &lt;code&gt;server.R&lt;/code&gt;), run &lt;code&gt;deployApp()&lt;/code&gt; to launch the app online.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Bugs and session logs.&lt;/strong&gt; Most often they won’t be bugs actually, but fancies, as it were. For instance, some special characters have to get even more special (technically, UTF-8 encoding). For a character such as ‘μ’, Shiny prefers ‘Âμ’, or better, the Unicode &lt;code&gt;expression(&#34;\u03bc&#34;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Cling to your logs by calling the line below, which you may keep at hand in your ‘Shiny deployer.R’ script.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;showLogs(appPath = getwd(), appFile = NULL, appName = NULL, account = NULL,
entries = 50, streaming = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At best, the log output will mention any typos and unaccepted characters, pointing to specific lines in your code.&lt;/p&gt;
&lt;p&gt;It may take a couple of intense days to get a first Shiny app running. Although the usual rabbit holes do exist, years of Shiny have already yielded a sizeable body of free resources online (tutorials, blogs, vlogs). Moreover, there’s also &lt;a href=&#34;https://community.rstudio.com/&#34;&gt;the RStudio Community&lt;/a&gt;, and then StackOverflow etc., where you can post any needs/despair. Post your code, log, and explanation, and you’ll be rescued out in a couple of days. Long live those contributors.&lt;/p&gt;
&lt;p&gt;It’s sometimes enough to upload a bare app, but you might then think it can look better.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5&lt;/strong&gt; (optional). &lt;strong&gt;Advance.&lt;/strong&gt; Use tabs to combine multiple apps on one webpage, use different widgets, include a download option, etc. Tutorials like &lt;a href=&#34;https://www.youtube.com/watch?v=Q9sRKkaNveI&#34;&gt;this one on Youtube&lt;/a&gt; can take you there, especially those that provide the code, as in the description of that video. Use those scripts as templates. For example, I made use of tabs on the top of the dashboard in order to keep the side bar from having too many widgets. The appearance of these tabs can be adjusted. More importantly, the inputs in the sidebar can be modified depending on the active tab, by means of ‘reactivity’ conditions.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
mainPanel(

	tags$style(HTML(&amp;#39;
	    .tabbable &amp;gt; .nav &amp;gt; li &amp;gt; a                  		{background-color:white; color:#3E454E}
	    .tabbable &amp;gt; .nav &amp;gt; li &amp;gt; a:hover            		{background-color:#002555; color:white}
	    .tabbable &amp;gt; .nav &amp;gt; li[class=active] &amp;gt; a 		{background-color:#ECF4FF; color:black}
	    .tabbable &amp;gt; .nav &amp;gt; li[class=active] &amp;gt; a:hover	{background-color:#E7F1FF; color:black}
	&amp;#39;)),

	tabsetPanel(id=&amp;#39;tabvals&amp;#39;,

            tabPanel(value=1, h4(strong(&amp;#39;Group &amp;amp; Electrode&amp;#39;)), br(), plotOutput(&amp;#39;plot_GroupAndElectrode&amp;#39;),
			h5(a(strong(&amp;#39;See plots with 95% Confidence Intervals&amp;#39;), href=&amp;#39;https://osf.io/2tpxn/&amp;#39;,
			target=&amp;#39;_blank&amp;#39;), style=&amp;#39;text-decoration: underline;&amp;#39;), 
			downloadButton(&amp;#39;downloadPlot.1&amp;#39;, &amp;#39;Download HD plot&amp;#39;), br(), br(),
			# EEG montage
			img(src=&amp;#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&amp;#39;, height=500, width=1000)),

            tabPanel(value=2, h4(strong(&amp;#39;Participant &amp;amp; Area&amp;#39;)), br(), plotOutput(&amp;#39;plot_ParticipantAndLocation&amp;#39;),
			h5(a(strong(&amp;#39;See plots with 95% Confidence Intervals&amp;#39;), href=&amp;#39;https://osf.io/86ch9/&amp;#39;,
			target=&amp;#39;_blank&amp;#39;), style=&amp;#39;text-decoration: underline;&amp;#39;), 
			downloadButton(&amp;#39;downloadPlot.2&amp;#39;, &amp;#39;Download HD plot&amp;#39;), br(), br(),
			# EEG montage
			img(src=&amp;#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&amp;#39;, height=500, width=1000)),

            tabPanel(value=3, h4(strong(&amp;#39;Participant &amp;amp; Electrode&amp;#39;)), br(), plotOutput(&amp;#39;plot_ParticipantAndElectrode&amp;#39;),
			br(), downloadButton(&amp;#39;downloadPlot.3&amp;#39;, &amp;#39;Download HD plot&amp;#39;), br(), br(),
			# EEG montage
			img(src=&amp;#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&amp;#39;, height=500, width=1000)),

            tabPanel(value=4, h4(strong(&amp;#39;OLD Group &amp;amp; Electrode&amp;#39;)), br(), plotOutput(&amp;#39;plot_OLDGroupAndElectrode&amp;#39;),
			h5(a(strong(&amp;#39;See plots with 95% Confidence Intervals&amp;#39;), href=&amp;#39;https://osf.io/dvs2z/&amp;#39;,
			target=&amp;#39;_blank&amp;#39;), style=&amp;#39;text-decoration: underline;&amp;#39;), 
			downloadButton(&amp;#39;downloadPlot.4&amp;#39;, &amp;#39;Download HD plot&amp;#39;), br(), br(),
			# EEG montage
			img(src=&amp;#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&amp;#39;, height=500, width=1000))
	),
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;a href=&#34;https://shiny.rstudio.com/gallery/&#34;&gt;official Shiny gallery&lt;/a&gt; offers a great array of apps including their code (e.g., &lt;a href=&#34;https://shiny.rstudio.com/gallery/kmeans-example.html&#34;&gt;basic example&lt;/a&gt;). Another feature you may add is the option to download your plots, tables, data…&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
# In ui.R script

downloadButton(&amp;#39;downloadPlot.1&amp;#39;, &amp;#39;Download HD plot&amp;#39;)

#___________________________________________________


# In server.R script

spec_title = paste0(&amp;#39;ERP waveforms for &amp;#39;, input$var.Group, &amp;#39; Group, Electrode &amp;#39;, input$var.Electrodes.1, &amp;#39; (negative values upward; time windows displayed)&amp;#39;)

plot_GroupAndElectrode = ggplot(df2, aes(x=time, y=-microvolts, color=condition)) +
  geom_rect(xmin=160, xmax=216, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=270, xmax=370, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=350, xmax=550, ymin=8, ymax=-7.5, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=500, xmax=750, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_line(size=1, alpha = 1) + scale_linetype_manual(values=colours) +
  scale_y_continuous(limits=c(-8.38, 8.3), breaks=seq(-8,8,by=1), expand = c(0,0.1)) +
  scale_x_continuous(limits=c(-208,808),breaks=seq(-200,800,by=100), expand = c(0.005,0), labels= c(&amp;#39;-200&amp;#39;,&amp;#39;-100 ms&amp;#39;,&amp;#39;0&amp;#39;,&amp;#39;100 ms&amp;#39;,&amp;#39;200&amp;#39;,&amp;#39;300 ms&amp;#39;,&amp;#39;400&amp;#39;,&amp;#39;500 ms&amp;#39;,&amp;#39;600&amp;#39;,&amp;#39;700 ms&amp;#39;,&amp;#39;800&amp;#39;)) +
  ggtitle(spec_title) + theme_bw() + geom_vline(xintercept=0) +
  annotate(geom=&amp;#39;segment&amp;#39;, y=seq(-8,8,1), yend=seq(-8,8,1), x=-4, xend=8, color=&amp;#39;black&amp;#39;) +
  annotate(geom=&amp;#39;segment&amp;#39;, y=-8.2, yend=-8.38, x=seq(-200,800,100), xend=seq(-200,800,100), color=&amp;#39;black&amp;#39;) +
  geom_segment(x = -200, y = 0, xend = 800, yend = 0, size=0.5, color=&amp;#39;black&amp;#39;) +
  theme(legend.position = c(0.100, 0.150), legend.background = element_rect(fill=&amp;#39;#EEEEEE&amp;#39;, size=0),
	axis.title=element_blank(), legend.key.width = unit(1.2,&amp;#39;cm&amp;#39;), legend.text=element_text(size=17),
	legend.title = element_text(size=17, face=&amp;#39;bold&amp;#39;), plot.title= element_text(size=20, hjust = 0.5, vjust=2),
	axis.text.y = element_blank(), axis.text.x = element_text(size = 14, vjust= 2.12, face=&amp;#39;bold&amp;#39;, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;),
	axis.ticks=element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), 
	panel.grid.minor = element_blank(), plot.margin = unit(c(0.1,0.1,0,0), &amp;#39;cm&amp;#39;)) +
  annotate(&amp;#39;segment&amp;#39;, x=160, xend=216, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=270, xend=370, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=350, xend=550, y=-7.5, yend=-7.5, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=500, xend=750, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  scale_fill_manual(name = &amp;#39;Context / Target trial&amp;#39;, values=colours) +
  scale_color_manual(name = &amp;#39;Context / Target trial&amp;#39;, values=colours) +
  guides(linetype=guide_legend(override.aes = list(size=1.2))) +
   guides(color=guide_legend(override.aes = list(size=2.5))) +
# Print y axis labels within plot area:
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;\u2013&amp;#39; * &amp;#39;3 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = 3, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;) +
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;+3 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = -3, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;) +
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;\u2013&amp;#39; * &amp;#39;6 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = 6, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;)

print(plot_GroupAndElectrode)

output$downloadPlot.1 &amp;lt;- downloadHandler(
	filename &amp;lt;- function(file){
	paste0(input$var.Group, &amp;#39; group, electrode &amp;#39;, input$var.Electrodes.1, &amp;#39;, &amp;#39;, Sys.Date(), &amp;#39;.png&amp;#39;)},
   	content &amp;lt;- function(file){
      		png(file, units=&amp;#39;in&amp;#39;, width=13, height=5, res=900)
      		print(plot_GroupAndElectrode)
      		dev.off()},
	contentType = &amp;#39;image/png&amp;#39;)
  } )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apps can include any text, such as explanations of any length and web links. For instance, we can link back to the data repository, where the code for the app can be found.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/8.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An example of a &lt;a href=&#34;https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment/&#34;&gt;Shiny app is available&lt;/a&gt;, which may also be &lt;a href=&#34;https://mybinder.org/v2/gh/pablobernabeu/Modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing/master?urlpath=rstudio&#34;&gt;edited and run in this RStudio environment&lt;/a&gt;, inside the ‘Shiny-app’ folder.&lt;/p&gt;
&lt;p&gt;The Shiny server (shinyapps.io) allows publishing dashboards built with various frameworks besides Shiny proper. &lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/&#34;&gt;Flexdashboard&lt;/a&gt; and &lt;a href=&#34;https://rstudio.github.io/shinydashboard/&#34;&gt;Shinydashboard&lt;/a&gt; are two of these frameworks, which have visible advantages over basic Shiny, in terms of layout. An &lt;a href=&#34;https://pablobernabeu.shinyapps.io/Dutch-modality-exclusivity-norms/&#34;&gt;example with Flexdashboard is available&lt;/a&gt;.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ &lt;b&gt; Flexdashboard &lt;/b&gt; ★
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ ★ &lt;b&gt; Shiny &lt;/b&gt; ★ ★
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ ★ ★ &lt;b&gt; Flexdashboard-Shiny &lt;/b&gt; ★ ★ ★
&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pablobernabeu/data-is-present/master/dashboard%20gif.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;logistics&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Logistics&lt;/h4&gt;
&lt;p&gt;Memory capacity can become an issue as you go on, which will be flagged in the error logs as: ‘Shiny cannot use on-disk bookmarking’. This doesn’t necessarily lead you to a paid subscription or to &lt;a href=&#34;https://www.r-bloggers.com/alternative-approaches-to-scaling-shiny-with-rstudio-shiny-server-shinyproxy-or-custom-architecture/&#34;&gt;host the website on a custom server&lt;/a&gt;. Try pruning the data file, outsourcing data sections across the five available apps.&lt;/p&gt;
&lt;p&gt;App providers have specific terms of use. To begin, Shiny has a free starter license with limited use, where free apps can handle a certain amount of data, and up to five apps may be created. Beyond that, RStudio offers a wide range of &lt;a href=&#34;http://www.shinyapps.io/#_pricing&#34;&gt;subscriptions&lt;/a&gt; starting at $9/month. For its part, Tableau in principle deals only with &lt;a href=&#34;https://www.tableau.com/pricing&#34;&gt;subscriptions&lt;/a&gt; from $35/month on. While they offer 1-year licenses to students and instructors for free, these don’t include web hosting, unlike Shiny’s free plan. &lt;a href=&#34;https://www.linkedin.com/pulse/r-shiny-v-tableau-dawn-graphics-anand-gupta?lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3BCDbB2MVuQA6l%2BRNxwqWzQg%3D%3D&#34;&gt;Further comparisons&lt;/a&gt; of these platforms are available online. Last, I’ll just mention a third language, &lt;a href=&#34;https://d3js.org/&#34;&gt;D3&lt;/a&gt;, which is powerful, and may also be used &lt;a href=&#34;https://rstudio.github.io/r2d3/&#34;&gt;through R&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the case of very heavy data or frequent public use, if you don’t want to host your Shiny app externally, you might consider rendering a PDF with your visualisations instead.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
pdf(&amp;quot;List of plots per page&amp;quot;, width=13, height=5)
print(plot1)
print(plot2)
# ...
print(plot150)
dev.off()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;High-resolution plots can be rendered into a PDF document in a snap. Conveniently, all text is indexed, so it can be searched (Ctrl+f / Cmd+f / 🔍) (&lt;a href=&#34;https://osf.io/2tpxn/&#34;&gt;see example&lt;/a&gt;). Furthermore, you may also &lt;a href=&#34;http://www.ilovepdf.com/&#34;&gt;merge the rendered PDF&lt;/a&gt; with any other documents.&lt;/p&gt;
&lt;div class=&#34;document-viewer-container&#34; style=&#34;height: 80vh; min-height: 400px;&#34;&gt;
&lt;iframe src=&#34;https://cdn.jsdelivr.net/gh/pablobernabeu/Modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing@master/ERPs/Plots/List%20plots%20per%20participant%20and%20brain%20area.pdf&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;border: none&#34; title=&#34;Document Viewer&#34; loading=&#34;lazy&#34;&gt;
&lt;p&gt;
Your browser does not support embedded PDFs. &lt;a href=&#34;https://cdn.jsdelivr.net/gh/pablobernabeu/Modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing@master/ERPs/Plots/List%20plots%20per%20participant%20and%20brain%20area.pdf&#34; target=&#34;_blank&#34;&gt;Download the PDF&lt;/a&gt; instead.
&lt;/p&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-in-slides&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Summary in &lt;a href=&#34;https://www.slideshare.net/PabloBernabeu/presenting-data-interactively-online-using-r-shiny-126064157&#34;&gt;slides&lt;/a&gt;&lt;/h3&gt;
&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/mDJ6IF1RGTiAR8&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;div style=&#34;margin-bottom:5px&#34;&gt;
&lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/PabloBernabeu/presenting-data-interactively-online-using-r-shiny-126064157&#34; title=&#34;Presenting data interactively online using R Shiny&#34; target=&#34;_blank&#34;&gt;Presenting data interactively online using R Shiny&lt;/a&gt; &lt;/strong&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
