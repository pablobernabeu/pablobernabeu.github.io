<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>python | Pablo Bernabeu</title>
    <link>https://pablobernabeu.github.io/tags/python/</link>
      <atom:link href="https://pablobernabeu.github.io/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <description>python</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-uk</language><copyright>Pablo Bernabeu, 2015—2025. Licence: [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/). Email: pcbernabeu@gmail.com. Cookies only used by third-party systems such as [Disqus](https://help.disqus.com/en/articles/1717155-use-of-cookies).</copyright><lastBuildDate>Fri, 28 Mar 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://pablobernabeu.github.io/img/default_preview_image.png</url>
      <title>python</title>
      <link>https://pablobernabeu.github.io/tags/python/</link>
    </image>
    
    <item>
      <title>Prototype workflow for semi-automatic processing of speech and co-speech gestures</title>
      <link>https://pablobernabeu.github.io/2025/prototype-workflow-for-semi-automatic-processing-of-speech-and-cospeech-gestures/</link>
      <pubDate>Fri, 28 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2025/prototype-workflow-for-semi-automatic-processing-of-speech-and-cospeech-gestures/</guid>
      <description>


&lt;div id=&#34;integrating-speech-and-gesture-processing-for-linguistic-analysis-using-python&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Integrating Speech and Gesture Processing for Linguistic Analysis using Python&lt;/h2&gt;
&lt;p&gt;Understanding the interplay between speech and gesture is crucial for linguistic and cognitive research. The current prototype, available &lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures&#34;&gt;on GitHub&lt;/a&gt;, aims to automate the analysis of temporal alignment between spoken demonstrative pronouns and pointing gestures in video recordings. By integrating computer vision (via Google’s &lt;a href=&#34;https://ai.google.dev/edge/mediapipe/solutions/guide&#34;&gt;MediaPipe&lt;/a&gt;) and speech recognition (using &lt;a href=&#34;https://alphacephei.com/vosk/models&#34;&gt;language-specific Vosk models&lt;/a&gt;) using Python, the workflow provides enriched video annotations and alignment data, offering valuable insights into deictic communication.&lt;/p&gt;
&lt;p&gt;For reference, the GitHub repository includes an &lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/ELAN&#34;&gt;&lt;em&gt;ELAN&lt;/em&gt; folder&lt;/a&gt; containing output from a traditional annotation process using the &lt;a href=&#34;https://archive.mpi.nl/tla/elan&#34;&gt;ELAN program&lt;/a&gt;. Ultimately, the performance of the semi-automated prototype must be validated against manual annotations created using ELAN or a similar program. For reference, below is a plot of manually-annotated data for the alignment between demonstrative pronouns and pointing gestures (see &lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/ELAN/plot_alignment.R&#34;&gt;R code for the plot&lt;/a&gt;).&lt;/p&gt;
&lt;div style=&#34;margin-top: 4%;&#34;&gt;

&lt;/div&gt;
&lt;p&gt;&lt;img src=&#39;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/ELAN/plots/1_scatterplot.png?raw=true&#39; alt=&#39;Plot of manually-annotated data for the alignment between demonstrative pronouns and pointing gestures.&#39; style=&#39;margin-top:2px;&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-it-works-running-the-program&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How It Works: Running the Program&lt;/h2&gt;
&lt;p&gt;The prototype system, which is available &lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures&#34;&gt;on GitHub&lt;/a&gt;, requires primary data in the form of video and corresponding audio files, which should be placed in &lt;code&gt;mnt/primary data&lt;/code&gt;. They video-audio pairs should be named in the same way (e.g., &lt;code&gt;1.mp4&lt;/code&gt; and &lt;code&gt;1.wav&lt;/code&gt;). The video should feature a person in a medium or medium close-up shot.&lt;/p&gt;
&lt;p&gt;Running the following command initiates the processing pipeline:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python main.py --audio_folder &amp;quot;mnt/primary data/audio&amp;quot; \
               --video_folder &amp;quot;mnt/primary data/video&amp;quot; \
               --model &amp;quot;mnt/primary data/vosk-model-de-0.21&amp;quot; \
               --words_of_interest &amp;quot;der,die,das,den,dem,denen,dessen,deren,dieser,diese,dieses,diesen,diesem&amp;quot; \
               --output &amp;quot;mnt/output&amp;quot; \
               --max_time_diff 800&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This command processes the data and stores results in the designated output directory.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;breaking-down-the-processing-pipeline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Breaking Down the Processing Pipeline&lt;/h2&gt;
&lt;div id=&#34;audio-transcription-and-word-onset-extraction-audio_processing.py&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1. Audio Transcription and Word Onset Extraction (&lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/audio_processing.py&#34;&gt;&lt;code&gt;audio_processing.py&lt;/code&gt;&lt;/a&gt;)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The speech recognition model transcribes spoken content and identifies demonstrative pronouns from a predefined list.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Onset times of these pronouns are extracted to facilitate alignment analysis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Outputs include a plain text transcript and a WebVTT subtitle file.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;script-container&#34; data-src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fprototype-process-pronouns-gestures%2Fblob%2Fmain%2Faudio_processing.py&amp;amp;style=a11y-dark&amp;amp;type=code&amp;amp;showFullPath=on&amp;amp;showCopy=on&amp;amp;showLineNumbers=on&amp;amp;showFileMeta=on&#34;&gt;
&lt;button class=&#34;toggle-script&#34;&gt;Show script&lt;/button&gt;
&lt;div class=&#34;script-wrapper&#34;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;gesture-detection-video_processing.py&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2. Gesture Detection (&lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/video_processing.py&#34;&gt;&lt;code&gt;video_processing.py&lt;/code&gt;&lt;/a&gt;)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker&#34;&gt;MediaPipe’s hand landmark estimation&lt;/a&gt; detects pointing gestures based on the position of the wrist (landmark &lt;code&gt;0&lt;/code&gt;) and the tip of the index finger (landmark &lt;code&gt;8&lt;/code&gt;). The &lt;a href=&#34;https://mediapipe-studio.webapps.google.com/demo/hand_landmarker&#34;&gt;online demonstration&lt;/a&gt; is worth a check.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A pointing gesture is recognised at the moment when these landmarks are maximally distant from each other.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;script-container&#34; data-src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fprototype-process-pronouns-gestures%2Fblob%2Fmain%2Fvideo_processing.py&amp;amp;style=a11y-dark&amp;amp;type=code&amp;amp;showFullPath=on&amp;amp;showCopy=on&amp;amp;showLineNumbers=on&amp;amp;showFileMeta=on&#34;&gt;
&lt;button class=&#34;toggle-script&#34;&gt;Show script&lt;/button&gt;
&lt;div class=&#34;script-wrapper&#34;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;alignment-analysis-alignment_analysis.py&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;3. Alignment Analysis (&lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/alignment_analysis.py&#34;&gt;&lt;code&gt;alignment_analysis.py&lt;/code&gt;&lt;/a&gt;)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The extracted demonstrative pronoun onsets are compared with detected gesture apices. Both categories are paired on a case-by-case basis if the distance between them is smaller than the maximum gap (&lt;code&gt;max_time_diff&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Temporal differences between speech and gesture events are calculated.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A CSV file containing word-gesture alignment data is generated.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A scatter plot is created for each recording to illustrate the alignment between the words of interest and the closest pointing gesture within the &lt;code&gt;max_time_diff&lt;/code&gt; window.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;margin-top: 4%;&#34;&gt;

&lt;/div&gt;
&lt;p&gt;&lt;img src=&#39;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/mnt/output/1_scatterplot.png?raw=true&#39; alt=&#39;Plot of data for the alignment between demonstrative pronouns and pointing gestures, obtained using a semi-automated workflow in Python.&#39; style=&#39;margin-top:2px; margin-bottom:-2px;&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;div class=&#34;script-container&#34; data-src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fprototype-process-pronouns-gestures%2Fblob%2Fmain%2Falignment_analysis.py&amp;amp;style=a11y-dark&amp;amp;type=code&amp;amp;showFullPath=on&amp;amp;showCopy=on&amp;amp;showLineNumbers=on&amp;amp;showFileMeta=on&#34;&gt;
&lt;button class=&#34;toggle-script&#34;&gt;Show script&lt;/button&gt;
&lt;div class=&#34;script-wrapper&#34;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;video-processing-and-annotation-video_editing.py&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;4. Video Processing and Annotation (&lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/video_editing.py&#34;&gt;&lt;code&gt;video_editing.py&lt;/code&gt;&lt;/a&gt;)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The system overlays the transcribed speech as subtitles.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Gesture peaks are highlighted to make alignment patterns visible.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The original audio is merged into the video for reference.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;script-container&#34; data-src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fprototype-process-pronouns-gestures%2Fblob%2Fmain%2Fvideo_editing.py&amp;amp;style=a11y-dark&amp;amp;type=code&amp;amp;showFullPath=on&amp;amp;showCopy=on&amp;amp;showLineNumbers=on&amp;amp;showFileMeta=on&#34;&gt;
&lt;button class=&#34;toggle-script&#34;&gt;Show script&lt;/button&gt;
&lt;div class=&#34;script-wrapper&#34;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;automated-execution-main.py&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;5. Automated Execution (&lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/main.py&#34;&gt;&lt;code&gt;main.py&lt;/code&gt;&lt;/a&gt;)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The main script coordinates the entire process.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Multiple audio-video file pairs can be processed simultaneously.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Results are systematically organised in the output directory.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;script-container&#34; data-src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fprototype-process-pronouns-gestures%2Fblob%2Fmain%2Fmain.py&amp;amp;style=a11y-dark&amp;amp;type=code&amp;amp;showFullPath=on&amp;amp;showCopy=on&amp;amp;showLineNumbers=on&amp;amp;showFileMeta=on&#34;&gt;
&lt;button class=&#34;toggle-script&#34;&gt;Show script&lt;/button&gt;
&lt;div class=&#34;script-wrapper&#34;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;current-challenges-and-next-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Current Challenges and Next Steps&lt;/h2&gt;
&lt;div id=&#34;improving-pronoun-identification&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1. Improving Pronoun Identification&lt;/h4&gt;
&lt;p&gt;One limitation of the current system is the overidentification of demonstrative pronouns. In languages such as English, French and German, many definite articles are mistakenly included because they share the same form as demonstrative pronouns. This issue could be addressed by replacing the current current fuzzy &lt;code&gt;words_of_interest&lt;/code&gt; list with a more precise list, where each pronoun is contextualised by its preceding and subsequent words.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;enhancing-gesture-detection-accuracy&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2. Enhancing Gesture Detection Accuracy&lt;/h4&gt;
&lt;p&gt;The system underidentifies pointing gestures, which impacts the overall analysis. Improving MediaPipe’s detection implementation and incorporating additional filtering methods—such as movement velocity thresholds—could significantly enhance accuracy.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This prototype represents an important step towards automating the analysis of speech-gesture interactions. By bridging linguistic and computer vision technologies, the system offers a scalable method for studying deictic communication, paving the way for further refinements in multimodal linguistic analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to correctly encode triggers in Python and send them to BrainVision through serial port (useful for OpenSesame and PsychoPy)</title>
      <link>https://pablobernabeu.github.io/2023/encode-triggers-in-python-and-send-them-to-brainvision-through-serial-port-useful-for-opensesame-and-psychopy/</link>
      <pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/encode-triggers-in-python-and-send-them-to-brainvision-through-serial-port-useful-for-opensesame-and-psychopy/</guid>
      <description>


</description>
    </item>
    
    <item>
      <title>Assigning participant-specific parameters automatically in OpenSesame</title>
      <link>https://pablobernabeu.github.io/2023/assigning-participant-specific-parameters-automatically-in-opensesame/</link>
      <pubDate>Wed, 17 May 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/assigning-participant-specific-parameters-automatically-in-opensesame/</guid>
      <description>


&lt;p&gt;OpenSesame offers options to counterbalance properties of the stimulus across participants. However, in cases of more involved assignments of session parameters across participants, it becomes necessary to write a bit of Python code in an inline script, which should be placed at the top of the timeline. In such a script, the participant-specific parameters are loaded in from a csv file. Below is a minimal example of the csv file.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;17%&#34; /&gt;
&lt;col width=&#34;22%&#34; /&gt;
&lt;col width=&#34;20%&#34; /&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;24%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;participant&lt;/th&gt;
&lt;th&gt;language&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;training_list&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;test_list&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;experiment_list&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;7&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;8&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Below is the corresponding inline script. The code &lt;code&gt;.iloc[0]&lt;/code&gt; at the end of the lines is used to select a cell.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Assign participant-specific parameters. For each participant-specific factor in the 
# stimulus files, take the level corresponding to the index position specified in the 
# participant parameters file.

import csv  # handle csv file
import pandas as pd  # handle data frames

participant_parameters = pd.read_csv(exp.get_file(&amp;#39;stimuli/parameters per participant.csv&amp;#39;))

var.participant = var.subject_nr

var.language = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;language&amp;#39;].iloc[0]

var.training_list = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;training_list&amp;#39;].iloc[0]

var.test_list = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;test_list&amp;#39;].iloc[0]

var.experiment_list = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;experiment_list&amp;#39;].iloc[0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, the variables created in the script can be used in &lt;kbd&gt;Run if&lt;/kbd&gt; conditions (e.g., &lt;code&gt;[language] == &#39;Mini-English&#39;&lt;/code&gt;), as replacements inside file names (e.g., &lt;code&gt;[language] training, List [training_list].csv&lt;/code&gt;), and as input in sketchpads (e.g., &lt;code&gt;Current list: [training_list]&lt;/code&gt;).&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
