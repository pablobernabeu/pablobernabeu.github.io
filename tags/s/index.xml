<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>s | Pablo Bernabeu</title>
    <link>https://pablobernabeu.github.io/tags/s/</link>
      <atom:link href="https://pablobernabeu.github.io/tags/s/index.xml" rel="self" type="application/rss+xml" />
    <description>s</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-uk</language><copyright>Pablo Bernabeu, 2015—2026. Licence: [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/). Email: pcbernabeu@gmail.com. Cookies only used by third-party systems such as [Disqus](https://help.disqus.com/en/articles/1717155-use-of-cookies).</copyright><lastBuildDate>Wed, 12 Nov 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://pablobernabeu.github.io/img/default_preview_image.png</url>
      <title>s</title>
      <link>https://pablobernabeu.github.io/tags/s/</link>
    </image>
    
    <item>
      <title>Secure and scalable speech transcription for local and HPC</title>
      <link>https://pablobernabeu.github.io/2025/speech-transcription-python/</link>
      <pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2025/speech-transcription-python/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2025/speech-transcription-python/index.en_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2025/speech-transcription-python/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2025/speech-transcription-python/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;


&lt;div id=&#34;the-evolution-of-transcription-technology&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Evolution of Transcription Technology&lt;/h2&gt;
&lt;p&gt;The landscape of speech-to-text transcription has undergone a remarkable transformation in recent years, driven by the proliferation of generative artificial intelligence (genAI) tools. From basic dictation software to sophisticated neural networks, transcription technology has evolved to handle diverse audio conditions, multiple languages and complex speech patterns with unprecedented accuracy.&lt;/p&gt;
&lt;p&gt;At the forefront of this revolution stands &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;OpenAI’s Whisper model family&lt;/a&gt;, released as open-source tools that have democratised access to state-of-the-art &lt;a href=&#34;https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&#34;&gt;automatic speech recognition (ASR)&lt;/a&gt; capabilities. True to the “Open” in OpenAI’s original mission, these models have become the gold standard for transcription tasks, offering researchers and developers robust, multilingual speech recognition that rivals proprietary commercial solutions. The Whisper architecture, trained on 680,000 hours of multilingual audio data, represents a paradigm shift toward generalisable, production-ready ASR systems that can handle real-world audio conditions without extensive fine-tuning.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chatbots-and-limitations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chatbots and Limitations&lt;/h2&gt;
&lt;p&gt;Large Language Model chatbots such as &lt;a href=&#34;https://chat.openai.com/&#34;&gt;ChatGPT&lt;/a&gt; and &lt;a href=&#34;https://gemini.google.com/&#34;&gt;Google Gemini&lt;/a&gt; allow uploading recordings and having them transcribed using advanced models such as Whisper. However, this route has several limitations that make it unsuitable for serious research and production workflows.&lt;/p&gt;
&lt;p&gt;First, cloud-based chatbot interfaces impose strict file size limitations, typically restricting uploads to recordings of 25MB or less, which translates to roughly 20-30 minutes of audio content. This constraint renders them impractical for transcribing lengthy interviews, focus groups, or extended research sessions that often span multiple hours.&lt;/p&gt;
&lt;p&gt;Second, chatbot-based transcription provides significantly less reproducibility than local workflows. The exact model versions, processing parameters and post-processing steps remain opaque to users, making it impossible to replicate results or maintain consistent transcription quality across different sessions. This lack of transparency is particularly problematic for academic research where methodological rigor and reproducibility are paramount.&lt;/p&gt;
&lt;p&gt;Third, when working with confidential or sensitive recordings, cloud-based solutions introduce substantial privacy and compliance risks. While some platforms offer temporary chat modes that allegedly prevent model training using uploaded content, this approach still requires transmitting sensitive audio data to third-party servers, potentially violating institutional policies, research ethics requirements, or data protection regulations such as the &lt;a href=&#34;https://gdpr.eu/&#34;&gt;General Data Protection Regulation (GDPR)&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;addressing-the-limitations-python-supported-local-workflow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Addressing the Limitations: Python-Supported Local Workflow&lt;/h2&gt;
&lt;p&gt;The constraints of cloud-based transcription can be comprehensively addressed by implementing a secure, self-contained, local workflow that leverages the same advanced models while maintaining complete control over data processing and storage. This &lt;a href=&#34;https://github.com/pablobernabeu/secure_local_HPC_speech_transcription&#34;&gt;production-grade transcription system&lt;/a&gt; offers several compelling advantages over cloud alternatives:&lt;/p&gt;
&lt;div id=&#34;complete-data-sovereignty-and-gdpr-compliance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Complete Data Sovereignty and GDPR Compliance&lt;/h3&gt;
&lt;p&gt;By executing all processing on local or institutional infrastructure, the workflow ensures that sensitive audio content never leaves the controlled environment. This approach provides full GDPR compliance and satisfies the stringent data protection requirements common in academic research, healthcare and corporate environments. The system downloads pre-trained models once and runs them entirely offline, eliminating ongoing data transmission concerns.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unlimited-scale-and-batch-processing-capabilities&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Unlimited Scale and Batch Processing Capabilities&lt;/h3&gt;
&lt;p&gt;Unlike cloud services with arbitrary file size limitations, the local workflow can process audio files of any length and handle large-scale batch operations. The system supports parallel processing across multiple graphics processing unit (GPU) nodes in high-performance computing (HPC) environments, enabling researchers to transcribe hundreds of hours of audio content efficiently. The intelligent job scheduling system automatically detects available files and optimises resource allocation across computing clusters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reproducible-and-auditable-processing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reproducible and Auditable Processing&lt;/h3&gt;
&lt;p&gt;Every aspect of the transcription pipeline is configurable and documented, from model selection and audio enhancement parameters to text processing rules and privacy protection settings. This transparency enables researchers to maintain detailed methodological records, reproduce results across different time periods and adjust processing parameters to optimise for specific audio conditions or research requirements.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;advanced-quality-control-and-post-processing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Advanced Quality Control and Post-Processing&lt;/h3&gt;
&lt;p&gt;The workflow incorporates sophisticated quality improvement algorithms that address common artefacts introduced by generative artificial intelligence models. These include automatic detection and removal of spurious repetitions, intelligent punctuation correction and context-aware personal name masking that prevents false positives whilst maintaining conversation flow and readability.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;flexible-audio-enhancement-pipeline&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Flexible Audio Enhancement Pipeline&lt;/h3&gt;
&lt;p&gt;The system includes an optional audio enhancement stage that applies spectral noise reduction, dynamic range compression and signal amplification to improve transcription quality for challenging audio conditions. This preprocessing stage uses the first 0.5 seconds of each recording as a noise reference, enabling adaptive enhancement that adjusts to different recording environments.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multi-model-support-and-future-proofing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multi-Model Support and Future-Proofing&lt;/h3&gt;
&lt;p&gt;While optimised for OpenAI’s Whisper models, the architecture supports any &lt;a href=&#34;https://huggingface.co/&#34;&gt;HuggingFace&lt;/a&gt;-compatible &lt;a href=&#34;https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&#34;&gt;ASR model&lt;/a&gt;, enabling researchers to experiment with specialised models for domain-specific applications or incorporate newer model releases as they become available. The modular design ensures long-term sustainability and adaptability to evolving transcription technologies.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-output&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example Output&lt;/h2&gt;
&lt;p&gt;The repository includes &lt;a href=&#34;https://github.com/pablobernabeu/secure_local_HPC_speech_transcription/tree/main/example_output&#34;&gt;example output files&lt;/a&gt; demonstrating the system’s transcription quality and formatting. Below is an &lt;a href=&#34;https://github.com/pablobernabeu/secure_local_HPC_speech_transcription/blob/main/example_output/transcripts/The%20modular%20mini-grammar_%20Building%20testable%20and%20reproducible%20artificial%20languages%20using%20FAIR%20principles_transcript.txt&#34;&gt;example transcript&lt;/a&gt;:&lt;/p&gt;
&lt;textarea readonly style=&#39;border-color: lightgrey; overflow: hidden; color: darkblue; font-size: 90%; min-width: 100%; height: 80vh; white-space: pre-wrap; overflow-wrap: normal; padding-right: 0.5em; padding-left: 1em;&#39;&gt;&lt;/textarea&gt;
&lt;script&gt;
fetch(&#39;https://raw.githubusercontent.com/pablobernabeu/secure_local_HPC_speech_transcription/refs/heads/main/example_output/transcripts/The%20modular%20mini-grammar_%20Building%20testable%20and%20reproducible%20artificial%20languages%20using%20FAIR%20principles_transcript.txt&#39;)
  .then(response =&gt; response.text())
  .then(data =&gt; {
    document.querySelector(&#39;textarea&#39;).textContent = data;
  });
&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;technical-architecture&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Technical Architecture&lt;/h2&gt;
&lt;p&gt;The system is implemented as a monolithic &lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt; script, &lt;a href=&#34;https://github.com/pablobernabeu/secure_local_HPC_speech_transcription/blob/main/transcription.py&#34;&gt;&lt;code&gt;transcription.py&lt;/code&gt;&lt;/a&gt;, which integrates all core components into a single, self-contained processing engine. This architectural choice prioritises simplicity, maintainability and ease of deployment without external module dependencies. The script orchestrates a multi-stage pipeline and resolves several critical technical challenges through a unified command-line interface.&lt;/p&gt;
&lt;script src=&#39;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fsecure_local_HPC_speech_transcription%2Fblob%2Fmain%2Ftranscription.py%23L1096-L1117&amp;style=a11y-dark&amp;type=code&amp;showFullPath=on&amp;showCopy=on&amp;showLineNumbers=on&amp;showFileMeta=on&#39;&gt;&lt;/script&gt;
&lt;div id=&#34;core-technical-solutions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Core Technical Solutions&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Challenge 1: GenAI-Generated Repetitions&lt;/strong&gt;
A significant hurdle with generative AI models like Whisper is their tendency to produce spurious repetitions—a form of hallucination where the model gets “stuck” and repeats phrases.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: The &lt;code&gt;--fix-spurious-repetitions&lt;/code&gt; flag activates a sophisticated repetition detection algorithm. It analyses text patterns to distinguish between intentional, natural emphasis and AI-generated artefacts by considering the frequency, context, and structure of repeated segments.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Challenge 2: GenAI-Generated Language Switching&lt;/strong&gt;
Whisper models can erroneously switch languages when interpreting phonetically ambiguous sounds.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: The &lt;code&gt;--language&lt;/code&gt; argument constrains the model to the designated language, preventing unwanted language switches while maintaining transcription accuracy.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Challenge 3: Intelligent and Private Name Masking&lt;/strong&gt;
A key challenge is reliably identifying personal names while avoiding the masking of common words or technical terms (false positives).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: The system uses a sophisticated, multi-tiered approach. The default is a curated database of over 1,793 names across nine languages, refined to minimise false positives on common words. For broader coverage, optional databases from Facebook (over 1.7 million names) can be enabled, though this increases the risk of false positives. Users can also provide custom lists of names to exclude from masking, which is useful for preserving the names of public figures or research team members.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Challenge 4: Scalable High-Performance Computing (HPC) Integration&lt;/strong&gt;
The workflow is designed for large-scale batch processing in HPC environments using a Simple Linux Utility for Resource Management (SLURM) scheduler.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: A collection of submission scripts provides dynamic job array sizing, automatically matching the number of jobs to the number of input files. The system intelligently optimises resource use by prioritising GPU allocation with a graceful fallback to CPU, ensuring continuous operation. It also includes comprehensive error detection and recovery. Users can override default HPC resource allocations using the &lt;code&gt;--memory&lt;/code&gt; and &lt;code&gt;--time-limit&lt;/code&gt; arguments for exceptionally large or complex files.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Challenge 5: Reproducible and Stable Environments&lt;/strong&gt;
Creating a consistent Python environment across different platforms can be difficult due to dependency conflicts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: The project includes platform-agnostic setup scripts that automatically detect and adapt to different HPC module systems. They manage complex dependencies, particularly for &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt; and &lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit&#34;&gt;CUDA&lt;/a&gt;, and use version pinning to ensure stability and reproducibility.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Challenge 6: Speaker Attribution (Diarisation)&lt;/strong&gt;
Identifying who is speaking in a multi-speaker recording is a common requirement.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: The system integrates &lt;a href=&#34;https://github.com/pyannote/pyannote-audio&#34;&gt;&lt;code&gt;pyannote.audio&lt;/code&gt;&lt;/a&gt; for speaker diarisation, which can be enabled with the &lt;code&gt;--speaker-attribution&lt;/code&gt; flag. This feature requires a &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;HuggingFace user access token&lt;/a&gt; for the &lt;a href=&#34;https://huggingface.co/pyannote/speaker-diarization-3.1&#34;&gt;&lt;code&gt;pyannote/speaker-diarization-3.1&lt;/code&gt;&lt;/a&gt; model.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;describing-the-method-in-publications&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Describing the Method in Publications&lt;/h2&gt;
&lt;p&gt;For researchers incorporating this workflow into their methodology, the following description provides a comprehensive yet concise summary suitable for academic publications:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Audio recordings were transcribed using &lt;a href=&#34;https://huggingface.co/openai/whisper-large-v3&#34;&gt;OpenAI’s Whisper large-v3 model&lt;/a&gt;, a state-of-the-art automatic speech recognition system (Batista, 2024). The transcription workflow maintained full GDPR compliance through a local implementation where the pre-trained model was downloaded from the OpenAI repository on the &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face platform&lt;/a&gt; and executed entirely on institutional high-performance computing infrastructure, ensuring no audio data or transcription content was transmitted to or processed by third-party services.&lt;/p&gt;
&lt;p&gt;The pipeline incorporated several processing stages: optional audio enhancement for improved signal quality using spectral noise reduction and dynamic range compression, configurable language specification to prevent unwanted language switching artefacts, automatic detection and removal of spurious text repetitions generated by the AI model, comprehensive spelling corrections and text formatting, and privacy protection through intelligent personal name masking that replaced detected names with anonymised placeholders whilst avoiding false positives on common conversational words.&lt;/p&gt;
&lt;p&gt;Quality control measures included automatic repetition pattern detection to remove AI-generated artefacts, punctuation spacing corrections and context-aware text processing to maintain natural conversation flow. The system generated both plain text transcripts and formatted Microsoft Word documents with comprehensive processing metadata and timestamps for reproducibility and audit purposes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Batista, J. R. (2024). &lt;em&gt;Learn OpenAI Whisper: Transform your understanding of GenAI through robust and accurate speech processing solutions&lt;/em&gt;. Packt Publishing Ltd.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;licence&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Licence&lt;/h3&gt;
&lt;p&gt;This workflow is made available under the &lt;a href=&#34;https://github.com/pablobernabeu/secure_local_HPC_speech_transcription/blob/main/Licence.md&#34;&gt;Creative Commons 4.0 Attribution 4.0 International licence&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;citation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Citation&lt;/h3&gt;
&lt;div style=&#34;text-align: left;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.5281/zenodo.17624830&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/DOI/10.5281/zenodo.17624830.svg&#34; alt=&#34;DOI&#34; style=&#34;display: inline-block; margin: 0;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If you use this workflow in your research, please cite:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p class=&#34;hanging-indent&#34;&gt;
Bernabeu, P. (2025). &lt;em&gt;Secure and scalable speech transcription for local and HPC&lt;/em&gt; (Version 1.0.0) [Computer software]. Zenodo. &lt;a href=&#34;https://doi.org/10.5281/zenodo.17624830&#34; class=&#34;uri&#34;&gt;https://doi.org/10.5281/zenodo.17624830&lt;/a&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The recommended BibTeX entry is:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;@misc{secure_local_HPC_speech_transcription,
  author    = {Bernabeu, Pablo},
  title     = {Secure and scalable speech transcription for local and {HPC}},
  year      = {2025},
  publisher = {Zenodo},
  version   = {1.0.0},
  doi       = {10.5281/zenodo.17624830},
  url       = {https://doi.org/10.5281/zenodo.17624830}
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This comprehensive transcription workflow represents a mature, production-ready alternative to cloud-based transcription services, specifically designed to meet the demanding requirements of research and development environments. By combining state-of-the-art AI models with robust engineering practices within a monolithic, easily maintainable architecture, the system delivers high-quality transcription capabilities whilst maintaining complete control over data processing, privacy protection and quality assurance.&lt;/p&gt;
&lt;p&gt;The workflow’s emphasis on reproducibility, scalability and simplicity makes it particularly valuable for research applications where methodological rigour and long-term sustainability are essential. As speech recognition technology continues to evolve, this straightforward architecture ensures that researchers can incorporate new developments whilst maintaining the stability and clarity required for research workflows.&lt;/p&gt;
&lt;p&gt;For organisations seeking to leverage advanced transcription capabilities without compromising data sovereignty or processing control, this workflow provides a compelling foundation for building sophisticated speech processing pipelines that can grow and adapt with emerging technologies and evolving research requirements.&lt;/p&gt;
&lt;div style=&#34;text-align: center; margin: 2em 0;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/secure_local_HPC_speech_transcription&#34; target=&#34;_blank&#34; class=&#34;github-workflow-button&#34;&gt;
&lt;svg style=&#34;width: 1.2em; height: 1.2em; vertical-align: middle; margin-right: 0.5em; fill: currentColor;&#34; viewBox=&#34;0 0 16 16&#34;&gt;
&lt;path d=&#34;M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z&#34;/&gt;
&lt;/svg&gt;
View Workflow on GitHub
&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>&lt;code&gt;4authors-year-doi-url&lt;/code&gt;: Minimal, numeric CSL style for documents with extreme space constraints</title>
      <link>https://pablobernabeu.github.io/2025/4authors-year-doi-url/</link>
      <pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2025/4authors-year-doi-url/</guid>
      <description>


&lt;p&gt;The &lt;a href=&#34;https://github.com/pablobernabeu/4authors-year-doi-url/blob/main/4authors-year-doi-url.csl&#34;&gt;&lt;code&gt;4authors-year-doi-url&lt;/code&gt; CSL style&lt;/a&gt; is designed to be as compact as possible—ideal for posters, slides and abstracts—while retaining the three most critical pieces of information for a reference: &lt;strong&gt;who&lt;/strong&gt; (authors), &lt;strong&gt;when&lt;/strong&gt; (year) and &lt;strong&gt;where&lt;/strong&gt; (DOI/URL).&lt;/p&gt;
&lt;div id=&#34;features&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;In-Text Citations:&lt;/strong&gt; Simple, superscripted numbers (e.g., &lt;sup&gt;1&lt;/sup&gt;, &lt;sup&gt;2&lt;/sup&gt;, &lt;sup&gt;3&lt;/sup&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Bibliography:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Numeric: A numbered list, ordered by appearance in the text.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;No Titles: Article and book titles are completely omitted to save space.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1-3 Authors: All authors are listed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;4+ Authors: Lists the first 3 authors, followed by an ellipsis (…) and the last author.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Year:&lt;/strong&gt; The publication year is included in parentheses.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;(DOI) URL:&lt;/strong&gt; The reference ends with a DOI URL or another available URL.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;div id=&#34;in-text-citation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;In-Text Citation&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;We also recognise the many-to-many relationship between forms of knowledge and skills.&lt;sup&gt;2&lt;/sup&gt; To build a predictive model, we will analyse interactions between reader-level factors and a comprehensive set of computationally-derived text variables.&lt;sup&gt;5–8&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bibliography&lt;/h3&gt;
&lt;blockquote class=&#34;no-bullets&#34; style=&#34;font-size: 67%; padding-left: -45%; padding-top: 2%; padding-bottom: 0.03%;&#34;&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Orben, A., &amp;amp; Przybylski, A. K. (2019). &lt;a href=&#34;https://doi.org/10.1177/0956797619830329&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/0956797619830329&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hattan, C., &amp;amp; Kendeou, P. (2024). &lt;a href=&#34;https://doi.org/10.1080/00461520.2024.2418048&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1080/00461520.2024.2418048&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mathers, S. J., Kolancali, P., Jelley, F., … Murphy, V. A. (2025). &lt;a href=&#34;https://doi.org/10.1016/j.edurev.2025.100665&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.edurev.2025.100665&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Li, S., Hiver, P., &amp;amp; Papi, M. (Eds). (2022). &lt;a href=&#34;https://doi.org/10.4324/9781003270546&#34; class=&#34;uri&#34;&gt;https://doi.org/10.4324/9781003270546&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Nielsen, J. L., Christensen, R. V., &amp;amp; Poulsen, M. (2025). &lt;a href=&#34;https://doi.org/10.1002/rrq.70003&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1002/rrq.70003&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Kaup, B., Günther, F., &amp;amp; Dudschig, C. (2025). &lt;a href=&#34;https://doi.org/10.1016/bs.plm.2025.08.001&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/bs.plm.2025.08.001&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Martinc, M., Pollak, S., &amp;amp; Robnik-Šikonja, M. (2021). &lt;a href=&#34;https://doi.org/10.1162/coli_a_00398&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1162/coli_a_00398&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bernabeu, P. (2022). &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Blasi, D. E., Henrich, J., Adamou, E., … Majid, A. (2022). &lt;a href=&#34;https://doi.org/10.1016/j.tics.2022.09.015&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.tics.2022.09.015&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-use&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to Use&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Download the &lt;a href=&#34;https://github.com/pablobernabeu/4authors-year-doi-url/blob/main/4authors-year-doi-url.csl&#34;&gt;4authors-year-doi-url.csl file&lt;/a&gt; from GitHub.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add the file to your reference manager:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Zotero:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Go to Edit &amp;gt; Preferences.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Go to the Cite tab and click the Styles sub-tab.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Click the + button and select the .csl file you downloaded.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mendeley:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Go to View &amp;gt; Citation Style &amp;gt; More Styles….&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Go to the Custom Styles tab.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Drag and drop the .csl file onto the page.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Other Platforms: Most reference managers (e.g., EndNote, Papers) have an option to add a custom CSL style. Consult their documentation for specific instructions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Select &lt;code&gt;4authors-year-doi-url&lt;/code&gt; as the citation style in your reference manager and word processor.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Prototype workflow for semi-automatic processing of speech and co-speech gestures</title>
      <link>https://pablobernabeu.github.io/2025/prototype-workflow-for-semi-automatic-processing-of-speech-and-cospeech-gestures/</link>
      <pubDate>Fri, 28 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2025/prototype-workflow-for-semi-automatic-processing-of-speech-and-cospeech-gestures/</guid>
      <description>


&lt;div id=&#34;integrating-speech-and-gesture-processing-for-linguistic-analysis-using-python&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Integrating Speech and Gesture Processing for Linguistic Analysis using Python&lt;/h2&gt;
&lt;p&gt;Understanding the interplay between speech and gesture is crucial for linguistic and cognitive research. The current prototype, available &lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures&#34;&gt;on GitHub&lt;/a&gt;, aims to automate the analysis of temporal alignment between spoken demonstrative pronouns and pointing gestures in video recordings. By integrating computer vision (via Google’s &lt;a href=&#34;https://ai.google.dev/edge/mediapipe/solutions/guide&#34;&gt;MediaPipe&lt;/a&gt;) and speech recognition (using &lt;a href=&#34;https://alphacephei.com/vosk/models&#34;&gt;language-specific Vosk models&lt;/a&gt;) using Python, the workflow provides enriched video annotations and alignment data, offering valuable insights into deictic communication.&lt;/p&gt;
&lt;p&gt;For reference, the GitHub repository includes an &lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/ELAN&#34;&gt;&lt;em&gt;ELAN&lt;/em&gt; folder&lt;/a&gt; containing output from a traditional annotation process using the &lt;a href=&#34;https://archive.mpi.nl/tla/elan&#34;&gt;ELAN program&lt;/a&gt;. Ultimately, the performance of the semi-automated prototype must be validated against manual annotations created using ELAN or a similar program. For reference, below is a plot of manually-annotated data for the alignment between demonstrative pronouns and pointing gestures (see &lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/ELAN/plot_alignment.R&#34;&gt;R code for the plot&lt;/a&gt;).&lt;/p&gt;
&lt;div style=&#34;margin-top: 4%;&#34;&gt;

&lt;/div&gt;
&lt;p&gt;&lt;img src=&#39;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/ELAN/plots/1_scatterplot.png?raw=true&#39; alt=&#39;Plot of manually-annotated data for the alignment between demonstrative pronouns and pointing gestures.&#39; style=&#39;margin-top:2px;&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-it-works-running-the-program&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How It Works: Running the Program&lt;/h2&gt;
&lt;p&gt;The prototype system, which is available &lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures&#34;&gt;on GitHub&lt;/a&gt;, requires primary data in the form of video and corresponding audio files, which should be placed in &lt;code&gt;mnt/primary data&lt;/code&gt;. They video-audio pairs should be named in the same way (e.g., &lt;code&gt;1.mp4&lt;/code&gt; and &lt;code&gt;1.wav&lt;/code&gt;). The video should feature a person in a medium or medium close-up shot.&lt;/p&gt;
&lt;p&gt;Running the following command initiates the processing pipeline:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python main.py --audio_folder &amp;quot;mnt/primary data/audio&amp;quot; \
               --video_folder &amp;quot;mnt/primary data/video&amp;quot; \
               --model &amp;quot;mnt/primary data/vosk-model-de-0.21&amp;quot; \
               --words_of_interest &amp;quot;der,die,das,den,dem,denen,dessen,deren,dieser,diese,dieses,diesen,diesem&amp;quot; \
               --output &amp;quot;mnt/output&amp;quot; \
               --max_time_diff 800&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This command processes the data and stores results in the designated output directory.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;breaking-down-the-processing-pipeline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Breaking Down the Processing Pipeline&lt;/h2&gt;
&lt;div id=&#34;audio-transcription-and-word-onset-extraction-audio_processing.py&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1. Audio Transcription and Word Onset Extraction (&lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/audio_processing.py&#34;&gt;&lt;code&gt;audio_processing.py&lt;/code&gt;&lt;/a&gt;)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The speech recognition model transcribes spoken content and identifies demonstrative pronouns from a predefined list.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Onset times of these pronouns are extracted to facilitate alignment analysis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Outputs include a plain text transcript and a WebVTT subtitle file.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;script-container&#34; data-src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fprototype-process-pronouns-gestures%2Fblob%2Fmain%2Faudio_processing.py&amp;amp;style=a11y-dark&amp;amp;type=code&amp;amp;showFullPath=on&amp;amp;showCopy=on&amp;amp;showLineNumbers=on&amp;amp;showFileMeta=on&#34;&gt;
&lt;button class=&#34;toggle-script&#34;&gt;Show script&lt;/button&gt;
&lt;div class=&#34;script-wrapper&#34;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;gesture-detection-video_processing.py&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2. Gesture Detection (&lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/video_processing.py&#34;&gt;&lt;code&gt;video_processing.py&lt;/code&gt;&lt;/a&gt;)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker&#34;&gt;MediaPipe’s hand landmark estimation&lt;/a&gt; detects pointing gestures based on the position of the wrist (landmark &lt;code&gt;0&lt;/code&gt;) and the tip of the index finger (landmark &lt;code&gt;8&lt;/code&gt;). The &lt;a href=&#34;https://mediapipe-studio.webapps.google.com/demo/hand_landmarker&#34;&gt;online demonstration&lt;/a&gt; is worth a check.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A pointing gesture is recognised at the moment when these landmarks are maximally distant from each other.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;script-container&#34; data-src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fprototype-process-pronouns-gestures%2Fblob%2Fmain%2Fvideo_processing.py&amp;amp;style=a11y-dark&amp;amp;type=code&amp;amp;showFullPath=on&amp;amp;showCopy=on&amp;amp;showLineNumbers=on&amp;amp;showFileMeta=on&#34;&gt;
&lt;button class=&#34;toggle-script&#34;&gt;Show script&lt;/button&gt;
&lt;div class=&#34;script-wrapper&#34;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;alignment-analysis-alignment_analysis.py&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;3. Alignment Analysis (&lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/alignment_analysis.py&#34;&gt;&lt;code&gt;alignment_analysis.py&lt;/code&gt;&lt;/a&gt;)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The extracted demonstrative pronoun onsets are compared with detected gesture apices. Both categories are paired on a case-by-case basis if the distance between them is smaller than the maximum gap (&lt;code&gt;max_time_diff&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Temporal differences between speech and gesture events are calculated.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A CSV file containing word-gesture alignment data is generated.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A scatter plot is created for each recording to illustrate the alignment between the words of interest and the closest pointing gesture within the &lt;code&gt;max_time_diff&lt;/code&gt; window.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;margin-top: 4%;&#34;&gt;

&lt;/div&gt;
&lt;p&gt;&lt;img src=&#39;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/mnt/output/1_scatterplot.png?raw=true&#39; alt=&#39;Plot of data for the alignment between demonstrative pronouns and pointing gestures, obtained using a semi-automated workflow in Python.&#39; style=&#39;margin-top:2px; margin-bottom:-2px;&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;div class=&#34;script-container&#34; data-src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fprototype-process-pronouns-gestures%2Fblob%2Fmain%2Falignment_analysis.py&amp;amp;style=a11y-dark&amp;amp;type=code&amp;amp;showFullPath=on&amp;amp;showCopy=on&amp;amp;showLineNumbers=on&amp;amp;showFileMeta=on&#34;&gt;
&lt;button class=&#34;toggle-script&#34;&gt;Show script&lt;/button&gt;
&lt;div class=&#34;script-wrapper&#34;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;video-processing-and-annotation-video_editing.py&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;4. Video Processing and Annotation (&lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/video_editing.py&#34;&gt;&lt;code&gt;video_editing.py&lt;/code&gt;&lt;/a&gt;)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The system overlays the transcribed speech as subtitles.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Gesture peaks are highlighted to make alignment patterns visible.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The original audio is merged into the video for reference.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;script-container&#34; data-src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fprototype-process-pronouns-gestures%2Fblob%2Fmain%2Fvideo_editing.py&amp;amp;style=a11y-dark&amp;amp;type=code&amp;amp;showFullPath=on&amp;amp;showCopy=on&amp;amp;showLineNumbers=on&amp;amp;showFileMeta=on&#34;&gt;
&lt;button class=&#34;toggle-script&#34;&gt;Show script&lt;/button&gt;
&lt;div class=&#34;script-wrapper&#34;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;automated-execution-main.py&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;5. Automated Execution (&lt;a href=&#34;https://github.com/pablobernabeu/prototype-process-pronouns-gestures/blob/main/main.py&#34;&gt;&lt;code&gt;main.py&lt;/code&gt;&lt;/a&gt;)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The main script coordinates the entire process.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Multiple audio-video file pairs can be processed simultaneously.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Results are systematically organised in the output directory.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;script-container&#34; data-src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fprototype-process-pronouns-gestures%2Fblob%2Fmain%2Fmain.py&amp;amp;style=a11y-dark&amp;amp;type=code&amp;amp;showFullPath=on&amp;amp;showCopy=on&amp;amp;showLineNumbers=on&amp;amp;showFileMeta=on&#34;&gt;
&lt;button class=&#34;toggle-script&#34;&gt;Show script&lt;/button&gt;
&lt;div class=&#34;script-wrapper&#34;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;current-challenges-and-next-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Current Challenges and Next Steps&lt;/h2&gt;
&lt;div id=&#34;improving-pronoun-identification&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1. Improving Pronoun Identification&lt;/h4&gt;
&lt;p&gt;One limitation of the current system is the overidentification of demonstrative pronouns. In languages such as English, French and German, many definite articles are mistakenly included because they share the same form as demonstrative pronouns. This issue could be addressed by replacing the current current fuzzy &lt;code&gt;words_of_interest&lt;/code&gt; list with a more precise list, where each pronoun is contextualised by its preceding and subsequent words.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;enhancing-gesture-detection-accuracy&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2. Enhancing Gesture Detection Accuracy&lt;/h4&gt;
&lt;p&gt;The system underidentifies pointing gestures, which impacts the overall analysis. Improving MediaPipe’s detection implementation and incorporating additional filtering methods—such as movement velocity thresholds—could significantly enhance accuracy.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This prototype represents an important step towards automating the analysis of speech-gesture interactions. By bridging linguistic and computer vision technologies, the system offers a scalable method for studying deictic communication, paving the way for further refinements in multimodal linguistic analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reducing the impedance in electroencephalography using a blunt needle, electrolyte gel and wiggling</title>
      <link>https://pablobernabeu.github.io/2024/lowering-impedance-in-electroencephalography-using-a-blunt-needle-electrolyte-gel-and-wiggling/</link>
      <pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2024/lowering-impedance-in-electroencephalography-using-a-blunt-needle-electrolyte-gel-and-wiggling/</guid>
      <description>


&lt;p&gt;Reducing the impedance in electroencephalography (EEG) is crucial for capturing high-quality brain activity signals. This process involves ensuring that electrodes make optimal contact with the skin without harming the participant. Below are a few tips to achieve this using a blunt needle, electrolyte gel and gentle wiggling.&lt;/p&gt;
&lt;div id=&#34;first-do-no-harm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;First, Do No Harm&lt;/h2&gt;
&lt;p&gt;The primary goal is to reduce the impedance without damaging the skin or causing discomfort. Only use disposable, blunt needles, and never let the needle pierce the skin. While wiggling the needle under each electrode, do not any apply any force down on the skin. To achieve this, hold the syringe with the minimum grip force needed to control it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;manage-the-angle-and-depth-of-the-needle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Manage the Angle and Depth of the Needle&lt;/h2&gt;
&lt;p&gt;The needle should be positioned at an angle of around 20–50 degrees relative to the participant’s skin. To attain such an angle, you may rotate the electrodes, move yourself or move the participant.&lt;/p&gt;
&lt;p&gt;Control the depth of the needle to prevent scratching the skin. Usually, only half of the needle has to be inside, as the key work area is right under the electrode.&lt;/p&gt;
&lt;div&gt;
&lt;div style=&#34;position: relative; padding-top: 56.25%; margin-top: 7%;&#34;&gt;
&lt;iframe src=&#34;https://www.youtube.com/embed/4KLtp-WnOOo&#34; frameborder=&#34;0&#34; allowfullscreen
      style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;apply-gel-strategically&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Apply Gel Strategically&lt;/h2&gt;
&lt;p&gt;Use just enough electrolyte gel to reduce the impedance, but avoid creating bridges between adjacent electrodes. A small amount of gel should be visible on the opening of each electrode, and any large lumps should be carefully removed. This ensures that each electrode maintains a distinct connection with the skin.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;control-the-swing-of-the-electrode&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Control the Swing of the Electrode&lt;/h2&gt;
&lt;p&gt;Place your finger on the electrode throughout the wiggling to feel and control its movement. This helps prevent excess gel from forming a bump under the electrode, which could lead to inaccurate readings.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wiggle-with-precision&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wiggle with Precision&lt;/h2&gt;
&lt;p&gt;Position yourself comfortably to work on each electrode, allowing for precise movements and minimising participant discomfort. Wiggle the needle in small circular motions to move hair aside and ensure good skin contact. Be careful not to scratch the skin in the process.&lt;/p&gt;
&lt;p&gt;By following these tips, you’ll achieve lower impedance without causing harm, ensuring high-quality EEG data and a better experience for the participant. In short, you’ll have moved the needle.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Passive constructions and asymmetries between languages</title>
      <link>https://pablobernabeu.github.io/2024/passive-constructions-and-asymmetries-between-languages/</link>
      <pubDate>Sat, 21 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2024/passive-constructions-and-asymmetries-between-languages/</guid>
      <description>


&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34; style=&#34;padding-bottom: 0; margin-bottom: 6%;&#34;&gt;
&lt;p&gt;Given all of the data that E&amp;amp;L [Evans &amp;amp; Levinson, 2009] cite, how could anyone maintain the notion of a universal grammar with linguistic content? Traditionally, there have been three basic strategies. First, just as we may force English grammar into the Procrustean bed of Latin grammar – that is how I was taught the structure of English in grade school – the grammars of the world’s so-called exotic languages may be forced into an abstract scheme based mainly on European languages.&lt;/p&gt;
&lt;p&gt;— &lt;strong&gt;Tomasello (2009)&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Researchers often make participants jump through hoops. Due to our personal blind spots, it seems easier to realise the full extent of these acrobatics when we consider the work of other researchers. In linguistic research, the acrobatics are often spurred by unnatural grammatical constructions. For instance, in Spanish, &lt;em&gt;be&lt;/em&gt;-based passive constructions—such as &lt;em&gt;La guerra fue empezada por …&lt;/em&gt; ‘The war was started by …’—are much less frequent than in English. In Spanish, &lt;em&gt;be&lt;/em&gt;-passives are mostly reserved for the formal, written discourse. Outside of that register, the left-dislocation of the patient is more commonly achieved through the following constructions. One construction hinges on the reflexive pronoun &lt;em&gt;se&lt;/em&gt;—e.g., &lt;em&gt;El apartamento se vendió&lt;/em&gt; ‘The apartment was sold’ (Takagaki, 2005). The other construction implements a pronominal object clitic (usually in preverbal position), as illustrated below.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; The latter construction, though not strictly passive, is thematically equivalent to many passives in English (Hidalgo Downing &amp;amp; Downing, 2012).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;images/guerra-la-empezo.png&#39; alt=&#39;Comparing the frequency of the English-like, *be*-based passive to the construction that implements a pronominal object clitic in preverbal position, in Spanish.&#39; style=&#39;margin-top:2px; margin-bottom:-2px;&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size:90%; color: #899499;&#34;&gt;Comparing the frequency of the English-like, &lt;em&gt;be&lt;/em&gt;-based passive to the construction that implements a pronominal object clitic in preverbal position.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The ratio of 8 to 5,300 results suggests that &lt;em&gt;be&lt;/em&gt;-passives are much less common than the construction implementing a preverbal clitic. In the light of this ratio, it is surprising to find linguistic research that seems to overlook the asymmetry between English and Spanish regarding passives. For instance, in a study on crosslinguistic syntactic priming in bilingual children, Vasilyeva et al. (2010) used stimuli such as &lt;em&gt;El gato fue lavado por el perro&lt;/em&gt; ‘The cat was washed by the dog’. Through a quick-and-dirty corpus analysis using a search engine, we can examine the limitations of these stimuli. Whereas the English form ‘The cat was washed’ is common enough, the literal translations &lt;em&gt;El gato fue lavado&lt;/em&gt; and &lt;em&gt;La gata fue lavada&lt;/em&gt; are essentially negligible.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;images/cat-washed.png&#39; alt=&#39;Comparing the frequency of passive constructions in English and in Spanish.&#39; style=&#39;margin-top:2px; margin-bottom:-2px;&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;images/gato-lavado.png&#39; alt=&#39;Comparing the frequency of passive constructions in English and in Spanish.&#39; style=&#39;margin-top:2px; margin-bottom:-2px;&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;images/gata-lavada.png&#39; alt=&#39;Comparing the frequency of passive constructions in English and in Spanish.&#39; style=&#39;margin-top:2px; margin-bottom:-2px;&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size:90%; color: #899499;&#34;&gt;Comparing the frequency of passive constructions in English and in Spanish.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Vasilyeva et al. (2010) observed that, when primed with English-like, &lt;em&gt;be&lt;/em&gt;-based passives in Spanish, the children increased their use of passive constructions in English. In contrast, exposure to English passives in the prime sentences did not lead to a greater use of the English-like passives in Spanish. The authors described this contrast as an &lt;em&gt;asymmetry&lt;/em&gt;, which has been profusely echoed in the subsequent literature on crosslinguistic syntactic priming (Serratrice, 2022). The caveat about this finding is that the stimuli &lt;em&gt;themselves&lt;/em&gt; contained an asymmetry, as the &lt;em&gt;be&lt;/em&gt;-passive is infelicitous in Spanish, certainly when compared to &lt;em&gt;be&lt;/em&gt;-passives in English (Hartsuiker &amp;amp; Bernolet, 2017; Vasilyeva et al., 2010), and especially among children, who use passives even less frequently.&lt;/p&gt;
&lt;p&gt;Syntactic isomorphism is less common than it may seem. That is, syntactic structures cannot always be mapped on a one-to-one basis across languages. Even when the same structures exist in two languages, their frequencies may be fundamentally different, as in the case of &lt;em&gt;be&lt;/em&gt;-based passives in English and Spanish. Thus, where possible, stimuli should capture the functional equivalents between languages, not just the surface-level structural forms, to avoid skewed results. By this means, it will become easier to interpret the results of crosslinguistic priming. At the same time, seeking crosslinguistic overlaps may be like looking for a needle in a haystack.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Evans, N., &amp;amp; Levinson, S. C. (2009). The myth of language universals: Language diversity and its importance for cognitive science. &lt;em&gt;Behavioral and Brain Sciences, 32&lt;/em&gt;(5), 429-448. &lt;a href=&#34;https://doi.org/10.1017/S0140525X0999094X&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1017/S0140525X0999094X&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hartsuiker, R. J., &amp;amp; Bernolet, S. (2017). The development of shared syntax in second language learning. &lt;em&gt;Bilingualism: Language and Cognition, 20&lt;/em&gt;(2), 219-234. &lt;a href=&#34;https://doi.org/10.1017/S1366728915000164&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1017/S1366728915000164&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hidalgo Downing, R., &amp;amp; Downing, A. (2012). Topic and topicality in text: A contrastive study of English and Spanish narrative texts. &lt;em&gt;Linguistics and the Human Sciences, 6&lt;/em&gt;, 193–217. &lt;a href=&#34;https://doi.org/10.1558/lhs.v6i1-3.193&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1558/lhs.v6i1-3.193&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Serratrice, L. (2022). What can syntactic priming tell us about crosslinguistic influence? In Messenger, K. (Ed.), &lt;em&gt;Syntactic priming in language acquisition: Representations, mechanisms and applications&lt;/em&gt; (pp. 129–156). Amsterdam: John Benjamins. &lt;a href=&#34;https://doi.org/10.1075/tilar.31&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1075/tilar.31&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Takagaki, T. (2005). On the productivity of the Spanish passive constructions. In Takagaki et al. (Eds.), &lt;em&gt;Corpus-based approaches to sentence structures&lt;/em&gt; (pp. 289–309). Amsterdam: John Benjamins. &lt;a href=&#34;https://doi.org/10.1075/ubli.2&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1075/ubli.2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tomasello, M. (2009). Universal grammar is dead. &lt;em&gt;Behavioral and Brain Sciences, 32&lt;/em&gt;(5), 470–471. &lt;a href=&#34;https://doi.org/10.1017/S0140525X09990744&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1017/S0140525X09990744&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Vasilyeva, M., Waterfall, H., Gámez, P. B., Gómez, L. E., Bowers, E., &amp;amp; Shimpi, P. (2010). Cross-linguistic syntactic priming in bilingual children. &lt;em&gt;Journal of Child Language, 37&lt;/em&gt;(5), 1047–1064. &lt;a href=&#34;https://doi.org/10.1017/S0305000909990213&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1017/S0305000909990213&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;For a couple of more ideas about pronominal object clitics, see &lt;a href=&#34;https://pablobernabeu.github.io/2023/pronominal-object-clitics-in-preverbal-position-are-a-hard-nut-to-crack-for-google-translate&#34;&gt;this other post&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A makeshift EEG lab</title>
      <link>https://pablobernabeu.github.io/2024/makeshift-eeg-lab/</link>
      <pubDate>Mon, 12 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2024/makeshift-eeg-lab/</guid>
      <description>


&lt;p&gt;Say, you need to set up a makeshift EEG lab in an office? Easy-peasy—only, try to move the hardware as little as possible, especially laptops with dongles sticking out. The rest is a trail of snapshots devoid of captions, a sink, a shower room and other paraphernalia, as this is only an ancillary, temporary, extraordinary little lab, and all those staples are within reach in our mainstream lab (see &lt;a href=&#34;#references&#34;&gt;Ledwidge et al., 2018&lt;/a&gt;; &lt;a href=&#34;#references&#34;&gt;Luck, 2014&lt;/a&gt;).&lt;/p&gt;
&lt;!-- Main Article Images (clickable to Open Modal) --&gt;
&lt;div class=&#34;image-gallery&#34;&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/1.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/2.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/3.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/3a.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/4.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/5.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/6.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/7.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34; style=&#34;margin-top: 4%; margin-bottom: 2%; padding-bottom: 0;&#34;&gt;
&lt;p&gt;&lt;i class=&#39;fas fa-smile&#39;&gt;&lt;/i&gt;  The needles below are &lt;em&gt;blunt&lt;/em&gt;—that is, they do not pierce the skin. They’re just used to put some gel into the cap to enable the detection of electrical activity in the brain. You can explore the non-invasive, painless procedure of EEG &lt;a href=&#34;https://pablobernabeu.github.io/2024/lowering-impedance-in-electroencephalography-using-a-blunt-needle-electrolyte-gel-and-wiggling&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;!-- Main Article Images (clickable to Open Modal) --&gt;
&lt;div class=&#34;image-gallery&#34;&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/8.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/8a.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/9.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/10.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/10a.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/11.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/12.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/13.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/14.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/15.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/16.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/17.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/18.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/19.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/20.webp&#39; alt=&#39;makeshift EEG lab in an office&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Need to return to the main lab? No problem! The tidyverse has got your back.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)

makeshift_lab %&amp;gt;%
  pivot_longer(!superfluous, names_to = &amp;#39;registrar&amp;#39;, values_to = &amp;#39;Mephis&amp;#39;) %&amp;gt;%
  filter(view == &amp;#39;outside&amp;#39;) %&amp;gt;%
  pull(leg)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;imageContainer&#34;&gt;
&lt;p&gt;&lt;img src=&#39;images/cosy%20EEG%20booth.jpg&#39; alt=&#39;cosy EEG booth&#39;&gt;
&lt;i class=&#39;expand-icon fas fa-expand-alt&#39;&gt;&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;!-- Modal --&gt;
&lt;div id=&#34;myModal&#34; class=&#34;imageModal&#34;&gt;
&lt;div class=&#34;imageModal-text imageClose&#34;&gt;
Click here or press &lt;kbd style=&#39;font-size: 90%; color:white; background-color:#303030;&#39;&gt;Esc&lt;/kbd&gt; to close
&lt;/div&gt;
&lt;div class=&#34;imageModal-wrapper&#34;&gt;
&lt;div class=&#34;imageModal-content-wrapper&#34;&gt;
&lt;!-- Images will be dynamically added here by JavaScript --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a class=&#39;prev&#39;&gt;❮&lt;/a&gt;
&lt;a class=&#39;next&#39;&gt;❯&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Ledwidge, P., Foust, J., &amp;amp; Ramsey, A. (2018). Recommendations for developing an EEG laboratory at a primarily undergraduate institution. &lt;em&gt;Journal of Undergraduate Neuroscience Education, 17&lt;/em&gt;(1), A10. &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6312138&#34; class=&#34;uri&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6312138&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Luck, S. J. (2014). Online Chapter 16: Setting up and running an ERP lab. In S. J. Luck (Ed.), &lt;em&gt;An introduction to
the event-related potential technique&lt;/em&gt;. Cambridge, MA: MIT Press. Retrieved from &lt;a href=&#34;http://mitp-content-server.mit.edu:18180/books/content/sectbyfn?collid=books_pres_0&amp;amp;fn=Ch_16_0.pdf&amp;amp;id=8575&#34; class=&#34;uri&#34;&gt;http://mitp-content-server.mit.edu:18180/books/content/sectbyfn?collid=books_pres_0&amp;amp;fn=Ch_16_0.pdf&amp;amp;id=8575&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R functions for checking and fixing vmrk files from BrainVision</title>
      <link>https://pablobernabeu.github.io/2024/r-functions-for-checking-and-fixing-vmrk-files-from-brainvision/</link>
      <pubDate>Sun, 30 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2024/r-functions-for-checking-and-fixing-vmrk-files-from-brainvision/</guid>
      <description>


&lt;p&gt;Electroencephalography (EEG) has become a cornerstone for understanding the intricate workings of the human brain in the field of neuroscience. However, EEG software and hardware come with their own set of constraints, particularly in the management of markers, also known as triggers. This article aims to shed light on these limitations and future prospects of marker management in EEG studies, while also introducing R functions that can help deal with vmrk files from BrainVision.&lt;/p&gt;
&lt;p&gt;Markers, serving as timestamps that indicate specific events during data collection, play a crucial role in EEG studies. These events could range from the onset of a stimulus to the participant’s response. However, one of the major constraints in current EEG systems is the limitation of markers to numbers between 1 and 255. This limitation is due to the fact that markers are typically stored as 8-bit unsigned integers in computer memory, which can only represent numbers in the range of 0 to 255. However, the number 0 is usually reserved for system use, leaving only the numbers 1 to 255 available for markers.&lt;/p&gt;
&lt;p&gt;This numerical constraint can pose significant challenges in the interpretation of markers, especially in complex experimental designs where a multitude of events need to be marked and differentiated. It necessitates careful documentation of each marker’s purpose prior to running the study. This means that researchers must meticulously map each number to a specific event or condition in their experiment, which can be a daunting task, especially for complex studies with numerous conditions and events.&lt;/p&gt;
&lt;p&gt;Looking towards the future, one might wonder if it will become possible to send markers with semantic information, instead of being constrained to numbers between 1 and 255. This would allow researchers to encode more detailed information in each marker, such as the type of stimulus presented or the specific condition being tested. Such a development could revolutionize the way we conduct and analyze EEG studies, offering greater flexibility in experimental design and more nuanced insights into brain activity.&lt;/p&gt;
&lt;p&gt;Below, we’ll demonstrate some functions from &lt;a href=&#34;https://github.com/pablobernabeu/EEG-tools-and-tips&#34;&gt;this GitHub repository&lt;/a&gt; that help inspect and fix vmrk files. We’ll work with the vmrk file shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Read the file into a vector of lines
readLines(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/EEG-tools-and-tips/main/example.vmrk&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Brain Vision Data Exchange Marker File, Version 1.0&amp;quot;                               
##  [2] &amp;quot;&amp;quot;                                                                                  
##  [3] &amp;quot;[Common Infos]&amp;quot;                                                                    
##  [4] &amp;quot;DataFile=3.eeg&amp;quot;                                                                    
##  [5] &amp;quot;Codepage=utf-8&amp;quot;                                                                    
##  [6] &amp;quot;&amp;quot;                                                                                  
##  [7] &amp;quot;[Marker Infos]&amp;quot;                                                                    
##  [8] &amp;quot;; Each entry: Mk&amp;lt;Marker number&amp;gt;=&amp;lt;Type&amp;gt;,&amp;lt;Description&amp;gt;,&amp;lt;Position in data points&amp;gt;,&amp;quot;   
##  [9] &amp;quot;; &amp;lt;Size in data points&amp;gt;, &amp;lt;Channel number (0 = marker is related to all channels)&amp;gt;,&amp;quot;
## [10] &amp;quot;; &amp;lt;Date (YYYYMMDDhhmmssuuuuuu)&amp;gt;&amp;quot;                                                   
## [11] &amp;quot;; Fields are delimited by commas, some fields might be omited (empty).&amp;quot;            
## [12] &amp;quot;; Commas in type or description text are coded as \&amp;quot;\\1\&amp;quot;.&amp;quot;                        
## [13] &amp;quot;Mk1=New Segment,,1,1,0,20240318111955090000&amp;quot;                                       
## [14] &amp;quot;Mk2=Stimulus,S254,20419,1,0&amp;quot;                                                       
## [15] &amp;quot;Mk3=Stimulus,S  5,22332,1,0&amp;quot;                                                       
## [16] &amp;quot;Mk4=Stimulus,S 42,23095,1,0&amp;quot;                                                       
## [17] &amp;quot;Mk5=Stimulus,S143,23100,1,0&amp;quot;                                                       
## [18] &amp;quot;Mk6=Stimulus,S  2,23106,1,0&amp;quot;                                                       
## [19] &amp;quot;Mk7=Stimulus,S102,23111,1,0&amp;quot;                                                       
## [20] &amp;quot;Mk8=Stimulus,S  6,25882,1,0&amp;quot;                                                       
## [21] &amp;quot;Mk9=Stimulus,S  5,28106,1,0&amp;quot;                                                       
## [22] &amp;quot;Mk10=Stimulus,S 50,29053,1,0&amp;quot;                                                      
## [23] &amp;quot;Mk11=Stimulus,S241,29058,1,0&amp;quot;                                                      
## [24] &amp;quot;Mk12=Stimulus,S  1,29063,1,0&amp;quot;                                                      
## [25] &amp;quot;Mk13=Stimulus,S101,29069,1,0&amp;quot;                                                      
## [26] &amp;quot;Mk14=Stimulus,S  6,31830,1,0&amp;quot;                                                      
## [27] &amp;quot;Mk15=Stimulus,S  5,34056,1,0&amp;quot;                                                      
## [28] &amp;quot;Mk16=Stimulus,S 49,35055,1,0&amp;quot;                                                      
## [29] &amp;quot;Mk17=Stimulus,S226,35060,1,0&amp;quot;                                                      
## [30] &amp;quot;Mk18=Stimulus,S  2,35066,1,0&amp;quot;                                                      
## [31] &amp;quot;Mk19=Stimulus,S103,35071,1,0&amp;quot;                                                      
## [32] &amp;quot;Mk20=Stimulus,S  6,37242,1,0&amp;quot;                                                      
## [33] &amp;quot;Mk21=Stimulus,S  5,39436,1,0&amp;quot;                                                      
## [34] &amp;quot;Mk22=Stimulus,S 43,40417,1,0&amp;quot;                                                      
## [35] &amp;quot;Mk23=Stimulus,S155,40423,1,0&amp;quot;                                                      
## [36] &amp;quot;Mk24=Stimulus,S  2,40429,1,0&amp;quot;                                                      
## [37] &amp;quot;Mk25=Stimulus,S103,40434,1,0&amp;quot;                                                      
## [38] &amp;quot;Mk26=Stimulus,S  6,42481,1,0&amp;quot;                                                      
## [39] &amp;quot;Mk27=Stimulus,S  5,44662,1,0&amp;quot;                                                      
## [40] &amp;quot;Mk28=Stimulus,S 40,45678,1,0&amp;quot;                                                      
## [41] &amp;quot;Mk29=Stimulus,S118,45683,1,0&amp;quot;                                                      
## [42] &amp;quot;Mk30=Stimulus,S  1,45688,1,0&amp;quot;                                                      
## [43] &amp;quot;Mk31=Stimulus,S103,45693,1,0&amp;quot;                                                      
## [44] &amp;quot;Mk32=Stimulus,S  6,47621,1,0&amp;quot;                                                      
## [45] &amp;quot;Mk33=Stimulus,S  5,49809,1,0&amp;quot;                                                      
## [46] &amp;quot;Mk34=Stimulus,S 50,50808,1,0&amp;quot;                                                      
## [47] &amp;quot;Mk35=Stimulus,S237,50813,1,0&amp;quot;                                                      
## [48] &amp;quot;Mk36=Stimulus,S  3,50818,1,0&amp;quot;                                                      
## [49] &amp;quot;Mk37=Stimulus,S101,50823,1,0&amp;quot;                                                      
## [50] &amp;quot;Mk38=Stimulus,S  6,53823,1,0&amp;quot;                                                      
## [51] &amp;quot;Mk39=Stimulus,S  5,56042,1,0&amp;quot;                                                      
## [52] &amp;quot;Mk40=Stimulus,S 40,57129,1,0&amp;quot;                                                      
## [53] &amp;quot;Mk41=Stimulus,S114,57134,1,0&amp;quot;                                                      
## [54] &amp;quot;Mk42=Stimulus,S  3,57140,1,0&amp;quot;                                                      
## [55] &amp;quot;Mk43=Stimulus,S103,57145,1,0&amp;quot;                                                      
## [56] &amp;quot;Mk44=Stimulus,S  6,59661,1,0&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;numbering-trials&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Numbering trials&lt;/h2&gt;
&lt;p&gt;The antediluvian limitation of markers often prevents us from storing the order of trials using markers. Thus, when we need to inspect or fix vmrk files, we must mentally divide the lines into trials. For instance, in our example vmrk file, all trials begin with the marker &lt;code&gt;S  5&lt;/code&gt;. The function below allows us to temporarily number trials by appending the number to the first marker of each trial. The parameters of the function allow us to select the &lt;code&gt;start_line&lt;/code&gt; to skip the metadata at the top of the vmrk file, as well as to select the number of &lt;code&gt;lines_per_trial&lt;/code&gt; and the &lt;code&gt;first_number&lt;/code&gt; to use in the first trial.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2FEEG-tools-and-tips%2Fblob%2Fmain%2Fnumber_trials.R%23L3-L24&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create temporary vmrk file with numbered trials

source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/EEG-tools-and-tips/main/number_trials.R&amp;#39;)

number_trials(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/EEG-tools-and-tips/main/example.vmrk&amp;#39;, 
              start_line = 15, lines_per_trial = 6, first_number = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Brain Vision Data Exchange Marker File, Version 1.0&amp;quot;                               
##  [2] &amp;quot;&amp;quot;                                                                                  
##  [3] &amp;quot;[Common Infos]&amp;quot;                                                                    
##  [4] &amp;quot;DataFile=3.eeg&amp;quot;                                                                    
##  [5] &amp;quot;Codepage=utf-8&amp;quot;                                                                    
##  [6] &amp;quot;&amp;quot;                                                                                  
##  [7] &amp;quot;[Marker Infos]&amp;quot;                                                                    
##  [8] &amp;quot;; Each entry: Mk&amp;lt;Marker number&amp;gt;=&amp;lt;Type&amp;gt;,&amp;lt;Description&amp;gt;,&amp;lt;Position in data points&amp;gt;,&amp;quot;   
##  [9] &amp;quot;; &amp;lt;Size in data points&amp;gt;, &amp;lt;Channel number (0 = marker is related to all channels)&amp;gt;,&amp;quot;
## [10] &amp;quot;; &amp;lt;Date (YYYYMMDDhhmmssuuuuuu)&amp;gt;&amp;quot;                                                   
## [11] &amp;quot;; Fields are delimited by commas, some fields might be omited (empty).&amp;quot;            
## [12] &amp;quot;; Commas in type or description text are coded as \&amp;quot;\\1\&amp;quot;.&amp;quot;                        
## [13] &amp;quot;Mk1=New Segment,,1,1,0,20240318111955090000&amp;quot;                                       
## [14] &amp;quot;Mk2=Stimulus,S254,20419,1,0&amp;quot;                                                       
## [15] &amp;quot;Mk3=Stimulus,S  5,22332,1,0  1&amp;quot;                                                    
## [16] &amp;quot;Mk4=Stimulus,S 42,23095,1,0&amp;quot;                                                       
## [17] &amp;quot;Mk5=Stimulus,S143,23100,1,0&amp;quot;                                                       
## [18] &amp;quot;Mk6=Stimulus,S  2,23106,1,0&amp;quot;                                                       
## [19] &amp;quot;Mk7=Stimulus,S102,23111,1,0&amp;quot;                                                       
## [20] &amp;quot;Mk8=Stimulus,S  6,25882,1,0&amp;quot;                                                       
## [21] &amp;quot;Mk9=Stimulus,S  5,28106,1,0  2&amp;quot;                                                    
## [22] &amp;quot;Mk10=Stimulus,S 50,29053,1,0&amp;quot;                                                      
## [23] &amp;quot;Mk11=Stimulus,S241,29058,1,0&amp;quot;                                                      
## [24] &amp;quot;Mk12=Stimulus,S  1,29063,1,0&amp;quot;                                                      
## [25] &amp;quot;Mk13=Stimulus,S101,29069,1,0&amp;quot;                                                      
## [26] &amp;quot;Mk14=Stimulus,S  6,31830,1,0&amp;quot;                                                      
## [27] &amp;quot;Mk15=Stimulus,S  5,34056,1,0  3&amp;quot;                                                   
## [28] &amp;quot;Mk16=Stimulus,S 49,35055,1,0&amp;quot;                                                      
## [29] &amp;quot;Mk17=Stimulus,S226,35060,1,0&amp;quot;                                                      
## [30] &amp;quot;Mk18=Stimulus,S  2,35066,1,0&amp;quot;                                                      
## [31] &amp;quot;Mk19=Stimulus,S103,35071,1,0&amp;quot;                                                      
## [32] &amp;quot;Mk20=Stimulus,S  6,37242,1,0&amp;quot;                                                      
## [33] &amp;quot;Mk21=Stimulus,S  5,39436,1,0  4&amp;quot;                                                   
## [34] &amp;quot;Mk22=Stimulus,S 43,40417,1,0&amp;quot;                                                      
## [35] &amp;quot;Mk23=Stimulus,S155,40423,1,0&amp;quot;                                                      
## [36] &amp;quot;Mk24=Stimulus,S  2,40429,1,0&amp;quot;                                                      
## [37] &amp;quot;Mk25=Stimulus,S103,40434,1,0&amp;quot;                                                      
## [38] &amp;quot;Mk26=Stimulus,S  6,42481,1,0&amp;quot;                                                      
## [39] &amp;quot;Mk27=Stimulus,S  5,44662,1,0  5&amp;quot;                                                   
## [40] &amp;quot;Mk28=Stimulus,S 40,45678,1,0&amp;quot;                                                      
## [41] &amp;quot;Mk29=Stimulus,S118,45683,1,0&amp;quot;                                                      
## [42] &amp;quot;Mk30=Stimulus,S  1,45688,1,0&amp;quot;                                                      
## [43] &amp;quot;Mk31=Stimulus,S103,45693,1,0&amp;quot;                                                      
## [44] &amp;quot;Mk32=Stimulus,S  6,47621,1,0&amp;quot;                                                      
## [45] &amp;quot;Mk33=Stimulus,S  5,49809,1,0  6&amp;quot;                                                   
## [46] &amp;quot;Mk34=Stimulus,S 50,50808,1,0&amp;quot;                                                      
## [47] &amp;quot;Mk35=Stimulus,S237,50813,1,0&amp;quot;                                                      
## [48] &amp;quot;Mk36=Stimulus,S  3,50818,1,0&amp;quot;                                                      
## [49] &amp;quot;Mk37=Stimulus,S101,50823,1,0&amp;quot;                                                      
## [50] &amp;quot;Mk38=Stimulus,S  6,53823,1,0&amp;quot;                                                      
## [51] &amp;quot;Mk39=Stimulus,S  5,56042,1,0  7&amp;quot;                                                   
## [52] &amp;quot;Mk40=Stimulus,S 40,57129,1,0&amp;quot;                                                      
## [53] &amp;quot;Mk41=Stimulus,S114,57134,1,0&amp;quot;                                                      
## [54] &amp;quot;Mk42=Stimulus,S  3,57140,1,0&amp;quot;                                                      
## [55] &amp;quot;Mk43=Stimulus,S103,57145,1,0&amp;quot;                                                      
## [56] &amp;quot;Mk44=Stimulus,S  6,59661,1,0&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;removing-trial-numbers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Removing trial numbers&lt;/h2&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2FEEG-tools-and-tips%2Fblob%2Fmain%2Fremove_trial_numbers.R%23L3-L20&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;Now, we will unnumber the trials. This can be especially useful after fixing any errors in the markers. For this demo, we’ll use an &lt;a href=&#34;https://github.com/pablobernabeu/EEG-tools-and-tips/blob/main/example_numbered_trials.vmrk&#34;&gt;example file with numbered trials&lt;/a&gt; that looks just like the output from &lt;code&gt;number_trials()&lt;/code&gt; shown above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Remove trial numbers from temporary vmrk file

source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/EEG-tools-and-tips/main/remove_trial_numbers.R&amp;#39;)

remove_trial_numbers(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/EEG-tools-and-tips/main/example_numbered_trials.vmrk&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Brain Vision Data Exchange Marker File, Version 1.0&amp;quot;                               
##  [2] &amp;quot;&amp;quot;                                                                                  
##  [3] &amp;quot;[Common Infos]&amp;quot;                                                                    
##  [4] &amp;quot;DataFile=3.eeg&amp;quot;                                                                    
##  [5] &amp;quot;Codepage=utf-8&amp;quot;                                                                    
##  [6] &amp;quot;&amp;quot;                                                                                  
##  [7] &amp;quot;[Marker Infos]&amp;quot;                                                                    
##  [8] &amp;quot;; Each entry: Mk&amp;lt;Marker number&amp;gt;=&amp;lt;Type&amp;gt;,&amp;lt;Description&amp;gt;,&amp;lt;Position in data points&amp;gt;,&amp;quot;   
##  [9] &amp;quot;; &amp;lt;Size in data points&amp;gt;, &amp;lt;Channel number (0 = marker is related to all channels)&amp;gt;,&amp;quot;
## [10] &amp;quot;; &amp;lt;Date (YYYYMMDDhhmmssuuuuuu)&amp;gt;&amp;quot;                                                   
## [11] &amp;quot;; Fields are delimited by commas, some fields might be omited (empty).&amp;quot;            
## [12] &amp;quot;; Commas in type or description text are coded as \&amp;quot;\\1\&amp;quot;.&amp;quot;                        
## [13] &amp;quot;Mk1=New Segment,,1,1,0,20240318111955090000&amp;quot;                                       
## [14] &amp;quot;Mk2=Stimulus,S254,20419,1,0&amp;quot;                                                       
## [15] &amp;quot;Mk3=Stimulus,S  5,22332,1,0&amp;quot;                                                       
## [16] &amp;quot;Mk4=Stimulus,S 42,23095,1,0&amp;quot;                                                       
## [17] &amp;quot;Mk5=Stimulus,S143,23100,1,0&amp;quot;                                                       
## [18] &amp;quot;Mk6=Stimulus,S  2,23106,1,0&amp;quot;                                                       
## [19] &amp;quot;Mk7=Stimulus,S102,23111,1,0&amp;quot;                                                       
## [20] &amp;quot;Mk8=Stimulus,S  6,25882,1,0&amp;quot;                                                       
## [21] &amp;quot;Mk9=Stimulus,S  5,28106,1,0&amp;quot;                                                       
## [22] &amp;quot;Mk10=Stimulus,S 50,29053,1,0&amp;quot;                                                      
## [23] &amp;quot;Mk11=Stimulus,S241,29058,1,0&amp;quot;                                                      
## [24] &amp;quot;Mk12=Stimulus,S  1,29063,1,0&amp;quot;                                                      
## [25] &amp;quot;Mk13=Stimulus,S101,29069,1,0&amp;quot;                                                      
## [26] &amp;quot;Mk14=Stimulus,S  6,31830,1,0&amp;quot;                                                      
## [27] &amp;quot;Mk15=Stimulus,S  5,34056,1,0&amp;quot;                                                      
## [28] &amp;quot;Mk16=Stimulus,S 49,35055,1,0&amp;quot;                                                      
## [29] &amp;quot;Mk17=Stimulus,S226,35060,1,0&amp;quot;                                                      
## [30] &amp;quot;Mk18=Stimulus,S  2,35066,1,0&amp;quot;                                                      
## [31] &amp;quot;Mk19=Stimulus,S103,35071,1,0&amp;quot;                                                      
## [32] &amp;quot;Mk20=Stimulus,S  6,37242,1,0&amp;quot;                                                      
## [33] &amp;quot;Mk21=Stimulus,S  5,39436,1,0&amp;quot;                                                      
## [34] &amp;quot;Mk22=Stimulus,S 43,40417,1,0&amp;quot;                                                      
## [35] &amp;quot;Mk23=Stimulus,S155,40423,1,0&amp;quot;                                                      
## [36] &amp;quot;Mk24=Stimulus,S  2,40429,1,0&amp;quot;                                                      
## [37] &amp;quot;Mk25=Stimulus,S103,40434,1,0&amp;quot;                                                      
## [38] &amp;quot;Mk26=Stimulus,S  6,42481,1,0&amp;quot;                                                      
## [39] &amp;quot;Mk27=Stimulus,S  5,44662,1,0&amp;quot;                                                      
## [40] &amp;quot;Mk28=Stimulus,S 40,45678,1,0&amp;quot;                                                      
## [41] &amp;quot;Mk29=Stimulus,S118,45683,1,0&amp;quot;                                                      
## [42] &amp;quot;Mk30=Stimulus,S  1,45688,1,0&amp;quot;                                                      
## [43] &amp;quot;Mk31=Stimulus,S103,45693,1,0&amp;quot;                                                      
## [44] &amp;quot;Mk32=Stimulus,S  6,47621,1,0&amp;quot;                                                      
## [45] &amp;quot;Mk33=Stimulus,S  5,49809,1,0&amp;quot;                                                      
## [46] &amp;quot;Mk34=Stimulus,S 50,50808,1,0&amp;quot;                                                      
## [47] &amp;quot;Mk35=Stimulus,S237,50813,1,0&amp;quot;                                                      
## [48] &amp;quot;Mk36=Stimulus,S  3,50818,1,0&amp;quot;                                                      
## [49] &amp;quot;Mk37=Stimulus,S101,50823,1,0&amp;quot;                                                      
## [50] &amp;quot;Mk38=Stimulus,S  6,53823,1,0&amp;quot;                                                      
## [51] &amp;quot;Mk39=Stimulus,S  5,56042,1,0&amp;quot;                                                      
## [52] &amp;quot;Mk40=Stimulus,S 40,57129,1,0&amp;quot;                                                      
## [53] &amp;quot;Mk41=Stimulus,S114,57134,1,0&amp;quot;                                                      
## [54] &amp;quot;Mk42=Stimulus,S  3,57140,1,0&amp;quot;                                                      
## [55] &amp;quot;Mk43=Stimulus,S103,57145,1,0&amp;quot;                                                      
## [56] &amp;quot;Mk44=Stimulus,S  6,59661,1,0&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;counting-markers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Counting markers&lt;/h2&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2FEEG-tools-and-tips%2Fblob%2Fmain%2Fcount_markers.R%23L3-L18&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/EEG-tools-and-tips/main/count_markers.R&amp;#39;)

count_markers(file = &amp;#39;https://raw.githubusercontent.com/pablobernabeu/EEG-tools-and-tips/main/example.vmrk&amp;#39;, 
              marker = &amp;#39;S  2&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Count of &amp;quot;S  2&amp;quot;: 3 instances&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;count_markers(file = &amp;#39;https://raw.githubusercontent.com/pablobernabeu/EEG-tools-and-tips/main/example.vmrk&amp;#39;, 
              marker = &amp;#39;S  3&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Count of &amp;quot;S  3&amp;quot;: 2 instances&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Preventing muscle artifacts in electroencephalography sessions</title>
      <link>https://pablobernabeu.github.io/2024/preventing-muscle-artifacts-in-electroencephalography-sessions/</link>
      <pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2024/preventing-muscle-artifacts-in-electroencephalography-sessions/</guid>
      <description>


&lt;p&gt;Electroencephalographic (EEG) signals are often contaminated by muscle artifacts such as blinks, jaw clenching and (of course) yawns, which generate electrical activity that can obscure the brain signals of interest. These artifacts typically manifest as large, abrupt changes in the EEG signal, complicating data interpretation and analysis. To reduce the likelihood of muscle artifacts, participants can be instructed during the preparatory phase of the session to minimize blinking and to keep their facial muscles relaxed. Furthermore, the design of the session can include verification stages that allow experimenters to verify the quality of the live EEG signal before letting the session continue.&lt;/p&gt;
&lt;div id=&#34;participant-briefing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Participant briefing&lt;/h2&gt;
&lt;div style=&#34;position: relative; margin-top: 20px; padding-top: 56.25%; margin-bottom: 40px;&#34;&gt;
&lt;iframe src=&#34;https://www.youtube-nocookie.com/embed/9Mbv6bUZlqY&#34; frameborder=&#34;0&#34; allowfullscreen style=&#34;position:absolute; top:0; left:0; width:100%; height:100%;&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;verification-stages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Verification stages&lt;/h2&gt;
&lt;p&gt;Verification stages allow the experimenters to verify the quality of the live EEG signal before letting the session continue. The following example includes an abbreviated reminder, &lt;span style=&#34;color: #B47DB6; background-color: orange;&#34;&gt; i.s.r &lt;/span&gt;, prompting the experimenter to verify the &lt;strong&gt;i&lt;/strong&gt;mpedance and the &lt;strong&gt;s&lt;/strong&gt;ignal before starting to &lt;strong&gt;r&lt;/strong&gt;ecord the signal. The orange stripes flag up the need for the experimenter to intervene.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;img/verification%20stage.png&#39; alt=&#39;Intervention screen in OpenSesame for experimenter to verify the quality of the electroencephalographic signal&#39; style=&#39;margin-top:2px;&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;impedance&lt;/em&gt; check is performed by verifying the impedance on each electrode, which has a dedicated function in most EEG software. Sometimes, especially if some time has elapsed since the initial impedance work, one or two electrodes have to be revised, which can be done quickly.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;signal&lt;/em&gt; check complements the previous check. Once all electrodes present a low-enough impedance and a calm-enough signal—usually taking a couple of minutes—, the experimenter begins recording the signal.&lt;/p&gt;
&lt;p&gt;Next, the experimenter presses the keys that are—unbeknownst to participants—needed to proceed with the session. The screenshot below shows an example setup in OpenSesame, whereby the letter &lt;kbd&gt;C&lt;/kbd&gt; must be pressed twice to advance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;img/OS%20verification%20stage.png&#39; alt=&#39;Intervention screen in OpenSesame for experimenter to verify the quality of the electroencephalographic signal&#39; style=&#39;margin-top:2px;&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;The advice is reinforced before returning the control of the screen to the participant (i.e., this screen is the last one that requires the experimenter’s intervention).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;img/verification%20stage%202.png&#39; alt=&#39;Intervention screen in OpenSesame for experimenter to verify the quality of the electroencephalographic signal&#39; style=&#39;margin-top:2px;&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;Later, during the breaks, the advice is reiterated, and a buffer period is introduced to best preserve the trials immediately following the breaks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;img/break.png&#39; alt=&#39;A buffer period is introduced to best preserve the trials immediately following the breaks&#39; style=&#39;margin-top:2px;&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outcome&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outcome&lt;/h2&gt;
&lt;p&gt;The combination of verbal and written instructions, along with verification stages, can greatly contribute to the quality of raw EEG data, which in turn helps preserve more data in the &lt;a href=&#34;https://pablobernabeu.github.io/2020/event-related-potentials-why-and-how-i-used-them/#preprocessing-erps&#34;&gt;preprocessing&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Job: Part-time research assistant in experimental research</title>
      <link>https://pablobernabeu.github.io/2024/job-part-time-research-assistant-in-experimental-research/</link>
      <pubDate>Wed, 27 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2024/job-part-time-research-assistant-in-experimental-research/</guid>
      <description>


&lt;p&gt;We are seeking to appoint a part-time research assistant to help us recruit participants and conduct an experiment. In the current project, led by Jorge González Alonso and funded by the Research Council of Norway, we investigate language learning and the neurophysiological basis of multilingualism. To this end, we are conducting an &lt;strong&gt;electroencephalography (EEG) experiment&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Your work as a research assistant will be mentored and supervised primarily by Pablo Bernabeu, and secondarily by the head of our project and the directors of our lab. The role involves &lt;strong&gt;20 hours a week&lt;/strong&gt; of on-site work in the Department of Language and Culture at UiT The Arctic University of Norway. The appointment should begin in early April 2024, or soon thereafter, and will span until mid-August (an extension thereafter might be possible).&lt;/p&gt;
&lt;p&gt;By the end of your appointment, you will be able to conduct EEG sessions on your own, and will have a better understanding of EEG, experimental research and the psychology of language. This experience will be most useful for subsequent work in academic research; for instance, in a PhD. Yet, the post also involves more general skills, such as planning, documentation and data management.&lt;/p&gt;
&lt;div id=&#34;person-specification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Person specification&lt;/h2&gt;
&lt;p&gt;Please consider the essential (required) criteria and the desirable criteria listed below.&lt;/p&gt;
&lt;div id=&#34;essential-criteria&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Essential criteria&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;not being currently enrolled in an on-site degree programme at UiT or any other university;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;availability for the on-site work at UiT throughout the appointment;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;attention to detail;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;computer literacy;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;written and spoken fluency in English&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;desirable-criteria&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Desirable criteria&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;previous experience in running EEG sessions, or related qualifications;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;previous experience in any experimental research, or related qualifications;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;schedule flexibility to accommodate the lab sessions;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;interest in experimental linguistics, psychology or neuroscience&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-apply&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to apply&lt;/h2&gt;
&lt;p&gt;Please submit your CV and a personal statement of up to 400 words to Pablo Bernabeu (&lt;a href=&#34;mailto:p.bernabeu@uit.no&#34; class=&#34;email&#34;&gt;p.bernabeu@uit.no&lt;/a&gt;) and Jorge González Alonso (&lt;a href=&#34;mailto:jorge.gonzalez.alonso@uit.no&#34; class=&#34;email&#34;&gt;jorge.gonzalez.alonso@uit.no&lt;/a&gt;), with the subject ‘RA post’. Any questions are also welcome. In the personal statement, please describe how your background fits with the above criteria. We will review applications on a rolling basis.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>rscopus_plus: An extension of the rscopus package</title>
      <link>https://pablobernabeu.github.io/2024/rscopus-plus-an-extension-of-the-rscopus-package/</link>
      <pubDate>Wed, 21 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2024/rscopus-plus-an-extension-of-the-rscopus-package/</guid>
      <description>


&lt;p&gt;Sometimes it’s useful to do a bibliometric analysis. To this end, the &lt;a href=&#34;https://github.com/pablobernabeu/rscopus_plus&#34;&gt;&lt;code&gt;rscopus_plus&lt;/code&gt;&lt;/a&gt; functions (Bernabeu, 2024) extend the R package &lt;a href=&#34;https://github.com/muschellij2/rscopus&#34;&gt;&lt;code&gt;rscopus&lt;/code&gt;&lt;/a&gt; (Muschelli, 2022) to administer the search quota and enable specific searches and comparisons.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/rscopus_plus/blob/main/scopus_search_plus.R&#34;&gt;&lt;code&gt;scopus_search_plus&lt;/code&gt;&lt;/a&gt; runs &lt;code&gt;rscopus::scopus_search&lt;/code&gt; as many times as necessary based on the number of results and the search quota.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/rscopus_plus/blob/main/scopus_search_DOIs.R&#34;&gt;&lt;code&gt;scopus_search_DOIs&lt;/code&gt;&lt;/a&gt; gets DOIs from &lt;code&gt;scopus_search_plus&lt;/code&gt;, which can then be imported into a reference manager, such as Zotero, to create a list of references.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/rscopus_plus/blob/main/scopus_search_additional_DOIs.R&#34;&gt;&lt;code&gt;scopus_search_additional_DOIs&lt;/code&gt;&lt;/a&gt; searches for additional DOIs.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/rscopus_plus/blob/main/scopus_comparison.R&#34;&gt;&lt;code&gt;scopus_comparison&lt;/code&gt;&lt;/a&gt; compares counts of publications on various topics throughout a certain period. The comparison terms are shown in the legend and in the lines, and they all include the reference query.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/rscopus_plus/blob/main/plot_scopus_comparison.R&#34;&gt;&lt;code&gt;plot_scopus_comparison&lt;/code&gt;&lt;/a&gt; draws a line plot with the output from &lt;code&gt;scopus_comparison&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34; style=&#34;padding-bottom: 0; margin-top: 6%; margin-bottom: 6%;&#34;&gt;
&lt;p&gt;&lt;em&gt;Note.&lt;/em&gt; Before using any of the first four functions, the user must set their Scopus API key confidentially (see &lt;a href=&#34;https://cran.r-project.org/web/packages/rscopus/vignettes/api_key.html&#34;&gt;rscopus guidelines&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-of-use&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example of use&lt;/h2&gt;
&lt;p&gt;As an example (also available &lt;a href=&#34;https://github.com/pablobernabeu/L2_L3_EF/blob/main/biblio_analysis.R&#34;&gt;on GitHub&lt;/a&gt;), we’ll visualise the prevalence of three executive functions in the literatures on second and third language throughout the past two decades.&lt;/p&gt;
&lt;p&gt;First, we’ll use &lt;a href=&#34;https://github.com/pablobernabeu/rscopus_plus/blob/main/scopus_comparison.R&#34;&gt;&lt;code&gt;scopus_comparison()&lt;/code&gt;&lt;/a&gt; (fragment shown below).&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Frscopus_plus%2Fblob%2Fmain%2Fscopus_comparison.R%23L2-L23&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;Next, we’ll use &lt;a href=&#34;https://github.com/pablobernabeu/rscopus_plus/blob/main/plot_scopus_comparison.R&#34;&gt;&lt;code&gt;plot_scopus_comparison()&lt;/code&gt;&lt;/a&gt; (fragment shown below).&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Frscopus_plus%2Fblob%2Fmain%2Fplot_scopus_comparison.R%23L4-L14&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rscopus)
# set_api_key(&amp;#39;your_key_here&amp;#39;)  # (see https://cran.r-project.org/web/packages/rscopus/vignettes/api_key.html)

# I&amp;#39;ll read in my personal Scopus key from a file. 
# If you do this, make sure not to share your file.

api_key = readLines(&amp;#39;scopus_key.txt&amp;#39;)
set_api_key(api_key)

library(dplyr)
library(patchwork)


# Load in Scopus search functions from https://github.com/pablobernabeu/rscopus_plus
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/rscopus_plus/main/scopus_comparison.R&amp;#39;)
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/rscopus_plus/main/plot_scopus_comparison.R&amp;#39;)


# General parameters
search_period = 1990:2023
quota = 20


# Prevalence of three executive functions in second language studies from 2000 to 2023. 

# In addition to &amp;quot;second language&amp;quot;, the reference query includes the terms 
# &amp;quot;learning&amp;quot; and &amp;quot;cognition&amp;quot; to make the scope of the search more relevant to 
# the topic of interest. 

reference_query = &amp;#39;&amp;quot;second language&amp;quot;&amp;#39;

comparison_terms = c( &amp;#39;&amp;quot;working memory&amp;quot;&amp;#39;, &amp;#39;inhibit*&amp;#39;, &amp;#39;&amp;quot;implicit learning&amp;quot;&amp;#39; )

N_comparison_terms = length(comparison_terms)

L2_EF = 
  scopus_comparison(reference_query, comparison_terms, 
                    search_period, quota, verbose = FALSE, 
                    reference_query_field_tag = &amp;#39;TITLE-ABS-KEY&amp;#39;)

saveRDS(L2_EF, &amp;#39;L2_EF.rds&amp;#39;)

L2_EF = readRDS(&amp;#39;L2_EF.rds&amp;#39;)  # it&amp;#39;s possible to load results directly

plot_L2_EF = 
  plot_scopus_comparison(L2_EF, 
                         pub_count_in_legend = FALSE, 
                         pub_count_in_lines = TRUE) +
  scale_color_manual(
    values = c( &amp;quot;[ref.] + &amp;#39;\&amp;quot;working memory\&amp;quot;&amp;#39;&amp;quot; = scales::hue_pal()(N_comparison_terms)[1],
                &amp;quot;[ref.] + &amp;#39;inhibit*&amp;#39;&amp;quot; = scales::hue_pal()(N_comparison_terms)[2], 
                &amp;quot;[ref.] + &amp;#39;\&amp;quot;implicit learning\&amp;quot;&amp;#39;&amp;quot; = scales::hue_pal()(N_comparison_terms)[3] )
  ) + 
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  # Prepare layout for the multi-plot combination
  theme(axis.text.x = element_blank(), axis.title.x = element_blank(), 
        legend.position = &amp;#39;none&amp;#39;, plot.margin = margin(0, 0, 15, 0))


# Prevalence of three executive functions in third language studies from 2000 to 2023. 

# In addition to &amp;quot;third language&amp;quot;, the reference query includes the terms &amp;quot;learning&amp;quot; 
# and &amp;quot;cognition&amp;quot; to make the scope of the search more relevant to the topic of 
# interest. 

reference_query = &amp;#39;&amp;quot;third language&amp;quot;&amp;#39;

# Other parameters identical to those used in the query above.

L3_EF = 
  scopus_comparison(reference_query, comparison_terms, 
                    search_period, quota, verbose = FALSE, 
                    reference_query_field_tag = &amp;#39;TITLE-ABS-KEY&amp;#39;)

saveRDS(L3_EF, &amp;#39;L3_EF.rds&amp;#39;)

L3_EF = readRDS(&amp;#39;L3_EF.rds&amp;#39;)  # it&amp;#39;s possible to load results directly

plot_L3_EF = 
  plot_scopus_comparison(L3_EF, 
                         pub_count_in_legend = FALSE, 
                         pub_count_in_lines = TRUE) +
  scale_color_manual(
    values = c( &amp;quot;[ref.] + &amp;#39;\&amp;quot;working memory\&amp;quot;&amp;#39;&amp;quot; = scales::hue_pal()(N_comparison_terms)[1],
                &amp;quot;[ref.] + &amp;#39;inhibit*&amp;#39;&amp;quot; = scales::hue_pal()(N_comparison_terms)[2], 
                &amp;quot;[ref.] + &amp;#39;\&amp;quot;implicit learning\&amp;quot;&amp;#39;&amp;quot; = scales::hue_pal()(N_comparison_terms)[3] )
  ) + 
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  # Prepare layout for the multi-plot combination
  theme(axis.text.x = element_text(margin = margin(7, 0, 0, 0, &amp;#39;pt&amp;#39;)),
        axis.title.x = element_text(margin = margin(8, 0, 0, 0, &amp;#39;pt&amp;#39;)),
        legend.position = &amp;#39;inside&amp;#39;, legend.position.inside = c(.82, .8))


# Combine plots

plot_L2_EF + plot_L3_EF + 
  plot_layout(ncol = 1, axes = &amp;#39;collect&amp;#39;) &amp;amp;
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(vjust = 0.5, size = 13), 
        plot.title = element_markdown(hjust = 0.5, size = 12),
        legend.text = element_text(size = 11,),
        legend.background = element_rect(color = &amp;#39;grey80&amp;#39;, fill = &amp;#39;grey99&amp;#39;), 
        legend.margin = margin(0, 5, 2, 0)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2024/rscopus-plus-an-extension-of-the-rscopus-package/index.en_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;720&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The total number of publications over the current period is shown between brackets after each query.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Bernabeu, P. (2024). &lt;em&gt;rscopus_plus&lt;/em&gt;. OSF. &lt;a href=&#34;https://doi.org/10.17605/OSF.IO/BUZQ6&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17605/OSF.IO/BUZQ6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Muschelli, J. (2022). &lt;em&gt;Package ’rscopus’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/rscopus/rscopus.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/rscopus/rscopus.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to end trial after timeout in jsPsych</title>
      <link>https://pablobernabeu.github.io/2024/how-to-end-trial-after-timeout-in-jspsych/</link>
      <pubDate>Mon, 29 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2024/how-to-end-trial-after-timeout-in-jspsych/</guid>
      <description>


</description>
    </item>
    
    <item>
      <title>A session logbook for a longitudinal study using conditional formatting in Excel</title>
      <link>https://pablobernabeu.github.io/2023/a-session-logbook-for-a-longitudinal-study-using-conditional-formatting-in-excel/</link>
      <pubDate>Sat, 02 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/a-session-logbook-for-a-longitudinal-study-using-conditional-formatting-in-excel/</guid>
      <description>


&lt;p&gt;Longitudinal studies consist of several sessions, and often involve session session conductors. To facilitate the planning, registration and tracking of sessions, a session logbook becomes even more necessary than usual. To this end, an Excel workbook with conditional formatting can help automatise some formats and visualise the progress.&lt;/p&gt;
&lt;p&gt;Below is an example that is &lt;a href=&#34;https://1drv.ms/x/s!AouK9kQQrXKooGKVLbdBzz-DStmj?e=mbNZE4&#34;&gt;available on OneDrive&lt;/a&gt;. To fully access this workbook, it may be downloaded via &lt;kbd&gt;File&lt;/kbd&gt; &amp;gt; &lt;kbd&gt;Save as&lt;/kbd&gt; &amp;gt; &lt;kbd&gt;Download a copy&lt;/kbd&gt;. Alternatively, the workbook can be exported to one’s own account on OneDrive.&lt;/p&gt;
&lt;p&gt;The conditional formats include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;fill in blanks in columns A, C or D to remove the background&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;fill in blanks in F and G, or I and J, or L and M, etc, to highlight the entire session in green&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;fill in any notes columns to highlight them in red&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe style=&#34;margin-top: 1%; margin-bottom: 3%;&#34; width=&#34;700&#34; height=&#34;365&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; src=&#34;https://1drv.ms/x/c/a872ad1044f68a8b/UQSLivZEEK1yIICoYhAAAAAAABsHnSAl4C4iZLc?em=2&amp;amp;AllowTyping=True&amp;amp;ActiveCell=&amp;#39;in&amp;#39;!A1&amp;amp;Item=&amp;#39;in&amp;#39;!A1%3AW7&amp;amp;wdHideGridlines=True&amp;amp;wdDownloadButton=True&amp;amp;wdInConfigurator=True&amp;amp;wdInConfigurator=True&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;These conditional formats can be consulted (and edited) from the &lt;kbd&gt;Home&lt;/kbd&gt; tab by opening &lt;kbd&gt;Conditional Formatting&lt;/kbd&gt; &amp;gt; &lt;kbd&gt;Manage Rules&lt;/kbd&gt;. At the top, the option &lt;kbd&gt;This Worksheet&lt;/kbd&gt; should be selected to view all formulas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Motivating a preregistration (especially in experimental linguistics)</title>
      <link>https://pablobernabeu.github.io/2023/motivating-a-preregistration-especially-in-experimental-linguistics/</link>
      <pubDate>Fri, 24 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/motivating-a-preregistration-especially-in-experimental-linguistics/</guid>
      <description>


&lt;div id=&#34;the-disorder&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The disorder&lt;/h2&gt;
&lt;p&gt;Studies in the cognitive sciences often feature multiple experimental conditions and other independent variables. Once the study progresses, the analytical liberties associated with all the conditions and variables are compounded by the myriad possible steps in data processing and analysis. All these combinations lead to a &lt;em&gt;garden of forking paths&lt;/em&gt;, and the &lt;strong&gt;researcher degrees of freedom&lt;/strong&gt; soar to unexpected highs.&lt;/p&gt;
&lt;p&gt;The system of professional incentives in academia largely ignores the issue of researcher degrees of freedom. When CVs are assessed, quantity weighs more than quality of research. When studies are assessed, statistical significance weighs more than methodological rigour. Thus, the low replication rates in various fields, including linguistics (Kobrock &amp;amp; Roettger, 2023), are hardly surprising (cf. Barsalou, 2019).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-treatment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The treatment&lt;/h2&gt;
&lt;p&gt;The number of researcher degrees of freedom, and their unforeseen influence, can be reduced a priori by publishing a preregistration before data collection has begun (or before the analysis in the case of meta-analyses and secondary-data analyes). The preregistration takes a bit of time, which poses a challenge because funding systems often require &lt;em&gt;doing things&lt;/em&gt; as soon and as fast as possible, driven by a questionable notion of scientific productivity.&lt;/p&gt;
&lt;div id=&#34;how-to-gather-the-strength&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to gather the strength&lt;/h3&gt;
&lt;p&gt;The best argument to motivate a preregistration may be that does not incur any extra time. It only requires frontloading an important portion of the work. This effort will be rewarded when the study is published, as preregistered analyses are received with greater trust by peer-reviewers and other readers.&lt;/p&gt;
&lt;p&gt;If any contributors of a project can gather just enough time and interest to initiate a preregistration in time, the researchers can attempt to pocket this important asset for their study. They will reap the reward in time, and so will their field of research at large.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-to-do&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What to do&lt;/h3&gt;
&lt;p&gt;Initially, rather than detailed analytical steps, it is the broader theoretical aspects that should be laid out. An abstract and an introduction should be written, describing why the study has been designed in such a way, what analyses will be performed, and what hypotheses are afforded by the literature. Next, some methodological details regarding the materials and the analyses should be added.&lt;/p&gt;
&lt;p&gt;The degree of detail in the preregistration can be determined by the researchers alone. Yet, ceteris paribus, the greater the detail in a preregistration, the greater the trustworthiness of the analyses and the results. Multiple preregistration guidelines exist by now, including some field-specific ones.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solace&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Solace&lt;/h3&gt;
&lt;p&gt;Preregistration is not perfect, but is a lesser evil that reduces the misuse of statistical analysis in science (Mertzen et al., 2021; Roettger, 2023).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Even the strongest blizzards start with a single snowflake. (Sara Raasch)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Rules are made to be broken—at least as and when necessary. That is, deviations from the preregistration are possible, and indeed very frequent (Bakker et al., 2020; van den Akker et al., 2023).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Bakker, M., Veldkamp, C. L. S., Assen, M. A. L. M. van, Crompvoets, E. A. V., Ong, H. H., Nosek, B. A., Soderberg, C. K., Mellor, D., &amp;amp; Wicherts, J. M. (2020). Ensuring the quality and specificity of preregistrations. &lt;em&gt;PLOS Biology, 18&lt;/em&gt;(12), e3000937. &lt;a href=&#34;https://doi.org/10.1371/journal.pbio.3000937&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pbio.3000937&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barsalou, L. W. (2019). Establishing generalizable mechanisms. &lt;em&gt;Psychological Inquiry, 30&lt;/em&gt;(4), 220–230. &lt;a href=&#34;https://doi.org/10.1080/1047840X.2019.1693857&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1080/1047840X.2019.1693857&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kobrock, K., &amp;amp; Roettger, T. B. (2023). Assessing the replication landscape in experimental linguistics. &lt;em&gt;Glossa Psycholinguistics, 2&lt;/em&gt;(1). &lt;a href=&#34;https://doi.org/10.5070/G6011135&#34; class=&#34;uri&#34;&gt;https://doi.org/10.5070/G6011135&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Mertzen, D., Lago, S., &amp;amp; Vasishth, S. (2021). The benefits of preregistration for hypothesis-driven bilingualism research. &lt;em&gt;Bilingualism: Language and Cognition, 24&lt;/em&gt;(5), 807–812. &lt;a href=&#34;https://doi.org/10.1017/S1366728921000031&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1017/S1366728921000031&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Roettger, T. B. (2021). Preregistration in experimental linguistics: Applications, challenges, and limitations. &lt;em&gt;Linguistics, 59&lt;/em&gt;(5), 1227–1249. &lt;a href=&#34;https://doi.org/10.1515/ling-2019-0048&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1515/ling-2019-0048&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;van den Akker, O. R., van Assen, M. A. L. M., Enting, M., de Jonge, M., Ong, H. H., Rüffer, F., Schoenmakers, M., Stoevenbelt, A. H., Wicherts, J. M., &amp;amp; Bakker, M. (2023). Selective hypothesis reporting in psychology: Comparing preregistrations and corresponding publications. &lt;em&gt;Advances in Methods and Practices in Psychological Science, 6&lt;/em&gt;(3), 25152459231187988. &lt;a href=&#34;https://doi.org/10.1177/25152459231187988&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/25152459231187988&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Do you speak a Scandinavian language(s) and English, but no other languages? Delta i et EEG-eksperiment</title>
      <link>https://pablobernabeu.github.io/2023/do-you-speak-a-scandinavian-language-s-and-english-but-no-other-languages-delta-i-et-eeg-eksperiment/</link>
      <pubDate>Tue, 14 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/do-you-speak-a-scandinavian-language-s-and-english-but-no-other-languages-delta-i-et-eeg-eksperiment/</guid>
      <description>


&lt;div id=&#34;ved-å-delta-i-vårt-eksperiment-og-gjøre-noen-enkle-oppgaver-på-en-datamaskin-kan-du-bidra-til-forskning-og-tjene-250-kr-i-timen-gavekort.-eeg-er-helt-smertefritt.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Ved å delta i vårt eksperiment og gjøre noen enkle oppgaver på en datamaskin, kan du bidra til forskning og tjene 250 kr i timen (gavekort). EEG er helt smertefritt.&lt;/h4&gt;
&lt;div id=&#34;eksperimentet-foregår-i-tromsø-ved-uit-norges-arktiske-universitet.&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Eksperimentet foregår i Tromsø, ved UiT Norges Arktiske Universitet.&lt;/h5&gt;
&lt;p&gt;Vi ser etter deltakere med følgende egenskaper:&lt;/p&gt;
&lt;div style=&#34;text-indent:-1.2em; margin-left:2em;&#34;&gt;
&lt;p&gt;☑ Alder 18–45 år;&lt;/p&gt;
&lt;p&gt;☑ Snakker norsk som førstespråk og engelsk flytende. Utenom disse språkene, kan deltakerne også snakke svensk og dansk, men ikke andre språk (utover noen få ord);&lt;/p&gt;
&lt;p&gt;☑ Ingen språkrelaterte lidelser eller vansker, som f.eks. dysleksi;&lt;/p&gt;
&lt;p&gt;☑ Ingen oppmerksomhetssvikt&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I de fleste tilfeller vil deltakelse innebære å gjennomføre fem økter over en måned, med bonusutbetaling på slutten.&lt;/p&gt;
&lt;p&gt;Dersom du oppfyller disse kriteriene og ønsker å delta, vennligst registrer deg nedenfor for å motta mer informasjon, eller kontakt Pablo på epost-adressen &lt;a href=&#34;mailto:p.bernabeu@uit.no&#34; class=&#34;email&#34;&gt;p.bernabeu@uit.no&lt;/a&gt;.&lt;/p&gt;
&lt;div style=&#34;margin-top: 4%;&#34;&gt;

&lt;/div&gt;
&lt;a href=&#39;https://rb.gy/9zbhfv&#39;&gt;
&lt;button style = &#34;background-color: white; color: black; border: 2px solid #4CAF50; border-radius: 12px;&#34;&gt;
&lt;h3 style=&#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt;
&lt;span style=&#34;color:#86ea8d;&#34;&gt;&lt;i class=&#34;fa fa-envelope&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;&lt;/span&gt;
Meld deg på
&lt;/h3&gt;
&lt;/button&gt;
&lt;p&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div style=&#34;margin-top: 5%;&#34;&gt;

&lt;/div&gt;
&lt;p&gt;Tusen takk for din interesse i denne studien.&lt;/p&gt;
&lt;div style=&#34;margin-top: 5%;&#34;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgrey;&#34;&gt;Takk til Mona Kirknes Fossum og Marit Westergaard for hjelpen med oversettelsen av denne annonsen.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Learning how to use Zotero</title>
      <link>https://pablobernabeu.github.io/2023/learning-how-to-use-zotero/</link>
      <pubDate>Fri, 10 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/learning-how-to-use-zotero/</guid>
      <description>


&lt;p&gt;Is it worth learning how to use a reference management system such as Zotero? Maybe.&lt;/p&gt;
&lt;p&gt;The hours you invest in learning how to use Zotero (approx. 10 hours) are likely to pay off, as they will save you a lot of time that you would otherwise spend formatting, revising and correcting references. In addition, this skill would become part of your skill set.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://library-guides.ucl.ac.uk/zotero/installing-zotero&#34;&gt;A great guide&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://library-guides.ucl.ac.uk/zotero/further-help&#34;&gt;Free, online webinars in which you could participate and ask questions&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://library-guides.ucl.ac.uk/zotero/using-zotero-with-word&#34;&gt;Zotero and Word&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://chrome.google.com/webstore/detail/zotero-connector/ekhagklcjbdpajgpjgmbionohlpdbjgc&#34;&gt;Zotero extension for the Chrome browser, to quickly add papers to your libraries&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>FAQs on mixed-effects models</title>
      <link>https://pablobernabeu.github.io/2023/faqs-on-mixed-effects-models/</link>
      <pubDate>Sat, 04 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/faqs-on-mixed-effects-models/</guid>
      <description>


&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;I am dealing with nested data, and I remember from an article by &lt;a href=&#34;https://doi.org/10.1016/S0022-5371(73)80014-3&#34;&gt;Clark (1973)&lt;/a&gt; that nested should be analysed using special models. I’ve looked into mixed-effects models, and I’ve reached a structure with random intercepts by subjects and by items. Is this fine?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 55px;&#34;&gt;
&lt;p&gt;In early days, researchers would aggregate the data across these repeated measures to prevent the violation of the assumption of independence of observations, which is one of the most important assumptions in statistics. With the advent of mixed-effects models, researchers began accounting for these repeated measures using random intercepts and slopes. However, problems of convergence led many researchers to remove random slopes. This became widespread until, over the past few years, we have realised that random slopes are necessary to prevent an inflation of the Type I error due to the violation of the assumption of independence (&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;Brauer &amp;amp; Curtin, 2018&lt;/a&gt;; &lt;a href=&#34;http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf&#34;&gt;Singmann &amp;amp; Kellen, 2019&lt;/a&gt;). Please see Table 17 in Brauer and Curtin (2018). Due to the present reasons, the models in the current article are anti-conservative. To redress this problem, please consider the inclusion of random slopes by participant for all between-items variables [e.g., &lt;code&gt;(stimulus_condition | participant)&lt;/code&gt;], and random slopes by item for all between-participants variables [e.g., &lt;code&gt;(extraversion | item)&lt;/code&gt;]. Interaction terms should also have the corresponding slopes, except when the variables in the interaction vary within different units, that is, one between participants and one between items (&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;Brauer &amp;amp; Curtin, 2018&lt;/a&gt;). Each of the random intercepts and random slopes included in the model should be noted in the main text, for instance using footnotes in the results table (see &lt;a href=&#34;https://bookdown.org/pablobernabeu/language-sensorimotor-conceptual-processing-statistical-power/study-2.1-semantic-priming.html#semanticpriming-results&#34;&gt;example&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;I calculated the &lt;em&gt;p&lt;/em&gt; values by comparing minimally-different models using the &lt;code&gt;anova&lt;/code&gt; function. Is this fine?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 55px;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.3758/s13428-016-0809-y&#34;&gt;Luke (2017)&lt;/a&gt; warns that the &lt;em&gt;p&lt;/em&gt; values calculated by model comparison—which are based on likelihood ratio tests—can be anti-conservative. Therefore, the Kenward-Roger and the Satterthwaite methods are recommended instead (both available in other packages, such as &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;lmerTest&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/afex/afex.pdf&#34;&gt;afex&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;The lme4 package only runs on one thread (CPU) but the computer has 8. Do you have any advice on making the model run using more of the threads? It’s taking a very long time. I’ve seen these two possible solutions online from 2018 (&lt;a href=&#34;https://stackoverflow.com/questions/48315268/how-can-i-make-r-using-more-than-1-core-8-available-on-a-ubuntu-rstudio-server&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://stat.ethz.ch/pipermail/r-sig-mixed-models/2018q3/027170.html&#34;&gt;here&lt;/a&gt;) but would like some advice if they have any or have attempted either of these solutions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 55px;&#34;&gt;
&lt;p&gt;From the information I have seen in the past as well as right now, parallelising (g)lmer intentionally would be very involved. There is certainly interest in it, as your resources show (also see &lt;a href=&#34;https://github.com/lme4/lme4/issues?q=is%3Aissue+parallel&#34;&gt;here&lt;/a&gt;). However, the current information suggests to me that it is not possible.&lt;/p&gt;
&lt;p&gt;Interestingly, some isolated cases of unintentional parallelisation have been documented, and the developers of the &lt;a href=&#34;https://cran.r-project.org/web/packages/lme4/lme4.pdf&#34;&gt;lme4&lt;/a&gt; package were &lt;a href=&#34;&#34;&gt;surprised about them&lt;/a&gt; because they have not created this feature (see &lt;a href=&#34;https://github.com/lme4/lme4/issues/492&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://github.com/lme4/lme4/issues/627&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I think the best approach may be running your model(s) in a high-performance computing (HPC) cluster. Although this would not reduce the amount of time required for each model, it would have two advantages. First, your own computers wouldn’t be busy for days, and second, you could even run several models at the same time without exhausting your own computers. I still have access to the HPC at my previous university, and it would be fine for me to send your model(s) there if that would help you. Feel free to let me know. Otherwise I can see that your university has this facility too.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;We took your advice and ran the model on a supercomputer - it took roughly 2.5 days, which is what it took for the model to run on my iMac and a gaming laptop Vivienne has.&lt;/p&gt;
&lt;p&gt;The model, however, didn’t converge. We have read that you can use &lt;code&gt;allFit()&lt;/code&gt; to try the fit with all available optimizers. Do you have any experience using this? If you did, I wondered where this would sit in the code for the model? How and where do I add this in to check all available optimizers, please?&lt;/p&gt;
&lt;p&gt;I have attached my code in a txt file and the data in excel for you to see, in case it is of any use.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 55px;&#34;&gt;
&lt;p&gt;The multi-optimizer check is indeed a way (albeit tentative) to probe into the convergence. Convergence has long been a fuzzy subject, as there are different standpoints depending on the degree of conservativeness that is sought after by the analysts.&lt;/p&gt;
&lt;p&gt;On Page 124 in my thesis (&lt;a href=&#34;https://osf.io/97u5c&#34; class=&#34;uri&#34;&gt;https://osf.io/97u5c&lt;/a&gt;), you can find this multi-optimizer check (also see this &lt;a href=&#34;https://pablobernabeu.github.io/2021/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit&#34;&gt;blog post&lt;/a&gt;). All the code is available on OSF. More generally, I discuss the issue of convergence throughout the thesis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;I have run the model with &lt;code&gt;optimizer=&#34;nloptwrap&#34;&lt;/code&gt; and &lt;code&gt;algorithm=&#34;NLOPT_LN_BOBYQA&#34;&lt;/code&gt; and received the following warning message (once the model ran) -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;In optwrap(optimizer, devfun, start, rho$lower, control = control, :
convergence code 5 from nloptwrap: NLOPT_MAXEVAL_REACHED: optimization stopped becasue maxeval (above) was reached.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does this mean that the model didn’t converge? I’m only asking because I wasn’t given a statement saying it didn’t converge, as it did with Nelder_Mead. It was stated (at the end of summary table)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Optimizer (Nelder_Mead) convergence code: 4 (failure to converge in 10000 evaluations)
failure to converge in 10000 evaluations&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 55px;&#34;&gt;
&lt;p&gt;Please try &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/semanticpriming_lmerTest.R#L109&#34;&gt;increasing the max number of iterations&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;blockquote style=&#34;color: black; background-color: #FFF9F3; margin-bottom: 30px;&#34;&gt;
&lt;p&gt;We increased the max number of iterations to 1e6 and then 1e7, and the model didn’t converge. But it has converged with &lt;code&gt;maxeval=1e8&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I wanted to ask please, do you know of any issues with the max iterations being this high and effecting the interpretability of the model? Or is it completely fine?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;padding-left: 60px;&#34;&gt;
&lt;blockquote style=&#34;color: black; background-color: #F4FFF3; margin-bottom: 55px;&#34;&gt;
&lt;p&gt;There are no side-effects to increasing the number of iterations (see Remedy 6 in &lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;Brauer &amp;amp; Curtin, 2018&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, L. A. (2018). Mixed models in psychology: An introduction to the world of multilevel models. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389-411. &lt;a href=&#34;https://doi.org/10.1037/met0000132&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000132&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Clark, H. H. (1973). The language-as-fixed-effect fallacy: A critique of language statistics in psychological research. &lt;em&gt;Journal of Verbal Learning and Verbal Behavior, 12&lt;/em&gt;(4), 335-359. &lt;a href=&#34;https://doi.org/10.1016/S0022-5371(73)80014-3&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/S0022-5371(73)80014-3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Luke, S. G. (2017). Evaluating significance in linear mixed-effects models in R. &lt;em&gt;Behavior Research Methods, 49&lt;/em&gt;(4), 1494–1502. &lt;a href=&#34;https://doi.org/10.3758/s13428-016-0809-y&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13428-016-0809-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Singmann, H., &amp;amp; Kellen, D. (2019). An introduction to mixed models for experimental psychology. In D. H. Spieler &amp;amp; E. Schumacher (Eds.), &lt;em&gt;New Methods in Cognitive Psychology&lt;/em&gt; (pp. 4–31). Hove, UK: Psychology Press. &lt;a href=&#34;http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf&#34; class=&#34;uri&#34;&gt;http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>FAIR standards for the creation of research materials, with examples</title>
      <link>https://pablobernabeu.github.io/2023/fair-standards-for-the-creation-of-research-materials-with-examples/</link>
      <pubDate>Thu, 02 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/fair-standards-for-the-creation-of-research-materials-with-examples/</guid>
      <description>


&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34; style=&#34;padding-bottom: 0; margin-bottom: 6%;&#34;&gt;
&lt;p&gt;In the fast-paced world of scientific research, establishing minimum standards for the creation of research materials is essential. Whether it’s stimuli, custom software for data collection, or scripts for statistical analysis, the quality and transparency of these materials significantly impact the reproducibility and credibility of research. This blog post explores the importance of adhering to FAIR (Findable, Accessible, Interoperable, Reusable) principles, and offers practical examples for researchers, with a focus on the cognitive sciences.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Notwithstanding the need for speed in most scientific projects, what should be the &lt;strong&gt;minimum acceptable standards&lt;/strong&gt; in the &lt;strong&gt;creation of research materials&lt;/strong&gt; such as stimuli, custom software for data collection (e.g., experiment in jsPsych, OpenSesame or psychoPy), or scripts for statistical analysis?&lt;/p&gt;
&lt;p&gt;The answer to this question is contingent upon the field of research, the purpose and the duration of the project, and many other contextual factors. So, to narrow down the scope and come at a general answer, let’s suppose we asked a researcher in the cognitive sciences (e.g., a linguist, a psychologist or a neuroscientist) who values open science. Perhaps, such a researchers would be satisfied with a method for the creation of materials that &lt;strong&gt;allows the creators of the materials, as well as their collaborators and any other stakeholders (e.g., any fellow scientists working in the same field), to explore, understand, reproduce, modify, and reuse the materials following their completion and thereafter&lt;/strong&gt;. Let’s review some of the implements that can help fulfil these standards.&lt;/p&gt;
&lt;div id=&#34;fairness&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;FAIRness&lt;/h1&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.go-fair.org/fair-principles&#34;&gt;FAIR Guiding Principles for scientific data management and stewardship&lt;/a&gt; exhaustively describe a protocol for making materials &lt;strong&gt;F&lt;/strong&gt;indable, &lt;strong&gt;A&lt;/strong&gt;ccessible, &lt;strong&gt;I&lt;/strong&gt;nteroperable and &lt;strong&gt;R&lt;/strong&gt;eusable. These terms cover the five allowances listed above, along with other important aspects.&lt;/p&gt;
&lt;p&gt;Let’s look at some instantiations of the FAIR principles.&lt;/p&gt;
&lt;div id=&#34;sharing-the-materials&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sharing the materials&lt;/h2&gt;
&lt;p&gt;A sine qua non condition is to share the materials publicly online, as far as possible. Repositories on servers such as OSF or GitHub are often sufficient to this end. Unfortunately, most studies in the cognitive sciences still do not share the complete materials.&lt;/p&gt;
&lt;p&gt;One of the reasons why sharing is so important is to prevent wrong assumptions by the audience that will consider the research. That is, when the materials of a study are not publicly shared online, the readers of the papers are left with two options: to assume that there were no errors or to assume that were some errors of an uncertain degree. The method followed in the creation of the materials should free the readers of this tribulation, by allowing them to consult the materials and their preparation in full, or as completely as possible.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reproducibility&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reproducibility&lt;/h2&gt;
&lt;p&gt;It is convenient to allow others, and our future selves, to reproduce the materials throughout their preparation and at any time thereafter. For this purpose, &lt;strong&gt;R&lt;/strong&gt; can be used to register in scripts as many as possible of the steps followed throughout the preparation of the materials. Far from being only a software for data analysis, R allows the preparation of texts, images, audios, etc. Humans err, by definition. That can be counted on. Conveniently, registering the steps followed during weeks or months of preparation allows us to offload part of the documentation efforts. It’s a way of video-recording, as it were, all the additions, subtractions, replacements, transformations and calculations performed with the raw materials, for the creation of the final materials.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generous-documentation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generous documentation&lt;/h2&gt;
&lt;p&gt;Under the curse of knowledge, the creators of research materials may believe that their materials are self-explanatory. Often they are more obscure than they think. To allow any other stakeholders, including their future selves, to exercise the five allowances listed above—i.e., explore, understand, reproduce, modify, and reuse the materials—, the preparation process and the end materials should be documented with enough detail. This can be done using README.txt files throughout the project. Using the &lt;code&gt;.txt&lt;/code&gt; format/extension is recommended because other formats, such as Microsoft Word, may not be (fully) available in some computers. To exemplify the format and the content of readme files, below is an excerpt from a longitudinal study on which I’ve been working.&lt;/p&gt;
&lt;textarea readonly style=&#39;border-color: lightgrey; overflow: auto; color: darkblue; font-size: 90%; min-width: 125%; height: 4600px; white-space: pre; overflow-wrap: normal; padding-right: 0.5em; padding-left: 1em;&#39;&gt;

-- Post-training test --

In Sessions 2, 3, 4 and 6, if the test is failed in the first attempt, the training and the test are 
repeated (following González Alonso et al., 2020). In such cases, the result is shown at the end 
of the second attempt. The session advances if the accuracy achieved in the second attempt exceeds 
80%, whereas the session stops if the accuracy is lower. In the latter situation, an &#39;End of session&#39; 
message is presented, flanked by two orange circles, and followed by an acknowledgement for the 
participant. Once the participant has read this screen, the experimenter quits the session by 
pressing &#39;ESC&#39; and then &#39;Q&#39;.


== Stimuli ==

The stimulus lists are described in the R functions that were used to create the stimuli, as well as
in the &#39;list&#39; column in the stimulus files.


== Participant-specific parameters for lab-based sessions ==

Each participant was assigned certain parameters in advance, including the mini-language, the order 
of the resting-state parts, and the stimulus lists. The code that was used to create this assignment 
is available in the &#39;stimulus_preparation&#39; folder. 

Due to the pre-assignment of the parameters, there is a fixed set of participant IDs that can be 
used in OpenSesame. These identification numbers range between 1 and 144. If an ID outside of this 
range is used, the OpenSesame session does not run.


== General procedure for lab-based sessions ==

At the beginning of lab-based Sessions 2, 3, 4 and 6, the experimenter will first signal the lab is 
busy using a light or a sign. Next, they will ascertain what participant and what session applies. 
This is done using the session logbook that is shared among all session conductors. This session 
logbook is instantly updated online using a cloud service, such as OneDrive. 

Next, the experimenter starts OpenSesame by opening the program directly (not by opening the 
session-specific file), and then opens the appropriate session within OpenSesame. This procedure 
helps prevent the opening of a standalone Python window, the closing of which would result in the 
closing of OpenSesame. Next, the experimenter opens BrainVision Recorder.

When the participant arrives in the lab, they are informed that they can use the toilet outside. 
The participant is also offered some water. 

Next, the size of their head is measured, and an appropriate cap is tried on the participant’s 
head. Next, the cap is placed on a dummy head, and the electrodes are attached to the cap. At 
that point, to prevent signal interference, the participant is kindly asked to either put their 
mobile devices (phone, tablet, smartwatch) in flight mode, or to leave them outside of the booth. 

Next, to protect the participant&#39;s clothes from any drops of gel, a towel is placed on their 
upper back, covering shoulders and upper torso. Both ends of the towel are clipped together at 
the front using two or three clothes pegs. Next, the cap is fitted on the participant&#39;s head. To 
prevent the cap from being pulled back during the session, the splitter box is attached to the 
towel on the participant&#39;s back, right below their head. Next, measures are taken to adjust the 
position of the cap evenly, first from the nasion to the inion, and then from the tip of an ear 
to the other ear. 

Next, the experimenter returns to OpenSesame and runs the session in full screen by clicking on 
the full green triangle at the top left. Then, a file explorer window opens, in which the 
experimenter must assign a subject number consistent with the session logbook, and must select 
the destination folder for the logfile. The destination folder is called &#39;logfiles&#39;. Any prompts 
to overwrite a logfile must normally be refused, or considered carefully, due to the risk of 
losing data from previous sessions.

In the first screen, the experimenter can disable some of the tasks. This option can be used if a 
session has ended abruptly, in which case the session can be resumed from a near checkpoint. In 
such a case, the experimenter must first note this incident in their logbook, and rename the log 
file that was produced on the first run, by appending &#39;_first_run&#39; to the name. This prevents 
overwriting the file on the second run. Next, they must open a new session, enter the same 
participant ID, and select the appropriate part from which to begin. This part must be the part 
immediately following the last part that was completed in full. For instance, if a session ended
abruptly during the experiment, the beginning selected on the second run would be the experiment. 
Once the session has finished completely, the first log file and the second log file must be 
safely merged into a single file, keeping only the fully completed tasks.

In the first instructional screen, participants are asked to refrain from asking any questions 
unless it is necessary, so that all participants can receive the same instructions.

At the beginning of the resting-state part in Session 2, and at the beginning of the Experiment 
part, instructions are presented on the screen that ask participants to stay as still as possible 
during the following task. The screen contains an orange-coloured square with the letters &#39;i.s.r&#39;, 
that remind the experimenter to check the impedance and the signal, and finally to begin recording 
the EEG signal. If the impedance of any electrodes is poor, the experimenter may enter the booth 
to lower the impedance of the electrodes affected. Otherwise, after validating the signal and the 
impedance, the experimenter can begin the recording in BrainVision, and press the letter &#39;C&#39; twice 
in the stimulus computer. At that point, a green circle will appear, along with instructions for 
the participant. 

Similarly, at the end of the eyes-closed resting-state measurement (which is five minutes long), 
the experimenter must intervene when they see the screen with the orange stripes, by knocking on 
the door to let the participant open their eyes. 

Furthermore, at the end of the resting-state part and at the end of the Experiment part, a screen
with a crossed-out R appears to remind the experimenter to stop recording the EEG. 

Notice that the above-mentioned stages, characterised by screens with orange stripes, require the 
experimenter&#39;s intervention. The experimenter must allow the participant to read any text on these 
screens. Next, the experimenter must press the letter &#39;C&#39; twice to let the session continue. This 
protocol provides the experimenter with control when necessary. The experimenter should be aware 
of the use of the letter &#39;C&#39; at these points, as the requirement is not signalled on the screen 
to prevent participants from pressing the letter themselves. 

During the experiment, it is important to monitor the EEG signal. If it ever becomes very noisy, 
the experimenter must wait until the next break and the participant to stop, so that the signal 
can be verified. If the noise in the signal is due to the participant&#39;s movement, they should be 
asked again to please stay as still possible. If the noise is due to an increase in the 
impedance of some electrodes, the impedance of those electrodes should be revised.

The experiment in Session 2 contains breaks every 40 trials, whereas the experiments in subsequent 
sessions contain breaks every 50 trials. During these breaks, the number of the current trial 
appears in grey on the bottom right corner of the screen.

If a session ends abruptly during the experiment, but there is not enough time to restart the 
session from the experiment, then the data should be uploaded to the repository. 


== Definition of items in OpenSesame (only for programming purposes, not for in-session use) ==

  -- Each major part of the session is contained in a sequence item that is named in capital 
     letters (e.g., &#39;PRETRAINING&#39;, &#39;TRAINING&#39;, &#39;TEST&#39;, &#39;EXPERIMENT&#39;).

  -- &#39;continue_space&#39;: allows proceeding to the following screen after pressing the space bar, 
     which should be done by the participant. In most cases, two presses are required, as 
     detailed on the screen.

  -- &#39;continue_c&#39;: allows proceeding to the following screen after pressing the letter &#39;C&#39;, 
     which should be done by the experimenter. In most cases, two presses are required, as 
     detailed on the screen.


== Variables in the OpenSesame log files ==

In the log files produced by OpenSesame, each part of the session (e.g., Test, Experiment) is 
identified in the variable &#39;session_part&#39;. The names of the response variables are &#39;response&#39;,
&#39;response_time&#39; and &#39;correct&#39;. Item-specific response variables follow the formats of 
&#39;response_[item_name]&#39;, &#39;response_time_[item_name]&#39; and &#39;correct_[item_name]&#39; 
(see https://osdoc.cogsci.nl/3.3/manual/variables/#response-variables).

The output is verbose and requires preprocessing of the data. For instance, the last response 
in each loop may appear twice in the output, due to the processing of the response. These 
duplicates can--and must--be cleaned up by discarding the rows that have the same trial number
as the preceding row.


== EEG triggers ==

Triggers are sent to the EEG recorder throughout the experiment. The system for sending 
triggers is set up in OpenSesame script within the inline script &#39;EEG_trigger_setup&#39;.

The key to the triggers is provided below.

  0: reset trigger port in BrainVision Recorder. This trigger is integrated in the 
     trigger-sending function.

  -- Resting-state EEG part --

    10: beginning of eyes-open resting-state EEG

    11: end of eyes-open resting-state EEG

    12: beginning of eyes-closed resting-state EEG

    13: end of eyes-closed resting-state EEG

  -- Experiment part --

    5: fixation mark

    -- ID of each target sentence (only applicable to target trials) --

        110--253: triggers ranging between 110 and 253, time-locked to the onset of the 
          word of interest in each trial.
&lt;/textarea&gt;
&lt;div id=&#34;comments-in-code-scripts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comments in code scripts&lt;/h3&gt;
&lt;p&gt;It is helpful for our future selves, for our collaborators, and for any other stakeholders associated with a project—which includes any fellow researchers worldwide—to include comments in code scripts. These comments should introduce the purpose of the script at the top, and the purpose of various components of the code. Some excerpts are shown below as examples.&lt;/p&gt;
&lt;script src=&#39;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Ffrequentist_bayesian_plot.R%23L3-L35&amp;style=a11y-dark&amp;type=code&amp;showFullPath=on&amp;showCopy=on&amp;showLineNumbers=on&amp;showFileMeta=on&#39;&gt;&lt;/script&gt;
&lt;script src=&#39;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Fdata%2Fsemanticpriming_data_preparation.R%23L29-L60&amp;style=a11y-dark&amp;type=code&amp;showFullPath=on&amp;showCopy=on&amp;showLineNumbers=on&amp;showFileMeta=on&#39;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;open-source-software&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Open-source software&lt;/h2&gt;
&lt;p&gt;Where possible, open-source software should be used. Open-source software is free, and hence more accessible. Open-source software can be classified in various dimensions, such as the size of the user base. The more users, the greater the support, because the core developers have more resources, and the users will often help each other in public forums such as Stack Exchange. For instance, a programming language such as R boasts millions of users worldwide who count on support in public forums and in R-specific forums such as the &lt;a href=&#34;https://community.rstudio.com&#34;&gt;Posit Community&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Other software are not as large. For instance, open-source software for psychological research (e.g., &lt;a href=&#34;https://osdoc.cogsci.nl/&#34;&gt;OpenSesame&lt;/a&gt;, &lt;a href=&#34;https://www.psychopy.org/&#34;&gt;psychoPy&lt;/a&gt;) are far smaller than R in terms of community. Yet, these software too can count on substantial support. For the more basic uses, most of the way has already been paved, and the existing documentation suffices. For more advanced uses, the smaller size of the community can become more obvious, as one needs to spend more time looking for solutions.&lt;/p&gt;
&lt;p&gt;Regardless of the size of the community, all else being equal, open-source software is the right choice to ensure access to one’s work for all (potential) stakeholders in the future. The other option, proprietary software, entails dependence on the services of a private company.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidiness-and-parsimony-in-computer-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidiness and parsimony in computer code&lt;/h2&gt;
&lt;p&gt;Code scripts should be as tidy and parsimonious as possible. For instance, to prevent overly long scripts that would impair the comprehension of the materials, it is useful to break down large projects into nested scripts, and &lt;code&gt;source&lt;/code&gt; (i.e., run) the smaller scripts in the larger scripts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compose all stimuli for Sessions 2, 3, 4 and 6

# Create participant-specific parameters
source(&amp;#39;stimulus_preparation/participant_parameters.R&amp;#39;)

# Frame base images
source(&amp;#39;stimulus_preparation/base_images.R&amp;#39;)

# Session 2
source(&amp;#39;stimulus_preparation/Session 2/Session2_compile_all_stimuli.R&amp;#39;)

# Session 3
source(&amp;#39;stimulus_preparation/Session 3/Session3_compile_all_stimuli.R&amp;#39;)

# Session 4
source(&amp;#39;stimulus_preparation/Session 4/Session4_compile_all_stimuli.R&amp;#39;)

# Session 6
source(&amp;#39;stimulus_preparation/Session 6/Session6_compile_all_stimuli.R&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tidiness-and-parsimony-in-project-directories&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidiness and parsimony in project directories&lt;/h2&gt;
&lt;p&gt;A directory tree is useful to display all the folders in a project. The tree can be produced in the RStudio ‘Terminal’ console using the following one-line command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;find . -type d | sed -e &amp;quot;s/[^-][^\/]*\//  |/g&amp;quot; -e &amp;quot;s/|\([^ ]\)/| - \1/&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output will look like the following (excerpt from &lt;a href=&#34;https://osf.io/gt5uf/wiki&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf/wiki&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.
  | - bayesian_priors
  |  | - plots
  | - semanticpriming
  |  | - analysis_with_visualsimilarity
  |  |  | - model_diagnostics
  |  |  |  | - results
  |  |  |  | - plots
  |  |  | - results
  |  |  | - plots
  |  |  | - correlations
  |  |  |  | - plots
  |  | - frequentist_bayesian_plots
  |  |  | - plots
  |  | - frequentist_analysis
  |  |  | - model_diagnostics
  |  |  |  | - results
  |  |  |  | - plots
  |  |  | - lexical_covariates_selection
  |  |  |  | - results
  |  |  |  | - plots
  |  |  | - results
  |  |  | - plots&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Adhering to best practices—including the FAIR principles—in the creation of research materials enhances transparency, accessibility and reproducibility in scientific research. These standards facilitate researchers’ work beyond the short term, and increase the reliability of scientific work, thus contributing to the best use of resources.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Two-second delay after logger in OpenSesame</title>
      <link>https://pablobernabeu.github.io/2023/two-second-delay-after-logger/</link>
      <pubDate>Wed, 01 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/two-second-delay-after-logger/</guid>
      <description>


</description>
    </item>
    
    <item>
      <title>Preprocessing the Norwegian Web as Corpus (NoWaC) in R</title>
      <link>https://pablobernabeu.github.io/2023/preprocessing-the-norwegian-web-as-corpus-nowac-in-r/</link>
      <pubDate>Thu, 28 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/preprocessing-the-norwegian-web-as-corpus-nowac-in-r/</guid>
      <description>


&lt;div id=&#34;the-present-script-can-be-used-to-preprocess-data-from-a-frequency-list-of-the-norwegian-as-web-corpus-nowac-guevara-2010.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The present script can be used to preprocess data from a frequency list of the Norwegian as Web Corpus (NoWaC; Guevara, 2010).&lt;/h3&gt;
&lt;p&gt;Before using the script, the frequency list should be downloaded from &lt;a href=&#34;https://www.hf.uio.no/iln/english/about/organization/text-laboratory/projects/nowac/nowac-frequency.html&#34;&gt;this URL&lt;/a&gt;. The list is described as ‘frequency list sorted primary alphabetic and secondary by frequency within each character’, and &lt;a href=&#34;https://www.tekstlab.uio.no/nowac/download/nowac-1.1.lemma.frek.sort_alf_frek.txt.gz&#34;&gt;this is the direct URL&lt;/a&gt;. The download requires signing in to an institutional network. Last, the downloaded file should be unzipped.&lt;/p&gt;
&lt;p&gt;The script is shown below.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fpreprocessing-NoWaC-Corpus-in-R%2Fblob%2Fmain%2Fpreprocessing_NoWaC_Corpus.R%23L19-L102&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Guevara, E. R. (2010). NoWaC: A large web-based corpus for Norwegian. In &lt;em&gt;Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop&lt;/em&gt; (pp. 1-7). &lt;a href=&#34;https://aclanthology.org/W10-1501&#34; class=&#34;uri&#34;&gt;https://aclanthology.org/W10-1501&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rodina, Y., &amp;amp; Westergaard, M. (2015). Grammatical gender in Norwegian: Language acquisition and language change. &lt;em&gt;Journal of Germanic Linguistics, 27&lt;/em&gt;(2), 145–187. &lt;a href=&#34;https://doi.org/10.1017/S1470542714000245&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1017/S1470542714000245&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rodina, Y., &amp;amp; Westergaard, M. (2021). Grammatical gender and declension class in language change: A study of the loss of feminine gender in Norwegian. &lt;em&gt;Journal of Germanic Linguistics, 33&lt;/em&gt;(3), 235–263. &lt;a href=&#34;https://doi.org/10.1017/S1470542719000217&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1017/S1470542719000217&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Python inline script for OpenSesame to send EEG triggers via serial port</title>
      <link>https://pablobernabeu.github.io/2023/an-inline-script-for-opensesame-to-send-eeg-triggers-via-serial-port/</link>
      <pubDate>Sat, 09 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/an-inline-script-for-opensesame-to-send-eeg-triggers-via-serial-port/</guid>
      <description>


&lt;p&gt;The &lt;a href=&#34;https://osdoc.cogsci.nl&#34;&gt;OpenSesame&lt;/a&gt; user base is skyrocketing but—of course—remains small in comparison to many other user bases that we are used to. Therefore, when developing an experiment in OpenSesame, there are still many opportunities to break the mould. When you need to do something beyond the standard operating procedure, it may take longer to find suitable resources than it takes when a more widespread tool is used. So, why would you still want to use OpenSesame? There are many reasons: it is free, open source, based on Python, stable enough thanks to more than a decade of usage and development, &lt;a href=&#34;https://github.com/open-cogsci/OpenSesame&#34;&gt;well maintained&lt;/a&gt;, and has a &lt;a href=&#34;https://forum.cogsci.nl&#34;&gt;community forum&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I recently faced a challenge when developing an EEG experiment that uses event-related potentials. I couldn’t find a way to send triggers through a &lt;em&gt;serial&lt;/em&gt; port. Online, I could find several resources, such as a &lt;a href=&#34;https://stonekate.github.io/blog/opensesame&#34;&gt;script by Kate Stone that draws on a parallel port&lt;/a&gt;, an &lt;a href=&#34;https://github.com/esdalmaijer/opensesame_serial_port_trigger&#34;&gt;outdated plugin that also draws on a parallel port&lt;/a&gt;, and resources based on Python scripts in &lt;a href=&#34;https://discourse.psychopy.org/t/sending-triggers-to-brainvision-eeg-system-from-psychopy-coderview/11011/11&#34;&gt;PsychoPy&lt;/a&gt; and in &lt;a href=&#34;https://pyserial.readthedocs.io/en/latest/shortintro.html&#34;&gt;base Python&lt;/a&gt;. By standing on these giants’ shoulders, and on even more shoulders from &lt;a href=&#34;https://stackoverflow.com/a/76829646/7050882&#34;&gt;StackOverflow&lt;/a&gt;, I put together the following inline script for OpenSesame. The code must be placed in the &lt;a href=&#34;https://osdoc.cogsci.nl/4.0/manual/python/about/#inline_script-items&#34;&gt;&lt;code&gt;Prepare&lt;/code&gt; phase&lt;/a&gt;. The &lt;code&gt;Run&lt;/code&gt; phase can be empty.&lt;/p&gt;
&lt;p&gt;This script is used to set up the system for sending triggers to the EEG recording software (e.g., BrainVision Recorder). The code automatically looks for an available serial port at the beginning of the session. If there are any available ports, a connection is established to the first port in the list. If there are no available ports, no connection is established, and a warning is shown on the screen, informing the user that no triggers will be sent to the EEG recorder. In the latter case, the triggers are only printed in the OpenSesame console.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2FEEG-tools-and-tips%2Fblob%2Fmain%2FOpenSesame_inline_script_to_send_EEG_triggers_via_serial_port%23L12-L300&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>How to correctly encode triggers in Python and send them to BrainVision through serial port (useful for OpenSesame and PsychoPy)</title>
      <link>https://pablobernabeu.github.io/2023/encode-triggers-in-python-and-send-them-to-brainvision-through-serial-port-useful-for-opensesame-and-psychopy/</link>
      <pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/encode-triggers-in-python-and-send-them-to-brainvision-through-serial-port-useful-for-opensesame-and-psychopy/</guid>
      <description>


</description>
    </item>
    
    <item>
      <title>ggplotting power curves from the simr package</title>
      <link>https://pablobernabeu.github.io/2023/ggplotting-power-curves-from-simr-package/</link>
      <pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/ggplotting-power-curves-from-simr-package/</guid>
      <description>


&lt;p&gt;The R package ‘simr’ has greatly facilitated power analysis for mixed-effects models using Monte Carlo simulation (which involves running hundreds or thousands of tests under slight variations of the data). The &lt;code&gt;powerCurve&lt;/code&gt; function is used to estimate the statistical power for various sample sizes in one go. Since the tests are run serially, they can take a VERY long time; approximately, the time it takes to run the model supplied once (say, a few hours) &lt;em&gt;times&lt;/em&gt; the number of simulations (&lt;code&gt;nsim&lt;/code&gt;, which should be higher than 200), and &lt;em&gt;times&lt;/em&gt; the number of different sample sizes examined. While there isn’t a built-in parallel method, the power curves for different sample sizes can be run separately, and the results can be progressively combined as each component finishes running (see &lt;a href=&#34;https://pablobernabeu.github.io/2021/parallelizing-simr-powercurve&#34;&gt;tutorial&lt;/a&gt;). The power curves produced by &lt;code&gt;simr&lt;/code&gt; are so good they deserve ‘ggplot2’ rendering. So, here’s a function for it.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2FpowercurvePlot%2Fblob%2Fmain%2FpowercurvePlot.R%23L3-L82&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;a-usage-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A usage example&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
library(simr)
library(ggplot2)

# Toy model with data from &amp;#39;simr&amp;#39; package
fit = lmer(y ~ x + (x | g), data = simdata)

# Extend sample size of `g`
fit_extended_g = extend(fit, along = &amp;#39;g&amp;#39;, n = 12)

fit_powercurve = 
  powerCurve(fit_extended_g, fixed(&amp;#39;x&amp;#39;), 
             along = &amp;#39;g&amp;#39;, breaks = c(4, 6, 8, 10, 12), 
             nsim = 50, seed = 123, progress = FALSE)

# Read in custom function to ggplot results from simr::powerCurve

source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/powercurvePlot/main/powercurvePlot.R&amp;#39;)

powercurvePlot(fit_powercurve, number_x_axis_levels = 6) +
  
  # Change some defaults
  
  xlab(&amp;quot;Number of levels in &amp;#39;g&amp;#39;&amp;quot;) +
  
  theme(plot.title = element_blank(),
        axis.title.x = element_text(size = 18), 
        axis.title.y = element_text(size = 18), 
        axis.text = element_text(size = 17))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2023/ggplotting-power-curves-from-simr-package/index.en_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to discretise the colour variable in sjPlot::plot_model into equally-sized intervals</title>
      <link>https://pablobernabeu.github.io/2023/how-to-discretise-colour-variable-in-sjplot-plot-model-into-equally-sized-intervals/</link>
      <pubDate>Sat, 24 Jun 2023 16:54:46 +0200</pubDate>
      <guid>https://pablobernabeu.github.io/2023/how-to-discretise-colour-variable-in-sjplot-plot-model-into-equally-sized-intervals/</guid>
      <description>


&lt;p&gt;Whereas the direction of main effects can be interpreted from the sign of the estimate, the interpretation of interaction effects often requires plots. This task is facilitated by the R package &lt;a href=&#34;https://strengejacke.github.io/sjPlot&#34;&gt;&lt;code&gt;sjPlot&lt;/code&gt;&lt;/a&gt;. For instance, using the &lt;code&gt;plot_model&lt;/code&gt; function, I plotted the interaction between two continuous variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
#&amp;gt; Loading required package: Matrix
library(sjPlot)
#&amp;gt; Learn more about sjPlot with &amp;#39;browseVignettes(&amp;quot;sjPlot&amp;quot;)&amp;#39;.
library(ggplot2)

theme_set(theme_sjplot())

# Create data partially based on code by Ben Bolker  
# from https://stackoverflow.com/a/38296264/7050882

set.seed(101)

spin = runif(800, 1, 24)

trait = rep(1:40, each = 20)

ID = rep(1:80, each = 10)

testdata &amp;lt;- data.frame(spin, trait, ID)

testdata$fatigue &amp;lt;- 
  testdata$spin * testdata$trait / 
  rnorm(800, mean = 6, sd = 2)

# Model
fit = lmer(fatigue ~ spin * trait + (1|ID),
           data = testdata, REML = TRUE)
#&amp;gt; boundary (singular) fit: see help(&amp;#39;isSingular&amp;#39;)

plot_model(fit, type = &amp;#39;pred&amp;#39;, terms = c(&amp;#39;spin&amp;#39;, &amp;#39;trait&amp;#39;))
#&amp;gt; Warning: Ignoring unknown parameters: linewidth&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;7VTcfLu.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;Created on 2023-06-24 with &lt;a href=&#34;https://reprex.tidyverse.org&#34;&gt;reprex v2.0.2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;However, I needed an extra feature, as sjPlot by default breaks down the colour (&lt;code&gt;fill&lt;/code&gt;) variable into few levels that do not include the minimum or the maximum values in my variable. What I would like to do is to stratify the colour variable into equally-sized levels that include the minimum and the maximum values.&lt;/p&gt;
&lt;p&gt;Furthermore, in the legend, I would also like to display the number of levels of a grouping variable (&lt;code&gt;ID&lt;/code&gt;) that are contained in each level of the colour variable.&lt;/p&gt;
&lt;p&gt;Below is a solution using &lt;a href=&#34;https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-ten-or-six-bins&#34;&gt;custom functions called &lt;code&gt;deciles_interaction_plot&lt;/code&gt; and &lt;code&gt;sextiles_interaction_plot&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
#&amp;gt; Loading required package: Matrix
library(sjPlot)
library(ggplot2)

theme_set(theme_sjplot())

# Create data partially based on code by Ben Bolker  
# from https://stackoverflow.com/a/38296264/7050882

set.seed(101)

spin = runif(800, 1, 24)

trait = rep(1:40, each = 20)

ID = rep(1:80, each = 10)

testdata &amp;lt;- data.frame(spin, trait, ID)

testdata$fatigue &amp;lt;- 
  testdata$spin * testdata$trait / 
  rnorm(800, mean = 6, sd = 2)

# Model
fit = lmer(fatigue ~ spin * trait + (1|ID),
           data = testdata, REML = TRUE)
#&amp;gt; boundary (singular) fit: see help(&amp;#39;isSingular&amp;#39;)

# plot_model(fit, type = &amp;#39;pred&amp;#39;, terms = c(&amp;#39;spin&amp;#39;, &amp;#39;trait&amp;#39;))

# Binning the colour variable into ten levels (deciles)

# Read in function from GitHub
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/main/R_functions/deciles_interaction_plot.R&amp;#39;)

deciles_interaction_plot(
  model = fit, 
  x = &amp;#39;spin&amp;#39;,
  fill = &amp;#39;trait&amp;#39;,
  fill_nesting_factor = &amp;#39;ID&amp;#39;
)
#&amp;gt; Loading required package: dplyr
#&amp;gt; 
#&amp;gt; Attaching package: &amp;#39;dplyr&amp;#39;
#&amp;gt; The following objects are masked from &amp;#39;package:stats&amp;#39;:
#&amp;gt; 
#&amp;gt;     filter, lag
#&amp;gt; The following objects are masked from &amp;#39;package:base&amp;#39;:
#&amp;gt; 
#&amp;gt;     intersect, setdiff, setequal, union
#&amp;gt; Loading required package: RColorBrewer
#&amp;gt; Loading required package: ggtext
#&amp;gt; Loading required package: Cairo
#&amp;gt; Warning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette Set1 is 9
#&amp;gt; Returning the palette you asked for with that many colors
#&amp;gt; Warning: Ignoring unknown parameters: linewidth
#&amp;gt; Scale for &amp;#39;y&amp;#39; is already present. Adding another scale for &amp;#39;y&amp;#39;, which will
#&amp;gt; replace the existing scale.
#&amp;gt; Scale for &amp;#39;colour&amp;#39; is already present. Adding another scale for &amp;#39;colour&amp;#39;,
#&amp;gt; which will replace the existing scale.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;niqWOzx.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# If you wanted or needed to make six levels (sextiles) instead 
# of ten, you could use the function sextiles_interaction_plot.

# Read in function from GitHub
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/main/R_functions/sextiles_interaction_plot.R&amp;#39;)

sextiles_interaction_plot(
  model = fit, 
  x = &amp;#39;spin&amp;#39;,
  fill = &amp;#39;trait&amp;#39;,
  fill_nesting_factor = &amp;#39;ID&amp;#39;
)
#&amp;gt; Warning: Ignoring unknown parameters: linewidth
#&amp;gt; Scale for &amp;#39;y&amp;#39; is already present. Adding another scale for &amp;#39;y&amp;#39;, which will
#&amp;gt; replace the existing scale.
#&amp;gt; Scale for &amp;#39;colour&amp;#39; is already present. Adding another scale for &amp;#39;colour&amp;#39;,
#&amp;gt; which will replace the existing scale.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;w8Ydo4F.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;Created on 2023-06-24 with &lt;a href=&#34;https://reprex.tidyverse.org&#34;&gt;reprex v2.0.2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to map more informative values onto fill argument of sjPlot::plot_model</title>
      <link>https://pablobernabeu.github.io/2023/how-to-map-more-informative-values-onto-fill-argument-of-sjplot-plot-model/</link>
      <pubDate>Sat, 24 Jun 2023 16:51:11 +0200</pubDate>
      <guid>https://pablobernabeu.github.io/2023/how-to-map-more-informative-values-onto-fill-argument-of-sjplot-plot-model/</guid>
      <description>


&lt;p&gt;Whereas the direction of main effects can be interpreted from the sign of the estimate, the interpretation of interaction effects often requires plots. This task is facilitated by the R package &lt;a href=&#34;https://strengejacke.github.io/sjPlot&#34;&gt;&lt;code&gt;sjPlot&lt;/code&gt;&lt;/a&gt;. For instance, using the &lt;code&gt;plot_model&lt;/code&gt; function, I plotted the interaction between a continuous variable and a categorical variable. The categorical variable was passed to the &lt;code&gt;fill&lt;/code&gt; argument of plot_model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
#&amp;gt; Loading required package: Matrix
library(sjPlot)
#&amp;gt; Install package &amp;quot;strengejacke&amp;quot; from GitHub (`devtools::install_github(&amp;quot;strengejacke/strengejacke&amp;quot;)`) to load all sj-packages at once!
library(ggplot2)

theme_set(theme_sjplot())

cake$recipe_recoded = ifelse(cake$recipe == &amp;#39;A&amp;#39;, -0.5,
                             ifelse(cake$recipe == &amp;#39;B&amp;#39;, 0,
                                    ifelse(cake$recipe == &amp;#39;C&amp;#39;, 0.5,
                                           NA)))

fit = lmer(angle ~ recipe_recoded * temp + 
             (1|recipe_recoded:replicate), 
           cake, REML= FALSE)

plot_model(fit, type = &amp;#39;pred&amp;#39;, terms = c(&amp;#39;temp&amp;#39;, &amp;#39;recipe_recoded&amp;#39;))
#&amp;gt; Warning: Ignoring unknown parameters: linewidth&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;SiigAMp.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;Created on 2023-06-24 with &lt;a href=&#34;https://reprex.tidyverse.org&#34;&gt;reprex v2.0.2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;However, I needed an extra feature, as the categorical variable was not quite informative because it was a sum-coded transformation. Thus, I wanted the legend of the plot to show the values of the original variable (i.e., A, B and C), instead of those of the sum-coded variable that had been used in the model (i.e., -0.5, 0 and 0.5).&lt;/p&gt;
&lt;p&gt;Below is a solution using a custom function called &lt;a href=&#34;https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-alias-variables&#34;&gt;&lt;code&gt;alias_interaction_plot&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
#&amp;gt; Loading required package: Matrix
library(sjPlot)
library(ggplot2)

theme_set(theme_sjplot())

cake$recipe_recoded = ifelse(cake$recipe == &amp;#39;A&amp;#39;, -0.5,
                             ifelse(cake$recipe == &amp;#39;B&amp;#39;, 0,
                                    ifelse(cake$recipe == &amp;#39;C&amp;#39;, 0.5,
                                           NA)))

fit = lmer(angle ~ recipe_recoded * temp + 
             (1|recipe_recoded:replicate), 
           cake, REML= FALSE)

# plot_model(fit, type = &amp;#39;pred&amp;#39;, terms = c(&amp;#39;temp&amp;#39;, &amp;#39;recipe_recoded&amp;#39;))

# Displaying the original variable instead

# Read in function from GitHub
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/main/R_functions/alias_interaction_plot.R&amp;#39;)

alias_interaction_plot(
  model = fit, 
  dataset = cake,
  x = &amp;#39;temp&amp;#39;,
  fill = &amp;#39;recipe_recoded&amp;#39;,
  fill_alias = &amp;#39;recipe&amp;#39;,
  fill_title = &amp;#39;recipe&amp;#39;
)
#&amp;gt; Loading required package: rlang
#&amp;gt; Loading required package: dplyr
#&amp;gt; 
#&amp;gt; Attaching package: &amp;#39;dplyr&amp;#39;
#&amp;gt; The following objects are masked from &amp;#39;package:stats&amp;#39;:
#&amp;gt; 
#&amp;gt;     filter, lag
#&amp;gt; The following objects are masked from &amp;#39;package:base&amp;#39;:
#&amp;gt; 
#&amp;gt;     intersect, setdiff, setequal, union
#&amp;gt; Loading required package: RColorBrewer
#&amp;gt; Loading required package: ggtext
#&amp;gt; Loading required package: Cairo
#&amp;gt; Warning: Ignoring unknown parameters: linewidth
#&amp;gt; Scale for &amp;#39;y&amp;#39; is already present. Adding another scale for &amp;#39;y&amp;#39;, which will
#&amp;gt; replace the existing scale.
#&amp;gt; Scale for &amp;#39;colour&amp;#39; is already present. Adding another scale for &amp;#39;colour&amp;#39;,
#&amp;gt; which will replace the existing scale.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;SS03gK6.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;Created on 2023-06-24 with &lt;a href=&#34;https://reprex.tidyverse.org&#34;&gt;reprex v2.0.2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to visually assess the convergence of a mixed-effects model by plotting various optimizers</title>
      <link>https://pablobernabeu.github.io/2023/how-to-visually-assess-the-convergence-of-a-mixed-effects-model-by-plotting-various-optimizers/</link>
      <pubDate>Sat, 24 Jun 2023 16:42:34 +0200</pubDate>
      <guid>https://pablobernabeu.github.io/2023/how-to-visually-assess-the-convergence-of-a-mixed-effects-model-by-plotting-various-optimizers/</guid>
      <description>


&lt;p&gt;To assess whether convergence warnings render the results invalid, or on the contrary, the results can be deemed valid in spite of the warnings, &lt;a href=&#34;https://cran.r-project.org/web/packages/lme4/lme4.pdf&#34;&gt;Bates et al. (2023)&lt;/a&gt; suggest refitting models affected by convergence warnings with a variety of optimizers. The authors argue that, if the different optimizers produce practically-equivalent results, the results are valid. The &lt;code&gt;allFit&lt;/code&gt; function from the ‘lme4’ package allows the refitting of models using a number of optimizers. To use the seven optimizers listed above, two extra packages must be installed: ‘dfoptim’ and ‘optimx’ (see lme4 manual). The output from &lt;code&gt;allFit&lt;/code&gt; contains several statistics on the fixed and the random effects fitted by each optimizer.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
#&amp;gt; Loading required package: Matrix
library(dfoptim)
library(optimx)

# Create data using code by Ben Bolker from 
# https://stackoverflow.com/a/38296264/7050882

set.seed(101)
spin = runif(600, 1, 24)
reg = runif(600, 1, 15)
ID = rep(c(&amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,&amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;7&amp;quot;, &amp;quot;8&amp;quot;, &amp;quot;9&amp;quot;, &amp;quot;10&amp;quot;))
day = rep(1:30, each = 10)
testdata &amp;lt;- data.frame(spin, reg, ID, day)
testdata$fatigue &amp;lt;- testdata$spin * testdata$reg/10 * rnorm(30, mean=3, sd=2)

# Model
fit = lmer(fatigue ~ spin * reg + (1|ID),
           data = testdata, REML = TRUE)

# Refit model using all available algorithms
multi_fit = allFit(fit)
#&amp;gt; bobyqa : [OK]
#&amp;gt; Nelder_Mead : [OK]
#&amp;gt; nlminbwrap : [OK]
#&amp;gt; nmkbw : [OK]
#&amp;gt; optimx.L-BFGS-B : [OK]
#&amp;gt; nloptwrap.NLOPT_LN_NELDERMEAD : [OK]
#&amp;gt; nloptwrap.NLOPT_LN_BOBYQA : [OK]

# Show results 
summary(multi_fit)$fixef
#&amp;gt;                               (Intercept)      spin       reg  spin:reg
#&amp;gt; bobyqa                          -2.975678 0.5926561 0.1437204 0.1834016
#&amp;gt; Nelder_Mead                     -2.975675 0.5926559 0.1437202 0.1834016
#&amp;gt; nlminbwrap                      -2.975677 0.5926560 0.1437203 0.1834016
#&amp;gt; nmkbw                           -2.975678 0.5926561 0.1437204 0.1834016
#&amp;gt; optimx.L-BFGS-B                 -2.975680 0.5926562 0.1437205 0.1834016
#&amp;gt; nloptwrap.NLOPT_LN_NELDERMEAD   -2.975666 0.5926552 0.1437196 0.1834017
#&amp;gt; nloptwrap.NLOPT_LN_BOBYQA       -2.975678 0.5926561 0.1437204 0.1834016

# Read in function from GitHub
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/plot.fixef.allFit/main/plot.fixef.allFit.R&amp;#39;)

plot.fixef.allFit(multi_fit, 
                  
                  select_predictors = c(&amp;#39;spin&amp;#39;, &amp;#39;reg&amp;#39;, &amp;#39;spin:reg&amp;#39;), 
                  
                  # Increase padding at top and bottom of Y axis
                  multiply_y_axis_limits = 1.3,
                  
                  y_title = &amp;#39;Fixed effect (*b*)&amp;#39;)
#&amp;gt; Loading required package: dplyr
#&amp;gt; 
#&amp;gt; Attaching package: &amp;#39;dplyr&amp;#39;
#&amp;gt; The following objects are masked from &amp;#39;package:stats&amp;#39;:
#&amp;gt; 
#&amp;gt;     filter, lag
#&amp;gt; The following objects are masked from &amp;#39;package:base&amp;#39;:
#&amp;gt; 
#&amp;gt;     intersect, setdiff, setequal, union
#&amp;gt; Loading required package: reshape2
#&amp;gt; Loading required package: stringr
#&amp;gt; Loading required package: scales
#&amp;gt; Loading required package: ggplot2
#&amp;gt; Loading required package: ggtext
#&amp;gt; Loading required package: patchwork&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;XYQDug2.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# Alternative using plot-specific Y axes and other modified settings

plot.fixef.allFit(multi_fit, 
                  
                  select_predictors = c(&amp;#39;spin&amp;#39;, &amp;#39;spin:reg&amp;#39;), 
                  
                  # Use plot-specific Y axis limits
                  shared_y_axis_limits = FALSE,
                  
                  decimal_places = 7, 
                  
                  # Move up Y axis title
                  y_title_hjust = 4.5,
                  
                  y_title = &amp;#39;Fixed effect (*b*)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;BYXJYxM.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;Created on 2023-06-26 with &lt;a href=&#34;https://reprex.tidyverse.org&#34;&gt;reprex v2.0.2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intermixing stimuli from two loops randomly in OpenSesame</title>
      <link>https://pablobernabeu.github.io/2023/intermixing-stimuli-from-two-loops-randomly-in-opensesame/</link>
      <pubDate>Sat, 24 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/intermixing-stimuli-from-two-loops-randomly-in-opensesame/</guid>
      <description>


&lt;p&gt;I’m developing a slightly tricky design in &lt;a href=&#34;https://osdoc.cogsci.nl&#34;&gt;OpenSesame&lt;/a&gt; (a Python-based experiment builder). My stimuli comprise two kinds of sentences that contain different elements, and different numbers of elements. These sentences must be presented word by word. Furthermore, I need to attach triggers to some words in the first kind of sentences but not in the second kind. Last, these kinds of sentences must be intermixed within a block (or a &lt;code&gt;sequence&lt;/code&gt;) of trials, because the first kind are targets and the second kind are fillers.&lt;/p&gt;
&lt;p&gt;I need to present stimuli randomly from either of two &lt;code&gt;loop&lt;/code&gt;s within a &lt;code&gt;sequence&lt;/code&gt;. Currently, when I try this arrangement using the interface, the resulting script ends up with &lt;code&gt;run loop1&lt;/code&gt; followed by &lt;code&gt;run loop2&lt;/code&gt;, but I can’t find a way to ensure that the overarching sequence pulls randomly from either loop over successive trials. In other words, my goal is to intermix the stimuli from both loops.&lt;/p&gt;
&lt;p&gt;I have searched for clues in the OpenSesame tutorials, in the OpenSesame forum, and elsewhere online, but to no avail.&lt;/p&gt;
&lt;div id=&#34;solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solution&lt;/h2&gt;
&lt;p&gt;Finally, I solved this by first distributing all the words in each sentence across different columns in the stimulus file, yielding ‘word1’, ‘word2’, etc. Next, in OpenSesame, I created a sketchpad for each word. Last, to prevent showing any words that were blank, I entered conditions such as &lt;code&gt;[word7] != NA&lt;/code&gt; in the &lt;kbd&gt;Run if&lt;/kbd&gt; field of the sequence.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simultaneously sampling from two variables in jsPsych</title>
      <link>https://pablobernabeu.github.io/2023/simultaneously-sampling-from-two-variables-in-jspsych/</link>
      <pubDate>Sat, 24 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/simultaneously-sampling-from-two-variables-in-jspsych/</guid>
      <description>


&lt;p&gt;I am using &lt;a href=&#34;https://www.jspsych.org&#34;&gt;jsPsych&lt;/a&gt; to create an experiment and I am struggling to sample from two variables simultaneously. Specifically, in each trial, I would like to present a &lt;code&gt;primeWord&lt;/code&gt; and a &lt;code&gt;targetWord&lt;/code&gt; by randomly sampling each of them from its own variable.&lt;/p&gt;
&lt;p&gt;I have looked into several resources—such as &lt;a href=&#34;https://www.jspsych.org/7.0/overview/timeline/index.html#sampling-without-replacement&#34;&gt;sampling without replacement&lt;/a&gt;, &lt;a href=&#34;https://github.com/jspsych/jsPsych/discussions/1076#discussioncomment-87298&#34;&gt;custom sampling&lt;/a&gt; and &lt;a href=&#34;https://github.com/jspsych/jsPsych/discussions/1911#discussioncomment-922511&#34;&gt;position indices&lt;/a&gt;—but to no avail. I’m a beginner at this, so it’s possible that one of these resources was relevant (especially the last one, I think).&lt;/p&gt;
&lt;p&gt;In addition to the parallel sampling, I wonder how I could save the same trial index in the &lt;code&gt;data&lt;/code&gt; of both &lt;code&gt;primeWord&lt;/code&gt; and &lt;code&gt;targetWord&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solution&lt;/h2&gt;
&lt;p&gt;Finally I solved this using position indices in a for-of loop (see below: &lt;code&gt;For loop that creates trial information iteratively over trials (3 steps)&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;

  &amp;lt;head&amp;gt;

    &amp;lt;!-- jsPsych plugins --&amp;gt;
    &amp;lt;script src=&amp;quot;../jspsych.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src=&amp;quot;../plugins/jspsych-html-keyboard-response.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src=&amp;quot;../plugins/jspsych-html-button-response.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;

    &amp;lt;!-- CSS --&amp;gt;
    &amp;lt;link rel=&amp;quot;stylesheet&amp;quot; href=&amp;quot;../css/jspsych.css&amp;quot;&amp;gt;

    &amp;lt;style&amp;gt;
      body.jspsych-display-element {
        color: #ececec;
        background-color: #2b2b2b;
      }

      #jspsych-html-keyboard-response-stimulus {
        font-size: 32px;
      }

      .fas,
      .far {
        color: #b6b6b6;
      }

    &amp;lt;/style&amp;gt;

  &amp;lt;/head&amp;gt;


  &amp;lt;!-- Beginning of the script that contains the core of the experiment --&amp;gt;
  &amp;lt;script&amp;gt;
    /* Create empty timeline object, which will be sequentially filled in using timeline.push() */
    var timeline = [];

    var instructions = {
      type: &amp;#39;html-button-response&amp;#39;,
      stimulus: [&amp;quot;&amp;lt;p&amp;gt;Each screen will show a word in lower case, such as &amp;#39;target&amp;#39;. Press &amp;lt;b&amp;gt;F&amp;lt;/b&amp;gt; if the word is primarily abstract&amp;lt;/p&amp;gt;&amp;quot; +
        &amp;#39;&amp;lt;p&amp;gt;or &amp;lt;b&amp;gt;J&amp;lt;/b&amp;gt; if it is primarily concrete. Each word is presented for up to five seconds.&amp;lt;/p&amp;gt;&amp;#39;
      ],
      choices: [&amp;#39;Ready to start&amp;#39;]
    }
    /* Add instructions to the timeline */
    timeline.push(instructions)


    /* Stimuli */

    /* Begin with a general list of words, and randomly split the 
    list into a set of prime words and a set of target words. */

    var allWords = [{
        word: &amp;#39;word 1&amp;#39;,
        correct_response: &amp;#39;abstract&amp;#39;
      },
      {
        word: &amp;#39;word 2&amp;#39;,
        correct_response: &amp;#39;abstract&amp;#39;
      },
      {
        word: &amp;#39;word 3&amp;#39;,
        correct_response: &amp;#39;abstract&amp;#39;
      },
      {
        word: &amp;#39;word 4&amp;#39;,
        correct_response: &amp;#39;abstract&amp;#39;
      },
      {
        word: &amp;#39;word 5&amp;#39;,
        correct_response: &amp;#39;abstract&amp;#39;
      },
      {
        word: &amp;#39;word 6&amp;#39;,
        correct_response: &amp;#39;concrete&amp;#39;
      },
      {
        word: &amp;#39;word 7&amp;#39;,
        correct_response: &amp;#39;concrete&amp;#39;
      },
      {
        word: &amp;#39;word 8&amp;#39;,
        correct_response: &amp;#39;concrete&amp;#39;
      },
      {
        word: &amp;#39;word 9&amp;#39;,
        correct_response: &amp;#39;concrete&amp;#39;
      },
      {
        word: &amp;#39;word 10&amp;#39;,
        correct_response: &amp;#39;concrete&amp;#39;
      }

    ]

    /* Shuffle all words */
    var shuffled_allWords = allWords.sort(function() {
      return 0.5 - Math.random()
    });

    /* Split up the list into two sets */
    var midpoint = Math.floor(shuffled_allWords.length / 2);

    /* Set number of trials per participant (must be smaller than half of all words) */
    var number_of_trials = 4;

    /* Create the set of prime words */
    var primeWords = shuffled_allWords.slice(0, number_of_trials);

    /* Make prime words uppercase, as in Hutchison et al.
    (2013; https://doi.org/10.3758/s13428-012-0304-z) */

    var primeWords = primeWords.map(item =&amp;gt; ({
      ...item,
      word: item.word.toUpperCase()
    }))

    /* Create the set of target words */
    var targetWords = shuffled_allWords.slice(midpoint, midpoint + number_of_trials);


    /* Next, create set of interstimulus intervals from a range between 60 and 1200 ms. 
    First, the range is split into as many integers as the number of trials, equally 
    for all participants. Afterwards, the list is shuffled within participants. */

    function makeArr(startValue, stopValue, cardinality) {
      var arr = [];
      var step = (stopValue - startValue) / (cardinality - 1);
      for (var i = 0; i &amp;lt; cardinality; i++) {
        arr.push(startValue + (step * i));
      }
      return arr;
    }

    ordered_interstimulus_intervals = makeArr(60, 1200, number_of_trials);

    interstimulus_interval =
      ordered_interstimulus_intervals.sort(function() {
        return 0.5 - Math.random()
      });


    /* For loop that creates trial information iteratively over trials (3 steps) */

    /* 1. Enable function to create iterable range, to be used in the for loop below */
    const Range = (start, end) =&amp;gt; ({
      *[Symbol.iterator]() {
        while (start &amp;lt; end)
          yield start++;
      }
    })

    /* 2. Initialise stimuli array */
    stimuli = [];

    /* 3. Run loop */
    for (const i of Range(0, number_of_trials)) {
      stimuli.push({
        primeWord: primeWords[i].word,
        targetWord: targetWords[i].word,
        interstimulus_interval: interstimulus_interval[i],
        correct_response: targetWords[i].correct_response,
        trial: i + 1 /* 1 is added because, otherwise, trials would else trials would start from 0 */
      })
    }


    /* Trial content: fixation, primeWord, interstimulus interval, targetWord, feedback.
    This constitutes a unique trial in the semantic priming paradigm. Yet, beware that 
    jsPsych provides a &amp;#39;trial_index&amp;#39; value in the output of the task. That index is 
    assigned to each part of every trial. Thus, in the present experiment, there are 
    five trial_index values per trial--namely, one for each part listed above. */

    /* Fixation cross */
    var fixation = {
      type: &amp;#39;html-keyboard-response&amp;#39;,
      stimulus: &amp;#39;+&amp;#39;,
      response_ends_trial: false,
      trial_duration: function() {
        /* Set fixations with a varying duration to boost participants&amp;#39; attention */
        return jsPsych.randomization.sampleWithoutReplacement([400, 450, 500, 550, 600], 1)[0];
      },
      post_trial_gap: 0,
      data: {
        trial: jsPsych.timelineVariable(&amp;#39;trial&amp;#39;)
      },
      css_classes: [&amp;#39;stimulus&amp;#39;],
      /* Computation run at the end of each trial */
      on_finish: function(data) {
        /* Log key presses, if any, by writing 1 into fixation_keypresses (else, write 0) */
        if (data.key_press == null) {
          var fixation_keypresses = 0;
        } else {
          var fixation_keypresses = 1;
        };
        data.fixation_keypresses = fixation_keypresses
      }
    };

    var primeWord = {
      type: &amp;#39;html-keyboard-response&amp;#39;,
      stimulus: jsPsych.timelineVariable(&amp;#39;primeWord&amp;#39;),
      response_ends_trial: false,
      trial_duration: 150,
      post_trial_gap: 0,
      data: {
        position: &amp;#39;prime&amp;#39;,
        trial: jsPsych.timelineVariable(&amp;#39;trial&amp;#39;)
      },
      css_classes: [&amp;#39;stimulus&amp;#39;],
      /* Computation run at the end of each trial */
      on_finish: function(data) {
        /* Log key presses, if any, by writing 1 into primeWord_keypresses (else, write 0) */
        if (data.key_press == null) {
          var primeWord_keypresses = 0;
        } else {
          var primeWord_keypresses = 1;
        };
        data.primeWord_keypresses = primeWord_keypresses
      }
    };

    var interstimulus_interval = {
      type: &amp;#39;html-keyboard-response&amp;#39;,
      stimulus: &amp;#39; &amp;#39;,
      response_ends_trial: false,
      trial_duration: jsPsych.timelineVariable(&amp;#39;interstimulus_interval&amp;#39;),
      post_trial_gap: 0,
      data: {
        interstimulus_interval: jsPsych.timelineVariable(&amp;#39;interstimulus_interval&amp;#39;),
        trial: jsPsych.timelineVariable(&amp;#39;trial&amp;#39;)
      },
      css_classes: [&amp;#39;stimulus&amp;#39;],
      /* Computation run at the end of each trial */
      on_finish: function(data) {
        /* Log key presses, if any, by writing 1 into interstimulus_interval_keypresses (else, write 0) */
        if (data.key_press == null) {
          var interstimulus_interval_keypresses = 0;
        } else {
          var interstimulus_interval_keypresses = 1;
        };
        data.interstimulus_interval_keypresses = interstimulus_interval_keypresses
      }
    };

    var targetWord = {
      type: &amp;#39;html-keyboard-response&amp;#39;,
      stimulus: jsPsych.timelineVariable(&amp;#39;targetWord&amp;#39;),
      choices: [&amp;#39;f&amp;#39;, &amp;#39;j&amp;#39;],
      trial_duration: 3000,
      post_trial_gap: 0,
      css_classes: [&amp;#39;stimulus&amp;#39;],
      data: {
        position: &amp;#39;target&amp;#39;,
        trial: jsPsych.timelineVariable(&amp;#39;trial&amp;#39;),
        correct_response: jsPsych.timelineVariable(&amp;#39;correct_response&amp;#39;)
      },
      /* Computation run at the end of each trial */
      on_finish: function(data) {
        if (data.key_press !== null) {
          /* Label correct responses */
          if (data.correct_response == &amp;#39;abstract&amp;#39; &amp;amp;&amp;amp; data.key_press == jsPsych.pluginAPI.convertKeyCharacterToKeyCode(&amp;#39;f&amp;#39;) ||
            data.correct_response == &amp;#39;concrete&amp;#39; &amp;amp;&amp;amp; data.key_press == jsPsych.pluginAPI.convertKeyCharacterToKeyCode(&amp;#39;j&amp;#39;)) {
            var accuracy = &amp;#39;correct&amp;#39;;
            /* Label incorrect responses */
          } else if (data.correct_response == &amp;#39;abstract&amp;#39; &amp;amp;&amp;amp; data.key_press == jsPsych.pluginAPI.convertKeyCharacterToKeyCode(&amp;#39;j&amp;#39;) ||
            data.correct_response == &amp;#39;concrete&amp;#39; &amp;amp;&amp;amp; data.key_press == jsPsych.pluginAPI.convertKeyCharacterToKeyCode(&amp;#39;f&amp;#39;)) {
            var accuracy = &amp;#39;incorrect&amp;#39;;
          }
          /* Label unanswered trials */
        } else {
          var accuracy = &amp;#39;unanswered&amp;#39;;
        };
        data.accuracy = accuracy;
        /* Count up premature responses per trial. The command &amp;#39;last(4)&amp;#39; is used below
        to consider only the current part of the &amp;#39;trial&amp;#39; (i.e., targetWord) and the 
        three previous parts (i.e., interstimulus_interval, primeWord and fixation). 
        Notice that the response entered in this part (targetWord) is not added into 
        the sum, as it is appropriate to respond to the target word. */
        data.premature_responses =
          jsPsych.data.get().last(4).filter(&amp;#39;fixation_keypresses&amp;#39; == 1).select(&amp;#39;fixation_keypresses&amp;#39;).sum() +
          jsPsych.data.get().last(4).filter(&amp;#39;primeWord_keypresses&amp;#39; == 1).select(&amp;#39;primeWord_keypresses&amp;#39;).sum() +
          jsPsych.data.get().last(4).filter(&amp;#39;interstimulus_interval_keypresses&amp;#39; == 1).select(&amp;#39;interstimulus_interval_keypresses&amp;#39;).sum();
      }
    };

    feedback = {
      type: &amp;#39;html-keyboard-response&amp;#39;,
      stimulus: function() {
        var last_trial_accuracy = jsPsych.data.getLastTrialData().values()[0].accuracy;
        if (last_trial_accuracy == &amp;#39;incorrect&amp;#39;) {
          return &amp;#39;&amp;lt;p style=&amp;quot;color:red; font-face:bold;&amp;quot;&amp;gt;X&amp;lt;/p&amp;gt;&amp;#39;;
        } else if (last_trial_accuracy == &amp;#39;unanswered&amp;#39;) {
          return &amp;#39;&amp;lt;p style=&amp;quot;color:red; font-face:bold;&amp;quot;&amp;gt;0&amp;lt;/p&amp;gt;&amp;#39;
        } else {
          return &amp;#39;&amp;#39;
        }
      },
      choices: jsPsych.NO_KEYS,
      trial_duration: function() {
        var last_trial_accuracy = jsPsych.data.getLastTrialData().values()[0].accuracy;
        if (last_trial_accuracy == &amp;#39;correct&amp;#39;) {
          return 0
        } else {
          return 800
        }
      }
    };

    var main_procedure = {
      timeline: [fixation, primeWord, interstimulus_interval, targetWord, feedback],
      timeline_variables: stimuli
    };
    timeline.push(main_procedure);


    var debrief = {
      type: &amp;#39;html-keyboard-response&amp;#39;,
      choices: [&amp;#39;c&amp;#39;],
      stimulus: function() {
        var total_correct = jsPsych.data.get().filter({
          accuracy: &amp;#39;correct&amp;#39;
        }).count();
        var total_incorrect = jsPsych.data.get().filter({
          accuracy: &amp;#39;incorrect&amp;#39;
        }).count();
        var total_unanswered = jsPsych.data.get().filter({
          accuracy: &amp;#39;unanswered&amp;#39;
        }).count();
        var accuracy_rate = Math.round(total_correct / (total_correct + total_incorrect + total_unanswered) * 100) + &amp;quot;%&amp;quot;;
        var message = &amp;quot;&amp;lt;div style=&amp;#39;font-size:20px;&amp;#39;&amp;gt;&amp;lt;p&amp;gt;All done!&amp;lt;/p&amp;gt;&amp;quot; +
          &amp;quot;&amp;lt;p&amp;gt;Your accuracy rate was &amp;quot; + accuracy_rate + &amp;quot; (&amp;quot; + total_correct + &amp;quot; correct trials, &amp;quot; + total_incorrect +
          &amp;quot; incorrect and &amp;quot; + total_unanswered + &amp;quot; unanswered).&amp;lt;/p&amp;gt;&amp;quot; +
          &amp;quot;&amp;lt;p&amp;gt;Press C to see the entire set of data generated by this experiment.&amp;lt;/p&amp;gt;&amp;lt;/div&amp;gt;&amp;quot;;
        return message;
      }
    }
    /* Add debrief to the timeline */
    timeline.push(debrief);


    /* Initialize experiment by incorporating the timeline
    and setting the data to be displayed at the end. */
    jsPsych.init({
      timeline: timeline,
      on_finish: function() {
        jsPsych.data.displayData();
      },
      default_iti: function() {
        /* Use varying intertrial intervals to reduce habituation effects */
        return jsPsych.randomization.sampleWithoutReplacement([1300, 1400, 1500, 1600, 1700], 1)[0];
      }
    });

  &amp;lt;/script&amp;gt;

&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Table joins with conditional &ldquo;fuzzy&rdquo; string matching in R</title>
      <link>https://pablobernabeu.github.io/2023/table-joins-with-conditional-fuzzy-string-matching-in-r/</link>
      <pubDate>Sat, 24 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/table-joins-with-conditional-fuzzy-string-matching-in-r/</guid>
      <description>


&lt;p&gt;Here’s an example of fuzzy-matching strings in R that I shared on &lt;a href=&#34;https://stackoverflow.com/a/76368552/7050882&#34;&gt;StackOverflow&lt;/a&gt;. In &lt;code&gt;stringdist_join&lt;/code&gt;, the &lt;code&gt;max_dist&lt;/code&gt; argument is used to constrain the degree of fuzziness.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fuzzyjoin)
library(dplyr)
#&amp;gt; 
#&amp;gt; Attaching package: &amp;#39;dplyr&amp;#39;
#&amp;gt; The following objects are masked from &amp;#39;package:stats&amp;#39;:
#&amp;gt; 
#&amp;gt;     filter, lag
#&amp;gt; The following objects are masked from &amp;#39;package:base&amp;#39;:
#&amp;gt; 
#&amp;gt;     intersect, setdiff, setequal, union
library(knitr)


small_tab = data.frame(Food.Name = c(&amp;#39;Corn&amp;#39;, &amp;#39;Squash&amp;#39;, &amp;#39;Peppers&amp;#39;), 
                       Food.Code = c(NA, NA, NA))


large_tab = data.frame(Food.Name = c(&amp;#39;Sweet Corn&amp;#39;, &amp;#39;Red Corn&amp;#39;, &amp;#39;Baby Corns&amp;#39;, 
                                     &amp;#39;Squash&amp;#39;, &amp;#39;Long Squash&amp;#39;, &amp;#39;Red Pepper&amp;#39;, 
                                     &amp;#39;Green Pepper&amp;#39;, &amp;#39;Red Peppers&amp;#39;), 
                       Food.Code = c(532, 532, 944, 111, 123, 654, 655, 654))

joined_tab = stringdist_join(small_tab, large_tab, by = &amp;#39;Food.Name&amp;#39;,
                             ignore_case = TRUE, method = &amp;#39;cosine&amp;#39;, 
                             max_dist = 0.5, distance_col = &amp;#39;dist&amp;#39;) %&amp;gt;%
  
  # Tidy columns 
  select(Food.Name = Food.Name.x, -Food.Name.y, 
         Food.Code = Food.Code.y, -dist) %&amp;gt;%
  
  # Only keep most frequent food code per food name
  group_by(Food.Name) %&amp;gt;% count(Food.Name, Food.Code) %&amp;gt;% 
  slice(which.max(n)) %&amp;gt;% select(-n) %&amp;gt;%
  
  # Order food names as in the small table
  arrange(factor(Food.Name, levels = small_tab$Food.Name))

# Show table with columns renamed
joined_tab %&amp;gt;%
  rename(&amp;#39;Food Name&amp;#39; = Food.Name, 
         &amp;#39;Food Code&amp;#39; = Food.Code) %&amp;gt;%
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Food Name&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Food Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Corn&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;532&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Squash&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;111&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Peppers&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;654&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;sup&gt;Created on 2023-05-31 with &lt;a href=&#34;https://reprex.tidyverse.org&#34;&gt;reprex v2.0.2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A new function to plot convergence diagnostics from lme4::allFit()</title>
      <link>https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/</link>
      <pubDate>Thu, 22 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/index_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/index_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/index_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;


&lt;p&gt;Linear mixed-effects models (LMM) offer a consistent way of performing regression and analysis of variance tests which allows accounting for non-independence in the data. Over the past decades, LMMs have subsumed most of the General Linear Model, with a steady increase in popularity (Meteyard &amp;amp; Davies, 2020). Since their conception, LMMs have presented the challenge of model &lt;em&gt;convergence&lt;/em&gt;. In essence, the issue of convergence boils down to the widespread tension between parsimony and completeness in data analysis. That is, on the one hand, a good model must allow an accurate, parsimonious analysis of each predictor, and thus, it must not be overfitted with too many parameters. Yet, on the other hand, the model must be complete enough to account for a sufficient amount of variation in the data. In LMMs, any predictors that entail non-independent observations (also known as repeated measures) will normally bring both fixed and random effects into the model. Where a few of these predictors coexist, models often struggle to find enough information in the data to account for every predictor—and especially, for every random effect. This difficulty translates into convergence warnings (Brauer &amp;amp; Curtin, 2018; Singmann &amp;amp; Kellen, 2019). In this article, I review the issue of convergence before presenting a new plotting function in R that facilitates the diagnosis of convergence by visualising the fixed effects fitted by different optimization algorithms (also dubbed optimizers).&lt;/p&gt;
&lt;div id=&#34;completeness-versus-parsimony&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Completeness versus parsimony&lt;/h2&gt;
&lt;p&gt;Both fixed and random effects comprise intercepts and slopes. The pressure exerted by each of those types of effects on the model is determined by the number of data points involved by each. First, slopes are more demanding than intercepts, as they involve a (far) larger number of data points. Second, random effects are more demanding than fixed effects, as random effects entail the number of estimates required for fixed effects &lt;em&gt;times&lt;/em&gt; the number of levels in the grouping factor. As a result, on the most lenient end of the scale lies the fixed intercept, and on the heaviest end lie the random slopes. Convergence warnings in LMMs are often due to the random slopes alone.&lt;/p&gt;
&lt;p&gt;Sounds easy, then! Not inviting the random slopes to the party should solve the problem. Indeed, since random slopes involve the highest number of estimates by far, removing them does often remove convergence warnings. This, however, leads to a different problem. Surrendering the information provided by random slopes can result in the violation of the assumption of independence of observations. For years, the removal of random slopes due to convergence warnings was standard practice. Currently, in contrast, proposals increasingly consider other options, such as removing random effects if they do not significantly improve the fit of the model (Matuschek et al., 2017), and keeping the random slopes in the model in spite of the convergence warnings to safeguard the assumption of independence (see Table 17 in Brauer &amp;amp; Curtin, 2018; Singmann &amp;amp; Kellen, 2019).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-multiple-optimizers-sanity-check-from-lme4allfit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The multiple-optimizers sanity check from &lt;code&gt;lme4::allFit()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Framed within the drive to maintain random slopes wherever possible, the developers of the ‘lme4’ package propose a sanity check that uses a part of the ‘lme4’ &lt;em&gt;engine&lt;/em&gt; called ‘optimizer’. Every model has a default optimizer, unless a specific one is chosen through &lt;code&gt;control = lmerControl(optimizer = &#39;...&#39;)&lt;/code&gt; (in lmer models) or &lt;code&gt;control = glmerControl(optimizer = &#39;...&#39;)&lt;/code&gt; (in glmer models). The seven widely-available optimizers are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bobyqa&lt;/li&gt;
&lt;li&gt;Nelder_Mead&lt;/li&gt;
&lt;li&gt;nlminbwrap&lt;/li&gt;
&lt;li&gt;nmkbw&lt;/li&gt;
&lt;li&gt;optimx.L-BFGS-B&lt;/li&gt;
&lt;li&gt;nloptwrap.NLOPT_LN_NELDERMEAD&lt;/li&gt;
&lt;li&gt;nloptwrap.NLOPT_LN_BOBYQA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To assess whether convergence warnings render the results invalid, or on the contrary, the results can be deemed valid in spite of the warnings, Bates et al. (2022) suggest refitting models affected by convergence warnings with a variety of optimizers. The authors argue that if the different optimizers produce practically-equivalent results, the results are valid. The &lt;code&gt;allFit&lt;/code&gt; function from the ‘lme4’ package allows the refitting of models using a number of optimizers. To use the seven optimizers listed above, two extra packages must be installed: ‘dfoptim’ and ‘optimx’ (see &lt;a href=&#34;https://cran.r-project.org/web/packages/lme4/lme4.pdf&#34;&gt;lme4 manual&lt;/a&gt;). The output from &lt;code&gt;allFit()&lt;/code&gt; contains several statistics on the fixed and the random effects fitted by each optimizer (see &lt;a href=&#34;https://github.com/lme4/lme4/issues/512#issue-425198940&#34;&gt;example&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-the-fixed-effects-from-allfit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plotting the fixed effects from allFit()&lt;/h2&gt;
&lt;p&gt;Several R users have ventured into &lt;a href=&#34;https://www.google.com/search?q=%22ggplot%22+%22allfit%22+optimizers&#34;&gt;plotting the allFit() output&lt;/a&gt; but there is not a function in ‘lme4’ yet at the time of writing. Bernabeu (2022) developed a &lt;a href=&#34;https://github.com/pablobernabeu/plot.fixef.allFit/blob/main/plot.fixef.allFit.R&#34;&gt;function&lt;/a&gt; that takes the output from &lt;code&gt;allFit()&lt;/code&gt;, tidies it, selects the fixed effects and plots them using ‘ggplot2’. The function is shown below, and can be copied through the &lt;code&gt;Copy Code&lt;/code&gt; button at the top right corner. It can be renamed by changing &lt;code&gt;plot.fixef.allFit&lt;/code&gt; to another valid name.&lt;/p&gt;
&lt;div style=&#34;height: 800px; border: 0.5px dotted grey; padding: 10px; resize: both; overflow: auto;&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot the results from the fixed effects produced by different optimizers. This function 
# takes the output from lme4::allFit(), tidies it, selects fixed effects and plots them.

plot.fixef.allFit = function(allFit_output, 
                             # Set the same Y axis limits in every plot
                             shared_y_axis_limits = TRUE,
                             # Multiply Y axis limits by a factor (only 
                             # available if shared_y_axis_limits = TRUE)
                             multiply_y_axis_limits = 1, 
                             # Number of decimal places
                             decimal_places = NULL,
                             # Select predictors
                             select_predictors = NULL, 
                             # Number of rows
                             nrow = NULL, 
                             # Y axis title
                             y_title = &amp;#39;Fixed effect&amp;#39;,
                             # Alignment of the Y axis title
                             y_title_hjust = NULL,
                             # Add number to the names of optimizers
                             number_optimizers = TRUE,
                             # Replace colon in interactions with x
                             interaction_symbol_x = TRUE) {
  
  require(lme4)
  require(dfoptim)
  require(optimx)
  require(dplyr)
  require(reshape2)
  require(stringr)
  require(scales)
  require(ggplot2)
  require(ggtext)
  require(patchwork)
  library(Cairo)
  
  # Tidy allFit output
  
  # Extract fixed effects from the allFit() output
  allFit_fixef = summary(allFit_output)$fixef %&amp;gt;%  # Select fixed effects in the allFit results
    reshape2::melt() %&amp;gt;%  # Structure the output as a data frame
    rename(&amp;#39;Optimizer&amp;#39; = &amp;#39;Var1&amp;#39;, &amp;#39;fixed_effect&amp;#39; = &amp;#39;Var2&amp;#39;)  # set informative names
  
  # If number_optimizers = TRUE, assign number to each optimizer and place it before its name
  if(number_optimizers == TRUE) {
    allFit_fixef$Optimizer = paste0(as.numeric(allFit_fixef$Optimizer), &amp;#39;. &amp;#39;, allFit_fixef$Optimizer)
  }
  
  # If select_predictors was supplied, select them along with the intercept (the latter required)
  if(!is.null(select_predictors)) {
    allFit_fixef = allFit_fixef %&amp;gt;% dplyr::filter(fixed_effect %in% c(&amp;#39;(Intercept)&amp;#39;, select_predictors))
  }
  
  # Order variables
  allFit_fixef = allFit_fixef[, c(&amp;#39;Optimizer&amp;#39;, &amp;#39;fixed_effect&amp;#39;, &amp;#39;value&amp;#39;)]
  
  # PLOT. The overall plot is formed of a first row containing the intercept and the legend 
  # (intercept_plot), and a second row containing the predictors (predictors_plot), 
  # which may in turn occupy several rows.
  
  # If multiply_y_axis_limits was supplied but shared_y_axis_limits = FALSE,
  # warn that shared_y_axis_limits is required.
  if(!multiply_y_axis_limits == 1 &amp;amp; shared_y_axis_limits == FALSE) {
    message(&amp;#39;The argument `multiply_y_axis_limits` has not been used because \n it requires `shared_y_axis_limits` set to TRUE.&amp;#39;)
  }
  
  # If extreme values were entered in y_title_hjust, show warning
  if(!is.null(y_title_hjust)) {
    if(y_title_hjust &amp;lt; 0.5 | y_title_hjust &amp;gt; 6) {
      message(&amp;#39;NOTE: For y_title_hjust, a working range of values is between 0.6 and 6.&amp;#39;)
    }
  }
  
  # If decimal_places was supplied, convert number to the format used in &amp;#39;scales&amp;#39; package
  if(!is.null(decimal_places)) {
    decimal_places = 
      ifelse(decimal_places == 1, 0.1, 
             ifelse(decimal_places == 2, 0.01, 
                    ifelse(decimal_places == 3, 0.001, 
                           ifelse(decimal_places == 4, 0.0001, 
                                  ifelse(decimal_places == 5, 0.00001, 
                                         ifelse(decimal_places == 6, 0.000001, 
                                                ifelse(decimal_places == 7, 0.0000001, 
                                                       ifelse(decimal_places == 8, 0.00000001, 
                                                              ifelse(decimal_places == 9, 0.000000001, 
                                                                     ifelse(decimal_places == 10, 0.0000000001,
                                                                            ifelse(decimal_places == 11, 0.00000000001,
                                                                                   ifelse(decimal_places == 12, 0.000000000001,
                                                                                          ifelse(decimal_places == 13, 0.0000000000001,
                                                                                                 ifelse(decimal_places == 14, 0.00000000000001,
                                                                                                        ifelse(decimal_places &amp;gt;= 15, 0.000000000000001, 
                                                                                                               0.001
                                                                                                        )))))))))))))))
  }
  
  # First row: intercept_plot
  
  # Select intercept data only
  intercept = allFit_fixef %&amp;gt;% dplyr::filter(fixed_effect == &amp;#39;(Intercept)&amp;#39;)
  
  intercept_plot = intercept %&amp;gt;%
    ggplot(., aes(fixed_effect, value, colour = Optimizer)) +
    geom_point(position = position_dodge(1)) +
    facet_wrap(~fixed_effect, scale = &amp;#39;free&amp;#39;) +
    guides(colour = guide_legend(title.position = &amp;#39;left&amp;#39;)) +
    theme_bw() + 
    theme(axis.title = element_blank(), axis.ticks.x = element_blank(),
          axis.text.x = element_blank(), 
          strip.text = element_text(size = 10, margin = margin(t = 4, b = 6)),
          strip.background = element_rect(fill = &amp;#39;grey96&amp;#39;),
          legend.margin = margin(0.3, 0, 0.8, 1, &amp;#39;cm&amp;#39;), 
          legend.title = element_text(size = unit(15, &amp;#39;pt&amp;#39;), angle = 90, hjust = 0.5))
  
  # Second row: predictors_plot
  
  # Select all predictors except intercept
  predictors = allFit_fixef %&amp;gt;% dplyr::filter(!fixed_effect == &amp;#39;(Intercept)&amp;#39;)
  
  # If interaction_symbol_x = TRUE (default), replace colon with times symbol x between spaces
  if(interaction_symbol_x == TRUE) {
    # Replace colon in interactions with \u00D7, i.e., x; then set factor class
    predictors$fixed_effect = predictors$fixed_effect %&amp;gt;% 
      str_replace_all(&amp;#39;:&amp;#39;, &amp;#39; \u00D7 &amp;#39;) %&amp;gt;% factor()
  }
  
  # Order predictors as in the original output from lme4::allFit()
  predictors$fixed_effect = factor(predictors$fixed_effect, 
                                   levels = unique(predictors$fixed_effect))
  
  # Set number of rows for the predictors excluding the intercept.
  # First, if nrow argument supplied, use it
  if(!is.null(nrow)) {
    predictors_plot_nrow = nrow - 1  # Subtract 1 as intercept row not considered
    
    # Else, if nrow argument not supplied, calculate sensible number of rows: i.e., divide number of
    # predictors (exc. intercept) by 2 and round up the result. For instance, 7 predictors --&amp;gt; 3 rows
  } else predictors_plot_nrow = (length(unique(predictors$fixed_effect)) / 2) %&amp;gt;% ceiling()
  
  predictors_plot = ggplot(predictors, aes(fixed_effect, value, colour = Optimizer)) +
    geom_point(position = position_dodge(1)) +
    facet_wrap(~fixed_effect, scale = &amp;#39;free&amp;#39;,
               # Note that predictors_plot_nrow was defined a few lines above
               nrow = predictors_plot_nrow, 
               # Wrap names of predictors with more than 54 characters into new lines
               labeller = labeller(fixed_effect = label_wrap_gen(width = 55))) +
    labs(y = y_title) +
    theme_bw() + 
    theme(axis.title.x = element_blank(), axis.text.x = element_blank(),
          axis.ticks.x = element_blank(),
          axis.title.y = ggtext::element_markdown(size = 14, margin = margin(0, 15, 0, 0)),
          strip.text = element_text(size = 10, margin = margin(t = 4, b = 6)),
          strip.background = element_rect(fill = &amp;#39;grey96&amp;#39;), legend.position = &amp;#39;none&amp;#39;)
  
  # Below, the function scale_y_continuous is applied conditionally to avoid overriding settings. First, 
  # if shared_y_axis_limits = TRUE and decimal_places was supplied, set the same Y axis limits in 
  # every plot and set decimal_places. By default, also expand limits by a seventh of its original 
  # limit, and allow further multiplication of limits through multiply_y_axis_limits.
  if(shared_y_axis_limits == TRUE &amp;amp; !is.null(decimal_places)) {
    
    intercept_plot = intercept_plot +
      scale_y_continuous(limits = c(min(allFit_fixef$value) - allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits,
                                    max(allFit_fixef$value) + allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits), 
                         # Set number of decimal places
                         labels = scales::label_number(accuracy = decimal_places))
    
    predictors_plot = predictors_plot + 
      scale_y_continuous(limits = c(min(allFit_fixef$value) - allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits,
                                    max(allFit_fixef$value) + allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits), 
                         # Set number of decimal places
                         labels = scales::label_number(accuracy = decimal_places))
    
    # Else, if shared_y_axis_limits = TRUE but decimal_places were not supplied, do as above but without
    # setting decimal_places.
  } else if(shared_y_axis_limits == TRUE &amp;amp; is.null(decimal_places)) {
    
    intercept_plot = intercept_plot +
      scale_y_continuous(limits = c(min(allFit_fixef$value) - allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits,
                                    max(allFit_fixef$value) + allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits),
                         # Set number of decimal places
                         labels = scales::label_number(accuracy = decimal_places))
    
    predictors_plot = predictors_plot + 
      scale_y_continuous(limits = c(min(allFit_fixef$value) - allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits,
                                    max(allFit_fixef$value) + allFit_fixef$value %&amp;gt;% abs %&amp;gt;% 
                                      max / 7 * multiply_y_axis_limits),
                         # Set number of decimal places
                         labels = scales::label_number(accuracy = decimal_places))
    
    # Else, if shared_y_axis_limits = FALSE and decimal_places was supplied, set decimal_places. 
  } else if(shared_y_axis_limits == FALSE &amp;amp; !is.null(decimal_places)) {
    
    # Set number of decimal places in both plots
    intercept_plot = intercept_plot +
      scale_y_continuous(labels = scales::label_number(accuracy = decimal_places))
    
    predictors_plot = predictors_plot +
      scale_y_continuous(labels = scales::label_number(accuracy = decimal_places))
  }
  
  # Plot matrix: based on number of predictors_plot_nrow, adjust height of Y axis title
  # (unless supplied), and assign space to intercept_plot and predictors_plot
  if(predictors_plot_nrow == 1) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 3.6))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 11, l = 0)      # predictors row(s)
    )
    
  } else if(predictors_plot_nrow == 2) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 1.4))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 16, l = 0)      # predictors row(s)
    )
    
  } else if(predictors_plot_nrow == 3) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 0.92))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 21, l = 0)      # predictors row(s)
    )
    
  } else if(predictors_plot_nrow == 4) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 0.8))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 26, l = 0)      # predictors row(s)
    )
    
  } else if(predictors_plot_nrow == 5) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 0.73))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 31, l = 0)      # predictors row(s)
    )
    
  } else if(predictors_plot_nrow &amp;gt; 5) {
    
    # If y_title_hjust supplied, use it
    if(!is.null(y_title_hjust)) {
      predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = y_title_hjust))
      # Otherwise, set a sensible height
    } else predictors_plot = predictors_plot + 
        theme(axis.title.y = ggtext::element_markdown(hjust = 0.65))
    
    layout = c(
      patchwork::area(t = 1.5, r = 8.9, b = 6.8, l = 0),  # intercept row
      patchwork::area(t = 7.3, r = 9, b = 36, l = 0)      # predictors row(s)
    )
    
    # Also, advise user to consider distributing predictors into several plots
    message(&amp;#39;  Many rows! Consider distributing predictors into several plots \n  using argument `select_predictors`&amp;#39;)
  } 
  
  # Add margin
  predictors_plot = predictors_plot + theme(plot.margin = margin(15, 15, 15, 15))
  
  # Return matrix of plots
  wrap_plots(intercept_plot, predictors_plot, design = layout,
             # The 2 below corresponds to intercept_plot and predictors_plot
             nrow = 2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;optional-arguments&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Optional arguments&lt;/h3&gt;
&lt;p&gt;Below are the optional arguments allowed by the function, with their default values.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Set the same Y axis limits in every plot
shared_y_axis_limits = TRUE,

# Multiply Y axis limits by a factor (only 
# available if shared_y_axis_limits = TRUE)
multiply_y_axis_limits = 1, 

# Number of decimal places
decimal_places = NULL,

# Select predictors
select_predictors = NULL, 

# Number of rows
nrow = NULL, 

# Y axis title
y_title = &amp;#39;Fixed effect&amp;#39;,

# Alignment of the Y axis title
y_title_hjust = NULL,

# Add number to the names of optimizers
number_optimizers = TRUE,

# Replace colon in interactions with x
interaction_symbol_x = TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The argument &lt;code&gt;shared_y_axis_limits&lt;/code&gt; deserves a comment. It allows using the same Y axis limits (i.e., range) in all plots or, alternatively, using plot-specific limits. The parameter is &lt;code&gt;TRUE&lt;/code&gt; by default to prevent overinterpretations of small differences across optimizers (see the first figure below). In contrast, when &lt;code&gt;shared_y_axis_limits = FALSE&lt;/code&gt;, plot-specific limits are used, which results in a narrower range of values in the Y axis (see the second figure below). Since data points will span the entire Y axis in that case, any difference across optimizers—regardless of its relative importance—might be perceived as large, unless the specific range of values in each plot is noticed.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;use-case&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Use case&lt;/h2&gt;
&lt;p&gt;Let’s test the function with a minimal model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create data using code by Ben Bolker from 
# https://stackoverflow.com/a/38296264/7050882

set.seed(101)
spin = runif(600, 1, 24)
reg = runif(600, 1, 15)
ID = rep(c(&amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;,&amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;7&amp;quot;, &amp;quot;8&amp;quot;, &amp;quot;9&amp;quot;, &amp;quot;10&amp;quot;))
day = rep(1:30, each = 10)
testdata &amp;lt;- data.frame(spin, reg, ID, day)
testdata$fatigue &amp;lt;- testdata$spin * testdata$reg/10 * rnorm(30, mean=3, sd=2)

# Model

library(lme4)

fit = lmer(fatigue ~ spin * reg + (1|ID),
           data = testdata, REML = TRUE)

# Refit model using all available algorithms
multi_fit = allFit(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## bobyqa : [OK]
## Nelder_Mead : [OK]
## nlminbwrap : [OK]
## nmkbw : [OK]
## optimx.L-BFGS-B : [OK]
## nloptwrap.NLOPT_LN_NELDERMEAD : [OK]
## nloptwrap.NLOPT_LN_BOBYQA : [OK]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(multi_fit)$fixef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                               (Intercept)      spin       reg  spin:reg
## bobyqa                          -2.975678 0.5926561 0.1437204 0.1834016
## Nelder_Mead                     -2.975675 0.5926559 0.1437202 0.1834016
## nlminbwrap                      -2.975677 0.5926560 0.1437203 0.1834016
## nmkbw                           -2.975678 0.5926561 0.1437204 0.1834016
## optimx.L-BFGS-B                 -2.975680 0.5926562 0.1437205 0.1834016
## nloptwrap.NLOPT_LN_NELDERMEAD   -2.975666 0.5926552 0.1437196 0.1834017
## nloptwrap.NLOPT_LN_BOBYQA       -2.975678 0.5926561 0.1437204 0.1834016&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The effects to be visualised are selected below using the argument &lt;code&gt;select_predictors&lt;/code&gt;. Notice that the intercept is plotted by default on the first row, along with the legend that lists all the optimizers used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Read in function from GitHub
source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/plot.fixef.allFit/main/plot.fixef.allFit.R&amp;#39;)

plot.fixef.allFit(multi_fit, 
                  
                  select_predictors = c(&amp;#39;spin&amp;#39;, &amp;#39;reg&amp;#39;, &amp;#39;spin:reg&amp;#39;), 
                  
                  # Increase padding at top and bottom of Y axis
                  multiply_y_axis_limits = 1.3,
                  
                  y_title = &amp;#39;Fixed effect (*b*)&amp;#39;,
                  
                  # Align y title
                  y_title_hjust = .9)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 4.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 4.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/index_files/figure-html/demo-plot.fixef.allFit-function-1-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot produced by &lt;code&gt;plot.fixef.allFit()&lt;/code&gt; by default replaces the colons in interaction effects (e.g., &lt;code&gt;spin:reg&lt;/code&gt;) with ’ × ’ to facilitate the visibility (this can be overriden by setting &lt;code&gt;interaction_symbol_x = FALSE&lt;/code&gt;). Yet, it is important to note that any interactions passed to &lt;code&gt;select_predictors&lt;/code&gt; must have the colon, as that is the symbol present in the &lt;code&gt;lme4::allFit()&lt;/code&gt; output.&lt;/p&gt;
&lt;p&gt;The output of &lt;code&gt;plot.fixef.allFit()&lt;/code&gt; is a &lt;a href=&#34;https://ggplot2.tidyverse.org/&#34;&gt;ggplot2&lt;/a&gt; object that can be stored for further use, as in the example below, in which new parameters are used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

plot_fit_convergence = 
  
  plot.fixef.allFit(multi_fit, 
                    
                    select_predictors = c(&amp;#39;spin&amp;#39;, &amp;#39;spin:reg&amp;#39;), 
                    
                    # Use plot-specific Y axis limits
                    shared_y_axis_limits = FALSE,
                    
                    decimal_places = 7, 
                    
                    # Move up Y axis title
                    y_title_hjust = -20,
                    
                    y_title = &amp;#39;Fixed effect (*b*)&amp;#39;)

# Print
plot_fit_convergence&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2023/a-new-function-to-plot-convergence-diagnostics-from-lme4-allfit/index_files/figure-html/demo-plot.fixef.allFit-function-2-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot can be saved to disk as pdf, png, etc. through `ggplot2::ggsave()`
# ggsave(&amp;#39;plot_fit_convergence.pdf&amp;#39;, plot_fit_convergence, 
#        device = cairo_pdf, width = 9, height = 9, dpi = 900)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Bates, D., Maechler, M., Bolker, B., Walker, S., Christensen, R. H. B., Singmann, H., Dai, B., Scheipl, F., Grothendieck, G., Green, P., Fox, J., Bauer, A., &amp;amp; Krivitsky, P. N. (2022). &lt;em&gt;Package ‘lme4’.&lt;/em&gt; CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/lme4/lme4.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/lme4/lme4.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P. (2022). Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://doi.org/10.1037/met0000159&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., &amp;amp; Bates, D. (2017). Balancing type 1 error and power in linear mixed models. &lt;em&gt;Journal of Memory and Language, 94&lt;/em&gt;, 305–315. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2017.01.001&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.jml.2017.01.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meteyard, L., &amp;amp; Davies, R. A. (2020). Best practice guidance for linear mixed-effects models in psychological science. &lt;em&gt;Journal of Memory and Language, 112&lt;/em&gt;, 104092. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2020.104092&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.jml.2020.104092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Singmann, H., &amp;amp; Kellen, D. (2019). An introduction to mixed models for experimental psychology. In D. H. Spieler &amp;amp; E. Schumacher (Eds.), &lt;em&gt;New methods in cognitive psychology&lt;/em&gt; (pp. 4–31). Psychology Press.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Assigning participant-specific parameters automatically in OpenSesame</title>
      <link>https://pablobernabeu.github.io/2023/assigning-participant-specific-parameters-automatically-in-opensesame/</link>
      <pubDate>Wed, 17 May 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/assigning-participant-specific-parameters-automatically-in-opensesame/</guid>
      <description>


&lt;p&gt;OpenSesame offers options to counterbalance properties of the stimulus across participants. However, in cases of more involved assignments of session parameters across participants, it becomes necessary to write a bit of Python code in an inline script, which should be placed at the top of the timeline. In such a script, the participant-specific parameters are loaded in from a csv file. Below is a minimal example of the csv file.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;17%&#34; /&gt;
&lt;col width=&#34;22%&#34; /&gt;
&lt;col width=&#34;20%&#34; /&gt;
&lt;col width=&#34;14%&#34; /&gt;
&lt;col width=&#34;24%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;participant&lt;/th&gt;
&lt;th&gt;language&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;training_list&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;test_list&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;experiment_list&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;7&lt;/td&gt;
&lt;td&gt;Mini-English&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;8&lt;/td&gt;
&lt;td&gt;Mini-Norwegian&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Below is the corresponding inline script. The code &lt;code&gt;.iloc[0]&lt;/code&gt; at the end of the lines is used to select a cell.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Assign participant-specific parameters. For each participant-specific factor in the 
# stimulus files, take the level corresponding to the index position specified in the 
# participant parameters file.

import csv  # handle csv file
import pandas as pd  # handle data frames

participant_parameters = pd.read_csv(exp.get_file(&amp;#39;stimuli/parameters per participant.csv&amp;#39;))

var.participant = var.subject_nr

var.language = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;language&amp;#39;].iloc[0]

var.training_list = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;training_list&amp;#39;].iloc[0]

var.test_list = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;test_list&amp;#39;].iloc[0]

var.experiment_list = participant_parameters.loc[participant_parameters[&amp;#39;participant&amp;#39;] == var.subject_nr][&amp;#39;experiment_list&amp;#39;].iloc[0]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, the variables created in the script can be used in &lt;kbd&gt;Run if&lt;/kbd&gt; conditions (e.g., &lt;code&gt;[language] == &#39;Mini-English&#39;&lt;/code&gt;), as replacements inside file names (e.g., &lt;code&gt;[language] training, List [training_list].csv&lt;/code&gt;), and as input in sketchpads (e.g., &lt;code&gt;Current list: [training_list]&lt;/code&gt;).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pronominal object clitics in preverbal position are a hard nut to crack for Google Translate</title>
      <link>https://pablobernabeu.github.io/2023/pronominal-object-clitics-in-preverbal-position-are-a-hard-nut-to-crack-for-google-translate/</link>
      <pubDate>Wed, 17 May 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/pronominal-object-clitics-in-preverbal-position-are-a-hard-nut-to-crack-for-google-translate/</guid>
      <description>


&lt;p&gt;Unlike English, some Romance languages not only allow—but sometimes require—pronominal object clitics in preverbal position (Hanson &amp;amp; Carlson, 2014; Labotka et al., 2023). That is, instead of saying &lt;em&gt;La maestra ha detto il nome&lt;/em&gt; (Italian) ‘The teacher has said the name’, Italian allows &lt;em&gt;Il nome lo ha detto la maestra&lt;/em&gt; (literally, ‘The name it has said the teacher’), which could translate as ‘The name has been said by the teacher’, ‘&lt;em&gt;The teacher&lt;/em&gt; has said the name’, or even ‘It is the teacher that has said the name’. The form &lt;em&gt;Il nome lo ha detto la maestra&lt;/em&gt; is a marked phrasing that increases the attention to the agent of the action. Furthermore, when the clitic is in preverbal position, the degree of focus on the agent is also dependent on the context. For instance, the focus is light in &lt;em&gt;Lo ha detto la maestra&lt;/em&gt;, whereas it is stronger in &lt;em&gt;Lo ha detto la maestra, non l’assistente&lt;/em&gt; “It’s the teacher that’s said it, not the assistant”.&lt;/p&gt;
&lt;p&gt;The agent-focus can sometimes be relinquished in translations (‘The teacher has said it’). In other cases, it must be preserved by applying an intonational or written emphasis on the agent (‘&lt;em&gt;The teacher&lt;/em&gt; has said it’), or by implementing the idiomatic form ‘The teacher said’, or a marked syntax—for instance, with an it-cleft, as in “It’s the teacher that’s said it”, or by adding ‘oneself’, as in ‘The teacher said it herself’.&lt;/p&gt;
&lt;p&gt;Now, how does Google Translate (GT) deal with these translations in May 2023? To translate &lt;em&gt;Lo ha detto la maestra&lt;/em&gt;, GT opts for ‘The teacher said’, which is a good, idiomatic option. When it comes to translating to Spanish, GT returns ‘El profesor dijo’, which is the direct equivalent of the English translation.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; This option is valid in some varieties of Spanish in America. Nonetheless, it must be noted that a more direct translation from the Italian form would have been very good in Spanish (&lt;em&gt;Lo ha dicho la maestra&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;GT has greater trouble when the content of the sentence is slightly less frequent. For instance, &lt;em&gt;Lo cerca la maestra&lt;/em&gt; ‘Him is seeking the teacher’ is stripped of its markedness in GT’s rendering in English—i.e., ‘The teacher is looking for him’. Preserving the agent-focus—e.g., “It’s the teacher that’s looking for him”—would require some syntactic liberties, and hence entail some risks. So, playing it safe is understandable. Our next step is checking the translation to some Romance languages that allow the same movement to preverbal position present in the original Italian sentence &lt;em&gt;Lo cerca la maestra&lt;/em&gt;. Aside from GT, the sentence could be well translated into Romanian as &lt;em&gt;Îl caută dăscăliţa&lt;/em&gt;, or into Spanish as &lt;em&gt;Lo busca la profesora&lt;/em&gt;. In contrast, for both translations, GT returns the equivalents of the English translation—i.e., &lt;em&gt;Profesorul îl caută&lt;/em&gt; and &lt;em&gt;El profesor lo busca&lt;/em&gt;, again discarding the focus on the agent—unnecessarily in these cases due to the overlap in the grammars.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;images/Screenshot%202023-05-11%20162627.png&#39; alt=&#39;Suggesting a better translation in Google Translate&#39; style=&#39;margin-top:2px; margin-bottom:-2px;&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size:90%; color: #899499;&#34;&gt;Submitting a better translation is always an option.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In fairness, machine translation is an absolute feat overall, provided enough caution is practised. With the expansion of language models, machine translation is only going to improve. So, how much of a piece of cake will it be for GT to crack some of these syntactic details in time, and to preserve syntactic forms across languages when the systems match?&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Hanson, A. E. S., &amp;amp; Carlson, M. T. (2014). The roles of first language and proficiency in L2 processing of Spanish clitics: Global effects. &lt;em&gt;Language Learning, 64&lt;/em&gt;(2), 310-342. &lt;a href=&#34;https://doi.org/10.1111/lang.12050&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/lang.12050&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Labotka, D., Sabo, E., Bonais, R., Gelman, S. A., &amp;amp; Baptista, M. (2023). Testing the effects of congruence in adult multilingual acquisition with implications for creole genesis. &lt;em&gt;Cognition, 235&lt;/em&gt;, 105387. &lt;a href=&#34;https://doi.org/10.1016/j.cognition.2023.105387&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.cognition.2023.105387&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;We can again ignore the errors of grammatical gender in the translations.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;We can again ignore the errors of grammatical gender in the translations.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Specifying version number in OSF download links</title>
      <link>https://pablobernabeu.github.io/2023/specifying-version-number-in-osf-download-links/</link>
      <pubDate>Tue, 16 May 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/specifying-version-number-in-osf-download-links/</guid>
      <description>


&lt;p&gt;In the preparation of projects, files are often downloaded from OSF. It is good to document the URL addresses that were used for the downloads. These URLs can be provided in a code script (&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/data/semanticpriming_data_preparation.R#L294-L295&#34;&gt;see example&lt;/a&gt;) or in a README file. Better yet, it’s possible to specify the version of each file in the URL. This specification helps reduce the possibility of inaccuracies later, should any files be modified afterwards.&lt;/p&gt;
&lt;p&gt;The versions of files can be consulted on the right-hand side of the file page on OSF, as shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/OSF%20revisions.png&#34; width=&#34;550&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, the appropriate version can be specified in the download link by appending &lt;code&gt;?version=&lt;/code&gt;&lt;strong&gt;X&lt;/strong&gt; at the end. For instance, the seventh version is specified in the link &lt;a href=&#34;https://osf.io/hx6tz/download?version=7&#34;&gt;https://osf.io/hx6tz/download&lt;code&gt;?version=7&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Covariates are necessary to validate the variables of interest and to prevent bogus theories</title>
      <link>https://pablobernabeu.github.io/2023/covariates-are-necessary-to-validate-the-variables-of-interest-and-to-prevent-bogus-theories/</link>
      <pubDate>Mon, 15 May 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/covariates-are-necessary-to-validate-the-variables-of-interest-and-to-prevent-bogus-theories/</guid>
      <description>


&lt;p&gt;The need for covariates—or &lt;em&gt;nuisance variables&lt;/em&gt;—in statistical analyses is twofold. The first reason is purely statistical and the second reason is academic.&lt;/p&gt;
&lt;p&gt;First, the use of covariates is often necessary when the variable(s) of interest in a study may be connected to, and affected by, some satellite variables (Bottini et al., 2022; Elze et al., 2017; Sassenhagen &amp;amp; Alday, 2016). This complex scenario is the most common one due to the multivariate, dynamic, interactive nature of the real world.&lt;/p&gt;
&lt;p&gt;Second, the use of covariates is often necessary to prevent the development of bogus, redundant theories. Academics are strongly rewarded for developing theories. As we know, wherever there are strong rewards, there are serious risks. An academic could—consciously or not—produce a theory that is too closely related to an existing theory. So closely related are these theories that the second version might not warrant a name of its own. In such a scenario, covariates are useful and indeed necessary to vet the unique nature of the second version. That is, the first and the second version must be tested in the same model, and the variables corresponding to the first version can be construed as &lt;em&gt;covariates&lt;/em&gt;. This allows both the developers of the theories and the readers to compare the effects corresponding to each version of the theory, and to assess the degree of separation between them.&lt;/p&gt;
&lt;p&gt;The perverted use of covariates (Stefan &amp;amp; Schönbrodt, 2023)—however frequent and harmful—stands completely orthogonal to the correct usage of covariates, in the same way that a stethoscope can be used for good or for bad purposes. It would be poorly informed and misleading to conflate the correct and the incorrect uses, or to reject the use of covariates altogether due to the incorrect uses.&lt;/p&gt;
&lt;p&gt;In conclusion, the effects of interest in correlational/observational studies can be subject to mediation and moderation by satellite variables. These variables cannot be manipulated in correlational/observational studies, but they can—and often should—be included as covariates in the statistical models, to ward off spurious results and to vet similar theories.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Bottini, R., Morucci, P., D’Urso, A., Collignon, O., &amp;amp; Crepaldi, D. (2022). The concreteness advantage in lexical decision does not depend on perceptual simulations. &lt;em&gt;Journal of Experimental Psychology: General, 151&lt;/em&gt;(3), 731–738. &lt;a href=&#34;https://doi.org/10.1037/xge0001090&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/xge0001090&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Elze, M. C., Gregson, J., Baber, U., Williamson, E., Sartori, S., Mehran, R., Nichols, M., Stone, G. W., &amp;amp; Pocock, S. J. (2017). Comparison of propensity score methods and covariate adjustment: Evaluation in 4 cardiovascular studies. &lt;em&gt;Journal of the American College of Cardiology, 69&lt;/em&gt;(3), 345-357. &lt;a href=&#34;https://doi.org/10.1016/j.jacc.2016.10.060&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.jacc.2016.10.060&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Sassenhagen, J., &amp;amp; Alday, P. M. (2016). A common misapplication of statistical inference: Nuisance control with null-hypothesis significance tests. &lt;em&gt;Brain and Language, 162&lt;/em&gt;, 42-45. &lt;a href=&#34;https://doi.org/10.1016/j.bandl.2016.08.001&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.bandl.2016.08.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Stefan, A. M., &amp;amp; Schönbrodt, F. D. (2023). Big little lies: A compendium and simulation of p-hacking strategies. &lt;em&gt;Royal Society Open Science, 10&lt;/em&gt;(2), 220346. &lt;a href=&#34;https://doi.org/10.1098/rsos.220346&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1098/rsos.220346&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Cannot open plots created with &lt;code&gt;brms::mcmc_plot&lt;/code&gt; due to lack of &lt;code&gt;discrete_range&lt;/code&gt; function</title>
      <link>https://pablobernabeu.github.io/2023/cannot-open-plots-created-with-brms-mcmc-plot-due-to-lack-of-discrete-range-function/</link>
      <pubDate>Tue, 11 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2023/cannot-open-plots-created-with-brms-mcmc-plot-due-to-lack-of-discrete-range-function/</guid>
      <description>


</description>
    </item>
    
    <item>
      <title>A table of results for Bayesian mixed-effects models: Grouping variables and specifying random slopes</title>
      <link>https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/</link>
      <pubDate>Thu, 29 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;Here I share the format applied to tables presenting the results of &lt;em&gt;Bayesian&lt;/em&gt; models in Bernabeu (2022; the &lt;a href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes&#34;&gt;table for frequentist models is covered in this other post&lt;/a&gt;). The sample table presents a Bayesian mixed-effects model that was fitted using the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34;&gt;&lt;code&gt;brms&lt;/code&gt;&lt;/a&gt; (Bürkner et al., 2022). The mixed effects were driven by the maximal principle (Brauer &amp;amp; Curtin, 2018). The format of the table resembles one of the examples published by the &lt;a href=&#34;https://apastyle.apa.org/style-grammar-guidelines/tables-figures/sample-tables&#34;&gt;American Psychological Association&lt;/a&gt;. However, there are also deviations from those examples. For instance, in the present table, the effects are grouped under informative labels to facilitate the readers’ comprehension, using the &lt;a href=&#34;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&#34;&gt;&lt;code&gt;kableExtra&lt;/code&gt;&lt;/a&gt; package (Zhu, 2022). Furthermore, the random slopes are specified using superscript letters and a footnote. The table can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;loading-packages-and-the-results-of-the-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading packages and the results of the models&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(knitr)
library(tibble)
library(stringr)
library(lmerTest)
library(kableExtra)

# Load Bayesian results summary

semanticpriming_summary_weaklyinformativepriors_exgaussian = 
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_summary_weaklyinformativepriors_exgaussian.rds?raw=true&amp;#39;)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adjusting-the-names-of-the-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adjusting the names of the effects&lt;/h3&gt;
&lt;p&gt;First, to facilitate the understanding of the results, the original names of the effects will be adjusted in the &lt;code&gt;brms&lt;/code&gt; summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Rename effects in plain language and specify the random slopes
# (if any) for each effect, in the footnote. For this purpose, 
# superscripts are added to the names of the appropriate effects.
# 
# In the interactions below, word-level variables are presented 
# first for the sake of consistency (the order does not affect 
# the results in any way). Also in the interactions, double 
# colons are used to inform the &amp;#39;bayesian_model_table&amp;#39; 
# function that the two terms in the interaction must be split 
# into two lines.

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_attentional_control&amp;#39;] = &amp;#39;Attentional control&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_attentional_control&amp;#39;] = &amp;#39;Attentional control&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_vocabulary_size&amp;#39;] = &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_recoded_participant_gender&amp;#39;] = &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_target_word_frequency&amp;#39;] = &amp;#39;Word frequency&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_target_number_syllables&amp;#39;] = &amp;#39;Number of syllables&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_word_concreteness_diff&amp;#39;] = &amp;#39;Word-concreteness difference&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_cosine_similarity&amp;#39;] = &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_visual_rating_diff&amp;#39;] = &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_recoded_interstimulus_interval&amp;#39;] = &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_word_concreteness_diff:z_vocabulary_size&amp;#39;] = 
  &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Word-concreteness difference : SOA&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_participant_gender&amp;#39;] = 
  &amp;#39;Word-concreteness difference : Gender&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_attentional_control:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Attentional control&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_attentional_control:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Attentional control&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_vocabulary_size:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_vocabulary_size:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_recoded_participant_gender:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : Gender&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_recoded_participant_gender:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : Gender&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed)[
  rownames(semanticpriming_summary_weaklyinformativepriors_exgaussian$fixed) == 
    &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian_model_table&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;bayesian_model_table()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;In Bernabeu (2022), the following custom function was used.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Fbayesian_model_table.R%23L3-L181&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;The above function was used to render a PDF output. In the current scenario, however, we have an HTML output. In the above function, the code used for the &lt;span class=&#34;math inline&#34;&gt;\(\widehat{R}\)&lt;/span&gt; tailored to the HTML output (&lt;code&gt;&amp;amp;Rcirc;&lt;/code&gt;) does not render properly.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Fbayesian_model_table.R%23L119-L121&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;Instead, the LaTeX code &lt;code&gt;$\\widehat{R}$&lt;/code&gt; must be used. Therefore, we’ll correct this error and load the function below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function used in the manuscript to present summaries from &amp;#39;brms&amp;#39; models 
# in APA-formatted tables. The only obligatory argument to be supplied is 
# a summary of a &amp;#39;brms&amp;#39; model.

bayesian_model_table = 
  
  function(model_summary, show_intercept = TRUE, select_effects = NULL, 
           order_effects = NULL, format = NULL, 
           
           # If interaction_symbol_x = TRUE, replace double colons with 
           # times symbols followed by line breaks and indentation. 
           # Then, replace single colons with times symbols.
           interaction_symbol_x = FALSE,
           
           caption = &amp;#39;Summary of the lmerTest model.&amp;#39;) {
    
    require(dplyr)
    require(knitr)
    require(tibble)
    require(stringr)
    require(lmerTest)
    require(kableExtra)
    
    # Create data frame
    model_summary = 
      data.frame(Effect = rownames(model_summary$fixed), 
                 Estimate = model_summary$fixed$Estimate, 
                 SE = model_summary$fixed$Est.Error, 
                 CrI_2.5 = model_summary$fixed$`l-95% CI`, 
                 CrI_97.5 = model_summary$fixed$`u-95% CI`, 
                 Rhat = model_summary$fixed$Rhat,
                 row.names = NULL)
    
    # Process credible intervals and present both inside square brackets
    
    model_summary$CrI_2.5 = model_summary$CrI_2.5 %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .) %&amp;gt;% 
      # Remove minus sign from pure zeros
      sub(&amp;#39;-0.00&amp;#39;, &amp;#39;0.00&amp;#39;, .)
    
    model_summary$CrI_97.5 = model_summary$CrI_97.5 %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .) %&amp;gt;% 
      # Remove minus sign from pure zeros
      sub(&amp;#39;-0.00&amp;#39;, &amp;#39;0.00&amp;#39;, .)
    
    model_summary$CrI_95 = paste0(&amp;#39;[&amp;#39;, model_summary$CrI_2.5, &amp;#39;, &amp;#39;, 
                                  model_summary$CrI_97.5, &amp;#39;]&amp;#39;)
    
    # If show_intercept = FALSE, remove it
    if(isFALSE(show_intercept)) {
      model_summary = model_summary %&amp;gt;% filter(!grepl(&amp;#39;Intercept&amp;#39;, Effect))
      
      # Put &amp;#39;Intercept&amp;#39; in parentheses
    } else if(!is.null(model_summary[model_summary$Effect == &amp;#39;Intercept&amp;#39;, &amp;#39;Effect&amp;#39;])) {
      model_summary[model_summary$Effect == &amp;#39;Intercept&amp;#39;, &amp;#39;Effect&amp;#39;] = &amp;#39;(Intercept)&amp;#39;
    }
    
    # If select_effects was supplied, apply it and order effects accordingly
    if(!is.null(select_effects)) {
      model_summary = model_summary %&amp;gt;% filter(Effect %in% select_effects) %&amp;gt;%
        arrange(factor(Effect, levels = select_effects))
    }
    
    # If order_effects was supplied, apply order
    if(!is.null(order_effects)) {
      model_summary = model_summary %&amp;gt;%
        arrange(factor(Effect, levels = order_effects))
    }
    
    # Round other values
    
    model_summary$Estimate = model_summary$Estimate %&amp;gt;% as.numeric %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .) %&amp;gt;% 
      # Remove minus sign from pure zeros
      sub(&amp;#39;-0.00&amp;#39;, &amp;#39;0.00&amp;#39;, .)
    
    model_summary$SE = model_summary$SE %&amp;gt;% as.numeric %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .)
    
    model_summary$Rhat = model_summary$Rhat %&amp;gt;% as.numeric %&amp;gt;% 
      # Round off and keep trailing zeros
      sprintf(&amp;#39;%.2f&amp;#39;, .)
    
    # Order columns
    model_summary = model_summary %&amp;gt;% select(Effect, Estimate, SE, CrI_95, Rhat)
    
    # Right-align all columns after first one
    align = c(&amp;#39;l&amp;#39;, &amp;#39;r&amp;#39;, &amp;#39;r&amp;#39;, &amp;#39;r&amp;#39;, &amp;#39;r&amp;#39;)
    
    # Establish latex or HTML format: if no format supplied, 
    # try to obtain it from knitr, or apply HTML
    if(missing(format) || is.null(format)) {
      if(knitr::is_latex_output()) {
        format = &amp;#39;latex&amp;#39;
      } else format = &amp;#39;html&amp;#39;
    }
    
    # HTML format
    if(format == &amp;#39;html&amp;#39;) {
      
      # If interaction_symbol_x = TRUE, replace double colons with times 
      # symbols followed by line breaks and indentation. Then, replace 
      # single colons with times symbols.
      if(interaction_symbol_x) {
        model_summary$Effect = model_summary$Effect %&amp;gt;% 
          gsub(&amp;#39;::&amp;#39;, &amp;#39; &amp;amp;times; &amp;lt;br&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;#39;, .) %&amp;gt;%
          gsub(&amp;#39;:&amp;#39;, &amp;#39; &amp;amp;times; &amp;#39;, .)
      }
      
      # Output table
      model_summary %&amp;gt;% 
        
        # Remove header of first column and rename other headers
        rename(&amp;#39; &amp;#39; = &amp;#39;Effect&amp;#39;, &amp;#39;&amp;amp;beta;&amp;#39; = &amp;#39;Estimate&amp;#39;, &amp;#39;&amp;lt;i&amp;gt;SE&amp;lt;/i&amp;gt;&amp;#39; = &amp;#39;SE&amp;#39;, 
               &amp;#39;95% CrI&amp;#39; = &amp;#39;CrI_95&amp;#39;, &amp;#39;$\\widehat{R}$&amp;#39; = &amp;#39;Rhat&amp;#39;) %&amp;gt;%
        
        # Present table
        kbl(digits = 2, booktabs = TRUE, escape = FALSE, align = align,
            
            # Caption of the table (default unless specified)
            caption = caption, 
            
            # Disable occasional line gap (https://stackoverflow.com/a/49018919/7050882)
            linesep = &amp;#39;&amp;#39;) %&amp;gt;%
        
        # Apply nice kableExtra format
        kable_styling() %&amp;gt;%
        
        # Center-align header row
        row_spec(0, align = &amp;#39;c&amp;#39;)
      
      # LaTeX format
    } else {
      
      # If interaction_symbol_x = TRUE, replace double colons with times 
      # symbols followed by line breaks and indentation. Then, replace 
      # single colons with times symbols.
      if(interaction_symbol_x) {
        model_summary$Effect = model_summary$Effect %&amp;gt;% 
          gsub(&amp;#39;::&amp;#39;, &amp;#39; $\\\\times$ \n \\\\hspace{0.3cm}&amp;#39;, .) %&amp;gt;%
          gsub(&amp;#39;:&amp;#39;, &amp;#39; $\\\\times$ &amp;#39;, .)
      }
      
      model_summary$Effect = model_summary$Effect %&amp;gt;%
        
        # Escape underscores to avoid error in table
        str_replace_all(&amp;#39;_&amp;#39;, &amp;#39;\\\\_&amp;#39;) %&amp;gt;%
        
        # Allow line breaks in the names of the effects
        # (used in the interactions)
        kableExtra::linebreak(align = &amp;#39;l&amp;#39;)
      
      # Output table
      model_summary %&amp;gt;% 
        
        # Remove header of first column and rename other headers
        rename(&amp;#39; &amp;#39; = &amp;#39;Effect&amp;#39;, &amp;#39;$\\upbeta$&amp;#39; = &amp;#39;Estimate&amp;#39;, &amp;#39;$SE$&amp;#39; = &amp;#39;SE&amp;#39;, 
               &amp;#39;95\\% CrI&amp;#39; = &amp;#39;CrI_95&amp;#39;, &amp;#39;$\\widehat R$&amp;#39; = &amp;#39;Rhat&amp;#39;) %&amp;gt;%
        
        # Present table
        kbl(digits = 2, booktabs = TRUE, escape = FALSE, align = align,
            
            # Caption of the table (default unless specified)
            caption = caption, 
            
            # Disable occasional line gap (https://stackoverflow.com/a/49018919/7050882)
            linesep = &amp;#39;&amp;#39;) %&amp;gt;%
        
        # Apply nice kableExtra format
        kable_styling() %&amp;gt;%
        
        # Center-align header row
        row_spec(0, align = &amp;#39;c&amp;#39;)
    }
  }&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-function-in-use&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function in use&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create table (using custom function from the &amp;#39;R_functions&amp;#39; folder)
bayesian_model_table(
  semanticpriming_summary_weaklyinformativepriors_exgaussian,
  order_effects = c(&amp;#39;(Intercept)&amp;#39;,
                    &amp;#39;Attentional control&amp;#39;,
                    &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Word frequency&amp;#39;,
                    &amp;#39;Number of syllables&amp;#39;,
                    &amp;#39;Word-concreteness difference&amp;#39;,
                    &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;,
                    &amp;#39;Word-concreteness difference : SOA&amp;#39;,
                    &amp;#39;Word-concreteness difference : Gender&amp;#39;,
                    &amp;#39;Language-based similarity :: Attentional control&amp;#39;,
                    &amp;#39;Visual-strength difference :: Attentional control&amp;#39;,
                    &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;,
                    &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;,
                    &amp;#39;Language-based similarity : Gender&amp;#39;,
                    &amp;#39;Visual-strength difference : Gender&amp;#39;,
                    &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;),
  
  # Replace colons in the names of interactions with times symbols
  interaction_symbol_x = TRUE, 
  
  # No title
  caption = NULL) %&amp;gt;%
  
  # Group predictors under headings
  pack_rows(&amp;#39;Individual differences&amp;#39;, 2, 4) %&amp;gt;% 
  pack_rows(&amp;#39;Target-word lexical covariates&amp;#39;, 5, 6) %&amp;gt;% 
  pack_rows(&amp;#39;Prime--target relationship&amp;#39;, 7, 9) %&amp;gt;% 
  pack_rows(&amp;#39;Task condition&amp;#39;, 10, 10) %&amp;gt;% 
  pack_rows(&amp;#39;Interactions&amp;#39;, 11, 21) %&amp;gt;% 
  
  # Apply white background to override default shading in HTML output
  row_spec(1:21, background = &amp;#39;white&amp;#39;) %&amp;gt;%
  
  # Highlight covariates
  row_spec(c(2, 5:7, 11:15), background = &amp;#39;#FFFFF1&amp;#39;) %&amp;gt;%
  
  # Format
  kable_classic(full_width = FALSE, html_font = &amp;#39;Cambria&amp;#39;) %&amp;gt;%
  
  # Footnote describing abbreviations, random slopes, etc. 
  # LaTeX code used to format the text.
  footnote(escape = FALSE, threeparttable = TRUE, general_title = &amp;#39;&amp;lt;br&amp;gt;&amp;#39;, 
           general = paste(&amp;#39;*Note*. &amp;amp;beta; = Estimate based on $z$-scored predictors; *SE* = standard error;&amp;#39;,
                           &amp;#39;CrI = credible interval. Yellow rows contain covariates. Some interactions are &amp;#39;,
                           &amp;#39;split over two lines, with the second line indented. &amp;lt;br&amp;gt;&amp;#39;, 
                           &amp;#39;&amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt; By-word random slopes were included for this effect.&amp;#39;,
                           &amp;#39;&amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt; By-participant random slopes were included for this effect.&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table lightable-classic&#34; style=&#34;margin-left: auto; margin-right: auto; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;border-bottom: 0;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
β
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;i&gt;SE&lt;/i&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
95% CrI
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\widehat{R}\)&lt;/span&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;background-color: white !important;&#34;&gt;
(Intercept)
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Individual differences&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Vocabulary size &lt;sup&gt;a&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Gender &lt;sup&gt;a&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Target-word lexical covariates&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word frequency
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.12, -0.11]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Number of syllables
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.06, 0.07]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Prime–target relationship&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.07, -0.06]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.01, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;1&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Task condition&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Stimulus onset asynchrony (SOA) &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.02, 0.04]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;11&#34;&gt;
&lt;td colspan=&#34;5&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Interactions&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × SOA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × &lt;br&gt;    Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × &lt;br&gt;    Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × SOA &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × SOA &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.00
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; &#34; colspan=&#34;100%&#34;&gt;
&lt;span style=&#34;font-style: italic;&#34;&gt;&lt;br&gt;&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; &#34; colspan=&#34;100%&#34;&gt;
&lt;sup&gt;&lt;/sup&gt; &lt;em&gt;Note&lt;/em&gt;. β = Estimate based on &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scored predictors; &lt;em&gt;SE&lt;/em&gt; = standard error; CrI = credible interval. Yellow rows contain covariates. Some interactions are split over two lines, with the second line indented. &lt;br&gt; &lt;sup&gt;a&lt;/sup&gt; By-word random slopes were included for this effect. &lt;sup&gt;b&lt;/sup&gt; By-participant random slopes were included for this effect.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
&lt;div id=&#34;shading-specific-rows&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Shading specific rows&lt;/h4&gt;
&lt;p&gt;Shading specific rows is done differently when the output is PDF, as shown below (see p. 170 in &lt;a href=&#34;https://eprints.lancs.ac.uk/id/eprint/177833/6/2022deJuanBernabeuPhD.pdf&#34;&gt;Bernabeu, 2022&lt;/a&gt;).&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fthesis%2Fappendix-E-Bayesian-analysis-results.Rmd%23L164-L165&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Session info&lt;/h3&gt;
&lt;p&gt;If you encounter any blockers while reproducing the table using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;, my current session info may be useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.3.2 (2023-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 11 x64 (build 22621)
## 
## Matrix products: default
## 
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.utf8 
## [2] LC_CTYPE=English_United Kingdom.utf8   
## [3] LC_MONETARY=English_United Kingdom.utf8
## [4] LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.utf8    
## 
## time zone: Europe/Oslo
## tzcode source: internal
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] kableExtra_1.3.4    lmerTest_3.1-3      lme4_1.1-35.1      
## [4] Matrix_1.6-4        stringr_1.5.1       tibble_3.2.1       
## [7] dplyr_1.1.4         knitr_1.45          xaringanExtra_0.7.0
## 
## loaded via a namespace (and not attached):
##  [1] sass_0.4.8          utf8_1.2.4          generics_0.1.3     
##  [4] xml2_1.3.6          blogdown_1.18       stringi_1.8.3      
##  [7] lattice_0.22-5      digest_0.6.33       magrittr_2.0.3     
## [10] evaluate_0.23       grid_4.3.2          bookdown_0.37      
## [13] fastmap_1.1.1       jsonlite_1.8.8      httr_1.4.7         
## [16] rvest_1.0.3         fansi_1.0.6         viridisLite_0.4.2  
## [19] scales_1.3.0        numDeriv_2016.8-1.1 jquerylib_0.1.4    
## [22] cli_3.6.2           rlang_1.1.2         munsell_0.5.0      
## [25] splines_4.3.2       withr_2.5.2         cachem_1.0.8       
## [28] yaml_2.3.8          tools_4.3.2         uuid_1.1-1         
## [31] nloptr_2.0.3        minqa_1.2.6         colorspace_2.1-0   
## [34] ggplot2_3.4.4       webshot_0.5.5       boot_1.3-28.1      
## [37] vctrs_0.6.5         R6_2.5.1            lifecycle_1.0.4    
## [40] MASS_7.3-60         pkgconfig_2.0.3     pillar_1.9.0       
## [43] bslib_0.6.1         gtable_0.3.4        glue_1.6.2         
## [46] Rcpp_1.0.11         systemfonts_1.0.5   xfun_0.41          
## [49] tidyselect_1.2.0    rstudioapi_0.15.0   htmltools_0.5.7    
## [52] nlme_3.1-164        svglite_2.1.3       rmarkdown_2.25     
## [55] compiler_4.3.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://doi.org/10.1037/met0000159&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bürkner, P.-C., Gabry, J., Weber, S., Johnson, A., Modrak, M., Badr, H. S., Weber, F., Ben-Shachar, M. S., &amp;amp; Rabel, H. (2022). &lt;em&gt;Package ’brms’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/brms/brms.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhu, H. (2022). &lt;em&gt;Package ’kableExtra’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A table of results for frequentist mixed-effects models: Grouping variables and specifying random slopes</title>
      <link>https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/</link>
      <pubDate>Thu, 29 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-frequentist-mixed-effects-models-grouping-variables-and-specifying-random-slopes/index.en_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;Here I share the format applied to tables presenting the results of &lt;em&gt;frequentist&lt;/em&gt; models in Bernabeu (2022; the &lt;a href=&#34;https://pablobernabeu.github.io/2022/a-table-of-results-for-bayesian-mixed-effects-models-grouping-variables-and-specifying-random-slopes&#34;&gt;table for Bayesian models is covered in this other post&lt;/a&gt;). The sample table presents a mixed-effects model that was fitted using the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; (Kuznetsova et al., 2022). The mixed effects were driven by the maximal principle (Brauer &amp;amp; Curtin, 2018). The format of the table resembles one of the examples published by the &lt;a href=&#34;https://apastyle.apa.org/style-grammar-guidelines/tables-figures/sample-tables&#34;&gt;American Psychological Association&lt;/a&gt;. However, there are also deviations from those examples. For instance, in the present table, the effects are grouped under informative labels to facilitate the readers’ comprehension, using the &lt;a href=&#34;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&#34;&gt;&lt;code&gt;kableExtra&lt;/code&gt;&lt;/a&gt; package (Zhu, 2022). Furthermore, the random slopes are specified using superscript letters and a footnote. The table can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;loading-packages-and-the-results-of-the-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading packages and the results of the models&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(knitr)
library(tibble)
library(stringr)
library(lmerTest)
library(kableExtra)

# Load frequentist coefficients (estimates and confidence intervals)

KR_summary_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/KR_summary_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

confint_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/confint_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adjusting-the-names-of-the-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adjusting the names of the effects&lt;/h3&gt;
&lt;p&gt;First, to facilitate the understanding of the results, the original names of the effects will be adjusted in the &lt;code&gt;lmerTest&lt;/code&gt; summary and in the confidence intervals object.&lt;/p&gt;
&lt;p&gt;Incidentally, the confidence intervals were obtained using the &lt;code&gt;confint.merMod&lt;/code&gt; function from the &lt;code&gt;lme4&lt;/code&gt; package, as neither &lt;code&gt;lmerTest&lt;/code&gt; nor &lt;code&gt;lme4&lt;/code&gt; currently provide confidence intervals in their default results output. However, computing the confidence intervals is uncomplicated (&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/semanticpriming_lmerTest.R#L126-L130&#34;&gt;see code&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Rename effects in plain language and specify the random slopes
# (if any) for each effect, in the footnote. For this purpose, 
# superscripts are added to the names of the appropriate effects.
# 
# In the interactions below, word-level variables are presented 
# first for the sake of consistency (the order does not affect 
# the results in any way). Also in the interactions, double 
# colons are used to inform the &amp;#39;frequentist_model_table&amp;#39; 
# function that the two terms in the interaction must be split 
# into two lines.

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_attentional_control&amp;#39;] = &amp;#39;Attentional control&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_vocabulary_size&amp;#39;] = &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_participant_gender&amp;#39;] = &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_target_word_frequency&amp;#39;] = &amp;#39;Word frequency&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_target_number_syllables&amp;#39;] = &amp;#39;Number of syllables&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_word_concreteness_diff&amp;#39;] = &amp;#39;Word-concreteness difference&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_cosine_similarity&amp;#39;] = &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_visual_rating_diff&amp;#39;] = &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_interstimulus_interval&amp;#39;] = &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_word_concreteness_diff:z_vocabulary_size&amp;#39;] = 
  &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Word-concreteness difference : SOA&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_participant_gender&amp;#39;] = 
  &amp;#39;Word-concreteness difference : Gender&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_attentional_control:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Attentional control&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_attentional_control:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Attentional control&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_vocabulary_size:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_vocabulary_size:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_participant_gender:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : Gender&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_participant_gender:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : Gender&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(KR_summary_semanticpriming_lmerTest$coefficients)[
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) == 
    &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;


# Next, change the names in the confidence intervals object

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_attentional_control&amp;#39;] = &amp;#39;Attentional control&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_vocabulary_size&amp;#39;] = &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_participant_gender&amp;#39;] = &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_target_word_frequency&amp;#39;] = &amp;#39;Word frequency&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_target_number_syllables&amp;#39;] = &amp;#39;Number of syllables&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_word_concreteness_diff&amp;#39;] = &amp;#39;Word-concreteness difference&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_cosine_similarity&amp;#39;] = &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_visual_rating_diff&amp;#39;] = &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_interstimulus_interval&amp;#39;] = &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_word_concreteness_diff:z_vocabulary_size&amp;#39;] = 
  &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_interstimulus_interval&amp;#39;] = 
  &amp;#39;Word-concreteness difference : SOA&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_word_concreteness_diff:z_recoded_participant_gender&amp;#39;] = 
  &amp;#39;Word-concreteness difference : Gender&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_attentional_control:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Attentional control&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_attentional_control:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Attentional control&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_vocabulary_size:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_vocabulary_size:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_participant_gender:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : Gender&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_participant_gender:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : Gender&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;] = 
  &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;

rownames(confint_semanticpriming_lmerTest)[
  rownames(confint_semanticpriming_lmerTest) == 
    &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;] = 
  &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;frequentist_model_table&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;frequentist_model_table()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The following custom function was used.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Ffrequentist_model_table.R%23L3-L224&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;loading-the-function-from-github&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Loading the function from GitHub&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/raw/main/R_functions/frequentist_model_table.R&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-function-in-use&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function in use&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create table (using custom function from the &amp;#39;R_functions&amp;#39; folder)
frequentist_model_table(
  KR_summary_semanticpriming_lmerTest, 
  confint_semanticpriming_lmerTest,
  order_effects = c(&amp;#39;(Intercept)&amp;#39;,
                    &amp;#39;Attentional control&amp;#39;,
                    &amp;#39;Vocabulary size &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Gender &amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Word frequency&amp;#39;,
                    &amp;#39;Number of syllables&amp;#39;,
                    &amp;#39;Word-concreteness difference&amp;#39;,
                    &amp;#39;Language-based similarity &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Visual-strength difference &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Stimulus onset asynchrony (SOA) &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Word-concreteness difference :: Vocabulary size&amp;#39;,
                    &amp;#39;Word-concreteness difference : SOA&amp;#39;,
                    &amp;#39;Word-concreteness difference : Gender&amp;#39;,
                    &amp;#39;Language-based similarity :: Attentional control&amp;#39;,
                    &amp;#39;Visual-strength difference :: Attentional control&amp;#39;,
                    &amp;#39;Language-based similarity :: Vocabulary size&amp;#39;,
                    &amp;#39;Visual-strength difference :: Vocabulary size&amp;#39;,
                    &amp;#39;Language-based similarity : Gender&amp;#39;,
                    &amp;#39;Visual-strength difference : Gender&amp;#39;,
                    &amp;#39;Language-based similarity : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;,
                    &amp;#39;Visual-strength difference : SOA &amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt;&amp;#39;),
  
  # Replace colons in the names of interactions with times symbols
  interaction_symbol_x = TRUE, 
  
  # No title
  caption = NULL) %&amp;gt;%
  
  # Group predictors under headings
  pack_rows(&amp;#39;Individual differences&amp;#39;, 2, 4) %&amp;gt;% 
  pack_rows(&amp;#39;Target-word lexical covariates&amp;#39;, 5, 6) %&amp;gt;% 
  pack_rows(&amp;#39;Prime--target relationship&amp;#39;, 7, 9) %&amp;gt;% 
  pack_rows(&amp;#39;Task condition&amp;#39;, 10, 10) %&amp;gt;% 
  pack_rows(&amp;#39;Interactions&amp;#39;, 11, 21) %&amp;gt;% 
  
  # Apply white background to override default shading in HTML output
  row_spec(1:21, background = &amp;#39;white&amp;#39;) %&amp;gt;%
  
  # Highlight covariates
  row_spec(c(2, 5:7, 11:15), background = &amp;#39;#FFFFF1&amp;#39;) %&amp;gt;%
  
  # Format
  kable_classic(full_width = FALSE, html_font = &amp;#39;Cambria&amp;#39;) %&amp;gt;%
  
  # Footnote describing abbreviations, random slopes, etc. 
  # LaTeX code used to format the text.
  footnote(escape = FALSE, threeparttable = TRUE, general_title = &amp;#39;&amp;lt;br&amp;gt;&amp;#39;, 
           general = paste(&amp;#39;*Note*. &amp;amp;beta; = Estimate based on $z$-scored predictors; *SE* = standard error;&amp;#39;,
                           &amp;#39;CI = confidence interval. Yellow rows contain covariates. Some interactions are &amp;#39;,
                           &amp;#39;split over two lines, with the second line indented. &amp;lt;br&amp;gt;&amp;#39;, 
                           &amp;#39;&amp;lt;sup&amp;gt;a&amp;lt;/sup&amp;gt; By-word random slopes were included for this effect.&amp;#39;,
                           &amp;#39;&amp;lt;sup&amp;gt;b&amp;lt;/sup&amp;gt; By-participant random slopes were included for this effect.&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table lightable-classic&#34; style=&#34;margin-left: auto; margin-right: auto; font-family: Cambria; width: auto !important; margin-left: auto; margin-right: auto;border-bottom: 0;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;text-align: center;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
β
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;i&gt;SE&lt;/i&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
95% CI
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;i&gt;t&lt;/i&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;text-align: center;&#34;&gt;
&lt;i&gt;p&lt;/i&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;background-color: white !important;&#34;&gt;
(Intercept)
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.112
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Individual differences&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.56
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.577
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Vocabulary size &lt;sup&gt;a&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.02
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.987
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Gender &lt;sup&gt;a&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.03
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.979
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Target-word lexical covariates&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word frequency
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.16, -0.15]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-49.40
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Number of syllables
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.07, 0.08]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
22.81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;3&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Prime–target relationship&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.01, 0.02]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
3.48
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.08, -0.07]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-22.44
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.01, 0.02]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
4.18
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;1&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Task condition&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Stimulus onset asynchrony (SOA) &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.04, 0.07]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
7.47
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
&amp;lt;.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;11&#34;&gt;
&lt;td colspan=&#34;6&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Interactions&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
1.31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.189
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × SOA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
2.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.010
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Word-concreteness difference × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.97
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.332
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × &lt;br&gt;    Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
-2.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.014
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;background-color: #FFFFF1 !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × &lt;br&gt;    Attentional control
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
[0.00, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
0.24
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;background-color: #FFFFF1 !important;&#34;&gt;
.810
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-2.34
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.020
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × &lt;br&gt;    Vocabulary size
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-1.37
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.172
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-0.79
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.433
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
1.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.144
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Language-based similarity × SOA &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[0.00, 0.01]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
3.22
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.001
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 2em;background-color: white !important;&#34; indentlevel=&#34;1&#34;&gt;
Visual-strength difference × SOA &lt;sup&gt;b&lt;/sup&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
[-0.01, 0.00]
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
-2.25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;background-color: white !important;&#34;&gt;
.025
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; &#34; colspan=&#34;100%&#34;&gt;
&lt;span style=&#34;font-style: italic;&#34;&gt;&lt;br&gt;&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; &#34; colspan=&#34;100%&#34;&gt;
&lt;sup&gt;&lt;/sup&gt; &lt;em&gt;Note&lt;/em&gt;. β = Estimate based on &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scored predictors; &lt;em&gt;SE&lt;/em&gt; = standard error; CI = confidence interval. Yellow rows contain covariates. Some interactions are split over two lines, with the second line indented. &lt;br&gt; &lt;sup&gt;a&lt;/sup&gt; By-word random slopes were included for this effect. &lt;sup&gt;b&lt;/sup&gt; By-participant random slopes were included for this effect.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
&lt;div id=&#34;shading-specific-rows&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Shading specific rows&lt;/h4&gt;
&lt;p&gt;Shading specific rows is done differently when the output is PDF, as shown below (see p. 62 in &lt;a href=&#34;https://eprints.lancs.ac.uk/id/eprint/177833/6/2022deJuanBernabeuPhD.pdf&#34;&gt;Bernabeu, 2022&lt;/a&gt;).&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fthesis%2FChapter-3-Study-2.Rmd%23L690-L691&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Session info&lt;/h3&gt;
&lt;p&gt;If you encounter any blockers while reproducing the table using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;, my current session info may be useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.3.2 (2023-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 11 x64 (build 22621)
## 
## Matrix products: default
## 
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.utf8 
## [2] LC_CTYPE=English_United Kingdom.utf8   
## [3] LC_MONETARY=English_United Kingdom.utf8
## [4] LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.utf8    
## 
## time zone: Europe/Oslo
## tzcode source: internal
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] kableExtra_1.3.4    lmerTest_3.1-3      lme4_1.1-35.1      
## [4] Matrix_1.6-4        stringr_1.5.1       tibble_3.2.1       
## [7] dplyr_1.1.4         knitr_1.45          xaringanExtra_0.7.0
## 
## loaded via a namespace (and not attached):
##  [1] sass_0.4.8          utf8_1.2.4          generics_0.1.3     
##  [4] xml2_1.3.6          blogdown_1.18       stringi_1.8.3      
##  [7] lattice_0.22-5      digest_0.6.33       magrittr_2.0.3     
## [10] evaluate_0.23       grid_4.3.2          bookdown_0.37      
## [13] fastmap_1.1.1       jsonlite_1.8.8      httr_1.4.7         
## [16] rvest_1.0.3         fansi_1.0.6         viridisLite_0.4.2  
## [19] scales_1.3.0        numDeriv_2016.8-1.1 jquerylib_0.1.4    
## [22] cli_3.6.2           rlang_1.1.2         munsell_0.5.0      
## [25] splines_4.3.2       withr_2.5.2         cachem_1.0.8       
## [28] yaml_2.3.8          tools_4.3.2         uuid_1.1-1         
## [31] nloptr_2.0.3        minqa_1.2.6         colorspace_2.1-0   
## [34] ggplot2_3.4.4       webshot_0.5.5       boot_1.3-28.1      
## [37] vctrs_0.6.5         R6_2.5.1            lifecycle_1.0.4    
## [40] MASS_7.3-60         pkgconfig_2.0.3     pillar_1.9.0       
## [43] bslib_0.6.1         gtable_0.3.4        glue_1.6.2         
## [46] Rcpp_1.0.11         systemfonts_1.0.5   xfun_0.41          
## [49] tidyselect_1.2.0    rstudioapi_0.15.0   htmltools_0.5.7    
## [52] nlme_3.1-164        svglite_2.1.3       rmarkdown_2.25     
## [55] compiler_4.3.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://doi.org/10.1037/met0000159&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kuznetsova, A., Brockhoff, P. B., &amp;amp; Christensen, R. H. B. (2022). &lt;em&gt;Package ’lmerTest’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zhu, H. (2022). &lt;em&gt;Package ’kableExtra’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Plotting two-way interactions from mixed-effects models using alias variables</title>
      <link>https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-alias-variables/</link>
      <pubDate>Mon, 26 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-alias-variables/</guid>
      <description>


&lt;p&gt;Whereas the direction of main effects can be interpreted from the sign of the estimate, the interpretation of interaction effects often requires plots. This task is facilitated by the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34;&gt;&lt;code&gt;sjPlot&lt;/code&gt;&lt;/a&gt; (Lüdecke, 2022). In Bernabeu (2022), the sjPlot function called &lt;code&gt;plot_model&lt;/code&gt; served as the basis for the creation of some &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/tree/main/R_functions&#34;&gt;custom functions&lt;/a&gt;. One of these functions is &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/R_functions/alias_interaction_plot.R&#34;&gt;&lt;code&gt;alias_interaction_plot&lt;/code&gt;&lt;/a&gt;, which allows the plotting of interactions between a continuous variable and a categorical variable. Importantly, the categorical variable is replaced with an alias variable. This feature allows the back-transformation of the categorical variable to facilitate the communication of the results, for instance, when the categorical variable was sum-coded, which has been recommended for mixed-effects models (Brauer &amp;amp; Curtin, 2018).&lt;/p&gt;
&lt;p&gt;Below, we’ll use the function with a model fitted using &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; (Kuznetsova et al., 2022), although the function also works with several other models (see &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34;&gt;sjPlot manual&lt;/a&gt;). The plot can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;alias-interaction-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Alias interaction plot&lt;/h2&gt;
&lt;div id=&#34;the-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Falias_interaction_plot.R&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;the-function-in-use&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function in use&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Ffrequentist_analysis%2Fsemanticpriming-interactions-with-SOA.R&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/plots/semanticpriming-interactions-with-SOA.pdf&#34;&gt;&lt;img src=&#34;Screenshot%202022-12-27%20234345.png&#34; width=&#34;550&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://doi.org/10.1037/met0000159&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kuznetsova, A., Brockhoff, P. B., &amp;amp; Christensen, R. H. B. (2022). &lt;em&gt;Package ’lmerTest’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lüdecke, D. (2022). &lt;em&gt;Package ’sjPlot’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Plotting two-way interactions from mixed-effects models using ten or six bins</title>
      <link>https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-ten-or-six-bins/</link>
      <pubDate>Mon, 26 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2022/plotting-two-way-interactions-from-mixed-effects-models-using-ten-or-six-bins/</guid>
      <description>


&lt;p&gt;Whereas the direction of main effects can be interpreted from the sign of the estimate, the interpretation of interaction effects often requires plots. This task is facilitated by the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34;&gt;&lt;code&gt;sjPlot&lt;/code&gt;&lt;/a&gt; (Lüdecke, 2022). In Bernabeu (2022), the sjPlot function called &lt;code&gt;plot_model&lt;/code&gt; served as the basis for the creation of some &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/tree/main/R_functions&#34;&gt;custom functions&lt;/a&gt;. Two of these functions are &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/R_functions/deciles_interaction_plot.R&#34;&gt;&lt;code&gt;deciles_interaction_plot&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/R_functions/sextiles_interaction_plot.R&#34;&gt;&lt;code&gt;sextiles_interaction_plot&lt;/code&gt;&lt;/a&gt;. These functions allow the plotting of interactions between two continuous variables. In the case of &lt;code&gt;deciles_interaction_plot&lt;/code&gt;, one of the variables is divided into ten bins, known as deciles, and the other variable is unchanged. In the case of &lt;code&gt;sextiles_interaction_plot&lt;/code&gt;, one of the variables is divided into six bins, or sextiles, and the other variable is unchanged. Sample size per bin can be printed in the legend, which is particularly useful for research involving individual differences.&lt;/p&gt;
&lt;p&gt;Below, we’ll use these functions with models fitted using &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; (Kuznetsova et al., 2022), although the functions also work with several other models (see &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34;&gt;sjPlot manual&lt;/a&gt;). The plots can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;deciles-interaction-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Deciles interaction plot&lt;/h2&gt;
&lt;div id=&#34;the-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Fdeciles_interaction_plot.R&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;the-function-in-use&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function in use&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Ffrequentist_analysis%2Fsemanticpriming-interactions-with-vocabulary-size.R&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/plots/semanticpriming-interactions-with-vocabulary-size.pdf&#34;&gt;&lt;img src=&#34;Screenshot%202022-12-27%20234321.png&#34; width=&#34;580&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sextiles-interaction-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sextiles interaction plot&lt;/h2&gt;
&lt;div id=&#34;the-function-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Fsextiles_interaction_plot.R&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;the-function-in-use-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The function in use&lt;/h3&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Flexicaldecision%2Ffrequentist_analysis%2Flexicaldecision-interactions-with-vocabulary-age.R&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/lexicaldecision/frequentist_analysis/plots/lexicaldecision-interactions-with-vocabulary-age.pdf&#34;&gt;&lt;img src=&#34;Screenshot%202022-12-27%20234252.png&#34; width=&#34;650&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kuznetsova, A., Brockhoff, P. B., &amp;amp; Christensen, R. H. B. (2022). &lt;em&gt;Package ’lmerTest’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lüdecke, D. (2022). &lt;em&gt;Package ’sjPlot’&lt;/em&gt;. CRAN. &lt;a href=&#34;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/sjPlot/sjPlot.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why can&#39;t we be friends? Plotting frequentist (lmerTest) and Bayesian (brms) mixed-effects models</title>
      <link>https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/</link>
      <pubDate>Fri, 23 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/index.en_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;


&lt;p&gt;Frequentist and Bayesian statistics are sometimes regarded as fundamentally different philosophies. Indeed, can both methods qualify as philosophies, or is one of them just a pointless ritual? Is frequentist statistics about &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; values only? Are frequentist estimates diametrically opposed to Bayesian posterior distributions? Are confidence intervals and credible intervals irreconcilable? Will R crash if &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34;&gt;&lt;code&gt;brms&lt;/code&gt;&lt;/a&gt; are simultaneously loaded? If only we could fit frequentist and Bayesian models to the same data and plot the results together, we might get a glimpse into these puzzles.&lt;/p&gt;
&lt;p&gt;All the analyses shown below can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;. The combination of the frequentist and the Bayesian estimates in the same plot is achieved using the following custom function from &lt;a href=&#34;https://pablobernabeu.github.io/publication/pablo-bernabeu-2022-phd-thesis&#34;&gt;Bernabeu (2022)&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;visualising-frequentist-and-bayesian-estimates-in-one-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualising frequentist and Bayesian estimates in one plot&lt;/h3&gt;
&lt;p&gt;Both frequentist and Bayesian statistics offer the options of hypothesis testing and parameter estimation (Cumming, 2014; Kruschke &amp;amp; Liddell, 2018; Rouder et al., 2018; Schmalz et al., 2022; Tendeiro &amp;amp; Kiers, 2019, 2022; van Ravenzwaaij &amp;amp; Wagenmakers, 2022). In the statistical analyses conducted by Bernabeu (2022), hypothesis testing was performed within the frequentist framework, whereas parameter estimation was performed within both the frequentist and the Bayesian frameworks (for other examples of the &lt;em&gt;estimation&lt;/em&gt; approach, see Milek et al., 2018; Pregla et al., 2021; Rodríguez-Ferreiro et al., 2020).&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2FR_functions%2Ffrequentist_bayesian_plot.R%23L3-L179&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;p&gt;Let’s load the function from GitHub and put it to the test.&lt;/p&gt;
&lt;div style=&#34;height: 800px; border: 0.5px dotted grey; padding: 10px; resize: both; overflow: auto;&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Presenting the frequentist and the Bayesian estimates in the same plot. 
# For this purpose, the frequentist results are merged into a plot from 
# brms::mcmc_plot()

source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/main/R_functions/frequentist_bayesian_plot.R&amp;#39;)

# install.packages(&amp;#39;devtools&amp;#39;)
# library(devtools)
# install_version(&amp;#39;tidyverse&amp;#39;, &amp;#39;1.3.1&amp;#39;)  # Due to breaking changes, Version 1.3.1 is required.
# install_version(&amp;#39;ggplot2&amp;#39;, &amp;#39;3.3.5&amp;#39;)  # Due to breaking changes, Version 3.3.5 is required.
library(tidyverse)
library(ggplot2)
library(Cairo)

# Load frequentist coefficients (estimates and confidence intervals)

KR_summary_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/KR_summary_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

confint_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/confint_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

# Below are the default names of the effects
# rownames(KR_summary_semanticpriming_lmerTest$coefficients)
# rownames(confint_semanticpriming_lmerTest)

# Load Bayesian posterior distributions

semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian = 
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian.rds?raw=true&amp;#39;)))

# Below are the default names of the effects
# levels(semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter)


# Reorder the components of interactions in the frequentist results to match 
# with the order present in the Bayesian results.

rownames(KR_summary_semanticpriming_lmerTest$coefficients) =
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;, 
              replacement = &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;, 
              replacement = &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;)

rownames(confint_semanticpriming_lmerTest)  = 
  rownames(confint_semanticpriming_lmerTest) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;, 
              replacement = &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;, 
              replacement = &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;)


# Create a vector containing the names of the effects. This vector will be passed 
# to the plotting function.

new_labels = 
  
  semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter %&amp;gt;% 
  unique %&amp;gt;%
  
  # Remove the default &amp;#39;b_&amp;#39; from the beginning of each effect
  str_remove(&amp;#39;^b_&amp;#39;) %&amp;gt;%
  
  # Put Intercept in parentheses
  str_replace(pattern = &amp;#39;Intercept&amp;#39;, replacement = &amp;#39;(Intercept)&amp;#39;) %&amp;gt;%
  
  # First, adjust names of variables (both in main effects and in interactions)
  str_replace(pattern = &amp;#39;z_target_word_frequency&amp;#39;,
              replacement = &amp;#39;Target-word frequency&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_target_number_syllables&amp;#39;,
              replacement = &amp;#39;Number of target-word syllables&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_word_concreteness_diff&amp;#39;,
              replacement = &amp;#39;Word-concreteness difference&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_cosine_similarity&amp;#39;,
              replacement = &amp;#39;Language-based similarity&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_visual_rating_diff&amp;#39;,
              replacement = &amp;#39;Visual-strength difference&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_attentional_control&amp;#39;,
              replacement = &amp;#39;Attentional control&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_vocabulary_size&amp;#39;,
              replacement = &amp;#39;Vocabulary size&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_participant_gender&amp;#39;,
              replacement = &amp;#39;Gender&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval&amp;#39;,
              replacement = &amp;#39;SOA&amp;#39;) %&amp;gt;%
  # Show acronym in main effect of SOA
  str_replace(pattern = &amp;#39;^SOA$&amp;#39;,
              replacement = &amp;#39;Stimulus onset asynchrony (SOA)&amp;#39;) %&amp;gt;%
  
  # Second, adjust order of effects in interactions. In the output from the model, 
  # the word-level variables of interest (i.e., &amp;#39;z_cosine_similarity&amp;#39; and 
  # &amp;#39;z_visual_rating_diff&amp;#39;) sometimes appeared second in their interactions. For 
  # better consistency, the code below moves those word-level variables (with 
  # their new names) to the first position in their interactions. Note that the 
  # order does not affect the results in any way.
  sub(&amp;#39;(\\w+.*):(Language-based similarity|Visual-strength difference)&amp;#39;, 
      &amp;#39;\\2:\\1&amp;#39;, 
      .) %&amp;gt;%
  
  # Replace colons denoting interactions with times symbols
  str_replace(pattern = &amp;#39;:&amp;#39;, replacement = &amp;#39; &amp;amp;times; &amp;#39;)


# Create plot
plot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian =
  frequentist_bayesian_plot(KR_summary_semanticpriming_lmerTest,
                            confint_semanticpriming_lmerTest,
                            semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian,
                            labels = new_labels, interaction_symbol_x = TRUE,
                            vertical_line_at_x = 0, x_title = &amp;#39;Effect size (&amp;amp;beta;)&amp;#39;,
                            legend_ncol = 1) + 
  theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models/index.en_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Frequentist and Bayesian estimates are not so polar opposites, are they? What is more, the larger differences between some estimates are the result of the priors that were set on the corresponding effects. With uninformative priors, the frequentist and the Bayesian estimates are virtually identical.&lt;/p&gt;
&lt;script src=&#39;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Fbayesian_analysis%2Fsemanticpriming_brms_weaklyinformativepriors_exgaussian.R%23L16-L35&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#39;&gt;&lt;/script&gt;
&lt;p&gt;Now it’s time to consider in earnest:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Is frequentist statistics about &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; values only? Are frequentist estimates diametrically opposed to Bayesian posterior distributions? Are confidence intervals and credible intervals irreconcilable? Will R crash if &lt;a href=&#34;https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf&#34;&gt;&lt;code&gt;lmerTest&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34;&gt;&lt;code&gt;brms&lt;/code&gt;&lt;/a&gt; are simultaneously loaded?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Session info&lt;/h3&gt;
&lt;p&gt;If you encounter any blockers while reproducing the above analyses using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;, my current session info may be useful. For instance, the legend of the plot may not show if the latest versions of the &lt;code&gt;ggplot2&lt;/code&gt; and the &lt;code&gt;tidyverse&lt;/code&gt; packages are used. Instead, &lt;code&gt;ggplot2 3.3.5&lt;/code&gt; and &lt;code&gt;tidyverse 1.3.1&lt;/code&gt; should be installed using &lt;code&gt;install_version(&#39;ggplot2&#39;, &#39;3.3.5&#39;)&lt;/code&gt; and &lt;code&gt;install_version(&#39;tidyverse&#39;, &#39;1.3.1&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 22621)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.utf8 
## [2] LC_CTYPE=English_United Kingdom.utf8   
## [3] LC_MONETARY=English_United Kingdom.utf8
## [4] LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] ggtext_0.1.2        Cairo_1.6-0         forcats_1.0.0      
##  [4] stringr_1.5.0       dplyr_1.1.1         purrr_1.0.1        
##  [7] readr_2.1.4         tidyr_1.3.0         tibble_3.2.1       
## [10] ggplot2_3.3.5       tidyverse_1.3.1     knitr_1.42         
## [13] xaringanExtra_0.7.0
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.10      lubridate_1.9.2  digest_0.6.31    utf8_1.2.3      
##  [5] plyr_1.8.8       R6_2.5.1         cellranger_1.1.0 ggridges_0.5.4  
##  [9] backports_1.4.1  reprex_2.0.2     evaluate_0.21    highr_0.10      
## [13] httr_1.4.6       blogdown_1.16    pillar_1.9.0     rlang_1.1.0     
## [17] uuid_1.1-0       readxl_1.4.2     rstudioapi_0.14  jquerylib_0.1.4 
## [21] rmarkdown_2.21   labeling_0.4.2   munsell_0.5.0    gridtext_0.1.5  
## [25] broom_1.0.4      compiler_4.2.3   modelr_0.1.11    xfun_0.38       
## [29] pkgconfig_2.0.3  htmltools_0.5.5  tidyselect_1.2.0 bookdown_0.33.3 
## [33] fansi_1.0.4      crayon_1.5.2     tzdb_0.4.0       dbplyr_2.3.2    
## [37] withr_2.5.0      commonmark_1.9.0 grid_4.2.3       jsonlite_1.8.4  
## [41] gtable_0.3.3     lifecycle_1.0.3  DBI_1.1.3        magrittr_2.0.3  
## [45] scales_1.2.1     cli_3.4.1        stringi_1.7.12   cachem_1.0.7    
## [49] farver_2.1.1     fs_1.6.1         xml2_1.3.3       bslib_0.4.2     
## [53] generics_0.1.3   vctrs_0.6.1      tools_4.2.3      glue_1.6.2      
## [57] markdown_1.5     hms_1.1.3        fastmap_1.1.1    yaml_2.3.7      
## [61] timechange_0.2.0 colorspace_2.1-0 rvest_1.0.3      haven_2.5.2     
## [65] sass_0.4.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: Multilevel analysis and statistical power&lt;/em&gt;. Lancaster University. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34; class=&#34;uri&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cumming, G. (2014). The new statistics: Why and how. &lt;em&gt;Psychological Science, 25&lt;/em&gt;(1), 7–29. &lt;a href=&#34;https://doi.org/10.1177/0956797613504966&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/0956797613504966&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kruschke, J. K., &amp;amp; Liddell, T. M. (2018). The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 25&lt;/em&gt;(1), 178–206.&lt;/p&gt;
&lt;p&gt;Milek, A., Butler, E. A., Tackman, A. M., Kaplan, D. M., Raison, C. L., Sbarra, D. A., Vazire, S., &amp;amp; Mehl, M. R. (2018). “Eavesdropping on happiness” revisited: A pooled, multisample replication of the association between life satisfaction and observed daily conversation quantity and quality. &lt;em&gt;Psychological Science, 29&lt;/em&gt;(9), 1451–1462. &lt;a href=&#34;https://doi.org/10.1177/0956797618774252&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/0956797618774252&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pregla, D., Lissón, P., Vasishth, S., Burchert, F., &amp;amp; Stadie, N. (2021). Variability in sentence comprehension in aphasia in German. &lt;em&gt;Brain and Language, 222&lt;/em&gt;, 105008. &lt;a href=&#34;https://doi.org/10.1016/j.bandl.2021.105008&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1016/j.bandl.2021.105008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rodríguez-Ferreiro, J., Aguilera, M., &amp;amp; Davies, R. (2020). Semantic priming and schizotypal personality: Reassessing the link between thought disorder and enhanced spreading of semantic activation. &lt;em&gt;PeerJ, 8&lt;/em&gt;, e9511. &lt;a href=&#34;https://doi.org/10.7717/peerj.9511&#34; class=&#34;uri&#34;&gt;https://doi.org/10.7717/peerj.9511&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Rouder, J. N., Haaf, J. M., &amp;amp; Vandekerckhove, J. (2018). Bayesian inference for psychology, part IV: Parameter estimation and Bayes factors. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 25&lt;/em&gt;(1), 102–113. &lt;a href=&#34;https://doi.org/10.3758/s13423-017-1420-7&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13423-017-1420-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Schmalz, X., Biurrun Manresa, J., &amp;amp; Zhang, L. (2021). What is a Bayes factor? &lt;em&gt;Psychological Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1037/met0000421&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000421&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tendeiro, J. N., &amp;amp; Kiers, H. A. L. (2019). A review of issues about null hypothesis Bayesian testing. &lt;em&gt;Psychological Methods, 24&lt;/em&gt;(6), 774–795. &lt;a href=&#34;https://doi.org/10.1037/met0000221&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000221&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tendeiro, J. N., &amp;amp; Kiers, H. A. L. (2022). On the white, the black, and the many shades of gray in between: Our reply to van Ravenzwaaij and Wagenmakers (2021). &lt;em&gt;Psychological Methods, 27&lt;/em&gt;(3), 466–475. &lt;a href=&#34;https://doi.org/10.1037/met0000505&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000505&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;van Ravenzwaaij, D., &amp;amp; Wagenmakers, E.-J. (2022). Advantages masquerading as “issues” in Bayesian hypothesis testing: A commentary on Tendeiro and Kiers (2019). &lt;em&gt;Psychological Methods, 27&lt;/em&gt;(3), 451–465. &lt;a href=&#34;https://doi.org/10.1037/met0000415&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/met0000415&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian workflow: Prior determination, predictive checks and sensitivity analyses</title>
      <link>https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/</link>
      <pubDate>Thu, 22 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/index.en_files/clipboard/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/index.en_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;window.xaringanExtraClipboard(null, {&#34;button&#34;:&#34;Copy Code&#34;,&#34;success&#34;:&#34;Copied!&#34;,&#34;error&#34;:&#34;Press Ctrl+C to Copy&#34;})&lt;/script&gt;


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
library(ggridges)
library(ggtext)
library(patchwork)
library(papaja)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This post presents a code-through of a Bayesian workflow in R, which can be reproduced using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;. The content is &lt;em&gt;closely&lt;/em&gt; based on &lt;span class=&#34;citation&#34;&gt;Bernabeu (&lt;a href=&#34;#ref-bernabeu2022a&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;, which was in turn based on lots of other references. In addition to those, you may wish to consider &lt;span class=&#34;citation&#34;&gt;Nicenboim et al. (&lt;a href=&#34;#ref-nicenboimInprep&#34;&gt;n.d.&lt;/a&gt;)&lt;/span&gt;, a book in preparation that is already available online (&lt;a href=&#34;https://vasishth.github.io/bayescogsci/book&#34; class=&#34;uri&#34;&gt;https://vasishth.github.io/bayescogsci/book&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In &lt;span class=&#34;citation&#34;&gt;Bernabeu (&lt;a href=&#34;#ref-bernabeu2022a&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;, a Bayesian analysis was performed to complement the estimates that had been obtained in the frequentist analysis. Whereas the goal of the frequentist analysis had been hypothesis testing, for which &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; values were used, the goal of the Bayesian analysis was parameter estimation. Accordingly, we estimated the posterior distribution of every effect, without calculating Bayes factors &lt;span class=&#34;citation&#34;&gt;(for other examples of the same &lt;em&gt;estimation approach&lt;/em&gt;, see &lt;a href=&#34;#ref-milekEavesdroppingHappinessRevisited2018&#34;&gt;Milek et al., 2018&lt;/a&gt;; &lt;a href=&#34;#ref-preglaVariabilitySentenceComprehension2021&#34;&gt;Pregla et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;; for comparisons between estimation and hypothesis testing, see &lt;a href=&#34;#ref-cummingNewStatisticsWhy2014&#34;&gt;Cumming, 2014&lt;/a&gt;; &lt;a href=&#34;#ref-kruschkeBayesianNewStatistics2018&#34;&gt;Kruschke &amp;amp; Liddell, 2018&lt;/a&gt;; &lt;a href=&#34;#ref-rouderBayesianInferencePsychology2018&#34;&gt;Rouder et al., 2018&lt;/a&gt;; &lt;a href=&#34;#ref-schmalzWhatBayesFactor2021&#34;&gt;Schmalz et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-tendeiroReviewIssuesNull2019&#34;&gt;Tendeiro &amp;amp; Kiers, 2019&lt;/a&gt;, &lt;a href=&#34;#ref-tendeiroOnTheWhite2022&#34;&gt;in press&lt;/a&gt;; &lt;a href=&#34;#ref-vanravenzwaaijAdvantagesMasqueradingIssues2021&#34;&gt;van Ravenzwaaij &amp;amp; Wagenmakers, 2021&lt;/a&gt;)&lt;/span&gt;. In the estimation approach, the estimates are interpreted by considering the position of their credible intervals in relation to the expected effect size. That is, the closer an interval is to an effect size of 0, the smaller the effect of that predictor. For instance, an interval that is symmetrically centred on 0 indicates a very small effect, whereas—in comparison—an interval that does not include 0 at all indicates a far larger effect.&lt;/p&gt;
&lt;p&gt;This analysis served two purposes: first, to ascertain the interpretation of the smaller effects—which were identified as unreliable in the power analyses—, and second, to complement the estimates obtained in the frequentist analysis. The latter purpose was pertinent because the frequentist models presented convergence warnings—even though it must be noted that a previous study found that frequentist and Bayesian estimates were similar despite convergence warnings appearing in the frequentist analysis &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;)&lt;/span&gt;. Furthermore, the complementary analysis was pertinent because the frequentist models presented residual errors that deviated from normality—even though mixed-effects models are fairly robust to such a deviation &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kniefViolatingNormalityAssumption2021&#34;&gt;Knief &amp;amp; Forstmeier, 2021&lt;/a&gt;; &lt;a href=&#34;#ref-schielzethRobustnessLinearMixed2020&#34;&gt;Schielzeth et al., 2020&lt;/a&gt;)&lt;/span&gt;. Owing to these precedents, we expected to find broadly similar estimates in the frequentist analyses and in the Bayesian ones. Across studies, each frequentist model has a Bayesian counterpart, with the exception of the secondary analysis performed in Study 2.1 (semantic priming) that included &lt;code&gt;vision-based similarity&lt;/code&gt; as a predictor. The R package ‘brms’, Version 2.17.0, was used for the Bayesian analysis &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burknerAdvancedBayesianMultilevel2018&#34;&gt;Bürkner, 2018&lt;/a&gt;; &lt;a href=&#34;#ref-burknerPackageBrms2022&#34;&gt;Bürkner et al., 2022&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Priors&lt;/h2&gt;
&lt;p&gt;Priors are one of the hardest nuts to crack in Bayesian statistics. First, it can be useful to inspect what priors can be set in the model. Second, it is important to visualise a reasonable set of priors based on the available literature or any other available sources. Third, just before fitting the model, the adequacy of a range of priors should be assessed using prior predictive checks. Fourth, posterior predictive checks were performed to assess the consistency between the observed data and new data predicted by the posterior distributions. Fifth, the influence of the priors on the results should be assessed through a prior sensitivity analysis &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-leeBayesianCognitiveModeling2014&#34;&gt;Lee &amp;amp; Wagenmakers, 2014&lt;/a&gt;; &lt;a href=&#34;#ref-schootBayesianStatisticsModelling2021&#34;&gt;Van de Schoot et al., 2021&lt;/a&gt;; also see &lt;a href=&#34;#ref-bernabeu2022a&#34;&gt;Bernabeu, 2022&lt;/a&gt;; &lt;a href=&#34;#ref-preglaVariabilitySentenceComprehension2021&#34;&gt;Pregla et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34;&gt;Stone et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneInteractionGrammaticallyDistinct2021&#34;&gt;Stone et al., 2021&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;checking-what-priors-can-be-set&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Checking what priors can be set&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;brms::get_prior&lt;/code&gt; function can be used to check what effects in the model can be assigned a prior. The output (see &lt;a href=&#34;http://paul-buerkner.github.io/brms/reference/get_prior.html&#34;&gt;example&lt;/a&gt;) will include the current (perhaps default) prior on each effect.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Fbayesian_analysis%2Fprior_predictive_checks%2Fsemanticpriming_priorpredictivecheck_informativepriors.R%23L28-L100&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;determining-the-priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Determining the priors&lt;/h2&gt;
&lt;p&gt;The priors were established by inspecting the effect sizes obtained in previous studies as well as the effect sizes obtained in our frequentist analyses of the present data (reported in &lt;a href=&#34;https://pablobernabeu.github.io/thesis/study-2.1-semantic-priming.html#study-2.1-semantic-priming&#34;&gt;Studies 2.1, 2.2 and 2.3&lt;/a&gt;). In the first regard, the previous studies that were considered were selected because the experimental paradigms, variables and analytical procedures they had used were similar to those used in our current studies. Specifically, regarding paradigms, we sought studies that implemented: (I) semantic priming with a lexical decision task—as in Study 2.1—, (II) semantic decision—as in Study 2.2—, or (III) lexical decision—as in Study 2.3. Regarding analytical procedures, we sought studies in which both the dependent and the independent variables were &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scored. We found two studies that broadly matched these criteria: &lt;span class=&#34;citation&#34;&gt;Lim et al. (&lt;a href=&#34;#ref-lim2020a&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; (see Table 5 therein) and &lt;span class=&#34;citation&#34;&gt;Pexman &amp;amp; Yap (&lt;a href=&#34;#ref-pexman2018a&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; (see Tables 6 and 7 therein). Out of these studies, &lt;span class=&#34;citation&#34;&gt;Pexman &amp;amp; Yap (&lt;a href=&#34;#ref-pexman2018a&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; contained the variables that were most similar to ours, which included vocabulary size (labelled ‘NAART’) and word frequency.&lt;/p&gt;
&lt;p&gt;Based on both these studies and on the &lt;a href=&#34;https://pablobernabeu.github.io/thesis/study-2.1-semantic-priming.html#study-2.1-semantic-priming&#34;&gt;frequentist analyses&lt;/a&gt;, a range of effect sizes was identified that spanned between β = -0.30 and β = 0.30. This range was centred around 0 as the variables were &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scored. The bounds of this range were determined by the largest effects, which appeared in &lt;span class=&#34;citation&#34;&gt;Pexman &amp;amp; Yap (&lt;a href=&#34;#ref-pexman2018a&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;. Pexman et al. conducted a semantic decision study, and split the data set into abstract and concrete words. The two largest effects they found were—first—a word concreteness effect in the concrete-words analysis of β = -0.41, and—second—a word concreteness effect in the abstract-words analysis of β = 0.20. Unlike Pexman et al., we did not split the data set into abstract and concrete words, but analysed these sets together. Therefore, we averaged between the aforementioned values, obtaining a range between β = -0.30 and β = 0.30.&lt;/p&gt;
&lt;p&gt;In the results of &lt;span class=&#34;citation&#34;&gt;Lim et al. (&lt;a href=&#34;#ref-lim2020a&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Pexman &amp;amp; Yap (&lt;a href=&#34;#ref-pexman2018a&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;, and in our frequentist results, some effects consistently presented a negative polarity (i.e., leading to shorter response times), whereas some other effects were consistently positive. We incorporated the direction of effects into the priors only in cases of large effects that had presented a consistent direction (either positive or negative) in previous studies and in our frequentist analyses in the present studies. These criteria were matched by the following variables: word frequency—with a negative direction, as higher word frequency leads to shorter RTs &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-brysbaertImpactWordPrevalence2016&#34;&gt;Brysbaert et al., 2016&lt;/a&gt;; &lt;a href=&#34;#ref-brysbaertWordFrequencyEffect2018a&#34;&gt;Brysbaert et al., 2018&lt;/a&gt;; &lt;a href=&#34;#ref-lim2020a&#34;&gt;Lim et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-mendesPervasiveEffectWord2021&#34;&gt;Mendes &amp;amp; Undorf, 2021&lt;/a&gt;; &lt;a href=&#34;#ref-pexman2018a&#34;&gt;Pexman &amp;amp; Yap, 2018&lt;/a&gt;)&lt;/span&gt;—, number of letters and number of syllables—both with positive directions &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bartonWordlengthEffectReading2014&#34;&gt;Barton et al., 2014&lt;/a&gt;; &lt;a href=&#34;#ref-beyersmannEvidenceEmbeddedWord2020&#34;&gt;Beyersmann et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-pexman2018a&#34;&gt;Pexman &amp;amp; Yap, 2018&lt;/a&gt;)&lt;/span&gt;—, and orthographic Levenshtein distance—with a positive direction &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cerniMotorExpertiseTyping2016&#34;&gt;Cerni et al., 2016&lt;/a&gt;; &lt;a href=&#34;#ref-dijkstraMultilinkComputationalModel2019&#34;&gt;Dijkstra et al., 2019&lt;/a&gt;; &lt;a href=&#34;#ref-kimEffectsLexicalFeatures2018&#34;&gt;Kim et al., 2018&lt;/a&gt;; &lt;a href=&#34;#ref-yarkoniMovingColtheartNew2008&#34;&gt;Yarkoni et al., 2008&lt;/a&gt;)&lt;/span&gt;. We did not incorporate information about the direction of the word concreteness effect, as this effect can follow different directions in abstract and concrete words &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-brysbaert2014a&#34;&gt;Brysbaert et al., 2014&lt;/a&gt;; &lt;a href=&#34;#ref-pexman2018a&#34;&gt;Pexman &amp;amp; Yap, 2018&lt;/a&gt;)&lt;/span&gt;, and we analysed both sets of words together. In conclusion, the four predictors that had directional priors were covariates. All the other predictors had priors centred on 0. Last, as a methodological matter, it is noteworthy that most of the psycholinguistic studies applying Bayesian analysis have not incorporated any directional information in priors &lt;span class=&#34;citation&#34;&gt;(e.g., &lt;a href=&#34;#ref-preglaVariabilitySentenceComprehension2021&#34;&gt;Pregla et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34;&gt;Stone et al., 2020&lt;/a&gt;; cf. &lt;a href=&#34;#ref-stoneInteractionGrammaticallyDistinct2021&#34;&gt;Stone et al., 2021&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;prior-distributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Prior distributions&lt;/h3&gt;
&lt;p&gt;The choice of priors can influence the results in consequential ways. To assess the extent of this influence, &lt;em&gt;prior sensitivity analyses&lt;/em&gt; have been recommended. These analyses are performed by comparing the effect of more and less strict priors—or, in other words, priors varying in their degree of informativeness. The degree of variation is adjusted through the standard deviation, and the means are not varied &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-leeBayesianCognitiveModeling2014&#34;&gt;Lee &amp;amp; Wagenmakers, 2014&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34;&gt;Stone et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-schootBayesianStatisticsModelling2021&#34;&gt;Van de Schoot et al., 2021&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In this way, we compared the results obtained using ‘informative’ priors (&lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; = 0.1), ‘weakly-informative’ priors (&lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; = 0.2) and ‘diffuse’ priors (&lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt; = 0.3). These standard deviations were chosen so that around 95% of values in the informative priors would fall within our initial range of effect sizes that spanned from -0.30 to 0.30. All priors are illustrated in the figure below. These priors resembled others from previous psycholinguistic studies &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-preglaVariabilitySentenceComprehension2021&#34;&gt;Pregla et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34;&gt;Stone et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneInteractionGrammaticallyDistinct2021&#34;&gt;Stone et al., 2021&lt;/a&gt;)&lt;/span&gt;. For instance, &lt;span class=&#34;citation&#34;&gt;Stone et al. (&lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; used the following priors: &lt;span class=&#34;math inline&#34;&gt;\(Normal\)&lt;/span&gt;(0, 0.1), &lt;span class=&#34;math inline&#34;&gt;\(Normal\)&lt;/span&gt;(0, 0.3) and &lt;span class=&#34;math inline&#34;&gt;\(Normal\)&lt;/span&gt;(0, 1). The range of standard deviations we used—i.e., 0.1, 0.2 and 0.3—was narrower than those of previous studies because our dependent variable and our predictors were &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-scored, resulting in small estimates and small &lt;span class=&#34;math inline&#34;&gt;\(SD\)&lt;/span&gt;s &lt;span class=&#34;citation&#34;&gt;(see &lt;a href=&#34;#ref-lim2020a&#34;&gt;Lim et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-pexman2018a&#34;&gt;Pexman &amp;amp; Yap, 2018&lt;/a&gt;)&lt;/span&gt;. These priors were used on the fixed effects and on the standard deviation parameters of the fixed effects. For the correlations among the random effects, an &lt;span class=&#34;math inline&#34;&gt;\(LKJ\)&lt;/span&gt;(2) prior was used &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-lewandowskiGeneratingRandomCorrelation2009&#34;&gt;Lewandowski et al., 2009&lt;/a&gt;)&lt;/span&gt;. This is a ‘regularising’ prior, as it assumes that high correlations among random effects are rare &lt;span class=&#34;citation&#34;&gt;(also used in &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34;&gt;Stone et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneInteractionGrammaticallyDistinct2021&#34;&gt;Stone et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-vasishthBayesianDataAnalysis2018&#34;&gt;Vasishth et al., 2018&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set seed number to ensure exact reproducibility 
# of the random distributions
set.seed(123)

# The code below plots all our types of priors. Each distribution 
# contains 10,000 simulations, resulting in 90,000 rows.

# The green vertical rectangle shows the range of plausible effect 
# sizes based on previous studies that applied a similar analysis 
# (Lim et al., 2020, https://doi.org/10.1177/1747021820906566; 
# Pexman &amp;amp; Yap, 2018, https://doi.org/10.1037/xlm0000499) as 
# well as on the frequentist analyses of the current data.

priors = data.frame(
  
  informativeness = 
    as.factor(c(rep(&amp;#39;Informative priors (*SD* = 0.1)&amp;#39;, 30000),
                rep(&amp;#39;Weakly-informative priors (*SD* = 0.2)&amp;#39;, 30000),
                rep(&amp;#39;Diffuse priors (*SD* = 0.3)&amp;#39;, 30000))), 
  
  direction = as.factor(c(rep(&amp;#39;negative&amp;#39;, 10000), 
                          rep(&amp;#39;neutral&amp;#39;, 10000),
                          rep(&amp;#39;positive&amp;#39;, 10000),
                          rep(&amp;#39;negative&amp;#39;, 10000), 
                          rep(&amp;#39;neutral&amp;#39;, 10000),
                          rep(&amp;#39;positive&amp;#39;, 10000),
                          rep(&amp;#39;negative&amp;#39;, 10000), 
                          rep(&amp;#39;neutral&amp;#39;, 10000),
                          rep(&amp;#39;positive&amp;#39;, 10000))),
  
  direction_and_distribution = 
    as.factor(c(rep(&amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.1)&amp;#39;, 10000), 
                rep(&amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.1)&amp;#39;, 10000),
                rep(&amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.1)&amp;#39;, 10000),
                rep(&amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.2)&amp;#39;, 10000),
                rep(&amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.2)&amp;#39;, 10000),
                rep(&amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.2)&amp;#39;, 10000),
                rep(&amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.3)&amp;#39;, 10000), 
                rep(&amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.3)&amp;#39;, 10000),
                rep(&amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.3)&amp;#39;, 10000))),
  
  estimate = c(rnorm(10000, m = -0.1, sd = 0.1),
               rnorm(10000, m = 0, sd = 0.1),
               rnorm(10000, m = 0.1, sd = 0.1),
               rnorm(10000, m = -0.1, sd = 0.2),
               rnorm(10000, m = 0, sd = 0.2),
               rnorm(10000, m = 0.1, sd = 0.2),
               rnorm(10000, m = -0.1, sd = 0.3),
               rnorm(10000, m = 0, sd = 0.3),
               rnorm(10000, m = 0.1, sd = 0.3))
)

# Order factor levels

priors$informativeness = 
  ordered(priors$informativeness, 
          levels = c(&amp;#39;Informative priors (*SD* = 0.1)&amp;#39;, 
                     &amp;#39;Weakly-informative priors (*SD* = 0.2)&amp;#39;, 
                     &amp;#39;Diffuse priors (*SD* = 0.3)&amp;#39;))

priors$direction = 
  ordered(priors$direction, 
          levels = c(&amp;#39;negative&amp;#39;, &amp;#39;neutral&amp;#39;, &amp;#39;positive&amp;#39;))

priors$direction_and_distribution =
  ordered(priors$direction_and_distribution,
          levels = c(&amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.1)&amp;#39;, 
                     &amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.1)&amp;#39;,
                     &amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.1)&amp;#39;,
                     &amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.2)&amp;#39;, 
                     &amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.2)&amp;#39;,
                     &amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.2)&amp;#39;,
                     &amp;#39;Negative (*M* = -0.1)&amp;lt;br&amp;gt;*Normal*(-0.1, 0.3)&amp;#39;, 
                     &amp;#39;Neutral (*M* = 0)&amp;lt;br&amp;gt;*Normal*(0, 0.3)&amp;#39;,
                     &amp;#39;Positive (*M* = 0.1)&amp;lt;br&amp;gt;*Normal*(0.1, 0.3)&amp;#39;))


# PLOT zone

colours = c(&amp;#39;#7276A2&amp;#39;, &amp;#39;black&amp;#39;, &amp;#39;#A27272&amp;#39;)
fill_colours = c(&amp;#39;#CCCBE7&amp;#39;, &amp;#39;#D7D7D7&amp;#39;, &amp;#39;#E7CBCB&amp;#39;)

# Initialise plot (`aes` specified separately to allow 
# use of `geom_rect` at the end)
ggplot() +
  
  # Turn to the distributions
  stat_density_ridges(data = priors, 
                      aes(x = estimate, y = direction_and_distribution, 
                          color = direction, fill = direction),
                      geom = &amp;#39;density_ridges_gradient&amp;#39;, alpha = 0.7, 
                      jittered_points = TRUE, quantile_lines = TRUE, 
                      quantiles = c(0.025, 0.975), show.legend = F) +
  scale_color_manual(values = colours) + 
  scale_fill_manual(values = fill_colours) + 
  # Adjust X axis to the random distributions obtained
  scale_x_continuous(limits = c(min(priors$estimate), 
                                max(priors$estimate)), 
                     n.breaks = 6, expand = c(0.04, 0.04)) +
  scale_y_discrete(expand = expansion(add = c(0.18, 1.9))) +
  # Facets containing the three models varying in informativeness
  facet_wrap(vars(informativeness), scales = &amp;#39;free&amp;#39;, dir = &amp;#39;v&amp;#39;) +
  # Vertical line at x = 0
  geom_vline(xintercept = 0, linetype = &amp;#39;dashed&amp;#39;, color = &amp;#39;grey50&amp;#39;) +
  xlab(&amp;#39;Effect size (&amp;amp;beta;)&amp;#39;) + 
  ylab(&amp;#39;Direction of the prior and corresponding distribution&amp;#39;) +
  theme_minimal() +
  theme(axis.title.x = ggtext::element_markdown(size = 12, margin = margin(t = 9)),
        axis.text.x = ggtext::element_markdown(size = 11, margin = margin(t = 4)),
        axis.title.y = ggtext::element_markdown(size = 12, margin = margin(r = 9)),
        axis.text.y = ggtext::element_markdown(lineheight = 1.6, colour = colours),
        strip.background = element_rect(fill = &amp;#39;grey98&amp;#39;, colour = &amp;#39;grey90&amp;#39;,
                                        linetype = &amp;#39;solid&amp;#39;),
        strip.text = element_markdown(size = 11, margin = margin(t = 7, b = 7)),
        panel.spacing.y = unit(9, &amp;#39;pt&amp;#39;), panel.grid.minor = element_blank(), 
        plot.margin = margin(8, 8, 9, 8)
  ) +
  
  # Shaded rectangle containing range of previous effects
  geom_rect(data = data.frame(x = 1), xmin = -0.3, xmax = 0.3, 
            ymin = -Inf, ymax = Inf, fill = &amp;#39;darkgreen&amp;#39;, alpha = .3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/index.en_files/figure-html/bayesian-priors-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Priors used in the three studies. The green vertical rectangle shows the range of plausible effect sizes based on previous studies and on our frequentist analyses. In the informative priors, around 95% of the values fall within the range.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;prior-predictive-checks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Prior predictive checks&lt;/h2&gt;
&lt;p&gt;The adequacy of each of these priors was assessed by performing prior predictive checks, in which we compared the observed data to the predictions of the model &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-schootBayesianStatisticsModelling2021&#34;&gt;Van de Schoot et al., 2021&lt;/a&gt;)&lt;/span&gt;. Furthermore, in these checks we also tested the adequacy of two model-wide distributions: the traditional Gaussian distribution (default in most analyses) and an exponentially modified Gaussian—dubbed ‘ex-Gaussian’—distribution &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-matzkePsychologicalInterpretationExGaussian2009&#34;&gt;Matzke &amp;amp; Wagenmakers, 2009&lt;/a&gt;)&lt;/span&gt;. The ex-Gaussian distribution was considered because the residual errors of the frequentist models were not normally distributed &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-loTransformNotTransform2015&#34;&gt;Lo &amp;amp; Andrews, 2015&lt;/a&gt;)&lt;/span&gt;, and because this distribution was found to be more appropriate than the Gaussian one in a previous, related study &lt;span class=&#34;citation&#34;&gt;(see supplementary materials of &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;)&lt;/span&gt;. The ex-Gaussian distribution had an identity link function, which preserves the interpretability of the coefficients, as opposed to a transformation applied directly to the dependent variable &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-loTransformNotTransform2015&#34;&gt;Lo &amp;amp; Andrews, 2015&lt;/a&gt;)&lt;/span&gt;. The results of these prior predictive checks revealed that the priors were adequate, and that the ex-Gaussian distribution was more appropriate than the Gaussian one, converging with &lt;span class=&#34;citation&#34;&gt;Rodríguez-Ferreiro et al. (&lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. Therefore, the ex-Gaussian distribution was used in the final models.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Fbayesian_analysis%2Fprior_predictive_checks%2Fsemanticpriming_priorpredictivecheck_informativepriors.R%23L105-L235&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;models-with-a-gaussian-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Models with a Gaussian distribution&lt;/h3&gt;
&lt;p&gt;The figures below show the prior predictive checks for the Gaussian models. These plots show the maximum, mean and minimum values of the observed data (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) and those of the predicted distribution (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;, which stands for &lt;em&gt;rep&lt;/em&gt;lications of the outcome). The way of interpreting these plots is by comparing the observed data to the predicted distribution. The specifics of this comparison vary across the three plots. First, in the upper plot, which shows the maximum values, the ideal scenario would show the observed maximum value (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) overlapping with the maximum value of the predicted distribution (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;). Second, in the middle plot, showing the mean values, the ideal scenario would show the observed mean value (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) overlapping with the mean value of the predicted distribution (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;). Last, in the lower plot, which shows the minimum values, the ideal scenario would have the observed minimum value (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) overlapping with the minimum value of the predicted distribution (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;). While the overlap need not be absolute, the closer the observed and the predicted values are on the X axis, the better. As such, the three predictive checks below—corresponding to models that used the default Gaussian distribution—show that the priors fitted the data acceptably but not very well.&lt;/p&gt;
&lt;embed src=&#34;https://pablobernabeu.github.io/language-sensorimotor-simulation-PhD-thesis/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_informativepriors.pdf&#34; width=&#34;100%&#34; height=&#34;740&#34; type=&#34;application/pdf&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the Gaussian, informative-prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;embed src=&#34;https://pablobernabeu.github.io/language-sensorimotor-simulation-PhD-thesis/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_weaklyinformativepriors.pdf&#34; width=&#34;100%&#34; height=&#34;740&#34; type=&#34;application/pdf&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the Gaussian, weakly-informative-prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;embed src=&#34;https://pablobernabeu.github.io/language-sensorimotor-simulation-PhD-thesis/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_diffusepriors.pdf&#34; width=&#34;100%&#34; height=&#34;740&#34; type=&#34;application/pdf&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the Gaussian, diffuse-prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;models-with-an-exponentially-modified-gaussian-i.e.-ex-gaussian-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Models with an exponentially-modified Gaussian (i.e., ex-Gaussian) distribution&lt;/h3&gt;
&lt;p&gt;In contrast to the above results, the figures below demonstrate that, when an ex-Gaussian distribution was used, the priors fitted the data far better, which converged with the results of a similar comparison performed by Rodríguez-Ferreiro et al. (2020; see supplementary materials of the latter study).&lt;/p&gt;
&lt;embed src=&#34;https://pablobernabeu.github.io/language-sensorimotor-simulation-PhD-thesis/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_informativepriors_exgaussian.pdf&#34; width=&#34;100%&#34; height=&#34;740&#34; type=&#34;application/pdf&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the ex-Gaussian, informative-prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;embed src=&#34;https://pablobernabeu.github.io/language-sensorimotor-simulation-PhD-thesis/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_weaklyinformativepriors_exgaussian.pdf&#34; width=&#34;100%&#34; height=&#34;740&#34; type=&#34;application/pdf&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the ex-Gaussian, weakly-informative-prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;embed src=&#34;https://pablobernabeu.github.io/language-sensorimotor-simulation-PhD-thesis/semanticpriming/bayesian_analysis/prior_predictive_checks/plots/semanticpriming_priorpredictivecheck_diffusepriors_exgaussian.pdf&#34; width=&#34;100%&#34; height=&#34;740&#34; type=&#34;application/pdf&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior predictive checks for the ex-Gaussian, diffuse-prior model from the semantic priming study. &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; = observed data; &lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt; = predicted data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-predictive-checks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Posterior predictive checks&lt;/h2&gt;
&lt;p&gt;Based on the results from the prior predictive checks, the ex-Gaussian distribution was used in the final models. Next, posterior predictive checks were performed to assess the consistency between the observed data and new data predicted by the posterior distributions &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-schootBayesianStatisticsModelling2021&#34;&gt;Van de Schoot et al., 2021&lt;/a&gt;)&lt;/span&gt;. The figure below presents the posterior predictive checks for the latter models. The interpretation of these plots is simple: the distributions of the observed (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) and the predicted data (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;) should be as similar as possible. As such, the plots below suggest that the results are trustworthy.&lt;/p&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Flanguage-sensorimotor-simulation-PhD-thesis%2Fblob%2Fmain%2Fsemanticpriming%2Fbayesian_analysis%2Fposterior_predictive_checks%2Fsemanticpriming_posteriorpredictivechecks.R%23L3-L68&amp;style=a11y-dark&amp;type=code&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showCopy=on&amp;fetchFromJsDelivr=on&#34;&gt;&lt;/script&gt;
&lt;embed src=&#34;https://pablobernabeu.github.io/language-sensorimotor-simulation-PhD-thesis/semanticpriming/bayesian_analysis/posterior_predictive_checks/plots/semanticpriming_posteriorpredictivechecks_allpriors_exgaussian.pdf&#34; width=&#34;100%&#34; height=&#34;360&#34; type=&#34;application/pdf&#34;&gt;
&lt;blockquote&gt;
&lt;p&gt;Posterior predictive checks for the (ex-Gaussian) models from the semantic priming study. The observed data (&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;) and the predicted data (&lt;span class=&#34;math inline&#34;&gt;\(y_{rep}\)&lt;/span&gt;) almost entirely overlap with each other, demonstrating a very good fit.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prior-sensitivity-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5. Prior sensitivity analysis&lt;/h2&gt;
&lt;p&gt;In the main analysis, the informative, weakly-informative and diffuse priors were used in separate models. In other words, in each model, all priors had the same degree of informativeness &lt;span class=&#34;citation&#34;&gt;(as done in &lt;a href=&#34;#ref-preglaVariabilitySentenceComprehension2021&#34;&gt;Pregla et al., 2021&lt;/a&gt;; &lt;a href=&#34;#ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34;&gt;Rodríguez-Ferreiro et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34;&gt;Stone et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-stoneInteractionGrammaticallyDistinct2021&#34;&gt;Stone et al., 2021&lt;/a&gt;)&lt;/span&gt;. In this way, a prior sensitivity analysis was performed to acknowledge the likely influence of the priors on the posterior distributions—that is, on the results &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-leeBayesianCognitiveModeling2014&#34;&gt;Lee &amp;amp; Wagenmakers, 2014&lt;/a&gt;; &lt;a href=&#34;#ref-stoneEffectDecayLexical2020&#34;&gt;Stone et al., 2020&lt;/a&gt;; &lt;a href=&#34;#ref-schootBayesianStatisticsModelling2021&#34;&gt;Van de Schoot et al., 2021&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We’ll first load a &lt;a href=&#34;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/R_functions/frequentist_bayesian_plot.R&#34;&gt;custom function (&lt;code&gt;frequentist_bayesian_plot&lt;/code&gt;)&lt;/a&gt; from GitHub.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;#39;https://raw.githubusercontent.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/main/R_functions/frequentist_bayesian_plot.R&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;height: 800px; border: 0.5px dotted grey; padding: 10px; resize: both; overflow: auto;&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Presenting the frequentist and the Bayesian estimates in the same plot. 
# For this purpose, the frequentist results are merged into a plot from 
# brms::mcmc_plot()

# install.packages(&amp;#39;devtools&amp;#39;)
# library(devtools)
# install_version(&amp;#39;tidyverse&amp;#39;, &amp;#39;1.3.1&amp;#39;)  # Due to breaking changes, Version 1.3.1 is required.
# install_version(&amp;#39;ggplot2&amp;#39;, &amp;#39;5.3.5&amp;#39;)  # Due to breaking changes, Version 5.3.5 is required.
library(tidyverse)
library(ggplot2)
library(Cairo)

# Load frequentist coefficients (estimates and confidence intervals)

KR_summary_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/KR_summary_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

confint_semanticpriming_lmerTest =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/frequentist_analysis/results/confint_semanticpriming_lmerTest.rds?raw=true&amp;#39;)))

# Below are the default names of the effects
# rownames(KR_summary_semanticpriming_lmerTest$coefficients)
# rownames(confint_semanticpriming_lmerTest)

# Load Bayesian posterior distributions

semanticpriming_posteriordistributions_informativepriors_exgaussian = 
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_posteriordistributions_informativepriors_exgaussian.rds?raw=true&amp;#39;)))

semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian =
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian.rds?raw=true&amp;#39;)))

semanticpriming_posteriordistributions_diffusepriors_exgaussian = 
  readRDS(gzcon(url(&amp;#39;https://github.com/pablobernabeu/language-sensorimotor-simulation-PhD-thesis/blob/main/semanticpriming/bayesian_analysis/results/semanticpriming_posteriordistributions_diffusepriors_exgaussian.rds?raw=true&amp;#39;)))

# Below are the default names of the effects
# levels(semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter)


# Reorder the components of interactions in the frequentist results to match 
# with the order present in the Bayesian results.

rownames(KR_summary_semanticpriming_lmerTest$coefficients) =
  rownames(KR_summary_semanticpriming_lmerTest$coefficients) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;, 
              replacement = &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;, 
              replacement = &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;)

rownames(confint_semanticpriming_lmerTest)  = 
  rownames(confint_semanticpriming_lmerTest) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_cosine_similarity&amp;#39;, 
              replacement = &amp;#39;z_cosine_similarity:z_recoded_interstimulus_interval&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval:z_visual_rating_diff&amp;#39;, 
              replacement = &amp;#39;z_visual_rating_diff:z_recoded_interstimulus_interval&amp;#39;)


# Create a vector containing the names of the effects. This vector will be passed 
# to the plotting function.

new_labels = 
  
  semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian$data$parameter %&amp;gt;% 
  unique %&amp;gt;%
  
  # Remove the default &amp;#39;b_&amp;#39; from the beginning of each effect
  str_remove(&amp;#39;^b_&amp;#39;) %&amp;gt;%
  
  # Put Intercept in parentheses
  str_replace(pattern = &amp;#39;Intercept&amp;#39;, replacement = &amp;#39;(Intercept)&amp;#39;) %&amp;gt;%
  
  # First, adjust names of variables (both in main effects and in interactions)
  str_replace(pattern = &amp;#39;z_target_word_frequency&amp;#39;,
              replacement = &amp;#39;Target-word frequency&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_target_number_syllables&amp;#39;,
              replacement = &amp;#39;Number of target-word syllables&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_word_concreteness_diff&amp;#39;,
              replacement = &amp;#39;Word-concreteness difference&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_cosine_similarity&amp;#39;,
              replacement = &amp;#39;Language-based similarity&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_visual_rating_diff&amp;#39;, 
              replacement = &amp;#39;Visual-strength difference&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_attentional_control&amp;#39;,
              replacement = &amp;#39;Attentional control&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_vocabulary_size&amp;#39;,
              replacement = &amp;#39;Vocabulary size&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_participant_gender&amp;#39;,
              replacement = &amp;#39;Gender&amp;#39;) %&amp;gt;%
  str_replace(pattern = &amp;#39;z_recoded_interstimulus_interval&amp;#39;,
              replacement = &amp;#39;SOA&amp;#39;) %&amp;gt;%
  # Show acronym in main effect of SOA
  str_replace(pattern = &amp;#39;^SOA$&amp;#39;,
              replacement = &amp;#39;Stimulus onset asynchrony (SOA)&amp;#39;) %&amp;gt;%
  
  # Second, adjust order of effects in interactions. In the output from the model, 
  # the word-level variables of interest (i.e., &amp;#39;z_cosine_similarity&amp;#39; and 
  # &amp;#39;z_visual_rating_diff&amp;#39;) sometimes appeared second in their interactions. For 
  # better consistency, the code below moves those word-level variables (with 
  # their new names) to the first position in their interactions. Note that the 
  # order does not affect the results in any way.
  sub(&amp;#39;(\\w+.*):(Language-based similarity|Visual-strength difference)&amp;#39;, 
      &amp;#39;\\2:\\1&amp;#39;, 
      .) %&amp;gt;%
  
  # Replace colons denoting interactions with times symbols
  str_replace(pattern = &amp;#39;:&amp;#39;, replacement = &amp;#39; &amp;amp;times; &amp;#39;) 


# Create plots, beginning with the informative-prior model

plot_semanticpriming_frequentist_bayesian_plot_informativepriors_exgaussian =
  frequentist_bayesian_plot(KR_summary_semanticpriming_lmerTest,
                            confint_semanticpriming_lmerTest,
                            semanticpriming_posteriordistributions_informativepriors_exgaussian,
                            labels = new_labels, interaction_symbol_x = TRUE,
                            vertical_line_at_x = 0, x_title = &amp;#39;Effect size (&amp;amp;beta;)&amp;#39;, 
                            x_axis_labels = 3, note_frequentist_no_prior = TRUE) +
  ggtitle(&amp;#39;Prior *SD* = 0.1&amp;#39;)

#####

plot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian =
  frequentist_bayesian_plot(KR_summary_semanticpriming_lmerTest,
                            confint_semanticpriming_lmerTest,
                            semanticpriming_posteriordistributions_weaklyinformativepriors_exgaussian,
                            labels = new_labels, interaction_symbol_x = TRUE,
                            vertical_line_at_x = 0, x_title = &amp;#39;Effect size (&amp;amp;beta;)&amp;#39;,
                            x_axis_labels = 3, note_frequentist_no_prior = TRUE) +
  ggtitle(&amp;#39;Prior *SD* = 0.2&amp;#39;) +
  theme(axis.text.y = element_blank())

#####

plot_semanticpriming_frequentist_bayesian_plot_diffusepriors_exgaussian =
  frequentist_bayesian_plot(KR_summary_semanticpriming_lmerTest,
                            confint_semanticpriming_lmerTest,
                            semanticpriming_posteriordistributions_diffusepriors_exgaussian,
                            labels = new_labels, interaction_symbol_x = TRUE,
                            vertical_line_at_x = 0, x_title = &amp;#39;Effect size (&amp;amp;beta;)&amp;#39;, 
                            x_axis_labels = 3, note_frequentist_no_prior = TRUE) +
  ggtitle(&amp;#39;Prior *SD* = 0.3&amp;#39;) + 
  theme(axis.text.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The figure below presents the posterior distribution of each effect in each model. The frequentist estimates are also shown to facilitate the comparison.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_semanticpriming_frequentist_bayesian_plot_informativepriors_exgaussian +
    plot_semanticpriming_frequentist_bayesian_plot_weaklyinformativepriors_exgaussian +
    plot_semanticpriming_frequentist_bayesian_plot_diffusepriors_exgaussian +
    
    plot_layout(ncol = 3, guides = &amp;#39;collect&amp;#39;) &amp;amp; theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2022/bayesian-workflow-prior-determination-predictive-checks-and-sensitivity-analyses/index.en_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Estimates from the frequentist analysis (in red) and from the Bayesian analysis (in blue) for the semantic priming study, in each model. The frequentist means (represented by points) are flanked by 95% confidence intervals. The Bayesian means (represented by vertical lines) are flanked by 95% credible intervals in light blue (in some cases, the interval is occluded by the bar of the mean)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;A blog post on the &lt;a href=&#34;https://pablobernabeu.github.io/2022/why-can-t-we-be-friends-plotting-frequentist-lmertest-and-bayesian-brms-mixed-effects-models&#34;&gt;frequentist-Bayesian plots is also available&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;session-info&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Session info&lt;/h3&gt;
&lt;p&gt;If you encounter any blockers while reproduce the above analyses using the materials at &lt;a href=&#34;https://osf.io/gt5uf&#34; class=&#34;uri&#34;&gt;https://osf.io/gt5uf&lt;/a&gt;, my current session info may be useful. For instance, the legend of the last plot may not show if the latest versions of the &lt;code&gt;ggplot2&lt;/code&gt; and the &lt;code&gt;tidyverse&lt;/code&gt; packages are used. Instead, &lt;code&gt;ggplot2 3.3.5&lt;/code&gt; and &lt;code&gt;tidyverse 1.3.1&lt;/code&gt; should be installed using &lt;code&gt;install_version(&#39;ggplot2&#39;, &#39;3.3.5&#39;)&lt;/code&gt; and &lt;code&gt;install_version(&#39;tidyverse&#39;, &#39;1.3.1&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.3 (2023-03-15 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 22621)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.utf8 
## [2] LC_CTYPE=English_United Kingdom.utf8   
## [3] LC_MONETARY=English_United Kingdom.utf8
## [4] LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] Cairo_1.6-0         forcats_1.0.0       stringr_1.5.0      
##  [4] purrr_1.0.1         readr_2.1.4         tidyr_1.3.0        
##  [7] tibble_3.2.1        tidyverse_1.3.1     papaja_0.1.1       
## [10] tinylabels_0.2.3    patchwork_1.1.2     ggtext_0.1.2       
## [13] ggridges_0.5.4      ggplot2_3.3.5       dplyr_1.1.1        
## [16] knitr_1.42          xaringanExtra_0.7.0
## 
## loaded via a namespace (and not attached):
##  [1] fs_1.6.1           lubridate_1.9.2    insight_0.19.2     httr_1.4.6        
##  [5] tools_4.2.3        backports_1.4.1    bslib_0.4.2        utf8_1.2.3        
##  [9] R6_2.5.1           DBI_1.1.3          colorspace_2.1-0   withr_2.5.0       
## [13] tidyselect_1.2.0   emmeans_1.8.7      compiler_4.2.3     cli_3.4.1         
## [17] rvest_1.0.3        xml2_1.3.3         sandwich_3.0-2     labeling_0.4.2    
## [21] bookdown_0.33.3    bayestestR_0.13.1  sass_0.4.6         scales_1.2.1      
## [25] mvtnorm_1.1-3      commonmark_1.9.0   digest_0.6.31      rmarkdown_2.21    
## [29] pkgconfig_2.0.3    htmltools_0.5.5    dbplyr_2.3.2       fastmap_1.1.1     
## [33] highr_0.10         rlang_1.1.0        readxl_1.4.2       rstudioapi_0.14   
## [37] jquerylib_0.1.4    farver_2.1.1       generics_0.1.3     zoo_1.8-11        
## [41] jsonlite_1.8.4     magrittr_2.0.3     parameters_0.21.1  Matrix_1.6-1      
## [45] Rcpp_1.0.10        munsell_0.5.0      fansi_1.0.4        lifecycle_1.0.3   
## [49] stringi_1.7.12     multcomp_1.4-23    yaml_2.3.7         MASS_7.3-60       
## [53] plyr_1.8.8         grid_4.2.3         crayon_1.5.2       lattice_0.21-8    
## [57] haven_2.5.2        splines_4.2.3      gridtext_0.1.5     hms_1.1.3         
## [61] pillar_1.9.0       uuid_1.1-0         markdown_1.5       estimability_1.4.1
## [65] effectsize_0.8.3   codetools_0.2-19   reprex_2.0.2       glue_1.6.2        
## [69] evaluate_0.21      blogdown_1.16      modelr_0.1.11      vctrs_0.6.1       
## [73] tzdb_0.4.0         cellranger_1.1.0   gtable_0.3.3       datawizard_0.8.0  
## [77] cachem_1.0.7       xfun_0.38          xtable_1.8-4       broom_1.0.4       
## [81] survival_3.5-7     timechange_0.2.0   TH.data_1.1-1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-bartonWordlengthEffectReading2014&#34; class=&#34;csl-entry&#34;&gt;
Barton, J. J. S., Hanif, H. M., Eklinder Björnström, L., &amp;amp; Hills, C. (2014). The word-length effect in reading: &lt;span&gt;A&lt;/span&gt; review. &lt;em&gt;Cognitive Neuropsychology&lt;/em&gt;, &lt;em&gt;31&lt;/em&gt;(5-6), 378–412. &lt;a href=&#34;https://doi.org/10.1080/02643294.2014.895314&#34;&gt;https://doi.org/10.1080/02643294.2014.895314&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bernabeu2022a&#34; class=&#34;csl-entry&#34;&gt;
Bernabeu, P. (2022). &lt;em&gt;Language and sensorimotor simulation in conceptual processing: &lt;span&gt;Multilevel&lt;/span&gt; analysis and statistical power&lt;/em&gt;. &lt;span&gt;Lancaster University&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/1795&#34;&gt;https://doi.org/10.17635/lancaster/thesis/1795&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-beyersmannEvidenceEmbeddedWord2020&#34; class=&#34;csl-entry&#34;&gt;
Beyersmann, E., Grainger, J., &amp;amp; Taft, M. (2020). Evidence for embedded word length effects in complex nonwords. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;35&lt;/em&gt;(2), 235–245. &lt;a href=&#34;https://doi.org/10.1080/23273798.2019.1659989&#34;&gt;https://doi.org/10.1080/23273798.2019.1659989&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brysbaertWordFrequencyEffect2018a&#34; class=&#34;csl-entry&#34;&gt;
Brysbaert, M., Mandera, P., &amp;amp; Keuleers, E. (2018). The word frequency effect in word processing: &lt;span&gt;An&lt;/span&gt; updated review. &lt;em&gt;Current Directions in Psychological Science&lt;/em&gt;, &lt;em&gt;27&lt;/em&gt;(1), 45–50. &lt;a href=&#34;https://doi.org/10.1177/0963721417727521&#34;&gt;https://doi.org/10.1177/0963721417727521&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brysbaertImpactWordPrevalence2016&#34; class=&#34;csl-entry&#34;&gt;
Brysbaert, M., Stevens, M., Mandera, P., &amp;amp; Keuleers, E. (2016). The impact of word prevalence on lexical decision times: &lt;span&gt;Evidence&lt;/span&gt; from the &lt;span&gt;Dutch Lexicon Project&lt;/span&gt; 2. &lt;em&gt;Journal of Experimental Psychology: Human Perception and Performance&lt;/em&gt;, &lt;em&gt;42&lt;/em&gt;(3), 441–458. &lt;a href=&#34;https://doi.org/10.1037/xhp0000159&#34;&gt;https://doi.org/10.1037/xhp0000159&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brysbaert2014a&#34; class=&#34;csl-entry&#34;&gt;
Brysbaert, M., Warriner, A. B., &amp;amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known &lt;span&gt;English&lt;/span&gt; word lemmas. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, &lt;em&gt;46&lt;/em&gt;, 904–911. &lt;a href=&#34;https://doi.org/10.3758/s13428-013-0403-5&#34;&gt;https://doi.org/10.3758/s13428-013-0403-5&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerAdvancedBayesianMultilevel2018&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C. (2018). Advanced &lt;span&gt;Bayesian&lt;/span&gt; multilevel modeling with the &lt;span&gt;R&lt;/span&gt; package &lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;. &lt;em&gt;The R Journal&lt;/em&gt;, &lt;em&gt;10&lt;/em&gt;(1), 395–411. &lt;a href=&#34;https://journal.r-project.org/archive/2018/RJ-2018-017/index.html&#34;&gt;https://journal.r-project.org/archive/2018/RJ-2018-017/index.html&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-burknerPackageBrms2022&#34; class=&#34;csl-entry&#34;&gt;
Bürkner, P.-C., Gabry, J., Weber, S., Johnson, A., Modrak, M., Badr, H. S., Weber, F., Ben-Shachar, M. S., &amp;amp; Rabel, H. (2022). &lt;em&gt;Package ’&lt;span class=&#34;nocase&#34;&gt;brms&lt;/span&gt;’&lt;/em&gt;. &lt;span&gt;CRAN&lt;/span&gt;. &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/brms.pdf&#34;&gt;https://cran.r-project.org/web/packages/brms/brms.pdf&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cerniMotorExpertiseTyping2016&#34; class=&#34;csl-entry&#34;&gt;
Cerni, T., Velay, J.-L., Alario, F.-X., Vaugoyeau, M., &amp;amp; Longcamp, M. (2016). Motor expertise for typing impacts lexical decision performance. &lt;em&gt;Trends in Neuroscience and Education&lt;/em&gt;, &lt;em&gt;5&lt;/em&gt;(3), 130–138. &lt;a href=&#34;https://doi.org/10.1016/j.tine.2016.07.007&#34;&gt;https://doi.org/10.1016/j.tine.2016.07.007&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cummingNewStatisticsWhy2014&#34; class=&#34;csl-entry&#34;&gt;
Cumming, G. (2014). The new statistics: &lt;span&gt;Why&lt;/span&gt; and how. &lt;em&gt;Psychological Science&lt;/em&gt;, &lt;em&gt;25&lt;/em&gt;(1), 7–29. &lt;a href=&#34;https://doi.org/10.1177/0956797613504966&#34;&gt;https://doi.org/10.1177/0956797613504966&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-dijkstraMultilinkComputationalModel2019&#34; class=&#34;csl-entry&#34;&gt;
Dijkstra, T., Wahl, A., Buytenhuijs, F., Halem, N. V., Al-Jibouri, Z., Korte, M. D., &amp;amp; Rekké, S. (2019). Multilink: &lt;span&gt;A&lt;/span&gt; computational model for bilingual word recognition and word translation. &lt;em&gt;Bilingualism: Language and Cognition&lt;/em&gt;, &lt;em&gt;22&lt;/em&gt;(4), 657–679. &lt;a href=&#34;https://doi.org/10.1017/S1366728918000287&#34;&gt;https://doi.org/10.1017/S1366728918000287&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kimEffectsLexicalFeatures2018&#34; class=&#34;csl-entry&#34;&gt;
Kim, M., Crossley, S. A., &amp;amp; Skalicky, S. (2018). Effects of lexical features, textual properties, and individual differences on word processing times during second language reading comprehension. &lt;em&gt;Reading and Writing&lt;/em&gt;, &lt;em&gt;31&lt;/em&gt;(5), 1155–1180. &lt;a href=&#34;https://doi.org/10.1007/s11145-018-9833-x&#34;&gt;https://doi.org/10.1007/s11145-018-9833-x&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kniefViolatingNormalityAssumption2021&#34; class=&#34;csl-entry&#34;&gt;
Knief, U., &amp;amp; Forstmeier, W. (2021). Violating the normality assumption may be the lesser of two evils. &lt;em&gt;Behavior Research Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3758/s13428-021-01587-5&#34;&gt;https://doi.org/10.3758/s13428-021-01587-5&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kruschkeBayesianNewStatistics2018&#34; class=&#34;csl-entry&#34;&gt;
Kruschke, J. K., &amp;amp; Liddell, T. M. (2018). The &lt;span&gt;Bayesian New Statistics&lt;/span&gt;: &lt;span&gt;Hypothesis&lt;/span&gt; testing, estimation, meta-analysis, and power analysis from a &lt;span&gt;Bayesian&lt;/span&gt; perspective. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, &lt;em&gt;25&lt;/em&gt;(1), 178–206. &lt;a href=&#34;https://doi.org/10.3758/s13423-016-1221-4&#34;&gt;https://doi.org/10.3758/s13423-016-1221-4&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-leeBayesianCognitiveModeling2014&#34; class=&#34;csl-entry&#34;&gt;
Lee, M. D., &amp;amp; Wagenmakers, E.-J. (2014). &lt;em&gt;Bayesian cognitive modeling: &lt;span&gt;A&lt;/span&gt; practical course&lt;/em&gt;. &lt;span&gt;Cambridge University Press&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1017/CBO9781139087759&#34;&gt;https://doi.org/10.1017/CBO9781139087759&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lewandowskiGeneratingRandomCorrelation2009&#34; class=&#34;csl-entry&#34;&gt;
Lewandowski, D., Kurowicka, D., &amp;amp; Joe, H. (2009). Generating random correlation matrices based on vines and extended onion method. &lt;em&gt;Journal of Multivariate Analysis&lt;/em&gt;, &lt;em&gt;100&lt;/em&gt;(9), 1989–2001. &lt;a href=&#34;https://doi.org/10.1016/j.jmva.2009.04.008&#34;&gt;https://doi.org/10.1016/j.jmva.2009.04.008&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lim2020a&#34; class=&#34;csl-entry&#34;&gt;
Lim, R. Y., Yap, M. J., &amp;amp; Tse, C.-S. (2020). Individual differences in &lt;span&gt;Cantonese Chinese&lt;/span&gt; word recognition: &lt;span&gt;Insights&lt;/span&gt; from the &lt;span&gt;Chinese Lexicon Project&lt;/span&gt;. &lt;em&gt;Quarterly Journal of Experimental Psychology&lt;/em&gt;, &lt;em&gt;73&lt;/em&gt;(4), 504–518. &lt;a href=&#34;https://doi.org/10.1177/1747021820906566&#34;&gt;https://doi.org/10.1177/1747021820906566&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-loTransformNotTransform2015&#34; class=&#34;csl-entry&#34;&gt;
Lo, S., &amp;amp; Andrews, S. (2015). To transform or not to transform: Using generalized linear mixed models to analyse reaction time data. &lt;em&gt;Frontiers in Psychology&lt;/em&gt;, &lt;em&gt;6&lt;/em&gt;, 1171. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2015.01171&#34;&gt;https://doi.org/10.3389/fpsyg.2015.01171&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-matzkePsychologicalInterpretationExGaussian2009&#34; class=&#34;csl-entry&#34;&gt;
Matzke, D., &amp;amp; Wagenmakers, E.-J. (2009). Psychological interpretation of the ex-&lt;span&gt;Gaussian&lt;/span&gt; and shifted &lt;span&gt;Wald&lt;/span&gt; parameters: &lt;span&gt;A&lt;/span&gt; diffusion model analysis. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, &lt;em&gt;16&lt;/em&gt;(5), 798–817. &lt;a href=&#34;https://doi.org/10.3758/PBR.16.5.798&#34;&gt;https://doi.org/10.3758/PBR.16.5.798&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mendesPervasiveEffectWord2021&#34; class=&#34;csl-entry&#34;&gt;
Mendes, P. S., &amp;amp; Undorf, M. (2021). On the pervasive effect of word frequency in metamemory. &lt;em&gt;Quarterly Journal of Experimental Psychology&lt;/em&gt;, 17470218211053329. &lt;a href=&#34;https://doi.org/10.1177/17470218211053329&#34;&gt;https://doi.org/10.1177/17470218211053329&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-milekEavesdroppingHappinessRevisited2018&#34; class=&#34;csl-entry&#34;&gt;
Milek, A., Butler, E. A., Tackman, A. M., Kaplan, D. M., Raison, C. L., Sbarra, D. A., Vazire, S., &amp;amp; Mehl, M. R. (2018). &lt;span&gt;“&lt;span&gt;Eavesdropping&lt;/span&gt; on happiness”&lt;/span&gt; revisited: &lt;span&gt;A&lt;/span&gt; pooled, multisample replication of the association between life satisfaction and observed daily conversation quantity and quality. &lt;em&gt;Psychological Science&lt;/em&gt;, &lt;em&gt;29&lt;/em&gt;(9), 1451–1462. &lt;a href=&#34;https://doi.org/10.1177/0956797618774252&#34;&gt;https://doi.org/10.1177/0956797618774252&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-nicenboimInprep&#34; class=&#34;csl-entry&#34;&gt;
Nicenboim, B., Schad, D., &amp;amp; Vasishth, S. (n.d.). &lt;em&gt;An introduction to &lt;span&gt;Bayesian&lt;/span&gt; data analysis for cognitive science&lt;/em&gt;. &lt;span&gt;Chapman and Hall/CRC Statistics in the Social and Behavioral Sciences Series&lt;/span&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-pexman2018a&#34; class=&#34;csl-entry&#34;&gt;
Pexman, P. M., &amp;amp; Yap, M. J. (2018). Individual differences in semantic processing: &lt;span&gt;Insights&lt;/span&gt; from the &lt;span&gt;Calgary&lt;/span&gt; semantic decision project. &lt;em&gt;Journal of Experimental Psychology: Learning, Memory, and Cognition&lt;/em&gt;, &lt;em&gt;44&lt;/em&gt;(7), 1091–1112. &lt;a href=&#34;https://doi.org/10.1037/xlm0000499&#34;&gt;https://doi.org/10.1037/xlm0000499&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-preglaVariabilitySentenceComprehension2021&#34; class=&#34;csl-entry&#34;&gt;
Pregla, D., Lissón, P., Vasishth, S., Burchert, F., &amp;amp; Stadie, N. (2021). Variability in sentence comprehension in aphasia in &lt;span&gt;German&lt;/span&gt;. &lt;em&gt;Brain and Language&lt;/em&gt;, &lt;em&gt;222&lt;/em&gt;, 105008. &lt;a href=&#34;https://doi.org/10.1016/j.bandl.2021.105008&#34;&gt;https://doi.org/10.1016/j.bandl.2021.105008&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rodriguez-ferreiroSemanticPrimingSchizotypal2020&#34; class=&#34;csl-entry&#34;&gt;
Rodríguez-Ferreiro, J., Aguilera, M., &amp;amp; Davies, R. (2020). Semantic priming and schizotypal personality: Reassessing the link between thought disorder and enhanced spreading of semantic activation. &lt;em&gt;PeerJ&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;, e9511. &lt;a href=&#34;https://doi.org/10.7717/peerj.9511&#34;&gt;https://doi.org/10.7717/peerj.9511&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rouderBayesianInferencePsychology2018&#34; class=&#34;csl-entry&#34;&gt;
Rouder, J. N., Haaf, J. M., &amp;amp; Vandekerckhove, J. (2018). Bayesian inference for psychology, part &lt;span&gt;IV&lt;/span&gt;: &lt;span&gt;Parameter&lt;/span&gt; estimation and &lt;span&gt;Bayes&lt;/span&gt; factors. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, &lt;em&gt;25&lt;/em&gt;(1), 102–113. &lt;a href=&#34;https://doi.org/10.3758/s13423-017-1420-7&#34;&gt;https://doi.org/10.3758/s13423-017-1420-7&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schielzethRobustnessLinearMixed2020&#34; class=&#34;csl-entry&#34;&gt;
Schielzeth, H., Dingemanse, N. J., Nakagawa, S., Westneat, D. F., Allegue, H., Teplitsky, C., Réale, D., Dochtermann, N. A., Garamszegi, L. Z., &amp;amp; Araya‐Ajoy, Y. G. (2020). Robustness of linear mixed‐effects models to violations of distributional assumptions. &lt;em&gt;Methods in Ecology and Evolution&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt;(9), 1141–1152. &lt;a href=&#34;https://doi.org/10.1111/2041-210X.13434&#34;&gt;https://doi.org/10.1111/2041-210X.13434&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schmalzWhatBayesFactor2021&#34; class=&#34;csl-entry&#34;&gt;
Schmalz, X., Biurrun Manresa, J., &amp;amp; Zhang, L. (2021). What is a &lt;span&gt;Bayes&lt;/span&gt; factor? &lt;em&gt;Psychological Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1037/met0000421&#34;&gt;https://doi.org/10.1037/met0000421&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-stoneEffectDecayLexical2020&#34; class=&#34;csl-entry&#34;&gt;
Stone, K., Malsburg, T. von der, &amp;amp; Vasishth, S. (2020). The effect of decay and lexical uncertainty on processing long-distance dependencies in reading. &lt;em&gt;PeerJ&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;, e10438. &lt;a href=&#34;https://doi.org/10.7717/peerj.10438&#34;&gt;https://doi.org/10.7717/peerj.10438&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-stoneInteractionGrammaticallyDistinct2021&#34; class=&#34;csl-entry&#34;&gt;
Stone, K., Veríssimo, J., Schad, D. J., Oltrogge, E., Vasishth, S., &amp;amp; Lago, S. (2021). The interaction of grammatically distinct agreement dependencies in predictive processing. &lt;em&gt;Language, Cognition and Neuroscience&lt;/em&gt;, &lt;em&gt;36&lt;/em&gt;(9), 1159–1179. &lt;a href=&#34;https://doi.org/10.1080/23273798.2021.1921816&#34;&gt;https://doi.org/10.1080/23273798.2021.1921816&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-tendeiroReviewIssuesNull2019&#34; class=&#34;csl-entry&#34;&gt;
Tendeiro, J. N., &amp;amp; Kiers, H. A. L. (2019). A review of issues about null hypothesis &lt;span&gt;Bayesian&lt;/span&gt; testing. &lt;em&gt;Psychological Methods&lt;/em&gt;, &lt;em&gt;24&lt;/em&gt;(6), 774–795. &lt;a href=&#34;https://doi.org/10.1037/met0000221&#34;&gt;https://doi.org/10.1037/met0000221&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-tendeiroOnTheWhite2022&#34; class=&#34;csl-entry&#34;&gt;
Tendeiro, J. N., &amp;amp; Kiers, H. A. L. (in press). On the white, the black, and the many shades of gray in between: &lt;span&gt;Our&lt;/span&gt; reply to van &lt;span&gt;Ravenzwaaij&lt;/span&gt; and &lt;span&gt;Wagenmakers&lt;/span&gt; (2021). &lt;em&gt;Psychological Methods&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-schootBayesianStatisticsModelling2021&#34; class=&#34;csl-entry&#34;&gt;
Van de Schoot, R., Depaoli, S., Gelman, A., King, R., Kramer, B., Märtens, K., Tadesse, M. G., Vannucci, M., Willemsen, J., &amp;amp; Yau, C. (2021). Bayesian statistics and modelling. &lt;em&gt;Nature Reviews Methods Primers&lt;/em&gt;, &lt;em&gt;1&lt;/em&gt;, 3. &lt;a href=&#34;https://doi.org/10.1038/s43586-020-00003-0&#34;&gt;https://doi.org/10.1038/s43586-020-00003-0&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vanravenzwaaijAdvantagesMasqueradingIssues2021&#34; class=&#34;csl-entry&#34;&gt;
van Ravenzwaaij, D., &amp;amp; Wagenmakers, E.-J. (2021). Advantages masquerading as &lt;span&gt;“issues”&lt;/span&gt; in &lt;span&gt;Bayesian&lt;/span&gt; hypothesis testing: &lt;span&gt;A&lt;/span&gt; commentary on &lt;span&gt;Tendeiro&lt;/span&gt; and &lt;span&gt;Kiers&lt;/span&gt; (2019). &lt;em&gt;Psychological Methods&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1037/met0000415&#34;&gt;https://doi.org/10.1037/met0000415&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vasishthBayesianDataAnalysis2018&#34; class=&#34;csl-entry&#34;&gt;
Vasishth, S., Nicenboim, B., Beckman, M. E., Li, F., &amp;amp; Kong, E. J. (2018). Bayesian data analysis in the phonetic sciences: &lt;span&gt;A&lt;/span&gt; tutorial introduction. &lt;em&gt;Journal of Phonetics&lt;/em&gt;, &lt;em&gt;71&lt;/em&gt;, 147–161. &lt;a href=&#34;https://doi.org/10.1016/j.wocn.2018.07.008&#34;&gt;https://doi.org/10.1016/j.wocn.2018.07.008&lt;/a&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-yarkoniMovingColtheartNew2008&#34; class=&#34;csl-entry&#34;&gt;
Yarkoni, T., Balota, D., &amp;amp; Yap, M. J. (2008). Moving beyond &lt;span&gt;Coltheart&lt;/span&gt;’s &lt;span&gt;N&lt;/span&gt;: &lt;span&gt;A&lt;/span&gt; new measure of orthographic similarity. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, &lt;em&gt;15&lt;/em&gt;(5), 971–979. &lt;a href=&#34;https://doi.org/10.3758/PBR.15.5.971&#34;&gt;https://doi.org/10.3758/PBR.15.5.971&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Avoiding (R) Markdown knitting errors using knit_deleting_service_files()</title>
      <link>https://pablobernabeu.github.io/2021/avoiding-knitting-errors-in-r-markdown-using-knit-deleting-service-files/</link>
      <pubDate>Fri, 17 Dec 2021 21:46:06 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2021/avoiding-knitting-errors-in-r-markdown-using-knit-deleting-service-files/</guid>
      <description>


&lt;p&gt;The function &lt;a href=&#34;https://github.com/pablobernabeu/knit_deleting_service_files/blob/main/knit_deleting_service_files.R&#34;&gt;&lt;code&gt;knit_deleting_service_files()&lt;/code&gt;&lt;/a&gt; helps avoid (R) Markdown &lt;a href=&#34;https://pablobernabeu.github.io/2021/tackling-knitting-errors-in-r-markdown/&#34;&gt;knitting errors&lt;/a&gt; caused by files and folders remaining from previous knittings (e.g., manuscript.tex, ZHJhZnQtYXBhLlJtZA==.Rmd, manuscript.synctex.gz). The only obligatory argument for this function is the name of a .Rmd or .md file. The optional argument is a path to a directory containing this file.&lt;/p&gt;
&lt;p&gt;The function first offers deleting potential service files and folders, for which the user’s approval is requested in the console (see screenshot below). Next, the document is knitted. Last, the function offers deleting potential service files and folders again.&lt;/p&gt;
&lt;p&gt;NOTE: The deletions, if accepted, are irreversible as they are made through &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/base/html/unlink.html&#34;&gt;&lt;code&gt;unlink()&lt;/code&gt;&lt;/a&gt;. Therefore, our familiar adage truly applies: this function comes with ABSOLUTELY NO WARRANTY. Please ensure you understand the &lt;a href=&#34;https://github.com/pablobernabeu/knit_deleting_service_files/blob/main/knit_deleting_service_files.R&#34;&gt;source code&lt;/a&gt; before using the function.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;images/Screenshot%202021-12-18%20at%2020.27.49.png&#39; alt=&#39;Screenshot of the function in use&#39; style=&#39;margin-top:2px; margin-bottom:-2px;&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size:90%; color: #899499;&#34;&gt;Screenshot of the function in use.&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;the-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The function&lt;/h2&gt;
&lt;script src=&#34;https://emgithub.com/embed-v2.js?target=https%3A%2F%2Fgithub.com%2Fpablobernabeu%2Fknit_deleting_service_files%2Fblob%2Fmain%2Fknit_deleting_service_files.R&amp;style=a11y-dark&amp;type=code&amp;showBorder=on&amp;showLineNumbers=on&amp;showFileMeta=on&amp;showFullPath=on&amp;showCopy=on&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Walking the line between reproducibility and efficiency in R Markdown: Three methods</title>
      <link>https://pablobernabeu.github.io/2021/walking-the-line-between-reproducibility-and-efficiency-in-r-markdown-three-methods/</link>
      <pubDate>Sun, 14 Nov 2021 00:10:08 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2021/walking-the-line-between-reproducibility-and-efficiency-in-r-markdown-three-methods/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2021/walking-the-line-between-reproducibility-and-efficiency-in-r-markdown-three-methods/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;As technology and research methods advance, the data sets tend to be larger and the methods more exhaustive. Consequently, the analyses take longer to run. This poses a challenge when the results are to be presented using &lt;a href=&#34;https://rmarkdown.rstudio.com/index.html&#34;&gt;R Markdown&lt;/a&gt;. One has to balance reproducibility and efficiency. On the one hand, it is desirable to keep the R Markdown document as self-contained as possible, so that those who may later examine the document can easily test and edit the code. On the other hand, it would be inefficient to create a document that is very slow to run or very long. The context of the task will determine how how time-consuming and long the code in an Rmd file can be. For instance, one could decide that the knitting can take up to 15 minutes, and each code chunk can span up to 30 lines.&lt;/p&gt;
&lt;p&gt;Several methods can be used in each document to accommodate different types of code. Three methods are presented below, ordered from easier-to-reproduce to easier-to-knit.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;For &lt;strong&gt;fast- and concise-enough code:&lt;/strong&gt; Provide the original code in the Rmd file. The code is run as the document is knitted. Example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; nrow(myData)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For &lt;strong&gt;fast-enough but very long code:&lt;/strong&gt; Store the code in a separate script and &lt;code&gt;source&lt;/code&gt; it in the Rmd file. The code is run as the document is knitted. Example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; source(&amp;#39;analysis/model_diagnostics.R&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For &lt;strong&gt;very slow and/or long code:&lt;/strong&gt; Store the code in a separate script and run it prior to knitting the Rmd file, so that the output from the code (e.g., a model, a plot) is saved and can be read into the Rmd. Example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; model_1 = readRDS(&amp;#39;results/model_1.rds&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Importantly, even the third method allows the reproducibility of the code. It just requires a bit of additional documentation to ensure that the end user can also access the script in which the result was produced (e.g., ‘analysis/model_1.R’).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tackling knitting errors in R Markdown</title>
      <link>https://pablobernabeu.github.io/2021/tackling-knitting-errors-in-r-markdown/</link>
      <pubDate>Sat, 13 Nov 2021 18:11:22 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2021/tackling-knitting-errors-in-r-markdown/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2021/tackling-knitting-errors-in-r-markdown/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;When knitting an R Markdown document after the first time, errors may sometimes appear. Three tips are recommended below.&lt;/p&gt;
&lt;div id=&#34;close-pdf-reader-window&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Close PDF reader window&lt;/h2&gt;
&lt;p&gt;When the document is knitted through the ‘Knit’ button, a PDF reader window opens to present the result. Closing this window can help resolve errors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;delete-service-files&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Delete service files&lt;/h2&gt;
&lt;p&gt;Every time the Rmd is knitted, some service files are created. Some of these files have the ‘.tex’ extension (e.g., index.tex, Appendix-B.tex), whereas others do not (e.g., index.log, ZHJhZnQtYXBhLlJtZA==.Rmd, index.synctex.gz). Deleting these files can help resolve errors (see a &lt;a href=&#34;https://pablobernabeu.github.io/2021/avoiding-knitting-errors-in-r-markdown-using-knit-deleting-service-files/&#34;&gt;possible function&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;delete-code-chunks-related-to-the-appendices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Delete code chunks related to the appendices&lt;/h2&gt;
&lt;p&gt;When knitting &lt;a href=&#34;https://github.com/crsh/papaja&#34;&gt;papaja&lt;/a&gt; documents containing any appendices, some code chunks related to the appendices will automatically appear at the end of the primary Rmd file—e.g.:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```{r echo = FALSE, results = &amp;#39;asis&amp;#39;, cache = FALSE}
papaja::render_appendix(&amp;#39;Appendix-A.Rmd&amp;#39;)
```&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Normally, these code chunks are automatically removed as the knitting finishes. However, if the chunks remain in place, deleting them manually can help resolve errors.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Parallelizing simr::powercurve() in R</title>
      <link>https://pablobernabeu.github.io/2021/parallelizing-simr-powercurve/</link>
      <pubDate>Fri, 23 Jul 2021 16:46:54 +0100</pubDate>
      <guid>https://pablobernabeu.github.io/2021/parallelizing-simr-powercurve/</guid>
      <description>


&lt;p&gt;The &lt;code&gt;powercurve&lt;/code&gt; function from the R package &lt;a href=&#34;https://cran.r-project.org/web/packages/simr/simr.pdf&#34;&gt;‘simr’&lt;/a&gt; (Green &amp;amp; MacLeod, 2016) can incur very long running times when the method used for the calculation of &lt;em&gt;p&lt;/em&gt; values is Kenward-Roger or Satterthwaite (see Luke, 2017). Here I suggest three ways for cutting down this time.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Where possible, use a high-performance (or high-end) computing cluster. This removes the need to use personal computers for these long jobs.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In case you’re using the &lt;code&gt;fixed()&lt;/code&gt; parameter of the &lt;code&gt;powercurve&lt;/code&gt; function, and calculating the power for different effects, run these at the same time (‘in parallel’) on different machines, rather than one after another.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Parallelize&lt;/em&gt; the &lt;code&gt;breaks&lt;/code&gt; argument. The &lt;code&gt;breaks&lt;/code&gt; argument of the &lt;code&gt;powercurve&lt;/code&gt; function allows the calculation of power for different levels of the grouping factor passed to &lt;code&gt;along&lt;/code&gt;. Some grouping factors are &lt;em&gt;participant&lt;/em&gt;, &lt;em&gt;trial&lt;/em&gt; and &lt;em&gt;item&lt;/em&gt;. The &lt;code&gt;breaks&lt;/code&gt; argument sets the different sample sizes for which power will be calculated. Parallelizing &lt;code&gt;breaks&lt;/code&gt; is done by running each number of levels in a separate function. When each has been run and saved, they are &lt;code&gt;c&lt;/code&gt;ombined to allow the plotting. This procedure is demonstrated below.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;parallelizing-breaks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parallelizing &lt;code&gt;breaks&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Let’s do a minimal example using a toy &lt;code&gt;lmer&lt;/code&gt; model. A power curve will be created for the fixed effect of &lt;code&gt;x&lt;/code&gt; along different sample sizes of the grouping factor &lt;code&gt;g&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Notice that the six sections of the power curve below are serially arranged, one after another. In contrast, to enable parallel processing, each power curve would be placed in a single script, and they would all be run at the same time.&lt;/p&gt;
&lt;p&gt;Although the power curves below run in a few minutes, the settings that are often used (e.g., a larger model; &lt;code&gt;fixed(&#39;x&#39;, &#39;sa&#39;)&lt;/code&gt; instead of &lt;code&gt;fixed(&#39;x&#39;)&lt;/code&gt;; &lt;code&gt;nsim = 500&lt;/code&gt; instead of &lt;code&gt;nsim = 50&lt;/code&gt;) take far longer. That is where parallel processing becomes useful.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
library(simr)

# Toy model with data from &amp;#39;simr&amp;#39; package
fm = lmer(y ~ x + (x | g), data = simdata)

# Extend sample size of `g`
fm_extended_g = extend(fm, along = &amp;#39;g&amp;#39;, n = 12)

# Parallelize `breaks` by running each number of levels in a separate function.

# 4 levels of g
pwcurve_4g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 4, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 6 levels of g
pwcurve_6g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 6, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 8 levels of g
pwcurve_8g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 8, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 10 levels of g
pwcurve_10g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 10, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 12 levels of g
pwcurve_12g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 12, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having saved each section of the power curve, we must now combine them to be able to plot them together (if you wish to automatise this procedure, consider &lt;a href=&#34;https://github.com/pablobernabeu/Language-and-vision-in-conceptual-processing-Multilevel-analysis-and-statistical-power/blob/main/R_functions/combine_powercurve_chunks.R&#34;&gt;this function&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a destination object using any of the power curves above.
all_pwcurve = pwcurve_4g

# Combine results
all_pwcurve$ps = c(pwcurve_4g$ps[1], pwcurve_6g$ps[1], pwcurve_8g$ps[1], 
                   pwcurve_10g$ps[1], pwcurve_12g$ps[1])

# Combine the different numbers of levels.
all_pwcurve$xval = c(pwcurve_4g$nlevels, pwcurve_6g$nlevels, pwcurve_8g$nlevels, 
                     pwcurve_10g$nlevels, pwcurve_12g$nlevels)

print(all_pwcurve)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Power for predictor &amp;#39;x&amp;#39;, (95% confidence interval),
## by number of levels in g:
##       4: 46.00% (31.81, 60.68) - 40 rows
##       6: 74.00% (59.66, 85.37) - 60 rows
##       8: 92.00% (80.77, 97.78) - 80 rows
##      10: 98.00% (89.35, 99.95) - 100 rows
##      12: 100.0% (92.89, 100.0) - 120 rows
## 
## Time elapsed: 0 h 0 m 4 s&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(all_pwcurve, xlab = &amp;#39;Levels of g&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2021/parallelizing-simr-powercurve/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For reproducibility purposes
sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.5.0 (2025-04-11 ucrt)
## Platform: x86_64-w64-mingw32/x64
## Running under: Windows 11 x64 (build 26100)
## 
## Matrix products: default
##   LAPACK version 3.12.1
## 
## locale:
## [1] LC_COLLATE=English_United Kingdom.utf8 
## [2] LC_CTYPE=English_United Kingdom.utf8   
## [3] LC_MONETARY=English_United Kingdom.utf8
## [4] LC_NUMERIC=C                           
## [5] LC_TIME=English_United Kingdom.utf8    
## 
## time zone: Europe/London
## tzcode source: internal
## 
## attached base packages:
## [1] stats     graphics  grDevices datasets  utils     methods   base     
## 
## other attached packages:
## [1] simr_1.0.7          lme4_1.1-35.5       Matrix_1.7-0       
## [4] knitr_1.49          xaringanExtra_0.8.0
## 
## loaded via a namespace (and not attached):
##  [1] tidyr_1.3.1       sass_0.4.9        utf8_1.2.4        generics_0.1.3   
##  [5] renv_1.1.4        stringi_1.8.4     blogdown_1.21     lattice_0.22-7   
##  [9] digest_0.6.37     magrittr_2.0.3    evaluate_1.0.1    grid_4.5.0       
## [13] bookdown_0.41     iterators_1.0.14  fastmap_1.2.0     plyr_1.8.9       
## [17] jsonlite_2.0.0    backports_1.5.0   Formula_1.2-5     mgcv_1.9-3       
## [21] purrr_1.0.2       fansi_1.0.6       jquerylib_0.1.4   abind_1.4-8      
## [25] cli_3.6.3         rlang_1.1.4       binom_1.1-1.1     splines_4.5.0    
## [29] plotrix_3.8-4     cachem_1.1.0      yaml_2.3.10       RLRsim_3.1-8     
## [33] parallel_4.5.0    tools_4.5.0       pbkrtest_0.5.3    uuid_1.2-1       
## [37] nloptr_2.1.1      minqa_1.2.8       dplyr_1.1.4       boot_1.3-30      
## [41] broom_1.0.7       vctrs_0.6.5       R6_2.5.1          lifecycle_1.0.4  
## [45] stringr_1.5.1     car_3.1-3         MASS_7.3-61       pkgconfig_2.0.3  
## [49] bslib_0.8.0       pillar_1.9.0      glue_1.8.0        Rcpp_1.0.13-1    
## [53] tidyselect_1.2.1  tibble_3.2.1      xfun_0.49         rstudioapi_0.17.1
## [57] htmltools_0.5.8.1 nlme_3.1-165      rmarkdown_2.29    carData_3.0-5    
## [61] compiler_4.5.0&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;just-the-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Just the code&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;
library(lme4)
library(simr)

# Toy model
fm = lmer(y ~ x + (x | g), data = simdata)

# Extend sample size of `g`
fm_extended_g = extend(fm, along = &amp;#39;g&amp;#39;, n = 12)

# Parallelize `breaks` by running each number of levels in a separate function.

# 4 levels of g
pwcurve_4g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 4, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 6 levels of g
pwcurve_6g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 6, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 8 levels of g
pwcurve_8g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 8, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 10 levels of g
pwcurve_10g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 10, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)

# 12 levels of g
pwcurve_12g = powerCurve(fm_extended_g, fixed(&amp;#39;x&amp;#39;), along = &amp;#39;g&amp;#39;, breaks = 12, 
                        nsim = 50, seed = 123, 
                        # No progress bar
                        progress = FALSE)


# Create a destination object using any of the power curves above.
all_pwcurve = pwcurve_4g

# Combine results
all_pwcurve$ps = c(pwcurve_4g$ps[1], pwcurve_6g$ps[1], pwcurve_8g$ps[1], 
                   pwcurve_10g$ps[1], pwcurve_12g$ps[1])

# Combine the different numbers of levels.
all_pwcurve$xval = c(pwcurve_4g$nlevels, pwcurve_6g$nlevels, pwcurve_8g$nlevels, 
                     pwcurve_10g$nlevels, pwcurve_12g$nlevels)


print(all_pwcurve)

plot(all_pwcurve, xlab = &amp;#39;Levels of g&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div class=&#34;hanging-indent&#34;&gt;
&lt;p&gt;Brysbaert, M., &amp;amp; Stevens, M. (2018). Power analysis and effect size in mixed effects models: A tutorial. &lt;em&gt;Journal of Cognition, 1&lt;/em&gt;(1), 9. &lt;a href=&#34;http://doi.org/10.5334/joc.10&#34; class=&#34;uri&#34;&gt;http://doi.org/10.5334/joc.10&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Green, P., &amp;amp; MacLeod, C. J. (2016). SIMR: An R package for power analysis of generalized linear mixed models by simulation. &lt;em&gt;Methods in Ecology and Evolution 7&lt;/em&gt;(4), 493–498, &lt;a href=&#34;https://doi.org/10.1111/2041-210X.12504&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1111/2041-210X.12504&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kumle, L., Vo, M. L. H., &amp;amp; Draschkow, D. (2021). Estimating power in (generalized) linear mixed models: An open introduction and tutorial in R. &lt;em&gt;Behavior Research Methods&lt;/em&gt;, 1–16. &lt;a href=&#34;https://doi.org/10.3758/s13428-021-01546-0&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13428-021-01546-0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Luke, S. G. (2017). Evaluating significance in linear mixed-effects models in R. &lt;em&gt;Behavior Research Methods, 49&lt;/em&gt;(4), 1494–1502. &lt;a href=&#34;https://doi.org/10.3758/s13428-016-0809-y&#34; class=&#34;uri&#34;&gt;https://doi.org/10.3758/s13428-016-0809-y&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The number of simulations set by &lt;code&gt;nsim&lt;/code&gt; should be larger (Brysbaert &amp;amp; Stevens, 2018; Green &amp;amp; MacLeod, 2016). In addition, the effect size for &lt;code&gt;x&lt;/code&gt; should be adjusted to the value that best fits with the planned study (Kumle et al., 2021).&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Brief Clarifications, Open Questions: Commentary on Liu et al. (2018)</title>
      <link>https://pablobernabeu.github.io/2021/brief-clarifications-open-questions-commentary-on-liu-et-al-2018/</link>
      <pubDate>Sat, 09 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2021/brief-clarifications-open-questions-commentary-on-liu-et-al-2018/</guid>
      <description>&lt;p&gt;Liu et al. (2018) present a study that implements the conceptual modality switch (CMS) paradigm, which has been used to investigate the modality-specific nature of conceptual representations (Pecher et al., 2003). Liu et al.&amp;lsquo;s experiment uses event-related potentials (ERPs; similarly, see Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013). In the design of the switch conditions, the experiment implements a corpus analysis to distinguish between purely-embodied modality switches and switches that are more liable to linguistic bootstrapping (also see Bernabeu et al., 2017; Louwerse &amp;amp; Connell, 2011). Furthermore, the sophisticated procedure used for the selection of stimuli could prove useful in future studies, and the application of Bayesian statistics is an interesting and promising novelty in the present research area.&lt;/p&gt;
&lt;p&gt;In reviewing the literature, Liu (2018) and Liu et al. (2018) contend that previous studies may be strongly biased due to methodological decisions in the analysis of ERPs. These decisions particularly regard the latency&amp;mdash;i.e., time windows&amp;mdash;and the topographic regions of interest&amp;mdash;i.e., subsets of electrodes. Thus, Liu et al. identify a &amp;lsquo;highly inconsistent&amp;rsquo; (p. 6) landscape in the ERP components that have been ascribed to the CMS effect in previous studies. Similarly, Liu (p. 47) writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Several studies have looked for the ERP manifestations of modality switching costs (Bernabeu, Willems, &amp;amp; Louwerse, 2017; Collins, Pecher, Zeelenberg, &amp;amp; Coulson, 2011; Hald, Hocking, Vernon, Marshall, &amp;amp; Garnham, 2013; Hald, Marshall, Janssen, &amp;amp; Garnham, 2011). However, what they found was not a clear picture. Not only was a significant effect found in the time window for the N400 component, but also a so-called early N400-like effect around 300ms (Bernabeu et al., 2017; Hald et al., 2011), the N1-P2 complex around 200ms (Bernabeu et al., 2017; Hald et al., 2013, 2011), as well as the late positivity component (LPC) after 600ms (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;drastic-conclusions&#34;&gt;Drastic conclusions&lt;/h3&gt;
&lt;p&gt;Liu et al. (2018) conclude that a confirmatory research approach is not warranted, and adopt a semi-exploratory approach, creating time windows of 50 ms each, rather than windows linked to known ERP components (Swaab et al., 2012), and using Bayesian statistics. Such a drastic conclusion appears to stem from the assumption that, if the CMS were robust enough, it would present in the same guise across studies. Such an assumption, however, may merit further examination, considering the multiplicity of known and unknown variables that may differ across experiments (Barsalou, 2019). This variability influences the &lt;em&gt;replication praxis&lt;/em&gt;, as it were. Indeed, Liu et al. themselves allude to one such variable (p. 7).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;These previous studies did not only examine the effect of modality switching but also other linguistic factors such as negated sentences, which could easily distort observed waveforms (Luck, 2005).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu et al.&amp;lsquo;s (2018) conclusions about the &amp;lsquo;inconsistent&amp;rsquo; results do not seem to duly weigh the differences across studies. None of the existing studies is a direct replication of another. The example quoted from Liu et al. above, regarding the presence of negated sentences, is one of the less important differences because one of the corresponding studies implemented the negation as a controlled, experimental condition, contrasting with affirmative sentences (Hald et al., 2013). Yet, other differences exist in virtually every aspect, from the types of modality switch used to the time-locking of ERPs. For instance, the studies vary in the implementation of the modality switches. Whereas Hald et al. (2011) distinguish between a switch and a non-switch condition, Bernabeu et al. (2017) analysed each type of switch separately&amp;mdash;i.e., auditory-to-visual, haptic-to-visual, visual-to-visual. Another difference across the studies is the onset point for ERPs. For instance, Bernabeu et al. time-locked ERPs to the point at which the modality switch is actually elicited in the CMS paradigm&amp;mdash;namely, the first word in target trials. In contrast, the other studies time-locked ERPs to the second word. In addition, these studies vary in their timelines&amp;mdash;i.e., presentation of words and inter-stimulus intervals&amp;mdash;, as well as in the words that were used as stimuli, in the preprocessing of ERPs, in the statistical analysis, and even in the language of testing in one case (all studies using English except Bernabeu et al., 2017, which used Dutch). Moreover, the studies differ in the sample size, ranging from ten finally-analysed participants (Hald et al, 2011) to 46 finally-analysed participants (Bernabeu et al., 2017). Last, the studies differ in the number of items per modality switch condition, ranging from 17 (in one of Liu et al.&amp;lsquo;s, 2018 conditions) to 40 per condition (Hald et al., 2011, 2013). Undoubtedly, seeing larger sample sizes and more stimulus items used in ERP studies is something to promote and celebrate. Last, it may be noted that Liu et al.&amp;lsquo;s stance on the inconsistency of previous results starkly contrasts with their use of a single study&amp;mdash;Hald et al. (2011)&amp;mdash;, with &lt;em&gt;N&lt;/em&gt; = 10, as the motivation for their sample size (Albers &amp;amp; Lakens, 2018).&lt;/p&gt;
&lt;h3 id=&#34;clarifications&#34;&gt;Clarifications&lt;/h3&gt;
&lt;p&gt;Liu et al. (2018) apply Bayesian statistics reportedly to help reduce the bias that may exist in the present literature. However, the reviews by Liu (2018) and Liu et al. appear to gloss over some important aspects regarding previous studies. For instance, Liu writes (p. 53):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The inflation of the probability of Type I error leads to an over-confidence in the interpretation of the results. For example, in the studies on modality switching costs, different time windows were chosen to test the early effect of modality switching costs. While Bernabeu et al. (2017), Hald et al. (2011) and Hald et al. (2013) examined the segment of ERP waveform between 190ms and 300ms or 160ms and 215ms based on visual inspection and found significant effects, Collins et al. (2011) chose a prescribed time window between 100ms and 200ms before the analysis and did not find the effect.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu also refers to the issue of multiple tests related to the multiple time windows and electrodes we find in ERP studies (p. 59):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It is typical for ERP studies to conduct multiple comparisons (e.g., running the same ANOVA repeatedly on different subsets of data like different time windows and different groups of electrodes). This would massively increase Type I error if no post hoc correction is conducted. However, if Bonferroni or other correction is conducted, it will render the study over-conservative, thus increasing the chance of Type II error. In the present thesis, 90 electrodes will be analysed individually, with 20 time slices in each trial. That results in 1800 NHSTs for each critical variable. A correction of multiple comparison will require a critical level of 2.78 x 10ˆ-5 for each test for a family-wise critical level of .05 (and an uncorrected test will almost definitely lead to false positive results). This stringent criterion could conceivably render it meaningless any p-values we can obtain from a statistical package.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Arguably, the scenario presented by Liu (2018), in which a researcher could conduct a purely data-driven analysis of ERP data, is extreme. The field of psycholinguistics, in general, does not have a tradition of purely data-driven analysis. Instead, it blends a humanistic background with a scientific methodology. As a result, the hypotheses and methods tend to be largely driven by the available literature. For instance, Bernabeu et al. (2017, quoted below from p. 1632) based their time windows and regions of interest on the most relevant of the preceding studies (also see Bernabeu, 2017).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Electrodes were divided into an anterior and a posterior area (also done in Hald et al., 2011). Albeit a superficial division, we found it sufficient for the research question. Time windows were selected as in Hald et al., except for the last window, which was extended up to 750 ms post word onset, instead of 700 ms, because the characteristic component of that latency tends to extend until then, as we confirmed by visual inspection of these results.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The literature-based approach follows the advice from Luck and Gaspelin (2017, p. 149), who wrote: &amp;lsquo;a researcher who wants to avoid significant but bogus effects would be advised to focus on testing a priori predictions without using the observed data to guide the selection of time windows or electrode sites&amp;rsquo; (also see Armstrong, 2014; for a more conservative stance, see Luck &amp;amp; Gaspelin, 2017). In addition, notice that the extension of the last window by 50 ms was informed by Swaab et al. (2012), who report results by which the P600 component (the main component occurring after the N400 in word reading) extended up to 800 ms.&lt;/p&gt;
&lt;p&gt;Next, Liu et al. (2018, pp. 6&amp;ndash;7) write:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;However, the findings of these components have been highly inconsistent. The N400 effect alone was found in the posterior region in some cases (Bernabeu et al., 2017; Hald et al., 2013), while in anterior region in others (Collins et al., 2011, Hald et al. (2011)).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yet, Bernabeu et al. (2017, p. 1632) had written:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In certain parts over the time course, the effect appeared in both anterior and posterior areas&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Liu et al. (2018) continue (p. 7):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In some cases, it was found in the typical window around 400ms (Collins et al., 2011), while in others an earlier window from 270ms to 370ms (Bernabeu et al., 2017; Hald et al., 2011).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, Bernabeu et al. (2017, p. 1632) had written:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The ERP results revealed a CMS effect from Time Window 1 on, larger after 350 ms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Regarding the time-locking of ERPs, Liu (2018, p. 43) writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Because the properties were usually salient for the concepts, the switching costs might have already been incurred when participants were processing the concept word. Bernabeu et al. (2017), in their recent replication of previous ERP studies, reversed the order of concept and property and did not find an immediate effect from the property onset. In future studies, it is recommended to adopt the reverse order, control the concept words so that they do not automatically activate the properties before the words are shown, or analyse epochs after both the concept and property words.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above excerpt seems to reveal a misunderstanding of Bernabeu et al.&amp;lsquo;s (2017) method and results (we may assume that, by &amp;lsquo;immediate&amp;rsquo;, Liu (2018) is referring to the 200 ms point or afterwards, since that is about as immediate as it gets; see Amsel et al., 2014; Swaab et al., 2012; Van Dam et al., 2014). Bernabeu et al.&amp;lsquo;s abstract mentioned (p. 1629):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Event-Related Potentials (ERPs) were time-locked to the onset of the first word (property) in the target trials so as to measure the effect online and to avoid a within-trial confound. A switch effect was found, characterized by more negative ERP amplitudes for modality switches than no-switches. It proved significant in four typical time windows from 160 to 750 milliseconds post word onset, with greater strength in posterior brain regions, and after 350 milliseconds.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Indeed, time-locking ERPs to the beginning of the trial was one of the principal features of Bernabeu et al.&amp;lsquo;s (2017) experiment. Complementing that, the property words were placed in the first trial because they are more perceptually loaded than concepts (Lynott &amp;amp; Connell, 2013), thus better suiting the main basis of the CMS paradigm. We know that the semantic processing of a word often commences within the first 200 ms (Amsel et al., 2014; Van Dam et al., 2014). Considering the importance of the time course in the grounding of conceptual representations (Hauk, 2016), it seems important to measure the CMS from the moment that it is elicited&amp;mdash;namely, in all experiments, from the first word of the target trial (Bernabeu, 2017; Bernabeu et al., 2017), rather than letting several hundreds of milliseconds elapse. Nonetheless, from a methodological perspective, it would be interesting to compare the two approaches in a dedicated study. This would precisely reveal the speed at which modality-specific meaning becomes activated during conceptual processing.&lt;/p&gt;
&lt;h3 id=&#34;outstanding-issues-random-effects-and-correction-for-multiple-tests&#34;&gt;Outstanding issues: Random effects and correction for multiple tests&lt;/h3&gt;
&lt;p&gt;A methodological issue affecting the statistical analysis of all the studies hereby considered (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013; Liu et al., 2018) is the absence of some applicable random effects. Some of the studies did not apply any random effects (Collins et al., 2011; Hald et al., 2011, 2013). Even in psycholinguistics, use of linear mixed-effects models is still increasing (Meteyard &amp;amp; Davies, 2020; Yarkoni, 2020). Yet, in those studies that did apply random effects, the corresponding  structure was not as exhaustive as it should have been, as they lacked random slopes. In Bernabeu et al. (2017), a model selection approach was applied (Matuschek et al., 2017), whereby each random effect was tested and only kept in the model if it significantly improved the fit. In Liu et al. (2018), random slopes were deemed unfeasible due to computational constraints (for background, see Brauer &amp;amp; Curtin, 2017). Applying a complete random effects structure is important for a robust statistical analysis (Barr et al., 2013; Yarkoni, 2020).&lt;/p&gt;
&lt;p&gt;Another issue is that of multiple tests. Where a small number of levels is used (e.g., time windows, topographic regions of interest) and these are informed by the literature, the advice has often been ambiguous as to whether a correction should be applied (e.g., Armstrong, 2014; for a more conservative stance, see Luck &amp;amp; Gaspelin, 2017). Indeed, no correction was applied in any of the four studies that used frequentist statistics (Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013).&lt;/p&gt;
&lt;h4 id=&#34;reanalysis-of-bernabeu-et-al-2017&#34;&gt;Reanalysis of Bernabeu et al. (2017)&lt;/h4&gt;
&lt;p&gt;The results of Bernabeu et al. (2017) were reanalysed after publication using a more complete random effects structure that incorporated by-participant random slopes for the modality-switch factor&amp;mdash;i.e., &lt;code&gt;(condition | participant)&lt;/code&gt;. The results were also corrected for multiple tests using the Holm-Bonferroni correction (Holm, 1979). For this purpose, the lowest p-value in the four time windows was multiplied by 4, the next p-value by 3, the next by 2, and the highest p-value was left as it unmodified. In this stepwise correction, if a nonsignificant p-value was reached, all the subsequent p-values became nonsignificant (see &lt;a href=&#34;https://osf.io/unvfs/&#34;&gt;analysis script&lt;/a&gt;). The &lt;a href=&#34;https://osf.io/qhe5s/&#34;&gt;results&lt;/a&gt; differed from the original, slopes-free models (available &lt;a href=&#34;https://osf.io/sx3nw/&#34;&gt;script&lt;/a&gt; and &lt;a href=&#34;https://osf.io/4v38d/&#34;&gt;results&lt;/a&gt;) in that the main effect of the modality switch factor became nonsignificant in the second time window (160&amp;ndash;216 ms) and in the fourth one (500&amp;ndash;750 ms), while remaining significant in the third time window (350&amp;ndash;550 ms). Incidentally, note that main effects may not be directly interpretable as the same same variables are present interactions (Kam &amp;amp; Franzese, 2007). Yet, the interactions of modality switch with participant group (quick/slow) and scalp location (anterior/posterior) retained the same significance.&lt;/p&gt;
&lt;h3 id=&#34;open-questions&#34;&gt;Open questions&lt;/h3&gt;
&lt;p&gt;Liu (2018) and Liu et al. (2018) raise interesting and important questions. Firstly, future research may be conducted to investigate what determines the variability of ERP results&amp;mdash;in terms of ERP components, time windows and topographic regions of interest. This research could include a comparison with other measurements, such as response times, to test whether ERPs are less reliable&amp;mdash;i.e., more variable across studies&amp;mdash;than response times. Similarly, future research may investigate whether the ERP literature is more biased than literature employing other measures, such as response times. In addition, future research could investigate whether moving to exploratory, Bayesian research designs is a necessary or sufficient condition to reduce bias in research and improve the precision of experimental measurements. Current alternatives to such an approach include direct (or conceptual) replications designed to achieve a higher power than previous studies (e.g., Chen et al., 2019). Arguably, policies determining funding decisions would need to change if we are to fully acknowledge the importance of &lt;em&gt;direct&lt;/em&gt; replication (Howe &amp;amp; Perfors, 2018; Kunert, 2016; Simons, 2014; Zwaan et al., 2018). Last, future research may investigate whether &amp;lsquo;clear picture&amp;rsquo; results are realistic, desirable or necessary, and whether unclear-picture results should be eschewed; or whether, on the contrary, clear-picture results may largely be the product of publication bias&amp;mdash;that is, the pressure to hide or misreport those aspects of a study that could challenge its acceptance by peer-reviewers or any other academics.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;div class = &#39;hanging-indent&#39;&gt;
&lt;p&gt;Albers, C., &amp;amp; Lakens, D. (2018). When power analyses based on pilot data are biased: Inaccurate effect size estimators and follow-up bias. &lt;em&gt;Journal of Experimental Social Psychology, 74&lt;/em&gt;, 187–195. &lt;a href=&#34;https://doi.org/10.1016/j.jesp.2017.09.004&#34;&gt;https://doi.org/10.1016/j.jesp.2017.09.004&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Amsel, B. D., Urbach, T. P., &amp;amp; Kutas, M. (2014). Empirically grounding grounded cognition: the case of color. &lt;em&gt;Neuroimage, 99&lt;/em&gt;, 149-157. &lt;a href=&#34;https://doi.org/10.1016/j.neuroimage.2014.05.025&#34;&gt;https://doi.org/10.1016/j.neuroimage.2014.05.025&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Armstrong, R. A. (2014). When to use the Bonferroni correction. &lt;em&gt;Ophthalmic and Physiological Optics, 34&lt;/em&gt;(5), 502-508. &lt;a href=&#34;https://doi.org/10.1111/opo.12131&#34;&gt;https://doi.org/10.1111/opo.12131&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barr, D. J., Levy, R., Scheepers, C., &amp;amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. &lt;em&gt;Journal of Memory and Language, 68&lt;/em&gt;, 255–278. &lt;a href=&#34;http://dx.doi.org/10.1016/j.jml.2012.11.001&#34;&gt;http://dx.doi.org/10.1016/j.jml.2012.11.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Barsalou, L. W. (2019). Establishing generalizable mechanisms. &lt;em&gt;Psychological Inquiry, 30&lt;/em&gt;(4), 220-230. &lt;a href=&#34;https://doi.org/10.1080/1047840X.2019.1693857&#34;&gt;https://doi.org/10.1080/1047840X.2019.1693857&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P. (2017). &lt;em&gt;Modality switches occur early and extend late in conceptual processing: evidence from ERPs&lt;/em&gt; [Master&#39;s thesis]. School of Humanities, Tilburg University. &lt;a href=&#34;https://psyarxiv.com/5gjvk&#34;&gt;https://psyarxiv.com/5gjvk&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P., Willems, R. M., &amp;amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, &amp;amp; E. J. Davelaar (Eds.), &lt;em&gt;Proceedings of the 39th Annual Conference of the Cognitive Science Society&lt;/em&gt; (pp. 1629-1634). Austin, TX: Cognitive Science Society. &lt;a href=&#34;https://doi.org/10.31234/osf.io/a5pcz&#34;&gt;https://doi.org/10.31234/osf.io/a5pcz&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Brauer, M., &amp;amp; Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. &lt;em&gt;Psychological Methods, 23&lt;/em&gt;(3), 389–411. &lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&#34;&gt;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer-Curtin-2018-on-LMEMs.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chen, S., Szabelska, A., Chartier, C. R., Kekecs, Z., Lynott, D., Bernabeu, P., … Schmidt, K. (2018). &lt;em&gt;Investigating object orientation effects across 14 languages&lt;/em&gt;. PsyArXiv. &lt;a href=&#34;https://doi.org/10.31234/osf.io/t2pjv/&#34;&gt;https://doi.org/10.31234/osf.io/t2pjv/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Collins, J., Pecher, D., Zeelenberg, R., &amp;amp; Coulson, S. (2011). Modality switching in a property verification task: an ERP study of what happens when candles flicker after high heels click. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2011.00010&#34;&gt;https://doi.org/10.3389/fpsyg.2011.00010&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hald, L. A., Hocking, I., Vernon, D., Marshall, J.-A., &amp;amp; Garnham, A. (2013). Exploring modality switching effects in negated sentences: further evidence for grounded representations. &lt;em&gt;Frontiers in Psychology, 4&lt;/em&gt;, 93. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2013.00093&#34;&gt;https://doi.org/10.3389/fpsyg.2013.00093&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hald, L. A., Marshall, J.-A., Janssen, D. P., &amp;amp; Garnham, A. (2011). Switching modalities in a sentence verification task: ERP evidence for embodied language processing. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.3389/fpsyg.2011.00045&#34;&gt;https://doi.org/10.3389/fpsyg.2011.00045&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hauk, O. (2016). Only time will tell–why temporal information is essential for our neuroscientific understanding of semantics. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;(4), 1072-1079. &lt;a href=&#34;https://doi.org/10.3758/s13423-015-0873-9&#34;&gt;https://doi.org/10.3758/s13423-015-0873-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Holm, S. (1979). A simple sequentially rejective multiple test procedure. &lt;em&gt;Scandinavian Journal of Statistics, 6&lt;/em&gt;, 65-70. &lt;a href=&#34;http://www.jstor.org/stable/4615733&#34;&gt;http://www.jstor.org/stable/4615733&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Howe, P. D., &amp;amp; Perfors, A. (2018). An argument for how (and why) to incentivise replication. &lt;em&gt;Behavioral and Brain Sciences, 41&lt;/em&gt;, e135-e135. &lt;a href=&#34;http://dx.doi.org/10.1017/S0140525X18000705&#34;&gt;http://dx.doi.org/10.1017/S0140525X18000705&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Kam, C. D., &amp;amp; Franzese, R. J. (2007). &lt;em&gt;Modeling and interpreting interactive hypotheses in regression analysis&lt;/em&gt;. Ann Arbor, MI: University of Michigan Press.&lt;/p&gt;
&lt;p&gt;Kunert, R. (2016). Internal conceptual replications do not increase independent replication success. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;(5), 1631-1638. &lt;a href=&#34;https://doi.org/10.3758/s13423-016-1030-9&#34;&gt;https://doi.org/10.3758/s13423-016-1030-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, P. (2018). &lt;em&gt;Embodied-linguistic conceptual representations during metaphor processing&lt;/em&gt;. Doctoral thesis, Lancaster University, UK. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/489&#34;&gt;https://doi.org/10.17635/lancaster/thesis/489&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Liu, P., Lynott, D., &amp;amp; Connell, L. (2018). Continuous neural activations of simulation-linguistic representations in modality switching costs. In P. Liu, &lt;em&gt;Embodied-linguistic conceptual representations during metaphor processing&lt;/em&gt;. Doctoral thesis, Lancaster University, UK. &lt;a href=&#34;https://doi.org/10.17635/lancaster/thesis/489&#34;&gt;https://doi.org/10.17635/lancaster/thesis/489&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Luck, S. J. (2005). Ten simple rules for designing ERP experiments. In T. C. Handy (Ed.), &lt;em&gt;Event-related potentials: A methods handbook&lt;/em&gt;.
MIT Press.&lt;/p&gt;
&lt;p&gt;Luck, S. J., &amp;amp; Gaspelin, N. (2017). How to get statistically significant effects in any ERP experiment (and why you shouldn&#39;t). &lt;em&gt;Psychophysiology, 54&lt;/em&gt;(1), 146-157. &lt;a href=&#34;https://doi.org/10.1111/psyp.12639&#34;&gt;https://doi.org/10.1111/psyp.12639&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., &amp;amp; Bates, D. (2017). Balancing type 1 error and power in linear mixed models. &lt;em&gt;Journal of Memory and Language, 94&lt;/em&gt;, 305–315. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2017.01.001&#34;&gt;https://doi.org/10.1016/j.jml.2017.01.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meteyard, L., &amp;amp; Davies, R. A. (2020). Best practice guidance for linear mixed-effects models in psychological science. &lt;em&gt;Journal of Memory and Language, 112&lt;/em&gt;, 104092. &lt;a href=&#34;https://doi.org/10.1016/j.jml.2020.104092&#34;&gt;https://doi.org/10.1016/j.jml.2020.104092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pecher, D., Zeelenberg, R., &amp;amp; Barsalou, L. W. (2003). Verifying different-modality properties for concepts produces switching costs. &lt;em&gt;Psychological Science, 14&lt;/em&gt;, 2, 119-24. &lt;a href=&#34;https://doi.org/10.1111/1467-9280.t01-1-01429&#34;&gt;https://doi.org/10.1111/1467-9280.t01-1-01429&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Simons, D. J. (2014). The value of direct replication. &lt;em&gt;Perspectives on Psychological Science, 9&lt;/em&gt;(1), 76–80. &lt;a href=&#34;https://doi.org/10.1177/1745691613514755&#34;&gt;https://doi.org/10.1177/1745691613514755&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Swaab, T. Y., Ledoux, K., Camblin, C. C., &amp;amp; Boudewyn, M. A. (2012). Language-related ERP components. In S. J. Luck &amp;amp; E. S. Kappenman (Eds.), &lt;em&gt;Oxford handbook of event-related potential components&lt;/em&gt; (pp. 397–440). Oxford University Press. &lt;a href=&#34;https://doi.org/10.1093/oxfordhb/9780195374148.013.0197&#34;&gt;https://doi.org/10.1093/oxfordhb/9780195374148.013.0197&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Van Dam, W. O., Brazil, I. A., Bekkering, H., &amp;amp; Rueschemeyer, S.-A. (2014). Flexibility in embodied language processing: context effects in lexical access. &lt;em&gt;Topics in Cognitive Science, 6&lt;/em&gt;(3), 407–424. &lt;a href=&#34;https://doi.org/10.1111/tops.12100&#34;&gt;https://doi.org/10.1111/tops.12100&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yarkoni, T. (2020). The generalizability crisis. &lt;em&gt;Behavioral and Brain Sciences&lt;/em&gt;, 1-37. &lt;a href=&#34;https://doi.org/10.1017/S0140525X20001685&#34;&gt;https://doi.org/10.1017/S0140525X20001685&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zwaan, R., Etz, A., Lucas, R., &amp;amp; Donnellan, M. (2018). Making replication mainstream. &lt;em&gt;Behavioral and Brain Sciences, 41&lt;/em&gt;, E120. &lt;a href=&#34;https://doi.org/10.1017/S0140525X17001972&#34;&gt;https://doi.org/10.1017/S0140525X17001972&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Collaboration while using R Markdown</title>
      <link>https://pablobernabeu.github.io/2020/collaboration-while-using-r-markdown/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2020/collaboration-while-using-r-markdown/</guid>
      <description>&lt;p&gt;In a highly recommendable &lt;a href=&#34;https://www.youtube.com/watch?v=Nj9J5iCSMB0&#34;&gt;presentation available on Youtube&lt;/a&gt;, &lt;strong&gt;Michael Frank&lt;/strong&gt; walks us through R Markdown. Below, I loosely summarise and partly elaborate on Frank&#39;s advice regarding collaboration among colleagues, some of whom may not be used to R Markdown (&lt;a href=&#34;https://www.youtube.com/watch?v=Nj9J5iCSMB0&amp;amp;feature=youtu.be&amp;amp;t=2900&amp;amp;ab_channel=MichaelFrank&#34;&gt;see relevant time point in Frank&#39;s presentation&lt;/a&gt;).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The first way is using GitHub, which has a great version control system, and even allows the rendering of Markdown text, if the file is given the extension &amp;lsquo;.md&amp;rsquo; on GitHub. Furthermore, GitHub has made private repositories with any number of collaborators free.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second way is copying the text part of the unrendered R Markdown doc (i.e., excluding any long code chunks) to Word or Google Docs, or any other trackable editor. The collaborators would then edit the text, and refrain from editing any of the R or Markdown code (i.e., any inline code, hashes, etc.). Changes would be tracked and accepted (any unwanted edits of the code may be undone), and transferred to the original document.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The third way is knitting the document to Word, which allows tracking changes, or otherwise knitting to PDF.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Notes about punctuation in formal writing</title>
      <link>https://pablobernabeu.github.io/2020/formal-punctuation/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2020/formal-punctuation/</guid>
      <description>&lt;p&gt;When writing formal pieces, some pitfalls in the punctuation are easy to avoid once you know them. Punctuation marks such as the comma, the semi-colon, the colon and the period are useful for organising phrases and clauses, facilitating the reading, and disambiguating. However, these marks are also liable to underuse, as in the case of run-on sentences; misuse, as in the comma splice; and overuse, as it often happens with the Oxford comma.&lt;/p&gt;
&lt;h2 style=&#34;padding-top:2%;&#34; id=&#34;run-on-sentences-insufficient-punctuation&#34;&gt;Run-on sentences: insufficient punctuation&lt;/h2&gt;
&lt;p&gt;Example (notice the absence of punctuation marks around &amp;lsquo;therefore&amp;rsquo;):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;* Specific agendas in media organisations lead to information being manipulated or hidden therefore individual research efforts are valuable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Possible alternative:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Specific agendas in media organisations lead to information being manipulated or hidden. Therefore, individual research efforts are valuable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;  &lt;a href=&#34;http://writing.umn.edu/sws/assets/pdf/quicktips/run-ons.pdf&#34;&gt;More information&lt;/a&gt;&lt;/p&gt;
&lt;h2 style=&#34;padding-top:3%;&#34; id=&#34;comma-splice-misused-punctuation&#34;&gt;Comma splice: misused punctuation&lt;/h2&gt;
&lt;p&gt;A type of run-on sentence, the comma splice is characterised by the use of of too-weak punctuation forms; often, using a comma instead of a semi-colon or a period.&lt;/p&gt;
&lt;p&gt;Example (notice the comma before &amp;lsquo;therefore&amp;rsquo;):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;* Specific agendas in media organisations lead to information being manipulated or hidden&lt;b&gt;,&lt;/b&gt; therefore individual research efforts are valuable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Possible alternative: see above.&lt;/p&gt;
&lt;p&gt;  &lt;a href=&#34;https://advice.writing.utoronto.ca/revising/comma-splices/&#34;&gt;More information&lt;/a&gt;&lt;/p&gt;
&lt;h2 style=&#34;padding-top:3%;&#34; id=&#34;oxford-comma-sometimes-overused&#34;&gt;Oxford comma: sometimes overused&lt;/h2&gt;
&lt;p&gt;The Oxford, or serial, comma (namely, the comma preceding the word &amp;lsquo;and&amp;rsquo; before the last item in a series) has the purpose of facilitating the reading and disambiguating. Where that purpose is not necessary, neither is the comma.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/post/formal-punctuation/fiver-without-Oxford-comma.jpg&#34; alt=&#34;Fiver without Oxford comma&#34; title=&#34;Fiver without Oxford comma&#34;&gt;&lt;/p&gt;
&lt;p&gt;Find out more:&lt;/p&gt;
&lt;p&gt;  &lt;a href=&#34;https://www.ox.ac.uk/sites/files/oxford/media_wysiwyg/University%20of%20Oxford%20Style%20Guide.pdf&#34;&gt;Oxford University guidelines&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;  &lt;a href=&#34;https://www.theguardian.com/commentisfree/2011/jul/04/oxford-comma-common-sense&#34;&gt;The Guardian guidelines&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;  &lt;a href=&#34;https://www.vanguard-pa.com/single-post/2019/03/06/Oxford-Comma-The-Kanye-West-of-Punctuation&#34;&gt;Overuse of Oxford comma&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
</description>
    </item>
    
    <item>
      <title>Stray meetings in Microsoft Teams</title>
      <link>https://pablobernabeu.github.io/2020/stray-meetings-in-microsoft-teams/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2020/stray-meetings-in-microsoft-teams/</guid>
      <description>&lt;p&gt;Unwanted, stranded meetings, overlapping with a general one in a channel, can occur when people click on the &lt;kbd&gt;Meet (now)&lt;/kbd&gt;/:camera: button, instead of clicking on the same &lt;kbd&gt;Join&lt;/kbd&gt; button in the chat field. This may especially happen to those who reach the channel first, or who cannot see the &lt;kbd&gt;Join&lt;/kbd&gt; button in the chat field because this field has been taken up by messages.&lt;/p&gt;
&lt;p&gt;To prevent students from creating or joining stray meetings, the channel lead can direct students to a single meeting link in the chat field, which could be upward ion this field, and to avoid the &lt;kbd&gt;Meet (now)&lt;/kbd&gt;/:camera: button at the top right in channels.&lt;/p&gt;
&lt;h3 id=&#34;further-background&#34;&gt;Further background&lt;/h3&gt;
&lt;h5 id=&#34;hi-all-in-microsoft-teams-if-multiple-people-click-the-meet-now-button-in-a-channel-what-happens&#34;&gt;“Hi All, in Microsoft Teams, if multiple people click the &amp;lsquo;Meet (now)&amp;rsquo; button in a channel, what happens?”&lt;/h5&gt;
&lt;p&gt;  &lt;a href=&#34;https://www.reddit.com/r/Office365/comments/gilp7o/microsoft_teams_simultaneous_channel_meetings/?utm_source=amp&amp;amp;utm_medium=&amp;amp;utm_content=post_body%E2%80%94&#34;&gt;See Reddit post&lt;/a&gt;&lt;/p&gt;
&lt;h5 id=&#34;more-advanced-tip-to-have-meeting-links-ready-in-each-channel&#34;&gt;More advanced tip to have meeting links ready in each channel&lt;/h5&gt;
&lt;p&gt;  &lt;a href=&#34;https://youtu.be/qo6yqh7erEY?t=184&#34;&gt;Watch tutorial from minute 3:04 to 4:50&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R Markdown amidst Madison parks</title>
      <link>https://pablobernabeu.github.io/2020/r-markdown-amidst-madison-parks/</link>
      <pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2020/r-markdown-amidst-madison-parks/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/rmarkdown-libs/plotly-binding/plotly.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/rmarkdown-libs/typedarray/typedarray.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://pablobernabeu.github.io/rmarkdown-libs/crosstalk/css/crosstalk.min.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/rmarkdown-libs/crosstalk/js/crosstalk.min.js&#34;&gt;&lt;/script&gt;
&lt;link
  href=&#34;https://pablobernabeu.github.io/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css&#34;
  rel=&#34;stylesheet&#34;
/&gt;
&lt;script src=&#34;https://pablobernabeu.github.io/rmarkdown-libs/plotly-main/plotly-latest.min.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;
  This document is part of teaching materials created for the workshop
  &lt;a
    href=&#34;https://github.com/pablobernabeu/CarpentryCon-2020-workshop-Open-Data-Reproducibility&#34;
    target=&#34;_top&#34;
    &gt;‘Open data and reproducibility v2.1: R Markdown, dashboards and Binder’&lt;/a
  &gt;, delivered at the
  &lt;a href=&#34;https://2020.carpentrycon.org/&#34;&gt;CarpentryCon 2020 conference&lt;/a&gt;. The
  purpose of this specific document is to practise R Markdown, including basic
  features such as Markdown markup and code chunks, along with more special
  features such as
  &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook/cross-ref.html&#34;
    &gt;cross-references for figures, tables, code chunks, etc&lt;/a
  &gt;. The code is
  &lt;a
    href=&#34;https://github.com/pablobernabeu/Data-is-present/blob/master/examples-documents-dashboards/R%20Markdown/R-Markdown-amidst-Madison-parks.Rmd&#34;
    &gt;on GitHub&lt;/a
  &gt;.
&lt;/p&gt;
&lt;p&gt;
  Since this conference was originally going to take place in Madison, let’s
  look at some
  &lt;a
    href=&#34;https://data-cityofmadison.opendata.arcgis.com/datasets/parks?geometry=-89.997%2C43.007%2C-88.679%2C43.183&#34;
    &gt;open data from the City of Madison&lt;/a
  &gt;.
&lt;/p&gt;
&lt;div id=&#34;park-types&#34; class=&#34;section level2&#34;&gt;
  &lt;h2&gt;Park types&lt;/h2&gt;
  &lt;p&gt;
    &lt;font style=&#34;color: grey&#34;
      &gt;[Placeholder text] Lorem ipsum dolor sit amet, consectetur adipiscing
      elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.
      Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut
      aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in
      voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur
      sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt
      mollit anim id est laborum.&lt;/font
    &gt;
    Figure &lt;a href=&#34;#fig:park-types&#34;&gt;1&lt;/a&gt; shows the number of parks within each
    type.
  &lt;/p&gt;
  &lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Showing the code because echo = TRUE 
dat = read.csv(&amp;#39;https://opendata.arcgis.com/datasets/9e00ff81868e49b7ba65d4e628b9e14f_6.csv&amp;#39;)

dat = 
  dat %&amp;gt;%
  group_by(Type) %&amp;gt;%
  mutate(parks_number = n())

ggplotly(
  ggplot(dat, aes(x=reorder(Type, parks_number), y=parks_number,
                  text = paste(&amp;#39;Number of parks =&amp;#39;, parks_number))) + 
    theme(axis.title.y=element_blank()) + stat_identity(geom=&amp;#39;bar&amp;#39;) + 
    labs(x=&amp;#39;Type&amp;#39;, y=&amp;#39;Number of parks&amp;#39;) + coord_flip(), 
  tooltip = &amp;#39;text&amp;#39;
)&lt;/code&gt;&lt;/pre&gt;
  &lt;div class=&#34;figure&#34;&gt;
    &lt;span style=&#34;display: block&#34; id=&#34;fig:park-types&#34;&gt;&lt;/span&gt;
    &lt;div
      id=&#34;htmlwidget-1&#34;
      style=&#34;width: 672px; height: 480px&#34;
      class=&#34;plotly html-widget&#34;
    &gt;&lt;/div&gt;
    &lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;
      {
        &#34;x&#34;: {
          &#34;data&#34;: [
            {
              &#34;orientation&#34;: &#34;h&#34;,
              &#34;width&#34;: [
                0.899999999999999, 0.899999999999999, 0.9, 0.899999999999999,
                0.899999999999999, 0.9, 0.9, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999, 0.9,
                0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999, 0.9,
                0.9, 0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.9, 0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.9, 0.9,
                0.899999999999999, 0.899999999999999, 0.899999999999999, 0.9,
                0.899999999999999, 0.9, 0.899999999999999, 0.9, 0.9,
                0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.9, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.9, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.9, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999, 0.9,
                0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.9, 0.899999999999999, 0.9,
                0.899999999999999, 0.899999999999999, 0.899999999999999, 0.9,
                0.9, 0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.9, 0.9, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999, 0.9,
                0.9, 0.899999999999999, 0.9, 0.899999999999999, 0.9,
                0.899999999999999, 0.9, 0.9, 0.9, 0.899999999999999,
                0.899999999999999, 0.9, 0.9, 0.899999999999999, 0.9, 0.9,
                0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999, 0.9,
                0.9, 0.9, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.9, 0.9, 0.899999999999999, 0.9,
                0.899999999999999, 0.899999999999999, 0.9, 0.899999999999999,
                0.9, 0.899999999999999, 0.9, 0.9, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.9, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.9, 0.899999999999999,
                0.9, 0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.9, 0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.9, 0.9, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999, 0.9,
                0.899999999999999, 0.899999999999999, 0.9, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.9, 0.899999999999999,
                0.9, 0.899999999999999, 0.9, 0.9, 0.9, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.9, 0.899999999999999, 0.9, 0.9, 0.9, 0.9,
                0.899999999999999, 0.9, 0.9, 0.9, 0.899999999999999,
                0.899999999999999, 0.9, 0.9, 0.9, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.9, 0.899999999999999,
                0.9, 0.899999999999999, 0.9, 0.9, 0.899999999999999,
                0.899999999999999, 0.9, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.9, 0.899999999999999,
                0.899999999999999, 0.9, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.9, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.9, 0.9,
                0.899999999999999, 0.899999999999999, 0.9, 0.9,
                0.899999999999999, 0.899999999999999, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.9, 0.899999999999999,
                0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.899999999999999,
                0.899999999999999, 0.899999999999999, 0.9, 0.9, 0.9, 0.9,
                0.899999999999999, 0.899999999999999
              ],
              &#34;base&#34;: [
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
              ],
              &#34;x&#34;: [
                100, 100, 22, 78, 78, 21, 31, 100, 78, 100, 78, 31, 100, 78,
                100, 78, 78, 100, 21, 31, 100, 78, 100, 2, 100, 100, 100, 100,
                78, 100, 100, 100, 78, 100, 100, 21, 2, 78, 78, 100, 21, 100,
                31, 100, 22, 1, 78, 100, 78, 78, 78, 100, 100, 13, 78, 100, 78,
                100, 22, 100, 78, 100, 31, 78, 100, 100, 78, 31, 100, 100, 100,
                78, 100, 100, 78, 21, 100, 21, 100, 78, 100, 21, 21, 100, 100,
                100, 1, 31, 100, 78, 100, 78, 78, 13, 21, 78, 22, 100, 22, 100,
                31, 31, 31, 78, 100, 31, 31, 100, 22, 31, 78, 100, 78, 100, 100,
                100, 78, 100, 100, 31, 21, 22, 78, 78, 78, 22, 13, 100, 31, 100,
                78, 31, 78, 21, 100, 21, 31, 78, 100, 78, 31, 100, 78, 78, 22,
                78, 21, 100, 100, 100, 31, 78, 100, 78, 100, 22, 22, 78, 100,
                78, 78, 13, 78, 100, 31, 78, 100, 78, 31, 100, 13, 100, 22, 13,
                13, 78, 78, 100, 78, 78, 78, 100, 100, 22, 78, 21, 13, 21, 22,
                78, 21, 13, 22, 100, 78, 31, 31, 13, 78, 78, 100, 100, 100, 78,
                100, 100, 100, 21, 100, 31, 100, 31, 22, 100, 78, 22, 100, 100,
                100, 78, 31, 100, 78, 31, 78, 100, 78, 78, 78, 100, 22, 78, 100,
                100, 100, 78, 100, 100, 100, 78, 78, 21, 31, 78, 78, 13, 22, 78,
                100, 100, 100, 78, 21, 100, 13, 21, 22, 31, 31, 31, 100, 78,
                100, 13, 22, 21, 22, 100, 78
              ],
              &#34;y&#34;: [
                9, 9, 6, 8, 8, 5, 7, 9, 8, 9, 8, 7, 9, 8, 9, 8, 8, 9, 5, 7, 9,
                8, 9, 3, 9, 9, 9, 9, 8, 9, 9, 9, 8, 9, 9, 5, 3, 8, 8, 9, 5, 9,
                7, 9, 6, 2, 8, 9, 8, 8, 8, 9, 9, 4, 8, 9, 8, 9, 6, 9, 8, 9, 7,
                8, 9, 9, 8, 7, 9, 9, 9, 8, 9, 9, 8, 5, 9, 5, 9, 8, 9, 5, 5, 9,
                9, 9, 1, 7, 9, 8, 9, 8, 8, 4, 5, 8, 6, 9, 6, 9, 7, 7, 7, 8, 9,
                7, 7, 9, 6, 7, 8, 9, 8, 9, 9, 9, 8, 9, 9, 7, 5, 6, 8, 8, 8, 6,
                4, 9, 7, 9, 8, 7, 8, 5, 9, 5, 7, 8, 9, 8, 7, 9, 8, 8, 6, 8, 5,
                9, 9, 9, 7, 8, 9, 8, 9, 6, 6, 8, 9, 8, 8, 4, 8, 9, 7, 8, 9, 8,
                7, 9, 4, 9, 6, 4, 4, 8, 8, 9, 8, 8, 8, 9, 9, 6, 8, 5, 4, 5, 6,
                8, 5, 4, 6, 9, 8, 7, 7, 4, 8, 8, 9, 9, 9, 8, 9, 9, 9, 5, 9, 7,
                9, 7, 6, 9, 8, 6, 9, 9, 9, 8, 7, 9, 8, 7, 8, 9, 8, 8, 8, 9, 6,
                8, 9, 9, 9, 8, 9, 9, 9, 8, 8, 5, 7, 8, 8, 4, 6, 8, 9, 9, 9, 8,
                5, 9, 4, 5, 6, 7, 7, 7, 9, 8, 9, 4, 6, 5, 6, 9, 8
              ],
              &#34;text&#34;: [
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 2&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 2&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 1&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 13&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 1&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 13&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 13&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 13&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 13&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 13&#34;,
                &#34;Number of parks = 13&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 13&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 13&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 13&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 13&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 13&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 31&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 13&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 21&#34;,
                &#34;Number of parks = 22&#34;,
                &#34;Number of parks = 100&#34;,
                &#34;Number of parks = 78&#34;
              ],
              &#34;type&#34;: &#34;bar&#34;,
              &#34;textposition&#34;: &#34;none&#34;,
              &#34;marker&#34;: {
                &#34;autocolorscale&#34;: false,
                &#34;color&#34;: &#34;rgba(89,89,89,1)&#34;,
                &#34;line&#34;: { &#34;width&#34;: 1.88976377952756, &#34;color&#34;: &#34;transparent&#34; }
              },
              &#34;showlegend&#34;: false,
              &#34;xaxis&#34;: &#34;x&#34;,
              &#34;yaxis&#34;: &#34;y&#34;,
              &#34;hoverinfo&#34;: &#34;text&#34;,
              &#34;frame&#34;: null
            }
          ],
          &#34;layout&#34;: {
            &#34;margin&#34;: {
              &#34;t&#34;: 26.2283105022831,
              &#34;r&#34;: 7.30593607305936,
              &#34;b&#34;: 40.1826484018265,
              &#34;l&#34;: 92.7853881278539
            },
            &#34;plot_bgcolor&#34;: &#34;rgba(235,235,235,1)&#34;,
            &#34;paper_bgcolor&#34;: &#34;rgba(255,255,255,1)&#34;,
            &#34;font&#34;: {
              &#34;color&#34;: &#34;rgba(0,0,0,1)&#34;,
              &#34;family&#34;: &#34;&#34;,
              &#34;size&#34;: 14.6118721461187
            },
            &#34;xaxis&#34;: {
              &#34;domain&#34;: [0, 1],
              &#34;automargin&#34;: true,
              &#34;type&#34;: &#34;linear&#34;,
              &#34;autorange&#34;: false,
              &#34;range&#34;: [-5, 105],
              &#34;tickmode&#34;: &#34;array&#34;,
              &#34;ticktext&#34;: [&#34;0&#34;, &#34;25&#34;, &#34;50&#34;, &#34;75&#34;, &#34;100&#34;],
              &#34;tickvals&#34;: [0, 25, 50, 75, 100],
              &#34;categoryorder&#34;: &#34;array&#34;,
              &#34;categoryarray&#34;: [&#34;0&#34;, &#34;25&#34;, &#34;50&#34;, &#34;75&#34;, &#34;100&#34;],
              &#34;nticks&#34;: null,
              &#34;ticks&#34;: &#34;outside&#34;,
              &#34;tickcolor&#34;: &#34;rgba(51,51,51,1)&#34;,
              &#34;ticklen&#34;: 3.65296803652968,
              &#34;tickwidth&#34;: 0.66417600664176,
              &#34;showticklabels&#34;: true,
              &#34;tickfont&#34;: {
                &#34;color&#34;: &#34;rgba(77,77,77,1)&#34;,
                &#34;family&#34;: &#34;&#34;,
                &#34;size&#34;: 11.689497716895
              },
              &#34;tickangle&#34;: -0,
              &#34;showline&#34;: false,
              &#34;linecolor&#34;: null,
              &#34;linewidth&#34;: 0,
              &#34;showgrid&#34;: true,
              &#34;gridcolor&#34;: &#34;rgba(255,255,255,1)&#34;,
              &#34;gridwidth&#34;: 0.66417600664176,
              &#34;zeroline&#34;: false,
              &#34;anchor&#34;: &#34;y&#34;,
              &#34;title&#34;: {
                &#34;text&#34;: &#34;Number of parks&#34;,
                &#34;font&#34;: {
                  &#34;color&#34;: &#34;rgba(0,0,0,1)&#34;,
                  &#34;family&#34;: &#34;&#34;,
                  &#34;size&#34;: 14.6118721461187
                }
              },
              &#34;hoverformat&#34;: &#34;.2f&#34;
            },
            &#34;yaxis&#34;: {
              &#34;domain&#34;: [0, 1],
              &#34;automargin&#34;: true,
              &#34;type&#34;: &#34;linear&#34;,
              &#34;autorange&#34;: false,
              &#34;range&#34;: [0.4, 9.6],
              &#34;tickmode&#34;: &#34;array&#34;,
              &#34;ticktext&#34;: [
                &#34;OTHER&#34;,
                &#34;SPECIAL&#34;,
                &#34;SPORTS COMPLEX&#34;,
                &#34;TRAFFICWAY&#34;,
                &#34;CONSERVATION&#34;,
                &#34;OPEN SPACE&#34;,
                &#34;COMMUNITY&#34;,
                &#34;NEIGHBORHOOD&#34;,
                &#34;MINI&#34;
              ],
              &#34;tickvals&#34;: [1, 2, 3, 4, 5, 6, 7, 8, 9],
              &#34;categoryorder&#34;: &#34;array&#34;,
              &#34;categoryarray&#34;: [
                &#34;OTHER&#34;,
                &#34;SPECIAL&#34;,
                &#34;SPORTS COMPLEX&#34;,
                &#34;TRAFFICWAY&#34;,
                &#34;CONSERVATION&#34;,
                &#34;OPEN SPACE&#34;,
                &#34;COMMUNITY&#34;,
                &#34;NEIGHBORHOOD&#34;,
                &#34;MINI&#34;
              ],
              &#34;nticks&#34;: null,
              &#34;ticks&#34;: &#34;outside&#34;,
              &#34;tickcolor&#34;: &#34;rgba(51,51,51,1)&#34;,
              &#34;ticklen&#34;: 3.65296803652968,
              &#34;tickwidth&#34;: 0.66417600664176,
              &#34;showticklabels&#34;: true,
              &#34;tickfont&#34;: {
                &#34;color&#34;: &#34;rgba(77,77,77,1)&#34;,
                &#34;family&#34;: &#34;&#34;,
                &#34;size&#34;: 11.689497716895
              },
              &#34;tickangle&#34;: -0,
              &#34;showline&#34;: false,
              &#34;linecolor&#34;: null,
              &#34;linewidth&#34;: 0,
              &#34;showgrid&#34;: true,
              &#34;gridcolor&#34;: &#34;rgba(255,255,255,1)&#34;,
              &#34;gridwidth&#34;: 0.66417600664176,
              &#34;zeroline&#34;: false,
              &#34;anchor&#34;: &#34;x&#34;,
              &#34;title&#34;: {
                &#34;text&#34;: &#34;&#34;,
                &#34;font&#34;: { &#34;color&#34;: null, &#34;family&#34;: null, &#34;size&#34;: 0 }
              },
              &#34;hoverformat&#34;: &#34;.2f&#34;
            },
            &#34;shapes&#34;: [
              {
                &#34;type&#34;: &#34;rect&#34;,
                &#34;fillcolor&#34;: null,
                &#34;line&#34;: { &#34;color&#34;: null, &#34;width&#34;: 0, &#34;linetype&#34;: [] },
                &#34;yref&#34;: &#34;paper&#34;,
                &#34;xref&#34;: &#34;paper&#34;,
                &#34;x0&#34;: 0,
                &#34;x1&#34;: 1,
                &#34;y0&#34;: 0,
                &#34;y1&#34;: 1
              }
            ],
            &#34;showlegend&#34;: false,
            &#34;legend&#34;: {
              &#34;bgcolor&#34;: &#34;rgba(255,255,255,1)&#34;,
              &#34;bordercolor&#34;: &#34;transparent&#34;,
              &#34;borderwidth&#34;: 1.88976377952756,
              &#34;font&#34;: {
                &#34;color&#34;: &#34;rgba(0,0,0,1)&#34;,
                &#34;family&#34;: &#34;&#34;,
                &#34;size&#34;: 11.689497716895
              }
            },
            &#34;hovermode&#34;: &#34;closest&#34;,
            &#34;barmode&#34;: &#34;relative&#34;
          },
          &#34;config&#34;: {
            &#34;doubleClick&#34;: &#34;reset&#34;,
            &#34;modeBarButtonsToAdd&#34;: [&#34;hoverclosest&#34;, &#34;hovercompare&#34;],
            &#34;showSendToCloud&#34;: false
          },
          &#34;source&#34;: &#34;A&#34;,
          &#34;attrs&#34;: {
            &#34;e434646d209&#34;: { &#34;x&#34;: {}, &#34;y&#34;: {}, &#34;text&#34;: {}, &#34;type&#34;: &#34;bar&#34; }
          },
          &#34;cur_data&#34;: &#34;e434646d209&#34;,
          &#34;visdat&#34;: { &#34;e434646d209&#34;: [&#34;function (y) &#34;, &#34;x&#34;] },
          &#34;highlight&#34;: {
            &#34;on&#34;: &#34;plotly_click&#34;,
            &#34;persistent&#34;: false,
            &#34;dynamic&#34;: false,
            &#34;selectize&#34;: false,
            &#34;opacityDim&#34;: 0.2,
            &#34;selected&#34;: { &#34;opacity&#34;: 1 },
            &#34;debounce&#34;: 0
          },
          &#34;shinyEvents&#34;: [
            &#34;plotly_hover&#34;,
            &#34;plotly_click&#34;,
            &#34;plotly_selected&#34;,
            &#34;plotly_relayout&#34;,
            &#34;plotly_brushed&#34;,
            &#34;plotly_brushing&#34;,
            &#34;plotly_clickannotation&#34;,
            &#34;plotly_doubleclick&#34;,
            &#34;plotly_deselect&#34;,
            &#34;plotly_afterplot&#34;,
            &#34;plotly_sunburstclick&#34;
          ],
          &#34;base_url&#34;: &#34;https://plot.ly&#34;
        },
        &#34;evals&#34;: [],
        &#34;jsHooks&#34;: []
      }
    &lt;/script&gt;
    &lt;p class=&#34;caption&#34;&gt;Figure 1: Number of parks within each type.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;twenty-largest-parks&#34; class=&#34;section level2&#34;&gt;
  &lt;h2&gt;Twenty largest parks&lt;/h2&gt;
  &lt;p&gt;
    &lt;font style=&#34;color: grey&#34;
      &gt;[Placeholder text] Lorem ipsum dolor sit amet, consectetur adipiscing
      elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.
      Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut
      aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in
      voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur
      sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt
      mollit anim id est laborum.&lt;/font
    &gt;
    Table &lt;a href=&#34;#tab:largest-parks&#34;&gt;1&lt;/a&gt; shows the twenty largest parks
    types, along with their type and acreage. The code doesn’t show below
    because &lt;code&gt;echo = FALSE&lt;/code&gt;.
  &lt;/p&gt;
  &lt;table&gt;
    &lt;caption&gt;
      &lt;span id=&#34;tab:largest-parks&#34;&gt;Table 1: &lt;/span
      &gt;The twenty largest parks in Madison.
    &lt;/caption&gt;
    &lt;thead&gt;
      &lt;tr class=&#34;header&#34;&gt;
        &lt;th align=&#34;left&#34;&gt;Name&lt;/th&gt;
        &lt;th align=&#34;left&#34;&gt;Type&lt;/th&gt;
        &lt;th align=&#34;right&#34;&gt;Acreage&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr class=&#34;odd&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Cherokee Marsh - North Unit&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;CONSERVATION&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;946.58&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;even&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Cherokee Marsh - South Unit (School Road Unit)&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;CONSERVATION&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;261.27&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;odd&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Elver Park&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;250.82&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;even&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Northeast Park&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;237.76&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;odd&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Warner Park&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;213.49&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;even&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Door Creek Park&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;159.97&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;odd&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Cherokee Marsh - Mendota Unit&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;CONSERVATION&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;122.08&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;even&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Owen Conservation Park&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;CONSERVATION&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;96.79&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;odd&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Reindahl (Amund) Park&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;90.74&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;even&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Olbrich Park&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;90.01&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;odd&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Yahara Hills Park (West)&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;82.20&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;even&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Sycamore Park&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;71.42&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;odd&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Turville Point Conservation Park&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;CONSERVATION&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;64.28&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;even&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Edna Taylor Conservation Park&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;CONSERVATION&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;60.27&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;odd&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Quann Park&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;55.43&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;even&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Demetral Park&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;49.18&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;odd&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Prairie Ridge Conservation Park&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;CONSERVATION&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;48.76&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;even&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Olin Park&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;47.12&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;odd&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Hiestand Park&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;46.27&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr class=&#34;even&#34;&gt;
        &lt;td align=&#34;left&#34;&gt;Vilas (Henry) Park&lt;/td&gt;
        &lt;td align=&#34;left&#34;&gt;COMMUNITY&lt;/td&gt;
        &lt;td align=&#34;right&#34;&gt;45.67&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
  &lt;p&gt;
    We could decide to display the code for the table at this point (or any
    other), which can be done using the parameters
    &lt;code&gt;ref.label = &#39;largest-parks&#39;, echo = TRUE, eval = FALSE&lt;/code&gt; in the
    chunk options (&lt;a
      href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook/reuse-chunks.html&#34;
      &gt;Xie, Dervieux, &amp;amp; Riederer, 2020&lt;/a
    &gt;).
  &lt;/p&gt;
  &lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  summarize(Name = Park_Name, Type, Acreage) %&amp;gt;%
  arrange(-Acreage, Name, Type) %&amp;gt;%
  select(Name, Type, Acreage) %&amp;gt;%
  head(20) %&amp;gt;%
  kable(caption = &amp;#39;The twenty largest parks in Madison.&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to engage Research Group Leaders in sustainable software practices</title>
      <link>https://pablobernabeu.github.io/2020/how-to-engage-research-group-leaders-in-sustainable-software-practices/</link>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2020/how-to-engage-research-group-leaders-in-sustainable-software-practices/</guid>
      <description>


</description>
    </item>
    
    <item>
      <title>Incentives for good research software practices</title>
      <link>https://pablobernabeu.github.io/2020/incentives-for-good-research-software-practices/</link>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2020/incentives-for-good-research-software-practices/</guid>
      <description>


</description>
    </item>
    
    <item>
      <title>Data is present: Workshops and datathons</title>
      <link>https://pablobernabeu.github.io/2020/data-is-present-workshops-and-datathons/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2020/data-is-present-workshops-and-datathons/</guid>
      <description>


&lt;div id=&#34;enhanced-data-presentation-using-reproducible-documents-and-dashboards&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Enhanced data presentation using reproducible documents and dashboards&lt;/h2&gt;
&lt;div id=&#34;calendar&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calendar&lt;/h3&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;9%&#34; /&gt;
&lt;col width=&#34;23%&#34; /&gt;
&lt;col width=&#34;39%&#34; /&gt;
&lt;col width=&#34;27%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Date&lt;/th&gt;
&lt;th&gt;Title&lt;/th&gt;
&lt;th&gt;Event and location&lt;/th&gt;
&lt;th&gt;Registration&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;26 Nov 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pablobernabeu.github.io/presentation/2020-11-26-mixed-effects-models-in-r-and-a-new-tool-for-data-simulation&#34;&gt;Mixed-effects models in R, and a new tool for data simulation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;New Tricks Seminars, Dept. Psychology, Lancaster University [online]&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;8 Oct 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pablobernabeu.github.io/presentation/2020-10-08-reproducibilidad-en-torno-a-una-aplicacion-web/&#34;&gt;Reproducibilidad en torno a una aplicación web&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Reprohack en español, LatinR Conference 2020 [online]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.eventbrite.com.ar/e/reprohack-en-espanol-latinr-2020-tickets-121741832097&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;13 Aug 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/pablobernabeu/CarpentryCon-2020-workshop-Open-Data-Reproducibility&#34;&gt;Open data and reproducibility: R Markdown, data dashboards and Binder v2.1&lt;/a&gt; (co-led with Florencia D’Andrea)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://2020.carpentrycon.org/schedule/&#34;&gt;CarpentryCon@Home&lt;/a&gt;, The Carpentries [online]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://2020.carpentrycon.org/schedule/&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;26 July 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/pablobernabeu/UKCLC2020-workshop-Open-data-and-reproducibility&#34;&gt;Open data and reproducibility: R Markdown, data dashboards and Binder&lt;/a&gt; (co-led with Eirini Zormpa)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.ukclc2020.com/pre-conference&#34;&gt;UK Cognitive Linguistics Conference&lt;/a&gt;, University of Birmingham [online]&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.ukclc2020.com/registration&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;6 May 2020&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pablobernabeu.github.io/PDF/LU-May-Markdown-workshop-programme.pdf&#34;&gt;R Markdown&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Lancaster University [online]&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;Event cancelled&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://newcastle2020.satrdays.org/&#34;&gt;Open data and reproducibility 2.0&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://newcastle2020.satrdays.org/&#34;&gt;SatRday Newcastle upon Tyne&lt;/a&gt;, Newcastle University&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://newcastle2020.satrdays.org/&#34;&gt;Link&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div style=&#34;margin-top: 5%;&#34;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;This project offers free activities to learn and practise reproducible data presentation. &lt;a href=&#34;https://www.software.ac.uk/about/fellows/pablo-bernabeu&#34;&gt;Pablo Bernabeu&lt;/a&gt; organises these events in the context of a &lt;a href=&#34;https://www.software.ac.uk/programmes-and-events/fellowship-programme&#34;&gt;Software Sustainability Institute Fellowship&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;open-source-software&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Open-source software&lt;/h4&gt;
&lt;p&gt;Programming languages such as &lt;a href=&#34;https://www.r-project.org/about.html&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;https://www.python.org/&#34;&gt;Python&lt;/a&gt; offer free, powerful resources for data processing, visualisation and analysis. Experience in these programs is highly valued in data-intensive disciplines.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;open-data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Open data&lt;/h4&gt;
&lt;p&gt;Original data has become a &lt;a href=&#34;https://www.nature.com/articles/d41586-019-01506-x&#34;&gt;public good in many research fields&lt;/a&gt; thanks to cultural and technological advances. On the internet, we can find innumerable data sets from sources such as scientific journals and repositories (e.g., &lt;a href=&#34;https://osf.io&#34;&gt;OSF&lt;/a&gt;), local and national governments (e.g., &lt;a href=&#34;https://data.london.gov.uk/&#34;&gt;London&lt;/a&gt;, UK [&lt;a href=&#34;https://www.ukdataservice.ac.uk/&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://data.gov.uk/&#34;&gt;2&lt;/a&gt;]), non-governmental organisations (e.g., &lt;a href=&#34;https://data.world/datasets/ngo&#34;&gt;data.world&lt;/a&gt;), etc. Researchers inside and outside academia nowadays share a lot of their data under attribution licences (e.g., &lt;a href=&#34;https://creativecommons.org/&#34;&gt;Creative Commons&lt;/a&gt;, the UK &lt;a href=&#34;http://www.nationalarchives.gov.uk/doc/open-government-licence/version/1/&#34;&gt;Open Government Licence&lt;/a&gt;, etc.). This allows any external analysts to access these raw data, create (additional) visualisations and analyses, and share these. In society, making data more accessible can &lt;a href=&#34;https://digitalcommons.law.yale.edu/cgi/viewcontent.cgi?article=1140&amp;amp;context=yhrdlj&#34;&gt;demonstrably benefit citizens&lt;/a&gt; (despite &lt;a href=&#34;https://firstmonday.org/ojs/index.php/fm/article/view/3316/2764#author&#34;&gt;limitations&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;activities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Activities&lt;/h2&gt;
&lt;p&gt;Activities comprise free &lt;strong&gt;workshops&lt;/strong&gt; and &lt;a href=&#34;#datathons-creating-reproducible-documents-and-dashboards&#34;&gt;&lt;strong&gt;datathons&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;workshops&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Workshops&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.r-project.org&#34;&gt;R is a programming language&lt;/a&gt; greatly equipped for the creation of reproducible documents and dashboards. Four workshops are offered that cover a suite of interrelated tools—R, R Markdown, data dashboards and Binder environments—, all underlain by reproducible workflows and open-source software.&lt;/p&gt;
&lt;p&gt;Each workshop includes &lt;strong&gt;taught and practical sections&lt;/strong&gt;. The practice provides a chance for participants to experience and address common issues with the code. The level of taught sections is largely tailored to participants; similarly, practice sections are individually adaptable by means of easier and tougher tasks. The duration is also flexible, and some of the workshops can be combined into the same session.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://rstudio.com/&#34;&gt;RStudio&lt;/a&gt; interface is used in all workshops. Multi-levelled, real code examples are used. Throughout the workshops, and especially in the practice sections, individual questions will be encouraged.&lt;/p&gt;
&lt;div id=&#34;workshop-1-introduction-to-r&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 1: Introduction to R&lt;/h4&gt;
&lt;p&gt;This workshop can serve as an introduction to R or a revision. It demonstrates what can be done in R, and provides resources for individual training. Since the duration is limited, online courses are also recommended (&lt;a href=&#34;https://www.coursera.org/courses?query=r&#34;&gt;see examples&lt;/a&gt; and &lt;a href=&#34;https://learner.coursera.help/hc/en-us/articles/209819033-Apply-for-Financial-Aid-or-a-Scholarship&#34;&gt;fee waivers for full content&lt;/a&gt;).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://adv-r.had.co.nz/Data-structures.html&#34;&gt;Data structures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.sthda.com/english/wiki/installing-and-using-r-packages&#34;&gt;Packages&lt;/a&gt;: general-purpose examples (e.g., &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt;) and more specific ones (e.g., for &lt;a href=&#34;https://cran.r-project.org/web/packages/lsr/lsr.pdf&#34;&gt;statistics&lt;/a&gt; or &lt;a href=&#34;https://cran.r-project.org/web/packages/GEOmap/GEOmap.pdf&#34;&gt;geography&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/rio/vignettes/rio.html&#34;&gt;Loading and writing data, in native and foreign formats&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Tidy&lt;/em&gt; format&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Wide&lt;/em&gt; versus &lt;em&gt;long&lt;/em&gt; format. For most processes in R, &lt;a href=&#34;https://r4ds.had.co.nz/tidy-data.html&#34;&gt;data needs to be in a tidy, long format&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://doi.org/10.1371/journal.pbio.3000202.g001&#39;&gt;&lt;img width = &#39;45%&#39; src = &#39;https://journals.plos.org/plosbiology/article/file?id=10.1371/journal.pbio.3000202.g001&amp;type=large&#39; alt = &#39;Illustration of wide and tidy data formats, from Postma and Goedhart (2019)&#39; style = &#39;margin-top:-0.1px;&#39; /&gt;&lt;/a&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:80%; color:darkgrey; text-align:center; margin-top:-24px; margin-bottom:20px;&#34;&gt;
Illustration from &lt;a href=&#34;https://doi.org/10.1371/journal.pbio.3000202.g001&#34;&gt;Postma and Goedhart (2019)&lt;/a&gt;.
&lt;/p&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://psyteachr.github.io/msc-data-skills/joins.html#joins&#34;&gt;Combining data sets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cookbook-r.com/Manipulating_data/Summarizing_data/&#34;&gt;Data summaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://philmcaleer.github.io/ug2-practical/visualisation-through-ggplot2.html&#34;&gt;Plots with &lt;code&gt;ggplot2::ggplot()&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://plot.ly/ggplot2/&#34;&gt;Interactive plots with &lt;code&gt;plotly::ggplotly()&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learningstatisticswithr-bookdown.netlify.com/part-v-statistical-tools.html&#34;&gt;Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer_and_Curtin_LMEMs-2017-Psych_Methods.pdf&#34;&gt;Linear mixed-effects models&lt;/a&gt; (see also &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0749596X20300061?dgcid=coauthor#b0670&#34;&gt;a review of practices&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://swcarpentry.github.io/r-novice-inflammation/02-func-R/&#34;&gt;How functions work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Debugging&lt;/em&gt;. Code errors are known as bugs. They can tiresome, but also interesting sometimes! :sweat_smile: Some tips for the first many years of experience include: reading and investigating error messages, in both source and console windows; controlling letter case and typos; closing parentheses and inverted commas; ensuring to have the necessary packages installed and loaded; following the format required by each function. To debug, break up code into subcomponents and test each of those to find out the source of the error. Once we act on that, the best outcome is seeing the code work, but sometimes different errors overlap, in which case we may see one error disappearing before another one appears. Debugging soon leads to proficient information seeking. The search process often begins on an internet search engine and extends to user communities, package documentation, tutorials, blogs… (see &lt;a href=&#34;https://youtu.be/Nj9J5iCSMB0?t=2687&#34;&gt;video explanation&lt;/a&gt;). &lt;a href=&#34;https://adv-r.hadley.nz/debugging.html&#34;&gt;Advanced debugging tools are also available&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Vast availability of free resources on the internet, from &lt;a href=&#34;https://www.coursera.org/courses?query=r%20programming&#34;&gt;Coursera&lt;/a&gt; and other MOOC sites, &lt;a href=&#34;https://education.rstudio.com/&#34;&gt;RStudio&lt;/a&gt;, &lt;a href=&#34;https://psyteachr.github.io/&#34;&gt;University of Glasgow&lt;/a&gt;, &lt;a href=&#34;http://swcarpentry.github.io/r-novice-inflammation/&#34;&gt;Carpentries&lt;/a&gt;, etc.&lt;/li&gt;
&lt;li&gt;Community: &lt;a href=&#34;https://stackoverflow.com/&#34;&gt;StackOverflow&lt;/a&gt;, &lt;a href=&#34;https://community.rstudio.com/&#34;&gt;RStudio Community&lt;/a&gt;, &lt;a href=&#34;https://github.com&#34;&gt;Github issues&lt;/a&gt; (e.g., for R packages), etc. Using and contributing back.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rstudio.cloud/&#34;&gt;RStudio Cloud&lt;/a&gt;: a personal RStudio environment on the internet&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;workshop-2-r-markdown-documents&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 2: R Markdown documents&lt;/h4&gt;
&lt;p&gt;Set your input and output in stone using &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;R Markdown&lt;/a&gt;. The analysis reports may be enriched with website features (HTML/CSS) and published as HTML, PDF or Word documents. Moreover, with R packages such as &lt;code&gt;bookdown&lt;/code&gt;, &lt;code&gt;bookdownplus&lt;/code&gt;, &lt;code&gt;blogdown&lt;/code&gt; and &lt;code&gt;flexdashboard&lt;/code&gt;, documents can be formatted as &lt;a href=&#34;https://awesome-blogdown.com/&#34;&gt;websites&lt;/a&gt;, &lt;a href=&#34;https://bookdown.org/&#34;&gt;digital papers and books&lt;/a&gt; and &lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/&#34;&gt;data dashboards&lt;/a&gt;. Other useful packages include &lt;code&gt;rmarkdown&lt;/code&gt;, &lt;code&gt;knitr&lt;/code&gt;, &lt;code&gt;DT&lt;/code&gt;, &lt;code&gt;kableExtra&lt;/code&gt; and &lt;code&gt;ggplot2&lt;/code&gt;. Further background: &lt;a href=&#34;https://www.youtube.com/watch?v=Nj9J5iCSMB0&#34;&gt;presentation by Michael Frank&lt;/a&gt;, &lt;a href=&#34;https://www.eddjberry.com/talks/reproducible-writing-with-rmarkdown.html#1&#34;&gt;slides by Ed Berry&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As well as facilitating the reproducibility of analyses and results to third parties, R Markdown is helpful &lt;em&gt;during the creation&lt;/em&gt; of a report. In particular, it reduces the chances of errors and the number of repetitive tasks. For instance, any part of the data can be inputted in the text directly from the source, rather than manually copying it (e.g., &lt;code&gt;`r mean(dat[dat$location==&#39;Havana&#39;, &#39;measure&#39;])`&lt;/code&gt; (&lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/r-code.html&#34;&gt;expand&lt;/a&gt;). Thus, if and when the analysis needs to be changed or updated, the report can be automatically updated at the click of a button. In another area, the captions for figures and tables can be automatised using &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook/cross-ref.html&#34;&gt;cross-reference labels&lt;/a&gt; (e.g., Table &lt;code&gt;\@ref(tab:mtcars)&lt;/code&gt;). This secures the match between the text and the captions of figures and tables, and it automatically updates the numbering whenever and wherever a new figure or table is introduced.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://bookdownplus.netlify.com/portfolio/&#39;&gt;&lt;img width = &#39;50%&#39; src = &#39;https://github.com/pablobernabeu/bookdownplus/blob/master/inst2/copernicus/showcase/copernicus2.png?raw=true&#39; alt = &#39;Example of paper created with bookdownplus (image retrieved from R bookdownplus package)&#39;/&gt;&lt;/a&gt;
&lt;p align=&#34;center&#34; style=&#34;text-align:center; margin-top:-30px; margin-bottom:30px;&#34;&gt;
Image from bookdownplus package (&lt;a href=&#34;https://bookdownplus.netlify.com/portfolio/&#34; class=&#34;uri&#34;&gt;https://bookdownplus.netlify.com/portfolio/&lt;/a&gt;).
&lt;/p&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;workshop-3-introduction-to-data-dashboards&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 3: Introduction to data dashboards&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01782/full&#34;&gt;Data dashboards are web applications used to visualise data&lt;/a&gt; in detail through tables and plots. They assist in explaining and accounting for our data processing and analysis. They don’t require any coding from the end user. While most dashboards and web applications present existing data, a few of them serve the purpose of creating or simulating new data (see &lt;a href=&#34;https://github.com/pablobernabeu/Experimental-data-simulation&#34;&gt;example&lt;/a&gt;).&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/&#39;&gt; &lt;img width = &#39;90%&#39; src = &#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/8.png&#39; alt = &#39;Illustration of the usage of dashboards alongside data repositories&#39; /&gt; &lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;These all-reproducible dashboards are published as websites, and thus, they can include hyperlinks and downloadable files. Some of the R packages used are &lt;code&gt;knitr&lt;/code&gt;, &lt;code&gt;DT&lt;/code&gt;, &lt;code&gt;kableExtra&lt;/code&gt;, &lt;code&gt;reactable&lt;/code&gt;, &lt;code&gt;ggplot2&lt;/code&gt;, &lt;code&gt;plotly&lt;/code&gt;, &lt;code&gt;rmarkdown&lt;/code&gt;, &lt;code&gt;flexdashboard&lt;/code&gt; and &lt;code&gt;shiny&lt;/code&gt;. The aim of this workshop is to practise creating different forms of dashboards—&lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/&#34;&gt;Flexdashboard&lt;/a&gt; and &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;Shiny&lt;/a&gt;—the latter of which offers &lt;a href=&#34;https://mastering-shiny.org/&#34;&gt;greater features&lt;/a&gt;, and to practise also with the hosting platforms fitting each type—such as personal websites, &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;, &lt;a href=&#34;https://mybinder.org/&#34;&gt;Binder&lt;/a&gt;, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;Shinyapps&lt;/a&gt; and &lt;a href=&#34;https://rstudio.com/products/shiny/shiny-server/&#34;&gt;custom servers&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A great thing about dashboards is that they may be made very simple, but they can also be taken to the next level using some HTML, CSS or Javascript code (on top of the back-end code present in the R packages used), which is addressed in the next workshop.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;workshop-4-binder-environments-and-improving-data-dashboards&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Workshop 4: Binder environments and improving data dashboards&lt;/h4&gt;
&lt;div id=&#34;binder&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Binder&lt;/h5&gt;
&lt;p&gt;&lt;a href=&#34;https://mybinder.org/&#34;&gt;Binder&lt;/a&gt; is a tool to facilitate public access to software environments—for instance, by publishing an RStudio environment on the internet. Binder can also host Shiny apps. It is generously free &lt;a href=&#34;https://discourse.jupyter.org/t/mybinder-org-cost-updates/2426&#34;&gt;for users&lt;/a&gt;. After looking at the &lt;a href=&#34;https://github.com/binder-examples/r&#34;&gt;nuts and bolts of a deployment&lt;/a&gt;, participants will be able to deploy their own Binder environments and check the result by the end of the workshop. For this purpose, it’s recommended to have data and R code ready, ideally in a GitHub repository.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;improving-data-dashboards&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Improving data dashboards&lt;/h5&gt;
&lt;p&gt;We will practise how to improve the functionality of dashboards using some HTML, CSS and Javascript code, which is &lt;a href=&#34;https://www.w3schools.com/whatis/&#34;&gt;the basis of websites&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!-- Javascript function to enable a hovering tooltip --&amp;gt;
&amp;lt;script&amp;gt;
$(document).ready(function(){
$(&amp;#39;[data-toggle=&amp;quot;tooltip1&amp;quot;]&amp;#39;).tooltip();
});
&amp;lt;/script&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;a href = &#39;https://shiny.rstudio.com/gallery/&#39;&gt; &lt;img align = &#39;center&#39; width = &#39;60%&#39; src = &#39;https://raw.githubusercontent.com/pablobernabeu/data-is-present/master/dashboard%20gif.gif&#39; alt = &#39;Examples of data dashboards&#39; /&gt; &lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trade-offs-among-dashboards&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Trade-offs among dashboards&lt;/h5&gt;
&lt;p&gt;Next, we will practise with three dashboard types—&lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/using.html&#34;&gt;Flexdashboard&lt;/a&gt;, &lt;a href=&#34;https://shiny.rstudio.com/tutorial/&#34;&gt;Shiny&lt;/a&gt; and &lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/shiny.html&#34;&gt;Flexdashboard-Shiny&lt;/a&gt;—and with the suitable hosting platforms. Firstly, the strength of Flexdashboard (&lt;a href=&#34;http://rpubs.com/pcbernabeu/Dutch-modality-exclusivity-norms&#34;&gt;example&lt;/a&gt;) is its basis on R Markdown, yielding an unmatched user interface (&lt;em&gt;front-end&lt;/em&gt;). Secondly, the strength of Shiny (&lt;a href=&#34;https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment/&#34;&gt;example&lt;/a&gt;) is the input reactivity (&lt;em&gt;back-end&lt;/em&gt;) it offers, allowing users to download sections of data they select, in various formats. Last, Flexdashboard-Shiny (&lt;a href=&#34;https://pablobernabeu.shinyapps.io/dutch-modality-exclusivity-norms/&#34;&gt;example&lt;/a&gt;) combines the best of both worlds.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ &lt;b&gt; Flexdashboard &lt;/b&gt; ★
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ ★ &lt;b&gt; Shiny &lt;/b&gt; ★ ★
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ ★ ★ &lt;b&gt; Flexdashboard-Shiny &lt;/b&gt; ★ ★ ★
&lt;/p&gt;
&lt;p&gt;Flexdashboard types are rendered as an HTML document—simple websites—, and can therefore be easily published on personal sites or &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;. This is convenient because no special hosting is required. In contrast, Shiny and Flexdashboard-Shiny types offer greater features, but require Shiny servers. Fortunately, the shinyapps.io server is available for free, up to some &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;usage limit&lt;/a&gt;. This server can host any of the three dashboards mentioned here. Another good option is presented by Binder environments, which can host the Shiny-type dashboards with no (explicit) limit. Yet, the Flexdashboard-Shiny type cannot be hosted in this server (&lt;a href=&#34;https://github.com/jupyter/repo2docker/issues/799&#34;&gt;as of January 2020, at least&lt;/a&gt;). Consequently, greater functionality may come at a cost for dashboards that have any considerable traffic, whereas dashboards with low traffic may do well on shinyapps.io. Knowing these trade-offs can help navigate &lt;a href=&#34;https://support.rstudio.com/hc/en-us/articles/217592947-What-are-the-limits-of-the-shinyapps-io-Free-plan-&#34;&gt;usage limits&lt;/a&gt;, save on web hosting fees, and increase the availability of our dashboards online, as we can offer fall-back versions on different platforms, as in the example below:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;… &lt;em&gt;&lt;a href=&#34;https://pablobernabeu.shinyapps.io/dutch-modality-exclusivity-norms/&#34;&gt;preferred-dashboard&lt;/a&gt; (in case of downtime, please visit this &lt;a href=&#34;http://rpubs.com/pcbernabeu/Dutch-modality-exclusivity-norms&#34;&gt;alternative&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Transforming dashboards into the different versions can be as easy as enabling or disabling some features, especially input reactivity. For instance, if we want to downgrade a Flexdashboard-Shiny to a Flexdashboard, to publish it outside of a Shiny server (see &lt;a href=&#34;https://github.com/pablobernabeu/Modality-exclusivity-norms-Bernabeu-et-al/blob/master/Dutch-modality-exclusivity-norms-RPubs.Rmd&#34;&gt;example&lt;/a&gt;), we must delete &lt;code&gt;runtime:shiny&lt;/code&gt; from the header, and disable reactive features, as below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
``` r
# Number of words selected on sidebar
# reactive(cat(paste0(&amp;#39;Words selected below: &amp;#39;, nrow(selected_props()))))
```&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;free-accounts-and-tips&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Free accounts and tips&lt;/h5&gt;
&lt;p&gt;Hosting sites have specific terms of use. For instance, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;shinyapps.io&lt;/a&gt; has a free starter license with limited use. Free apps can handle a large but limited amount of data, and up to five apps may be created. Beyond this, RStudio offers a wide range of subscriptions starting at $9/month.&lt;/p&gt;
&lt;p&gt;Memory and traffic limits of the free shinyapps.io account can sometimes present problems when heavy data data sets are used, or there are many visits to the app. The memory overload issue is often flagged as &lt;code&gt;Shiny cannot use on-disk bookmarking&lt;/code&gt;, whereas excessive traffic may see the app not loading. Fortunately, usage limits need not always require a paid subscription or a &lt;a href=&#34;https://www.r-bloggers.com/alternative-approaches-to-scaling-shiny-with-rstudio-shiny-server-shinyproxy-or-custom-architecture/&#34;&gt;custom server&lt;/a&gt;, thanks to the following workarounds:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;develop app locally as far as possible, and only deploy to shinyapps.io only at the last stage;&lt;/li&gt;
&lt;li&gt;prune data set, leaving only the necessary data;&lt;/li&gt;
&lt;li&gt;if necessary, unlink data by splitting it into different sets, reducing computational demands;&lt;/li&gt;
&lt;li&gt;if necessary, use various apps (five are allowed in each free shinyapps.io account);&lt;/li&gt;
&lt;li&gt;if necessary, link from the app to a PDF with visualisations requiring heavy, interlinked data. High-resolution plots can be rendered into a PDF document in a snap, using code such as below.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;pdf(&amp;#39;List of plots per page&amp;#39;, width = 13, height = 5)
print(plot1)
print(plot2)
# ...
print(plot150)
dev.off()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conveniently, all text in a PDF—even in plots—is indexed, so it can be searched [ Ctrl+f / Cmd+f / 🔍 ] (see &lt;a href=&#34;https://osf.io/2tpxn/&#34;&gt;example&lt;/a&gt;). Furthermore, you may also &lt;a href=&#34;http://www.ilovepdf.com/&#34;&gt;merge the rendered PDF with any other documents&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;prerequisites-and-suggestions-for-participation-in-each-workshop&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Prerequisites and suggestions for participation in each workshop&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Necessary:&lt;/em&gt; laptop or computer with &lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;https://rstudio.com/products/rstudio/download/&#34;&gt;RStudio&lt;/a&gt; installed, or access to &lt;a href=&#34;https://rstudio.cloud/&#34;&gt;RStudio Cloud&lt;/a&gt;; familiarity with the content of the preceding workshops through the web links herein.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Suggested:&lt;/em&gt; having your own data and R code ready (on a Github repository if participating in Workshop 4); participation in some of the preceding workshops.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;datathons-creating-reproducible-documents-and-dashboards&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Datathons: creating reproducible documents and dashboards&lt;/h3&gt;
&lt;p&gt;In these coding meetups, participants collaborate to create reproducible documents or dashboards using the data and software they prefer (see &lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/tree/master/examples-documents-dashboards&#34;&gt;examples&lt;/a&gt;). Since the work can be split across different people and sections, some nice products may be achieved within a session. Any programming languages may be used.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Data used:&lt;/strong&gt; academic or non-academic data of your own or from open-access sources such as &lt;a href=&#34;https://osf.io&#34;&gt;OSF&lt;/a&gt;, scientific journals, governments, international institutions, NGOs, etc.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inspired by the great &lt;a href=&#34;https://reprohack.github.io/reprohack-hq/&#34;&gt;Reprohacks&lt;/a&gt;, content suggestions are encouraged. That is, if you’d like to have a reproducible document or dashboard created for a certain, open-access data set, please let us know, and some participants may take it on. Suggestions may be posted as &lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/issues&#34;&gt;issues&lt;/a&gt; or emailed to &lt;a href=&#34;mailto:p.bernabeu@lancaster.ac.uk&#34; class=&#34;email&#34;&gt;p.bernabeu@lancaster.ac.uk&lt;/a&gt;.
&lt;br&gt;&lt;/br&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Purposes&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;collaborating to visualise data in novel ways using reproducible documents or interactive dashboards. For this purpose, participants sometimes draw on additional data to look at a bigger picture;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;reflecting on the process by reviewing the techniques applied and challenges encountered.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt; A key aspect of datathons is the creation of output. Documents and dashboards are (co-)authored by the participants who work on them, who can then publish them on their websites, or on &lt;a href=&#34;https://rpubs.com/&#34;&gt;RPubs&lt;/a&gt;, &lt;a href=&#34;https://mybinder.org/&#34;&gt;Binder&lt;/a&gt;, &lt;a href=&#34;https://www.shinyapps.io/&#34;&gt;Shinyapps&lt;/a&gt; or &lt;a href=&#34;https://rstudio.com/products/shiny/shiny-server/&#34;&gt;custom servers&lt;/a&gt;. Time constraints notwithstanding, a lot of this output may be very enticing for further development by the same participants, or even by other people if the code is shared online. Just like with data, an attribution licence can be attached to the code.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;prerequisites-and-suggestions-for-participation-in-datathons&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Prerequisites and suggestions for participation in datathons&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Necessary:&lt;/em&gt; basic knowledge of reproducible documents or dashboards.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Suggested:&lt;/em&gt; familiarity with the development of reproducible documents or dashboards; an idea about the data you’d like to work with and the kind of document or dashboard you want to create.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;contact&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Contact&lt;/h2&gt;
&lt;p&gt;Please submit any queries or requests by &lt;a href=&#34;https://github.com/pablobernabeu/Data-is-present/issues&#34;&gt;posting an issue&lt;/a&gt; or emailing &lt;a href=&#34;mailto:p.bernabeu@lancaster.ac.uk&#34; class=&#34;email&#34;&gt;p.bernabeu@lancaster.ac.uk&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Event-related potentials: Why and how I used them</title>
      <link>https://pablobernabeu.github.io/2020/event-related-potentials-why-and-how-i-used-them/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2020/event-related-potentials-why-and-how-i-used-them/</guid>
      <description>&lt;p&gt;Event-related potentials (ERPs) offer a unique insight in the study of human cognition. Let&#39;s look at their reason-to-be for the purposes of research, and how they are defined and processed. Most of this content is based on my master&#39;s thesis, which I could fortunately conduct at the Max Planck Institute for Psycholinguistics (see &lt;a href=&#39;https://psyarxiv.com/5gjvk/&#39;&gt;thesis&lt;/a&gt; or &lt;a href=&#39;https://psyarxiv.com/a5pcz/&#39;&gt;conference paper&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;electroencephalography&#34;&gt;Electroencephalography&lt;/h2&gt;
&lt;p&gt;The brain produces electrical activity all the time, which can be measured via electrodes on the scalp—a method known as electroencephalography (EEG). These pulses are produced for every one of our states and actions, in a voltage at a micro (µ) scale, typically between 10 µV (0.000010) and 100 µV (0.000100; Aurlien et al., 2004). The overlapping pulses happen at extremely high frequencies; indeed, the signal can be measured once per millisecond. The high frequency of this signal is very interesting for the study of some cognitive processes in particular, for which the time course is (or may be) critical. One such example is conceptual processing, namely, the process of understanding the meaning of words.&lt;/p&gt;
&lt;p&gt;Research has revealed the relation between certain EEG patterns and cognitive states and functions. Brain activity includes dozens of types, but broadly, it can be divided into neural oscillations and event-related potentials. Specific oscillations (also known as brain waves) are associated with &lt;em&gt;states&lt;/em&gt; such as wakefulness, sleep, arousal, relaxation, etc. (Roohi-Azizi et al., 2017). Event-related potentials instead represent more finite &lt;em&gt;events&lt;/em&gt;, such as the presentation of as a stimulus. In cognitive neuroscience, both oscillations and ERPs are studied, whereas in cognitive psychology, ERPs are much more common than oscillations. Let&#39;s dive into ERPs below.&lt;/p&gt;
&lt;h3 id=&#34;event-related-potentials&#34;&gt;Event-related potentials&lt;/h3&gt;
&lt;p&gt;In the lab, ERPs are elicited using controlled designs. In each trial, a series of stimuli are presented. At a fixed point therein, an EEG measurement begins and spans for a certain period. In turn, in the analysis, this measurement period is divided into &lt;em&gt;time windows&lt;/em&gt;, which often correspond to specific ERP components (e.g., N400 window).&lt;/p&gt;
&lt;p&gt;In psycholinguistics, for instance, a typical scenario is the presentation of words, and ERPs are systematically &lt;em&gt;time-locked&lt;/em&gt; to the same position in consecutive trials, often the onset of a word. By this means, the experimental manipulation is collected, and the non-experimental variation—&amp;lsquo;noise&amp;rsquo;—is largely cancelled out by the aggregation of multiple trials that share the experimental manipulation.&lt;/p&gt;
&lt;p&gt;The chief reason to employ the ERP method is the measurement of cognitive processes online, that is, precisely as they unfold. This is fitting in the context of language comprehension, where some important processes last for less than a second.&lt;/p&gt;
&lt;h2 id=&#34;time-course-of-word-processing&#34;&gt;Time course of word processing&lt;/h2&gt;
&lt;p&gt;Processing a word takes around 800 milliseconds (ms). Within that time, earlier processes (compared to later ones) have been ascribed greater relevance to the core process of understanding a word (Mahon &amp;amp; Caramazza, 2008). This assumes that broader processes start only after more immediate ones have started (but see Lebois, Wilson‐Mendenhall &amp;amp; Barsalou, 2014). The most immediate process is the recognition of a string of letters, which seems to start within 90 ms post word onset in early auditory cortex and the Visual Word Form Area (Willems et al., 2016). Then ensue further, fundamental stages known as &lt;em&gt;lexical&lt;/em&gt; and &lt;em&gt;semantic&lt;/em&gt; processes. Lexical processing is the identification of a string of letters as a known word, and it happens within around 160 ms post word onset. Next, at around 200 ms, we may see the beginning of semantic processing, which denotes a further step in the cognitive analysis of the word that is akin to &lt;em&gt;meaning&lt;/em&gt; (Hauk, 2016). These processes may overlap, as indeed suggested by the sensitivity of the N400 ERP (see also next section) to both lexical and semantic tasks (Kutas &amp;amp; Federmeier, 2011). Both processes also likely extend further in the processing timeline (Hauk, 2016). In spite of this overlap, however, lexical and semantic processing have often been linked to different cognitive phenomena. For instance, tasks promoting semantic processing (e.g., semantic decision, whereby participants describe words as concrete or abstract) have been found to engage sensorimotor simulation of the word&#39;s meaning (known as &lt;em&gt;embodiment&lt;/em&gt;) more strongly than lexical tasks do (Connell &amp;amp; Lynott, 2013; Pexman et al., 2019; Sato et al., 2008).&lt;/p&gt;
&lt;p&gt;Once the lexical and semantic stages have emerged, post-lexical, post-semantic processes follow (Mahon &amp;amp; Caramazza, 2008). These are mental imagery and episodic memory processes—both with an approximate emergence around 270 ms after word onset. The gradual progression from the identification of a word up to accessing its broadest meaning is an important anchoring point in the current research on the alleged embodiment of meaning comprehension, even if we might hope to count on more definitive threshold points (Hauk, 2016).&lt;/p&gt;
&lt;p&gt;Word processing data are mainly based on written word processing, but spoken words are processed quite similarly, if slightly faster (Leonard et al., 2016; Pulvermüller et al., 2005; Shtyrov et al., 2004).&lt;/p&gt;
&lt;p&gt;The bigger take-home messages would be: (1) the processing of meaning might only start at around 160 ms post word onset, and (2) processes outside of meaning comprehension might only start at around 270 ms. These working references must be taken with some caution because particular semantic effects have been found at different stages (e.g., the conceptual modality switch, as in Hald et al., 2011; Collins, Pecher et al., 2011). Indeed, in an influential critique of blooming findings on embodiment, Mahon and Caramazza (2008) argued that even early effects might possibly be explained in terms of non-embodied processing. They contended that working memory processes that were ancillary rather than semantic could be quickly engaged with the function of ‘colouring’ a concept, not building it up. To further complicate the matter, we do not have absolute certainty on the later section of the time course. Thus, as Hauk (2016) reviews, the different stages likely overlap at certain points, with different degrees of relevance. For instance, lexical processing may continue even once semantic processing has started, but would naturally become less relevant. Indeed, the relation among these processes is likely more of a continuum than a set of clear-cut modules. In a nutshell, the time course is important with some experimental effects in word processing, and, to that extent, we depend on our knowledge of the basic time course of word processing.&lt;/p&gt;
&lt;h2 id=&#34;the-conceptual-modality-switch-paradigm-and-its-time-course&#34;&gt;The conceptual modality switch paradigm and its time course&lt;/h2&gt;
&lt;p&gt;In demonstrating the relevance of embodied cognition, a sizeable series of studies have shown that reading about different conceptual modalities (e.g., auditory ‘loud bell’ followed by visual ‘pink background’) incurs processing costs (Pecher et al., 2003). Importantly, this manipulation does not concern the presentation mode of the stimulus, maintained constant, but the intrinsic semantic modality of the stimulus concepts. The conceptual modality switch effect has often been replicated (Pecher et al., 2004; Solomon &amp;amp; Barsalou, 2004; Marques, 2006; Vermeulen et al., 2007; van Dantzig et al., 2008; Lynott &amp;amp; Connell, 2009; Ambrosi et al., 2011; Collins et al., 2011; Hald et al., 2011; Hald et al., 2013; Scerrati et al., 2015).&lt;/p&gt;
&lt;p&gt;Bernabeu, Willems and Louwerse (2017) addressed a caveat with the time course of the conceptual modality switch paradigm. In previous experiments, trials presented a concept word followed by a property word. ERPs were time-locked to the latter property word. This design may have left uncontrolled a switch produced already at the concept. Indeed, the property word was already supposed to be in the particular modality of the trial. That pitfall could have had two consequences: loss of power and loss of certainty on the time course of the effect. Thus, Bernabeu et al. created a design in which ERPs were time-locked to the first word in target trials (see some &lt;a href=&#34;https://www.researchgate.net/post/Conceptual_modality_switch_effect_measured_at_first_word&#34;&gt;early input from researchers online&lt;/a&gt;). The purpose of this relocation was not to completely annul the possibility of post-core sensory processes, but to increase the time accuracy by measuring the modality switch from the point at which it is elicited.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;designoverview.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Implementing this design had an ancillary effect on the measurement of response times. A psycholinguistic experiment like this one requires controlling fundamental variables such as word frequency and length, by matching the means of these variables across experimental conditions. This must be controlled in the target words at least. As it is often the case, this control was only possible in the target words—the first one in target trials—, but it was not possible in the second word, which is the crucial one for response times. Response times could still be measured, but comparisons across conditions were not fully warranted. In sum, this was an ERP design.&lt;/p&gt;
&lt;h2 id=&#34;erp-components&#34;&gt;ERP components&lt;/h2&gt;
&lt;p&gt;When the ERP signal is plotted, it displays multiple wave shapes, or &lt;em&gt;waveforms&lt;/em&gt;, each with a peak flanked by falling tails. Each of these waves often corresponds to an ERP component, which is what cognitive scientists are often interested in.&lt;/p&gt;
&lt;p&gt;Multiple components are known, each having been found to consistently peak around specific points in time during a cognitive process. The peak is one of several features characterising each component. A sketch list is shown below (van Hell &amp;amp; Kroll, 2013).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Polarity:&lt;/strong&gt; The component either peaks in the positive or the negative pole of the signal. This polarity is relative to the &lt;em&gt;baseline&lt;/em&gt; point that is created in the preprocessing stage (see below);&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Latency:&lt;/strong&gt; the time course of the component, encompassing an onset, a peak and an overall duration. Time windows are normally set to match relevant components (e.g., the N400 window, etc.);&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Amplitude:&lt;/strong&gt; the voltage reached at a given time (e.g., the peak) or for a certain period (e.g., a time window);&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalp distribution, or topography:&lt;/strong&gt; the areas on the scalp (the scalp being a reasonable proxy for the brain) in which the component appears;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Functional role:&lt;/strong&gt; the cognitive functions that have been consistently associated with the component.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Examples of components in language processing include the N400, consistently linked to semantic processing, that is, seeking the meaning of words or sentences. The N400 is characterised by a large, negative amplitude peaking at around 400 ms post word onset, primarily found in central and posterior sites. N400 &lt;em&gt;effects&lt;/em&gt;, which are comparisons of the N400 component in different experimental conditions, have consistently appeared under violations of semantic expectations, i.e., related to meaning and events (Kutas &amp;amp; Federmeier, 2011; Swaab et al., 2012). Another well-known component in language is the P600, linked to syntactic processing, which allows the comprehension of sentences (Swaab et al., 2012). Other examples of components include lateralized readiness potentials, signalling motor preparation (Mordkoff &amp;amp; Gianaros, 2000), and the P3b component, which appears in the context of responses (van Vliet et al., 2014). Both of the latter components are relevant to researchers across domains, who often need to ward off &lt;em&gt;contamination&lt;/em&gt; from these components in their experiments. In Bernabeu et al.&amp;lsquo;s experiment, for instance, part of the reason why ERPs were time-locked to the first word in target trials was to prevent contamination from these components.&lt;/p&gt;
&lt;p&gt;ERP data sets are large, being the product of the number of electrodes (also called &amp;lsquo;channels&amp;rsquo;) times the number of time points, times the number of experimental conditions, and times the number of participants. In recent studies, the number of trials often adds to that product, whereas in previous experiments, the trials tended to be aggregated in each condition.&lt;/p&gt;
&lt;h2 id=&#34;eeg-montage&#34;&gt;EEG montage&lt;/h2&gt;
&lt;p&gt;The EEG montage is an important factor. The options are broadly characterised by three parameters of the electrodes:&lt;/p&gt;
&lt;p style=&#34;margin-left: 30px; line-height: 1.2; padding-bottom: 12px; padding-left: 15px; float: right; display: block;&#34;&gt;&lt;img src=&#34;EEG MPI open day photo.jpg&#34; alt=&#34;Pablo Bernabeu, 2015&#34; width=&#34;170px&#34; style=&#39;padding-bottom: 15px; margin-bottom: 0px;&#39; /&gt;&lt;span style=&#34;font-size: small; padding-left: 5px; padding-top: 0px; margin-top: 0px;&#34;&gt;Brainwaves exposed at an open day.&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Active / passive:&lt;/strong&gt; Active electrodes amplify the EEG signal directly at the scalp, whereas passive electrodes require the raw signal to be sent through a lead (i.e., a wire) and up to an amplifier. In the latter case, the lead acts as an antenna, picking up ambient electrical noise. This noise can hinder the subtle measurement of interest. This problem is solved by active electrodes (Laszlo et al., 2014).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Number:&lt;/strong&gt; Traditionally, montages with 32, 64 or 128 electrodes have been used. The more electrodes, the &amp;lsquo;denser&amp;rsquo; the montage, and the higher the spatial resolution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Wet / dry:&lt;/strong&gt; In some montages, the electrical conductance on the electrodes&amp;rsquo; contact point must be increased using some fluid solutions, such as a specific gel (often commercialised by the companies that also make EEG apparatuses). Conversely, other electrodes function in a dry way. Ensuring the proper conductance on wet electrodes has traditionally been very time-consuming for experimenters, often taking over half an hour of wiggling a blunt syringe distributing the saline solution around the tip. Traditionally, wet electrodes produced more reliable data than dry ones, but &lt;em&gt;the times they are a&#39;changing&lt;/em&gt; (di Flumeri, 2019).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An EEG/ERP experiment is time-consuming. The preparation (especially conductance-prompting with wet montages) and post-experiment procedures (especially washing the EEG cap) often take four or five times as long as the experiment proper. These procedures are especially long for higher-density, wet montages.&lt;/p&gt;
&lt;h2 id=&#34;preprocessing-erps&#34;&gt;Preprocessing ERPs&lt;/h2&gt;
&lt;p&gt;ERPs are not the first signal collected in experiments. They are obtained after considerable, systematic preprocessing of the EEG signal.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#39;https://www.researchgate.net/post/EEG_error_datasets_missing_channels_Its_strange_because_they_were_recorded_well_and_faulty_files_are_quite_as_heavy_as_the_good_ones_Any_ideas&#39;&gt;&lt;img src=&#34;https://www.researchgate.net/profile/Nikolay_Novitskiy/post/EEG_error_datasets_missing_channels_Its_strange_because_they_were_recorded_well_and_faulty_files_are_quite_as_heavy_as_the_good_ones_Any_ideas/attachment/59d6391b79197b8077996520/AS%3A400433085468672%401472482095219/image/41_64ch.png&#34; alt=&#34;Brain Vision waveforms&#34; width=&#39;70%&#39;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For Bernabeu et al. (2017), I used Brain Vision software, and followed the &lt;a href=&#34;https://erpinfo.org/resources&#34;&gt;tutorials from the well-known ERP Boot Camp&lt;/a&gt; of Steve Luck and Emily Kappenman. I applied the following pipeline for each participant:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;labeling channels (64 electrodes);&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;creating channel groups (anterior and posterior);&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;re-referencing the signal offline to the right mastoid (RM), having referenced online to the left mastoid (Ref);&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#39;EEG montage.png&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;
&lt;p&gt;separating my three experimental conditions;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ocular correction for blinks and significant, vertical or horizontal movements of the eyes (seminal method by Gratton et al., 1983, which is the default in Brain Vision);&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;baseline correction, which is a standardisation based on a certain period immediately before the onset of the target manipulation;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;further correction of artifacts such as motor action potentials (or lateralised readiness potentials) resulting from even the subtlest muscle activity.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This pipeline is reflected in the &lt;a href=&#34;https://osf.io/98fs6/&#34;&gt;scripts exported from Brain Vision&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  &amp;lt;Nodes&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMatch&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection/baselinecorr&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection/baselinecorr/baselinecorr_artif&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMatch/OcularCorrection/baselinecorr/baselinecorr_artif/minave_TotalMatch&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection/baselinecorr&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection/baselinecorr/baselinecorr_artif&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/EmbodiedMismatch/OcularCorrection/baselinecorr/baselinecorr_artif/minave_EmbodiedMismatch&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection/baselinecorr&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection/baselinecorr/baselinecorr_artif&amp;lt;/string&amp;gt;
    &amp;lt;string&amp;gt;1/Raw Data/labels/positions/rerefRM/TotalMismatch/OcularCorrection/baselinecorr/baselinecorr_artif/minave_TotalMismatch&amp;lt;/string&amp;gt;
  &amp;lt;/Nodes
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Word reading ERPs can look somewhat like this after the preprocessing (&lt;a href=&#39;https://osf.io/bz7ae/&#39;&gt;plots made in R&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;Four main waveform plots stacked.png&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;To visualise these waveforms throughout the different sections of the data, a &lt;a href=&#34;https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment&#34;&gt;dashboard is available&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;statistical-analysis&#34;&gt;Statistical analysis&lt;/h2&gt;
&lt;p&gt;With the myriad repeated measures involved in EEG, linear mixed-effects models are a good option, allowing the registration of electrodes and time points in the error term per participant (and trial, too, if these are not aggregated). The statistical analysis is &lt;a href=&#34;https://osf.io/sx3nw&#34;&gt;available on OSF&lt;/a&gt; (to view the plots, please &lt;a href=&#34;https://osf.io/download/sx3nw&#34;&gt;download the document&lt;/a&gt;).&lt;/p&gt;
&lt;iframe src=&#34;https://mfr.osf.io/render?url=https%3A%2F%2Fosf.io%2Fdownload%2Fsx3nw%2F%3Fdirect%26mode%3Drender&#34;
        width=&#34;100%&#34;
        scrolling=&#34;yes&#34;
        height=&#34;677px&#34;
        marginheight=&#34;0&#34;
        frameborder=&#34;0&#34;
        allowfullscreen
        webkitallowfullscreen
&gt;
&lt;/iframe&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Event-related potentials fulfil an important role in cognitive neuroscience and psychology, only surpassed by magnetic electroencephalography (MEG), which unites high temporal and spatial resolution. Learning how to use ERPs is demanding but even more rewarding. It certainly does not make for fast science, but allows the measurement of experimental effects online, that is, as they unfold.&lt;/p&gt;
&lt;p&gt;You can learn about and overcome multiple challenges. One of the issues I faced once regarded some channels (electrodes) that appeared to be missing from the data. I posted a &lt;a href=&#34;https://www.researchgate.net/post/EEG_error_datasets_missing_channels_Its_strange_because_they_were_recorded_well_and_faulty_files_are_quite_as_heavy_as_the_good_ones_Any_ideas&#34;&gt;question on ResearchGate&lt;/a&gt;, and emailed Brain Products, the maker of Brain Vision Recorder, which I was using.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hi everyone,&lt;/p&gt;
&lt;p&gt;If you could please give me a hand with this error, I would be very grateful. I have EEG from a psychological experiment, recorded with BrainVision Recorder, and being analyzed with BrainVision Analyzer 2. Most of the recordings are perfectly fine, but a few present a big error. Out of 64 original electrodes, only two appear. These are the right mastoid (RM) and the left eye sensor (LEOG). Both are bipolar electrodes. RM is to be re-referenced to the online reference electrode, while LEOG is to be re-referenced to the right eye electrode.&lt;/p&gt;
&lt;p&gt;I just can&#39;t fathom the error because all electrodes worked fine during the recording. Also, the data sets with the error are quite as heavy in terms of bytes as those without the error. Further, why should the RM and LEOG channels remain perfectly well as they do?&lt;/p&gt;
&lt;p&gt;This issue might seem like a simple zoom I&#39;ve bypassed, or similar&amp;hellip; But unfortunately the channels are just not there. I&#39;ve confirmed it as I tried to copy the pipeline from the good data sets onto the faulty ones, where I got the error &amp;lsquo;No channels enabled.&amp;rsquo; In case you had access to the BVA analysis software, please find the raw files for one of the faulty data sets here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Thanks to invaluable help from a &lt;a href=&#34;https://www.researchgate.net/post/EEG_error_datasets_missing_channels_Its_strange_because_they_were_recorded_well_and_faulty_files_are_quite_as_heavy_as_the_good_ones_Any_ideas&#34;&gt;ResearchGate contributor&lt;/a&gt; and the Brain Products team, I could put the pieces back together.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Update: Problem solved.&lt;/p&gt;
&lt;p&gt;As Nikolay said, the error originated in Recorder (I had used the workspace from the previous experimenter), and the problem was solved by setting the label and position of each channel.&lt;/p&gt;
&lt;p&gt;I tried editing the .vhdr file in raw (it seemed nice and quick to directly assign the channel names as labels) but i didn&#39;t quite find the way. Therefore, with a tip from the Brain Products team, I went about it within the program.&lt;/p&gt;
&lt;p&gt;First, I used the transform function &amp;lsquo;Edit channels&amp;rsquo; to rename all labels and set each within their coordinates. I did that for just one subject (it doesn&#39;t take as long as it sounds). Afterwards, I created a &amp;lsquo;History template&amp;rsquo; out of that process, and copied it to all other nodes.
At any rate, never getting out of the comfort workspace again&amp;hellip; :D&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;div class = &#39;hanging-indent&#39;&gt;
&lt;p&gt;Ambrosi, S., Kalenine, S., Blaye, A., &amp;amp; Bonthoux, F. (2011). Modality switching cost during property verification by 7 years of age. &lt;em&gt;International Journal of Behavioral Development, 35&lt;/em&gt;, 1, 78-83.&lt;/p&gt;
&lt;p&gt;Aurlien, H., Gjerde, I., Aarseth, J., Eldøen, G., Karlsen, B., Skeidsvoll, H., &amp;amp; Gilhus, N. (2003).
EEG background activity described by a large computerized database. &lt;em&gt;Clinical Neurophysiology, 115&lt;/em&gt;, 665–673.&lt;/p&gt;
&lt;p&gt;Bernabeu, P., Willems, R. M., &amp;amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, &amp;amp; E. J. Davelaar (Eds.), &lt;em&gt;Proceedings of the 39th Annual Conference of the Cognitive Science Society&lt;/em&gt; (pp. 1629-1634). Austin, TX: Cognitive Science Society.&lt;/p&gt;
&lt;p&gt;Collins, J., Pecher, D., Zeelenberg, R., &amp;amp; Coulson, S. (2011). Modality switching in a property verification task: an ERP study of what happens when candles flicker after high heels click. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Connell, L., &amp;amp; Lynott, D. (2013). Flexible and fast: Linguistic shortcut affects both shallow and deep conceptual processing. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 20&lt;/em&gt;, 542-550.&lt;/p&gt;
&lt;p&gt;Di Flumeri, G., Aricò, P., Borghini, G., Sciaraffa, N., Di Florio, A., &amp;amp; Babiloni, F. (2019). The Dry Revolution: Evaluation of Three Different EEG Dry Electrode Types in Terms of Signal Spectral Features, Mental States Classification and Usability. &lt;em&gt;Sensors (Basel, Switzerland), 19&lt;/em&gt;(6), 1365.&lt;/p&gt;
&lt;p&gt;Gratton, G., Coles, M. G., &amp;amp; Donchin, E. (1983). A new method for offline removal of ocular artefact. &lt;em&gt;Electroencephalography and Clinical Neurophysiology, 55&lt;/em&gt;, 4, 468-484.&lt;/p&gt;
&lt;p&gt;Hald, L. A., Hocking, I., Vernon, D., Marshall, J.-A., &amp;amp; Garnham, A. (2013). Exploring modality switching effects in negated sentences: further evidence for grounded representations. &lt;em&gt;Frontiers in Psychology, 4&lt;/em&gt;, 93.&lt;/p&gt;
&lt;p&gt;Hald, L. A., Marshall, J.-A., Janssen, D. P., &amp;amp; Garnham, A. (2011). Switching modalities in a sentence verification task: ERP evidence for embodied language processing. &lt;em&gt;Frontiers in Psychology, 2&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Hauk, O. (2016). Only time will tell—Why temporal information is essential for our neuroscientific understanding of semantics. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;, 4, 1072-1079.&lt;/p&gt;
&lt;p&gt;Kutas, M., &amp;amp; Federmeier, K. D. (2011). Thirty years and counting: finding meaning in the N400 component of the event-related brain potential (ERP). &lt;em&gt;Annual Review of Psychology, 62&lt;/em&gt;, 621–647.&lt;/p&gt;
&lt;p&gt;Laszlo, S., Ruiz-Blondet, M., Khalifian, N., Chu, F., &amp;amp; Jin, Z. (2014). A direct comparison of active and passive amplification electrodes in the same amplifier system. &lt;em&gt;Journal of Neuroscience Methods, 235&lt;/em&gt;, 298-307.&lt;/p&gt;
&lt;p&gt;Lebois, L. A. M., Wilson-Mendenhall, C. D., &amp;amp; Barsalou, L. W. (2014). Are automatic conceptual cores the gold standard of semantic processing? The context-dependence of spatial meaning in grounded congruency effects. &lt;em&gt;Cognitive Science, 39&lt;/em&gt;, 8, 1764-801.&lt;/p&gt;
&lt;p&gt;Leonard, M. K., Baud, M. O., Sjerps, M. J., &amp;amp; Chang, E. F. (2016). Perceptual restoration of masked speech in human cortex. &lt;em&gt;Nature Communications, 7&lt;/em&gt;, 13619.&lt;/p&gt;
&lt;p&gt;Luck, S. J. &amp;amp; Kappenman, E.S. (Eds.), &lt;em&gt;Oxford Handbook of Event-Related Potential Components&lt;/em&gt;. New York: Oxford University Press&lt;/p&gt;
&lt;p&gt;Mahon, B.Z., &amp;amp; Caramazza, A. (2008). A critical look at the Embodied Cognition Hypothesis and a new proposal for grounding conceptual content. &lt;em&gt;Journal of Physiology - Paris, 102&lt;/em&gt;, 59-70.&lt;/p&gt;
&lt;p&gt;Marques, J. F. (2006). Specialization and semantic organization: Evidence for multiple semantics linked to sensory modalities. *&lt;em&gt;Memory &amp;amp; Cognition, 34&lt;/em&gt;, 1, 60-67.&lt;/p&gt;
&lt;p&gt;Mordkoff, J. T., &amp;amp; Gianaros, P. J. (2000). Detecting the onset of the lateralized readiness potential: A comparison of available methods and procedures. &lt;em&gt;Psychophysiology, 37&lt;/em&gt;(3), 347–360.&lt;/p&gt;
&lt;p&gt;Pecher, D., Zeelenberg, R., &amp;amp; Barsalou, L. W. (2003). Verifying different-modality properties for concepts produces switching costs. &lt;em&gt;Psychological Science, 14&lt;/em&gt;, 2, 119-24.&lt;/p&gt;
&lt;p&gt;____ (2004). Sensorimotor simulations underlie conceptual representations: Modality-specific effects of prior activation. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 11&lt;/em&gt;, 1, 164-167.&lt;/p&gt;
&lt;p&gt;Pexman, P. M., Muraki, E. J., Sidhu, D. M., Siakaluk, P. D., &amp;amp; Yap, M. J. (2019). Quantifying sensorimotor experience: Body-object interaction ratings for more than 9,000 English words. &lt;em&gt;Behavior Research Methods, 51&lt;/em&gt;, 453-466.&lt;/p&gt;
&lt;p&gt;Pulvermüller, F., Shtyrov, Y., &amp;amp; Hauk, O. (2009). Understanding in an instant: Neurophysiological evidence for mechanistic language circuits in the brain. &lt;em&gt;Brain and Language, 110&lt;/em&gt;, 2, 81–94.&lt;/p&gt;
&lt;p&gt;Roohi-Azizi, M., Azimi, L., Heysieattalab, S., &amp;amp; Aamidfar, M. (2017). Changes of the brain&#39;s bioelectrical activity in cognition, consciousness, and some mental disorders. &lt;em&gt;Medical journal of the Islamic Republic of Iran, 31&lt;/em&gt;, 53.&lt;/p&gt;
&lt;p&gt;Sato, M., Mengarelli, M., Riggio, L., Gallese, V., &amp;amp; Buccino, G. (2008). Task related modulation of the motor system during language processing. &lt;em&gt;Brain and Language, 105&lt;/em&gt;, 83–90.&lt;/p&gt;
&lt;p&gt;Scerrati, E., Baroni, G., Borghi, A. M., Galatolo, R., Lugli, L., &amp;amp; Nicoletti, R. (2015). The modality-switch effect: visually and aurally presented prime sentences activate our senses. &lt;em&gt;Frontiers in Psychology, 6&lt;/em&gt;, 1668.&lt;/p&gt;
&lt;p&gt;Shtyrov, Y., Hauk, O., &amp;amp; Pulvermüller, F. (2004). Distributed neuronal networks for encoding category-specific semantic information: the mismatch negativity to action words. &lt;em&gt;European Journal of Neuroscience, 1&lt;/em&gt;, 4, 1083–1092.&lt;/p&gt;
&lt;p&gt;Solomon, K. O., &amp;amp; Barsalou, L. W. (2004). Perceptual simulation in property verification. &lt;em&gt;Memory &amp;amp; Cognition, 32&lt;/em&gt;, 244-259.&lt;/p&gt;
&lt;p&gt;Swaab, T.Y., Ledoux, K., Camblin, C.C., &amp;amp; Boudewyn, M.A. (2012) Language related ERP components. (Book Chapter). In Luck, S. J. &amp;amp; Kappenman, E.S. (Eds.), &lt;em&gt;Oxford Handbook of Event-Related Potential Components&lt;/em&gt; (pp. 397-440). New York: Oxford University Press&lt;/p&gt;
&lt;p&gt;Van Dantzig, S., Pecher, D., Zeelenberg, R., &amp;amp; Barsalou, L. W. (2008). Perceptual processing affects conceptual processing. &lt;em&gt;Cognitive Science, 32&lt;/em&gt;, 579–590.&lt;/p&gt;
&lt;p&gt;Van Hell, J. G., &amp;amp; Kroll, J. F. (2013). Using electrophysiological measures to track the mapping of words to concepts in the bilingual brain: a focus on translation. In J. Altarriba &amp;amp; L. Isurin (Eds.), &lt;em&gt;Memory, Language, and Bilingualism: Theoretical and Applied Approaches&lt;/em&gt; (pp. 126-160). New York: Cambridge University Press.&lt;/p&gt;
&lt;p&gt;Van Vliet, M., Manyakov, N., Storms, G., Fias, W., Wiersema, J., &amp;amp; Van Hulle, M. (2014). Response-Related Potentials during semantic priming: the effect of a speeded button response task on ERPs. &lt;em&gt;PLoS One, 9&lt;/em&gt;, 2, e87650.&lt;/p&gt;
&lt;p&gt;Vermeulen, N., Niedenthal, P. M., &amp;amp; Luminet, O. (2007). Switching between sensory and affective systems incurs processing costs. &lt;em&gt;Cognitive Science, 31&lt;/em&gt;, 1, 183-192.&lt;/p&gt;
&lt;p&gt;Willems, R. M., Frank, S. L., Nijhoff, A. D., Hagoort, P., &amp;amp; Van den Bosch, A. (2016). Prediction during natural language comprehension. &lt;em&gt;Cerebral Cortex, 26&lt;/em&gt;, 6, 2506-2516.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Naive principal component analysis in R</title>
      <link>https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Principal_component_analysis&#34;&gt;Principal Component Analysis (PCA)&lt;/a&gt; is a technique used to find the core components that underlie different variables. It comes in very useful whenever doubts arise about the true origin of three or more variables. There are two main methods for performing a PCA: naive or less naive. In the naive method, you first check some conditions in your data which will determine the essentials of the analysis. In the less-naive method, you set those yourself based on whatever prior information or purposes you had. The latter method is appropriate when you already have enough information about the intercorrelations, or when you are required to select a specific number of components. I will tackle the naive method, mainly by following the guidelines in &lt;a href=&#34;https://freethegeogbooks.files.wordpress.com/2016/08/book-for-r-language-stats.pdf&#34;&gt;Field, Miles, and Field (2012)&lt;/a&gt;, with updated code where necessary. A &lt;a href=&#34;https://freethegeogbooks.files.wordpress.com/2016/08/book-for-r-language-stats.pdf&#34;&gt;manual by Charles M. Friel&lt;/a&gt; (Sam Houston State University) was also useful.&lt;/p&gt;
&lt;p&gt;The ‘naive’ approach is characterized by a first stage that checks whether the PCA should actually be performed with your current variables, or if some should be removed. The variables that are accepted are taken to a second stage which identifies the number of principal components that seem to underlie your set of variables.&lt;/p&gt;
&lt;div id=&#34;stage-1.-determine-whether-pca-is-appropriate-at-all-considering-the-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;STAGE 1. Determine whether PCA is appropriate at all, considering the variables&lt;/h2&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;45%&#34; src=&#34;https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/1.jpg&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Variables should be &lt;strong&gt;inter-correlated enough but not too much.&lt;/strong&gt; Field et al. (2012) provide some thresholds, suggesting that no variable should have many correlations below .30, or &lt;em&gt;any&lt;/em&gt; correlation at all above .90. Thus, in the example here, variable Q06 should probably be excluded from the PCA.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Bartlett’s test&lt;/strong&gt;, on the nature of the intercorrelations, should be significant. Significance suggests that the variables are not an ‘identity matrix’ in which correlations are a sampling error.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;KMO&lt;/strong&gt; (Kaiser-Meyer-Olkin), a measure of sampling adequacy based on common variance (so similar purpose as Bartlett’s). As Field et al. review, ‘values between .5 and .7 are mediocre, values between .7 and .8 are good, values between .8 and .9 are great and values above .9 are superb’ (p. 761). There’s a general score as well as one per variable. The general one will often be good, whereas the individual scores may more likely fail. Any variable with a score below .5 should probably be removed, and the test should be run again.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Determinant:&lt;/strong&gt; A formula about multicollinearity. The result should preferably fall below .00001.
Note that some of these tests are run on the dataframe and others on a correlation matrix of the data, as distinguished below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Necessary libraries
library(ltm)
library(lattice)
library(psych)
library(car)
library(pastecs)
library(scales)
library(ggplot2)
library(arules)
library(plyr)
library(Rmisc)
library(GPArotation)
library(gdata)
library(MASS)
library(qpcR)
library(dplyr)
library(gtools)
library(Hmisc)

# Select variables of interest for the PCA
dataset = mydata[, c(&amp;#39;select_var1&amp;#39;, &amp;#39;select_var1&amp;#39;, 
  &amp;#39;select_var2&amp;#39;, &amp;#39;select_var3&amp;#39;, &amp;#39;select_var4&amp;#39;, 
  &amp;#39;select_var5&amp;#39;, &amp;#39;select_var6&amp;#39;, &amp;#39;select_var7&amp;#39;)]

# Create matrix: some tests will require it
data_matrix = cor(dataset, use = &amp;#39;complete.obs&amp;#39;)

# See intercorrelations
round(data_matrix, 2)

# Bartlett&amp;#39;s
cortest.bartlett(dataset)

# KMO (Kaiser-Meyer-Olkin)
KMO(data_matrix)

# Determinant
det(data_matrix)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-2.-identify-number-of-components-aka-factors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;STAGE 2. Identify number of components (aka factors)&lt;/h2&gt;
&lt;p&gt;In this stage, principal components (formally called ‘factors’ at this stage) are identified among the set of variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The identification is done through a basic, ‘unrotated’ PCA. The number of components set a priori must equal the number of variables that are being tested.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Start off with unrotated PCA

pc1 = psych::principal(dataset, nfactors = length(dataset), rotate=&amp;quot;none&amp;quot;)
pc1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is an example result:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  ## Principal Components Analysis
  ## Call: psych::principal(r = eng_prop, nfactors = 3, rotate = &amp;quot;none&amp;quot;)
  ## Standardized loadings (pattern matrix) based upon correlation matrix
  ##           PC1   PC2  PC3 h2       u2 com
  ## Aud_eng -0.89  0.13 0.44  1 -2.2e-16 1.5
  ## Hap_eng  0.64  0.75 0.15  1  1.1e-16 2.0
  ## Vis_eng  0.81 -0.46 0.36  1 -4.4e-16 2.0
  ## 
  ##                        PC1  PC2  PC3
  ## SS loadings           1.87 0.79 0.34
  ## Proportion Var        0.62 0.26 0.11
  ## Cumulative Var        0.62 0.89 1.00
  ## Proportion Explained  0.62 0.26 0.11
  ## Cumulative Proportion 0.62 0.89 1.00
  ## 
  ## Mean item complexity =  1.9
  ## Test of the hypothesis that 3 components are sufficient.
  ## 
  ## The root mean square of the residuals (RMSR) is  0 
  ##  with the empirical chi square  0  with prob &amp;lt;  NA 
  ## 
  ## Fit based upon off diagonal values = 1&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Among the columns, there are first the correlations between variables and components, followed by a column (h2) with the &lt;strong&gt;‘communalities’&lt;/strong&gt;. If less factors than variables had been selected, communality values would be below 1. Then there is the uniqueness column (u2): &lt;strong&gt;uniqueness&lt;/strong&gt; is equal to 1 minus the communality. Next is ‘com’, which reflects the &lt;strong&gt;complexity&lt;/strong&gt; with which a variable relates to the principal components. Those components are precisely found below. The first row contains the sums of squared loadings, or eigenvalues, namely, the total variance explained by each linear component. This value corresponds to the number of units explained out of all possible factors (which were three in the above example). The rows below all cut from the same cloth. &lt;em&gt;Proportion var&lt;/em&gt; = variance explained over a total of 1. This is the result of dividing the eigenvalue by the number of components. Multiply by 100 and you get the percentage of total variance explained, which becomes useful. In the example, 99% of the variance has been explained. Aside from the meddling maths, we should actually expect 100% there because the number of factors equaled the number of variables. &lt;em&gt;Cumulative var:&lt;/em&gt; variance added consecutively up to the last component. &lt;em&gt;Proportion explained:&lt;/em&gt; variance explained over what has actually been explained (only when variables = factors is this the same as Proportion var). &lt;em&gt;Cumulative proportion:&lt;/em&gt; the actually explained variance added consecutively up to the last component (Field et al., 2012).&lt;/p&gt;
&lt;p&gt;According to Field et al. (2012), two criteria can be used to determine the number of components that should be carried forward to the next stage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SS (sum of squares) loadings, with two possible cut-off points. On the one hand, following Kaiser’s threshold, we should select components with SS loadings &amp;gt; 1. In the example result shown above, only PC1 meets this criterion. A more lenient alternative is Kaiser’s threshold, whereby SS loadings &amp;gt; .7 are accepted. In the example result above, PC2 meet this criterion too.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Scree plot: retain as many components as the number of points after the point of inflection. To create a scree plot, call:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plot(pc1$values, type = &amp;#39;b&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;35%&#34; src=&#34;https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/2.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Imagine a straight line &lt;strong&gt;from the first point on the right.&lt;/strong&gt; Once this line bends considerably, count the points after the bend and up to the last point on the left. The number of points is the number of components to select. The example here is probably the most complicated (two components were finally chosen), but often it’s easier &lt;a href=&#34;https://www.google.nl/search?sca_esv=582945116&amp;amp;q=select+principal+components+scree+plot+point+inflection&amp;amp;tbm=isch&#34;&gt;see examples&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Based on either or both criteria, select the definitive number of components.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-3.-run-definitive-pca&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;STAGE 3. Run definitive PCA&lt;/h2&gt;
&lt;p&gt;Run a very similar command as you did before, but now with a more advanced method. The first PCA, a heuristic one, worked essentially on the inter-correlations. The definitive PCA, in contrast, will implement a prior shuffling known as ‘rotation’, to ensure that the result is robust enough (just like cards are shuffled). Explained variance is captured better this way. The go-to rotation method is the orthogonal, or ‘varimax’ (though others may be considered too).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Now with varimax rotation, Kaiser-normalized by default:
pc2 = psych::principal(dataset, nfactors=2, rotate = &amp;quot;varimax&amp;quot;, 
scores = TRUE)
pc2
pc2$loadings

# Sanity check
pc2$residual
pc2$fit
pc2$communality&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to Field et al. (2012), we would want:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Less than half of &lt;strong&gt;residuals&lt;/strong&gt; with absolute values &amp;gt; 0.05&lt;/li&gt;
&lt;li&gt;Model &lt;strong&gt;fit&lt;/strong&gt; &amp;gt; .9&lt;/li&gt;
&lt;li&gt;All &lt;strong&gt;communalities&lt;/strong&gt; &amp;gt; .7&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If any of this fails, consider changing the number of factors. Next, the rotated components that have been ‘extracted’ from the core of the set of variables can be added to the dataset. This would enable the use of these components as new variables that might prove powerful and useful (as in &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.1551-6709.2010.01157.x/full&#34;&gt;this research&lt;/a&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dataset = cbind(dataset, pc2$scores)
summary(dataset$RC1, dataset$RC2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-4.-determine-ascription-of-each-variable-to-components&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;STAGE 4. Determine ascription of each variable to components&lt;/h2&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;55%&#34; src=&#34;https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/3.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Check the main summary by just calling pc2, and see how each variable correlates with the rotated components. This is essential because it reveals how variables load on each component, or in other words, to which component a variable belongs. For instance, the table shown here belongs to a study about the meaning of words (Bernabeu, 2018). These results suggest that the visual and haptic modalities of words are quite related, whereas the auditory modality is relatively unique. When the analysis works out well, a cut-off point of &lt;em&gt;r&lt;/em&gt; = .8 may be applied for considering a variable as part of a component.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-5.-enjoy-the-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;STAGE 5. Enjoy the plot&lt;/h2&gt;
&lt;p&gt;The plot is perhaps the coolest part about PCA. It really makes an awesome illustration of the power of data analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ggplot(eng_props,
  aes(RC1, RC2, label = as.character(main_eng))) + stat_density2d (color = &amp;quot;gray87&amp;quot;) +
  geom_text(size = ifelse(eng_props$word_eng %in% w_set, 12, 7),
	fontface = ifelse(eng_props$word_eng %in% w_set, &amp;#39;bold&amp;#39;, &amp;#39;plain&amp;#39;)) +
  geom_point(data=eng_props[eng_props$word_eng %in% w_set,], pch=21, fill=NA, size=14, stroke=2, alpha=.6) +
  labs(subtitle=&amp;#39;(Data from Lynott &amp;amp; Connell, 2009)&amp;#39;, x = &amp;quot;Varimax-rotated Principal Component 1&amp;quot;, 
	y = &amp;quot;Varimax-rotated Principal Component 2&amp;quot;) +	theme_bw() +   
  theme( plot.background = element_blank(), panel.grid.major = element_blank(),
	panel.grid.minor = element_blank(), panel.border = element_blank(),
  	axis.line = element_line(color = &amp;#39;black&amp;#39;),
	axis.title.x = element_text(colour = &amp;#39;black&amp;#39;, size = 23, margin=margin(15,15,15,15)),
	axis.title.y = element_text(colour = &amp;#39;black&amp;#39;, size = 23, margin=margin(15,15,15,15)),
	axis.text.x = element_text(size=16), axis.text.y  = element_text(size=16),
	plot.title = element_text(hjust = 0.5, size = 32, face = &amp;quot;bold&amp;quot;, margin=margin(15,15,15,15)),
	plot.subtitle = element_text(hjust = 0.5, size = 20, margin=margin(2,15,15,15)) ) +
  geom_label_repel(data = eng_props[eng_props$word_eng %in% w_set,], aes(label = word_eng), size = 8, 
	alpha = 0.77, color = &amp;#39;black&amp;#39;, box.padding = 1.5 )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is an example combining PCA plots with code similar to the above. These plots illustrate something further with regard to the relationships among modalities. In property words, the different modalities spread out more clearly than they do in concept words. This makes sense because in language, properties define concepts (Bernabeu, 2018).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An example of these analyses is &lt;a href=&#34;https://pablobernabeu.shinyapps.io/Dutch-modality-exclusivity-norms&#34;&gt;available in available in this RStudio environment&lt;/a&gt;, in the &lt;code&gt;norms.R&lt;/code&gt; script.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Bernabeu, P. (2018). &lt;em&gt;Dutch modality exclusivity norms for 336 properties and 411 concepts&lt;/em&gt;. PsyArXiv. &lt;a href=&#34;https://doi.org/10.31234/osf.io/s2c5h&#34; class=&#34;uri&#34;&gt;https://doi.org/10.31234/osf.io/s2c5h&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Field, A. P., Miles, J., &amp;amp; Field, Z. (2012). &lt;em&gt;Discovering Statistics Using R&lt;/em&gt;. London, UK: Sage.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Review of the Landscape Model of reading: Composition, dynamics and application</title>
      <link>https://pablobernabeu.github.io/2018/review-of-the-landscape-model-of-reading-composition-dynamics-and-application/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2018/review-of-the-landscape-model-of-reading-composition-dynamics-and-application/</guid>
      <description>


&lt;p&gt;Throughout the 1990s, two opposing theories were used to explain how people understand texts, later bridged by the Landscape Model of reading (van den Broek, Young, Tzeng, &amp;amp; Linderholm, 1999). A review is offered below, including a schematic representation of the Landscape Model.&lt;/p&gt;
&lt;div id=&#34;memory-based-view&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Memory-based view&lt;/h2&gt;
&lt;p&gt;The memory-based view presented reading as an autonomous, unconscious, effortless process. Readers were purported to achieve an understanding of a text as a whole by combining the concepts, and implications readily afforded, in the text with their own background knowledge (Myers &amp;amp; O’Brien, 1998; O’Brien &amp;amp; Myers, 1999). This memory-based view did not include a conscious (re)activation of meaning. Arguably, this absence raises the question of how readers could recover the meaning if ever distracted while reading.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;constructionist-view&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Constructionist view&lt;/h2&gt;
&lt;p&gt;The constructionist view contended that readers make strategic, time-consuming efforts to access prior text and background knowledge (Graesser, Singer, &amp;amp; Trabasso, 1994; Singer, Graesser, &amp;amp; Trabasso, 1994). A possible challenge for this view is in leisure reading. If reading is effortful, how could it be explained when people become absorbed in leisure reading for hours, and enjoy it? How could this activity become as automatic and fast as it often does?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;landscape-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Landscape Model&lt;/h2&gt;
&lt;p&gt;The memory-based and the constructionist views were bridged in a proposal called the Landscape Model (van den Broek et al., 1999). With a step-based model, van den Broek et al. argued that strategic (re)activations are available to the reader, but need not always be used (see also Converse, 2018). Figure 1 illustrates the dynamic, often cyclical processes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#39;featured.PNG&#39; alt=&#34;Figure 1. Mindmap of van den Broek et al.’s Landscape Model of reading comprehension. Created by Pablo Bernabeu and retrieved from https://doi.org/10.6084/m9.figshare.1591215.&#34; style=&#39;margin-top:2px; margin-bottom:-2px;&#39;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size:90%; color: #899499;&#34;&gt;Figure 1. Mindmap of van den Broek et al.’s Landscape Model of reading comprehension. Created by Pablo Bernabeu and retrieved from &lt;a href=&#34;https://doi.org/10.6084/m9.figshare.1591215&#34; class=&#34;uri&#34;&gt;https://doi.org/10.6084/m9.figshare.1591215&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Landscape Model was applied to the context of discourse analysis by Yeari and van den Broek (2011). The authors noted that discourse analysts may find it useful to adopt a top-down, inductive approach to their task. That is, suppressing the natural, incremental route of meaning making in reading, discourse analysts may want to read through the text to gain a general idea first, before tackling a detailed analysis (see also Bell, 2011).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Bell, A. (2011). Re-constructing Babel: Discourse analysis, hermeneutics and the Interpretive Arc. &lt;em&gt;Discourse Studies, 13&lt;/em&gt;(5), 519–568. &lt;a href=&#34;https://doi.org/10.1177/1461445611412699&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/1461445611412699&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Converse, N. E. (2018). The Use of Explicit Comprehension Strategies During Oral Instruction of Informational Text Structures and the Effect on First-graders’ Listening Comprehension (Doctoral dissertation). Retrieved from &lt;a href=&#34;https://digitalcommons.usu.edu/etd/7305&#34; class=&#34;uri&#34;&gt;https://digitalcommons.usu.edu/etd/7305&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Graesser, A., Singer, M., &amp;amp; Trabasso, T. (1994). Constructing inferences during narrative comprehension. &lt;em&gt;Psychological Review, 101&lt;/em&gt;(3), 371–395. &lt;a href=&#34;https://doi.org/10.1037/0033-295X.101.3.371&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1037/0033-295X.101.3.371&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Myers, J. L., &amp;amp; O’Brien, E. J. (1998). Accessing the discourse representation during reading. &lt;em&gt;Discourse Processes, 26&lt;/em&gt;(2-3), 131–157. &lt;a href=&#34;https://doi.org/10.1080/01638539809545042&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1080/01638539809545042&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;O’Brien, E. J., &amp;amp; Myers, J. L. (1999). Text comprehension: A view from the bottom up. In S. R. Goldman, A. C. Graesser &amp;amp; P. van den Broek P (Eds.), &lt;em&gt;Narrative Comprehension, Causality, and Coherence: Essays in Honor of Tom Trabasso&lt;/em&gt; (pp. 35-53). Mahwah, NJ: Lawrence Erlbaum Associates.&lt;/p&gt;
&lt;p&gt;Singer, M., Graesser, A. C., &amp;amp; Trabasso, T. (1994). Minimal or global inference during reading. &lt;em&gt;Journal of Memory and Language, 33&lt;/em&gt;(4), 421–441. &lt;a href=&#34;https://doi.org/10.1006/jmla.1994.1020&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1006/jmla.1994.1020&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;van den Broek, P., Young, M., Tzeng, Y., &amp;amp; Linderholm, T. (1999). The landscape model of reading. In H. van Oostendorp &amp;amp; S. R. Goldman (Eds.), &lt;em&gt;The construction of mental representations during reading&lt;/em&gt; (pp. 71-98). Mahwah, NJ: Erlbaum.&lt;/p&gt;
&lt;p&gt;Yeari, M., &amp;amp; van den Broek, P. (2011). A cognitive account of discourse understanding and discourse interpretation: The Landscape Model of reading. &lt;em&gt;Discourse Studies, 13&lt;/em&gt;(5), 635-643. &lt;a href=&#34;https://doi.org/10.1177/1461445611412748&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/1461445611412748&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Denken met woorden</title>
      <link>https://pablobernabeu.github.io/2017/denken-met-woorden/</link>
      <pubDate>Mon, 20 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2017/denken-met-woorden/</guid>
      <description>


</description>
    </item>
    
    <item>
      <title>At Greg, 8 am</title>
      <link>https://pablobernabeu.github.io/2017/at-greg-8-am/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2017/at-greg-8-am/</guid>
      <description>
&lt;script src=&#34;https://pablobernabeu.github.io/2017/at-greg-8-am/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The clock strikes a certain hour, below all the Greg’s teaspoons at play. Results o’clock. The usual, please.&lt;/p&gt;
&lt;p&gt;Usual table. &lt;code&gt;summaryby&lt;/code&gt; (having to get the first peek in the cafeteria can only add zest). &lt;code&gt;summaryBy(RT ~ list(Ptp, Group, Cond), behdata, FUN=summary)&lt;/code&gt;. So, hardly any of the 95% Confidence Intervals contain 0. Does this really mean…?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‘For example, the hypothesis of equality of population means will be rejected at the 0.05 level if and only if a 95% CI for the mean difference does not contain 0.’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;— Dallal (2002; &lt;a href=&#34;http://www.jerrydallal.com/lhsp/pval.htm&#34; class=&#34;uri&#34;&gt;http://www.jerrydallal.com/lhsp/pval.htm&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Of course. The CI just has that and more. The window is showing a chilly 1999 morning. Let’s see the summary again. Wee standard deviations. By card, please.&lt;/p&gt;
&lt;p&gt;Mmm, the air outside is worth gingering up…&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The trials!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The assumption of independence spoils another morning.&lt;/p&gt;
&lt;p&gt;This new data consisted of response times (RT) that had been collected over several trials. The single dependent variable, RT, was accompanied by other variables which could be analyzed as independent variables. These included &lt;em&gt;Group&lt;/em&gt;, &lt;em&gt;Trial Number&lt;/em&gt;, and a within-subjects &lt;em&gt;Condition&lt;/em&gt;. &lt;strong&gt;What had to be done first off, in order to take the usual table?&lt;/strong&gt; &lt;em&gt;The trials!&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;assumption-of-independence-of-observations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assumption of independence of observations&lt;/h2&gt;
&lt;p&gt;One must account for any redundant measures below the level of participants (the experimental trials, in this case), so that the sample size (&lt;em&gt;N&lt;/em&gt;) used for any summary statistics match the number of participants (or the largest group, &lt;em&gt;n&lt;/em&gt;). Why? This is a &lt;a href=&#34;https://stats.stackexchange.com/questions/130019/standard-error-for-aggregated-proportions&#34;&gt;central assumption in statistics&lt;/a&gt;: observations must be independent. We can observe the independence assumption differently, depending on whether we’re summarizing data or performing statistical tests.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For &lt;em&gt;descriptive tables and plots&lt;/em&gt; (involving Standard Error/Deviation, Confidence Intervals, etc), &lt;em&gt;the data ought to be aggregated to the level from which you want to generalize&lt;/em&gt;. That level is—in this case and very often—&lt;em&gt;participants&lt;/em&gt;. Trials do not normally serve for statistical generalization (they’re good for experimental validity). This realization may come as a bummer if you have first seen the effect sizes in the un-aggregated data. The mirage (see red lines on the left table below) is caused by an inflated &lt;em&gt;N&lt;/em&gt; (cf. red lines on the right-hand table). As an illustration, the tables below summarize data with an actual sample &lt;em&gt;n&lt;/em&gt; = 23. However, the table on the right includes repeated measures that should have been aggregated, massively inflating &lt;em&gt;n&lt;/em&gt;. The inflation of the sample size equals the product of all repeated measures that failed to be aggregated under participants.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;inflated.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#39;30%&#39; src=&#39;SD.jpg&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Measures of variance such as the Standard Deviation divide by the sample size. Thus, the larger the sample (N), the smaller the Standard Deviation, Standard Error, Confidence Interval…—that is, the variation or noise.&lt;/p&gt;
&lt;p&gt;Aggregating is a snap. For example, with the aggregate() function in R, you just have to include all of your variables except that or those of the repeated measures:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;behdata_aggreg = aggregate(behdata$RT, list(behdata$Ptp, behdata$Group, behdata$Cond), 
  data=behdata, FUN=mean)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;In statistical tests, repeated measures below the participant level–e.g., trials–normally must be either factored in or aggregated. Barr and colleagues provide an easy, focused &lt;a href=&#34;http://talklab.psy.gla.ac.uk/simgen/faq.html#sec-3&#34;&gt;guide on this procedure&lt;/a&gt;. This is necessary because when the N in the analyses is augmented by unaccounted, redundant observations, &lt;em&gt;the famous assumption of independence of observations is violated&lt;/em&gt;, and the results may be invalid, as &lt;a href=&#34;https://arxiv.org/pdf/1601.01126.pdf&#34;&gt;Vasishth and Nicenboim (2016, p. 3)&lt;/a&gt; put it:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;‘if we were to do a t-test on the unaggregated data, we would violate the independence assumption and the result of the t-test would be invalid.’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now, usually the repetitions that concern us are the multiple trials or items in experiments, or other sub-participant measures. So what about participants–what are they never aggregated? &lt;a href=&#34;http://tandfonline.com.sci-hub.cc/doi/abs/10.1080/01933922.2016.1264520?journalCode=usgw20&#34;&gt;McCarthy, Whittaker, Boyle, and Eyal (2017, p.10)&lt;/a&gt; note:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‘It has also been proposed that researchers aggregate the responses of participants within the same group and use the groups/clusters as the unit of analysis (Stevens, 2007). However, because this would result in losing sample size at the participant level, this approach is not optimal given the already small numbers of groups typically studied in group work research.’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;different-procedure-in-linear-mixed-effects-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Different procedure in linear mixed-effects models&lt;/h2&gt;
&lt;p&gt;Aggregation is no longer necessary, where linear mixed-effects models can be used. These models allow us to &lt;a href=&#34;http://talklab.psy.gla.ac.uk/simgen/faq.html#sec-3&#34;&gt;account for any clusters (Participants, Trials, Items…) by signing them into the error term&lt;/a&gt; (&lt;a href=&#34;https://psych.wisc.edu/Brauer/BrauerLab/wp-content/uploads/2014/04/Brauer_and_Curtin_LMEMs-2017-Psych_Methods.pdf&#34;&gt;Brauer &amp;amp; Curtin, 2017&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs</title>
      <link>https://pablobernabeu.github.io/2017/modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing-evidence-from-erps/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2017/modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing-evidence-from-erps/</guid>
      <description>&lt;p&gt;Research has extensively investigated whether conceptual processing is modality-specific—that is, whether meaning is processed to a large extent on the basis of perceptual and motor affordances (Barsalou, 2016). This possibility challenges long-established theories. It suggests a strong link between physical experience and language which is not borne out of the paradigmatic arbitrariness of words (see Lockwood, Dingemanse, &amp;amp; Hagoort, 2016). Modality-specificity also clashes with models of language that have no link to sensory and motor systems (Barsalou, 2016).&lt;/p&gt;
&lt;a href=&#39;https://pablobernabeu.github.io/publication/bernabeu-etal-2017/&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #4CAF50; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt; 
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;/span&gt; Conference paper &lt;/h3&gt;&lt;/button&gt;&lt;/a&gt; &amp;nbsp; 
&lt;a href=&#39;https://pablobernabeu.github.io/publication/bernabeu-2017-mphil-thesis/&#39;&gt;
      &lt;button style = &#34;background-color: white; color: black; border: 2px solid #196F27; border-radius: 12px;&#34;&gt;
      &lt;h3 style = &#34;margin-top: 7px !important; margin-left: 9px !important; margin-right: 9px !important;&#34;&gt; 
      &lt;span style=&#34;color:#DBE6DA;&#34;&gt;&lt;/span&gt; Master&#39;s thesis &lt;/h3&gt;&lt;/button&gt;&lt;/a&gt; &amp;nbsp; 
&lt;br&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/post/Conceptual_modality_switch_effect_measured_at_first_word&#34;&gt;Early discussion on ResearchGate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://osf.io/97unm/&#34;&gt;Data and code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pablobernabeu.github.io/applications-and-dashboards/bernabeu-etal-2017-modalityswitch/&#34;&gt;Data dashboard&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the Conceptual Modality Switch (CMS) paradigm, participants perform a property verification task, deciding whether certain property words can reasonably describe concept words. Covertly, the conceptual modality of consecutive trials is manipulated in order to produce specific switches in conceptual modality. For instance, after the trial &lt;em&gt;Soundless Answer&lt;/em&gt;, which is primarily auditory, the following trial may match in modality—&lt;em&gt;Loud Welcome&lt;/em&gt;—or mismatch—&lt;em&gt;Fine Selection&lt;/em&gt; (visual).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;designoverview.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Modality switches incur processing costs, as reflected in brain signals linked to semantic violation, and in longer response times (RTs) (Scerrati, Lugli, Nicoletti, &amp;amp; Borghi, 2016). This effect suggests that perceptual features of concepts are accessed during conceptual processing. More recently, however, the CMS effect was reanalysed using a non-perceptual alternative. Louwerse and Connell (2011) found that language statistics (the co-occurrence of words in a language) were able to approximately predict visual/haptic, olfactory/gustatory, and auditory modalities, but not the subtler differences between visual and haptic and between olfactory and gustatory, which seemed to be reserved for perceptual simulations. Moreover, faster response times (RTs) were best explained by language statistics, whereas slower RTs were best explained by perceptual simulations.&lt;/p&gt;
&lt;p&gt;The time course of word processing is important. Research suggests that word processing spans one second, during which different processes—semantic and post-semantic—gradually accumulate (Hauk, 2016). The later an effect, the more reasons to question it. Yet, having an early emergence does not either make an effect lexicosemantic, as the meaning encoded could have gone through working memory before activating the actual system of interest, e.g., sensorimotor (Mahon &amp;amp; Caramazza, 2008). Research also suggests that modal systems may contribute to conceptual processing early on—within 200 ms (Vukovic, Feurra, Shpektor, Myachykov, &amp;amp; Shtyrov, 2017). Thus, measuring effects online may prove valuable.&lt;/p&gt;
&lt;h2 id=&#34;experiment&#34;&gt;Experiment&lt;/h2&gt;
&lt;p&gt;Bernabeu, Willems and Louwerse (2017) investigated whether CMS reflects a functionally relevant process of simulation or instead arises only after basic conceptual processing has been attained. We also examined whether different processing systems, amodal and modal, may compatibly operate.&lt;/p&gt;
&lt;p&gt;We measured CMS online by time-locking Event-Related brain Potentials (ERPs) to the onset of the first word in the target trials, in order to assess how strongly CMS may be influenced by post-semantic processes. Previous research would predict an increase in the CMS effect over time because earlier processing is relatively amodal (Louwerse &amp;amp; Hutchinson, 2012).&lt;/p&gt;
&lt;p&gt;We tested the compatibility of amodal and modal processing by drawing on Louwerse and Connell’s (2011) findings. In this conceptual replication, we split participants into a Quick and a Slow group based on RT. Maintaining CMS as a within-subjects factor, we predicted that the larger modality switches (e.g., auditory to visual) would be picked up equally by both groups, whereas the subtler switches (e.g., haptic to visual) would be picked up only—or more clearly—by the Slow group.&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;The stimuli were normed (Bernabeu, Louwerse, &amp;amp; Willems, in prep.). Three CMS conditions were created—Auditory-to-visual, Haptic-to-visual, Visual-to-visual—, each with 36 target trials. The property verification task was pretested valid (&lt;em&gt;N&lt;/em&gt; = 19).&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;All participants but one responded correctly to over half of the trials, with an overall accuracy of 63%.&lt;/p&gt;
&lt;p&gt;ERPs showed a CMS effect from time window 1 on, larger after 350 ms. It appeared with both switch conditions, and was characterized by a more negative amplitude for the switch conditions compared to the no-switch condition. It was generally stronger in the posterior brain regions, and in the Slow group. The results are illustrated in the figure below, which includes 95% Confidence Intervals and time windows.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;stackERPs.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#39;50%&#39; src=&#39;results.jpg&#39; /&gt;&lt;/p&gt;
&lt;p&gt;The analysis was done with Linear Mixed Effects models. Final models presented good fits, with R&lt;sup&gt;2&lt;/sup&gt; ranging from .748 to .862. First, the CMS effect in time window 1 was confirmed significant. Such an early emergence is unprecedented in the CMS literature, and it may have been enabled by the time-locking of ERPs to the first word in target trials. In this time window, the only process not lexicosemantic is possibly working memory (Hauk, 2016), and therefore this early emergence adds support to the possibility that CMS was directly caused by perceptual simulation.&lt;/p&gt;
&lt;p&gt;Whereas in time window 1, the effect was circumscribed to an interaction with Brain Area, by Time Window 2, a main effect of CMS emerged. In Windows 3 and 4, the only experimental effect was CMS.&lt;/p&gt;
&lt;p&gt;Bonferroni-corrected, planned ANOVA contrasts into CMS conditions revealed that the no-switch condition differed significantly from the switch conditions. By contrast, the switch conditions (Haptic-to-visual and Auditory-to-visual) hardly differed from each other, underscoring the CMS effect.&lt;/p&gt;
&lt;p&gt;Although the interaction of Group and CMS was only significant in Time Windows 1 and 2, Windows 2 to 4 presented a pattern fitting our predictions (Louwerse &amp;amp; Connell, 2011). While the Slow group picked up the switches across all modalities similarly, the Quick group picked up the Auditory-to-visual switch more clearly than the Haptic-to-visual switch.&lt;/p&gt;
&lt;h3 id=&#34;statistical-analysis&#34;&gt;Statistical analysis&lt;/h3&gt;
&lt;p&gt;The statistical analysis is &lt;a href=&#34;https://osf.io/sx3nw&#34;&gt;available on OSF&lt;/a&gt; (to view the plots, please &lt;a href=&#34;https://osf.io/download/sx3nw&#34;&gt;download the document&lt;/a&gt;).&lt;/p&gt;
&lt;iframe src=&#34;https://mfr.osf.io/render?url=https%3A%2F%2Fosf.io%2Fdownload%2Fsx3nw%2F%3Fdirect%26mode%3Drender&#34;
        width=&#34;100%&#34;
        scrolling=&#34;yes&#34;
        height=&#34;677px&#34;
        marginheight=&#34;0&#34;
        frameborder=&#34;0&#34;
        allowfullscreen
        webkitallowfullscreen
&gt;
&lt;/iframe&gt;
&lt;h2 id=&#34;discussion&#34;&gt;Discussion&lt;/h2&gt;
&lt;p&gt;Results broadly suggest that cognition may operate on qualitatively different systems for the same task. In conceptual processing, one of these systems appears to be modality-independent, potentially based on linguistic co-occurrences, whereas another system is modality-specific, linked to physical experience.&lt;/p&gt;
&lt;p&gt;A conference poster with further analyses is &lt;a href=&#34;https://osf.io/dj52n&#34;&gt;also available&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;div class = &#39;hanging-indent&#39;&gt;
&lt;p&gt;Barsalou, L. W. (2016). On staying grounded and avoiding quixotic dead ends. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Bernabeu, P., Louwerse, M. M., &amp;amp; Willems, R. M. (in prep.). Modality exclusivity norms for 747 properties and concepts in Dutch: a replication of English. Retrieved from &lt;a href=&#34;https://osf.io/brkjw/&#34;&gt;https://osf.io/brkjw/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Bernabeu, P., Willems, R. M., &amp;amp; Louwerse, M. M. (2017). Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs. In G. Gunzelmann, A. Howes, T. Tenbrink, &amp;amp; E. J. Davelaar (Eds.), &lt;em&gt;Proceedings of the 39th Annual Conference of the Cognitive Science Society&lt;/em&gt; (pp. 1629-1634). Austin, TX: Cognitive Science Society. &lt;a href=&#34;https://doi.org/10.31234/osf.io/a5pcz&#34;&gt;https://doi.org/10.31234/osf.io/a5pcz&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hauk, O. (2016). Only time will tell—Why temporal information is essential for our neuroscientific understanding of semantics. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review, 23&lt;/em&gt;, 4, 1072-1079.&lt;/p&gt;
&lt;p&gt;Lockwood, G., Hagoort, P., &amp;amp; Dingemanse, M. (2016). How iconicity helps people learn new words: neural correlates and individual differences in sound-symbolic bootstrapping. &lt;em&gt;Collabra, 2&lt;/em&gt;, 1, 7.&lt;/p&gt;
&lt;p&gt;Louwerse, M., &amp;amp; Connell, L. (2011). A taste of words: linguistic context and perceptual simulation predict the modality of words. &lt;em&gt;Cognitive Science, 35&lt;/em&gt;, 2, 381-98.&lt;/p&gt;
&lt;p&gt;Louwerse, M., &amp;amp; Hutchinson, S. (2012). Neurological evidence linguistic processes precede perceptual simulation in conceptual processing. &lt;em&gt;Frontiers in Psychology, 3&lt;/em&gt;, 385.&lt;/p&gt;
&lt;p&gt;Mahon, B. Z., &amp;amp; Caramazza, A. (2008). A critical look at the Embodied Cognition Hypothesis and a new proposal for grounding conceptual content. &lt;em&gt;Journal of Physiology - Paris, 102&lt;/em&gt;, 59-70.&lt;/p&gt;
&lt;p&gt;Scerrati, E., Lugli, L., Nicoletti, R., &amp;amp; Borghi, A. M. (2016). The Multilevel Modality-Switch Effect: What Happens When We See the Bees Buzzing and Hear the Diamonds Glistening. &lt;em&gt;Psychonomic Bulletin &amp;amp; Review&lt;/em&gt;, doi:10.3758/s13423-016-1150-2.&lt;/p&gt;
&lt;p&gt;Vukovic, V., Feurra, M., Shpektor, A., Myachykov, A., &amp;amp; Shtyrov, Y. (2017). Primary motor cortex functionally contributes to language comprehension: An online rTMS study. &lt;em&gt;Neuropsychologia, 96&lt;/em&gt;, 222-229.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The case for data dashboards: First steps in R Shiny</title>
      <link>https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/</guid>
      <description>


&lt;div style=&#34;font-size:110%;&#34;&gt;
&lt;b&gt; Dashboards for data visualisation, such as &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;R Shiny&lt;/a&gt; and &lt;a href=&#34;https://www.tableau.com/&#34;&gt;Tableau&lt;/a&gt;, allow the interactive exploration of data by means of drop-down lists and checkboxes, with no coding required from the final users. These web applications can be useful for both the data analyst and the public at large. &lt;/b&gt;
&lt;/div&gt;
&lt;div style=&#34;margin-top: 4%;&#34;&gt;

&lt;/div&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#34;25%&#34; src=&#34;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/1.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Visualisation apps run on internet browsers. This allows for three options: private viewing (useful during analysis), selective sharing (used within work groups), or internet publication. Among the available platforms, &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;R Shiny&lt;/a&gt; and &lt;a href=&#34;https://www.tableau.com/&#34;&gt;Tableau&lt;/a&gt; stand out due to being relatively accessible to new users. Apps serve a broad variety of purposes (see &lt;a href=&#34;https://shiny.rstudio.com/gallery/&#34;&gt;this gallery&lt;/a&gt; and &lt;a href=&#34;https://www.tableau.com/products/desktop&#34;&gt;this one&lt;/a&gt;). In science and beyond, these apps allow us to go &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01782/full&#34;&gt;the extra mile in sharing data&lt;/a&gt;. Alongside files and code shared in repositories, we can present the data in a website, in the form of plots or tables. This facilitates the public exploration of each section of the data (groups, participants, trials…) &lt;a href=&#34;https://www.nature.com/articles/d41586-019-01506-x&#34;&gt;to anyone interested, and allows researchers to account for their proceeding&lt;/a&gt; in the analysis.&lt;/p&gt;
&lt;p&gt;&lt;img width = &#39;70%&#39; src=&#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/2.png&#39; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#39;60%&#39; src=&#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/3.png&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Publishers and journals highly encourage authors to make the most of their data by facilitating its easy exploration by the readership–even though they don’t normally offer the possibility of hosting any web applications yet.&lt;/p&gt;
&lt;p&gt;Apps can also prove valuable to those analysing the data. For instance, my app helped me to identify the extent of noise in a section of the data. Instead of running through a heavy score of code, the drop-down lists of the app let me seamlessly surf through the different sections.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At a certain point, I found a data section that was consistently noisier than the rest, and eventually I had to discard it from further statistical analyses. Yet, instead of removing that from the app, I maintained it with a note attached. This particular trait in the data was rather salient.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/5.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Beyond such a salient feature in the data, a visualisation app may also help to spot subtler patterns such as third variables or individual differences.&lt;/p&gt;
&lt;p&gt;There are several platforms for creating apps (e.g., Tableau, D3.js, and R Shiny). I focus on R Shiny here for three reasons: it is affordable to use, fairly accessible to new users, and well suited for science as it is based on the R language (see for instance &lt;a href=&#34;https://doi.org/10.1080/10691898.2018.1436999&#34;&gt;this article&lt;/a&gt;).&lt;/p&gt;
&lt;div id=&#34;how-to-shiny&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;How to Shiny&lt;/h4&gt;
&lt;p&gt;&lt;img style = &#34;float: right; margin-left: 30px;&#34; width = &#39;30%&#39; src=&#39;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/6.png&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Shiny apps draw on any standard R code that you may already have. This is most commonly plots or tables, but other stuff such as images or Markdown texts are valid too. This is a nice thing to keep in mind when having to create a new app. Part of the job may already be done! The app is distributed among five different areas.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data file(s): These are any data files you’re using (e.g., with csv or rds extensions).&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;a.-server.r-script&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;1a. &lt;code&gt;server.R&lt;/code&gt; script&lt;/h5&gt;
&lt;p&gt;The &lt;code&gt;server&lt;/code&gt; script contains the central processes: plots, tables, etc. Code that existed independently of the app app may be brought into this script by slightly adapting it. At the top, call the &lt;code&gt;shiny&lt;/code&gt; library and any others used (e.g., ‘ggplot2’), and also read in the data. The snippet below shows the beginning of an example &lt;code&gt;server.R&lt;/code&gt; script.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
# server

library(shiny)
library(ggplot2)

EEG.ParticipantAndElectrode = readRDS(&amp;#39;EEG.ParticipantAndElectrode.rds&amp;#39;)
EEG.ParticipantAndBrainArea = readRDS(&amp;#39;EEG.ParticipantAndBrainArea.rds&amp;#39;)
EEG.GroupAndElectrode = readRDS(&amp;#39;EEG.GroupAndElectrode.rds&amp;#39;)
EEG.OLDGroupAndElectrode = readRDS(&amp;#39;EEG.OLDGroupAndElectrode.rds&amp;#39;)


server =

shinyServer(

  function(input, output) {

# plot_GroupAndElectrode:
    output$plot_GroupAndElectrode &amp;lt;- renderPlot({

dfelectrode &amp;lt;- aggregate(microvolts ~ electrode*time*condition, 
EEG.GroupAndElectrode[EEG.GroupAndElectrode$RT.based_Groups==input$var.Group,], mean)

df2 &amp;lt;- subset(dfelectrode, electrode == input$var.Electrodes.1)

df2$condition= as.factor(df2$condition)
df2$condition &amp;lt;- gsub(&amp;#39;visual2visual&amp;#39;, &amp;#39; Visual / Visual&amp;#39;, df2$condition)
df2$condition &amp;lt;- gsub(&amp;#39;haptic2visual&amp;#39;, &amp;#39; Haptic / Visual&amp;#39;, df2$condition)
df2$condition &amp;lt;- gsub(&amp;#39;auditory2visual&amp;#39;, &amp;#39; Auditory / Visual&amp;#39;, df2$condition)

df2$time &amp;lt;- as.integer(as.character(df2$time))
colours &amp;lt;- c(&amp;#39;firebrick1&amp;#39;, &amp;#39;dodgerblue&amp;#39;, &amp;#39;forestgreen&amp;#39;)
# green:visual2visual, blue:haptic2visual, red:auditory2visual

spec_title = paste0(&amp;#39;ERP waveforms for &amp;#39;, input$var.Group, &amp;#39; Group, Electrode &amp;#39;, input$var.Electrodes.1, &amp;#39; (negative values upward; time windows displayed)&amp;#39;)

plot_GroupAndElectrode = ggplot(df2, aes(x=time, y=-microvolts, color=condition)) +
  geom_rect(xmin=160, xmax=216, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=270, xmax=370, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=350, xmax=550, ymin=8, ymax=-7.5, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=500, xmax=750, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_line(size=1, alpha = 1) + scale_linetype_manual(values=colours) +
  scale_y_continuous(limits=c(-8.38, 8.3), breaks=seq(-8,8,by=1), expand = c(0,0.1)) +
  scale_x_continuous(limits=c(-208,808),breaks=seq(-200,800,by=100), expand = c(0.005,0), labels= c(&amp;#39;-200&amp;#39;,&amp;#39;-100 ms&amp;#39;,&amp;#39;0&amp;#39;,&amp;#39;100 ms&amp;#39;,&amp;#39;200&amp;#39;,&amp;#39;300 ms&amp;#39;,&amp;#39;400&amp;#39;,&amp;#39;500 ms&amp;#39;,&amp;#39;600&amp;#39;,&amp;#39;700 ms&amp;#39;,&amp;#39;800&amp;#39;)) +
  ggtitle(spec_title) + theme_bw() + geom_vline(xintercept=0) +
  annotate(geom=&amp;#39;segment&amp;#39;, y=seq(-8,8,1), yend=seq(-8,8,1), x=-4, xend=8, color=&amp;#39;black&amp;#39;) +
  annotate(geom=&amp;#39;segment&amp;#39;, y=-8.2, yend=-8.38, x=seq(-200,800,100), xend=seq(-200,800,100), color=&amp;#39;black&amp;#39;) +
  geom_segment(x = -200, y = 0, xend = 800, yend = 0, size=0.5, color=&amp;#39;black&amp;#39;) +
  theme(legend.position = c(0.100, 0.150), legend.background = element_rect(fill=&amp;#39;#EEEEEE&amp;#39;, size=0),
	axis.title=element_blank(), legend.key.width = unit(1.2,&amp;#39;cm&amp;#39;), legend.text=element_text(size=17),
	legend.title = element_text(size=17, face=&amp;#39;bold&amp;#39;), plot.title= element_text(size=20, hjust = 0.5, vjust=2),
	axis.text.y = element_blank(), axis.text.x = element_text(size = 14, vjust= 2.12, face=&amp;#39;bold&amp;#39;, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;),
	axis.ticks=element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), 
	panel.grid.minor = element_blank(), plot.margin = unit(c(0.1,0.1,0,0), &amp;#39;cm&amp;#39;)) +
  annotate(&amp;#39;segment&amp;#39;, x=160, xend=216, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=270, xend=370, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=350, xend=550, y=-7.5, yend=-7.5, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=500, xend=750, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  scale_fill_manual(name = &amp;#39;Context / Target trial&amp;#39;, values=colours) +
  scale_color_manual(name = &amp;#39;Context / Target trial&amp;#39;, values=colours) +
  guides(linetype=guide_legend(override.aes = list(size=1.2))) +
   guides(color=guide_legend(override.aes = list(size=2.5))) +
# Print y axis labels within plot area:
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;\u2013&amp;#39; * &amp;#39;3 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = 3, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;) +
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;+3 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = -3, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;) +
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;\u2013&amp;#39; * &amp;#39;6 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = 6, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;)

print(plot_GroupAndElectrode)

output$downloadPlot.1 &amp;lt;- downloadHandler(
	filename &amp;lt;- function(file){
	paste0(input$var.Group, &amp;#39; group, electrode &amp;#39;, input$var.Electrodes.1, &amp;#39;, &amp;#39;, Sys.Date(), &amp;#39;.png&amp;#39;)},
   	content &amp;lt;- function(file){
      		png(file, units=&amp;#39;in&amp;#39;, width=13, height=5, res=900)
      		print(plot_GroupAndElectrode)
      		dev.off()},
	contentType = &amp;#39;image/png&amp;#39;)
  } )

# ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://osf.io/uj8z4/&#34;&gt;— Whole script&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b.-ui.r-script&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;1b. &lt;code&gt;ui.R&lt;/code&gt; script&lt;/h5&gt;
&lt;p&gt;The &lt;code&gt;ui&lt;/code&gt; script defines the user interface. For instance, a factor column in the data that has multiple categories may be neatly displayed with a drop-down list on the side bar of the website. The interface may present a central plot before by a legend key below. The snippet below shows the beginning of an example &lt;code&gt;ui.R&lt;/code&gt; script.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
# UI

library(shiny)
library(ggplot2)

EEG.GroupAndElectrode = readRDS(&amp;#39;EEG.GroupAndElectrode.rds&amp;#39;)
EEG.ParticipantAndBrainArea = readRDS(&amp;#39;EEG.ParticipantAndBrainArea.rds&amp;#39;)
EEG.ParticipantAndElectrode = readRDS(&amp;#39;EEG.ParticipantAndElectrode.rds&amp;#39;)
EEG.OLDGroupAndElectrode = readRDS(&amp;#39;EEG.OLDGroupAndElectrode.rds&amp;#39;)


ui =

shinyUI(

   fluidPage(

    tags$head(tags$link(rel=&amp;#39;shortcut icon&amp;#39;, href=&amp;#39;https://image.ibb.co/fXUwzb/favic.png&amp;#39;)),  # web favicon
    tags$meta(charset=&amp;#39;UTF-8&amp;#39;),
    tags$meta(name=&amp;#39;description&amp;#39;, content=&amp;#39;This R Shiny visualisation dashboard presents data from a psycholinguistic ERP experiment (Bernabeu et al., 2017).&amp;#39;),
    tags$meta(name=&amp;#39;keywords&amp;#39;, content=&amp;#39;R, Shiny, ggplot2, visualisation, data, psycholinguistics, conceptual processing, modality switch, embodied cognition&amp;#39;),
    tags$meta(name=&amp;#39;viewport&amp;#39;, content=&amp;#39;width=device-width, initial-scale=1.0&amp;#39;),

    titlePanel(h3(strong(&amp;#39;Waveforms in detail from an ERP experiment on the Conceptual Modality Switch&amp;#39;), a(&amp;#39;(Bernabeu et al., 2017)&amp;#39;,
    href=&amp;#39;https://figshare.com/articles/EEG_study_on_conceptual_modality-switching_Bernabeu_et_al_in_prep_/4210863&amp;#39;, target=&amp;#39;_blank&amp;#39;,
    style = &amp;#39;color:#3E454E; text-decoration:underline; font-weight:normal&amp;#39;), 	align = &amp;#39;center&amp;#39;, style = &amp;#39;color:black&amp;#39;),

    windowTitle = &amp;#39;Visualization of ERP waveforms from experiment on Conceptual Modality Switch (Bernabeu et al., 2017)&amp;#39;),


    sidebarLayout(
	sidebarPanel(width = 2,


# Condition 1 for reactivity between tabs and sidebars

   conditionalPanel(
	condition = &amp;#39;input.tabvals == 1&amp;#39;,

	h5(a(strong(&amp;#39;See paper, statistics, all data.&amp;#39;), &amp;#39;Plots by group and brain area shown in paper.&amp;#39;,
	href=&amp;#39;https://figshare.com/articles/EEG_study_on_conceptual_modality-switching_Bernabeu_et_al_in_prep_/4210863&amp;#39;,
	target=&amp;#39;_blank&amp;#39;), align = &amp;#39;center&amp;#39;),
br(),

	selectInput(&amp;#39;var.Group&amp;#39;, label = &amp;#39;Group&amp;#39;, choices = list(&amp;#39;Quick&amp;#39;,&amp;#39;Slow&amp;#39;), selected = &amp;#39;Quick&amp;#39;),
	h6(&amp;#39;Quick G.: 23 participants&amp;#39;),
	h6(&amp;#39;Slow G.: 23 participants&amp;#39;),
br(),

	selectInput(&amp;#39;var.Electrodes.1&amp;#39;, label = h5(strong(&amp;#39;Electrode&amp;#39;), br(), &amp;#39;(see montage below)&amp;#39;),
                  choices = list(&amp;#39;1&amp;#39;,&amp;#39;2&amp;#39;,&amp;#39;3&amp;#39;,&amp;#39;4&amp;#39;,&amp;#39;5&amp;#39;,&amp;#39;6&amp;#39;,&amp;#39;7&amp;#39;,&amp;#39;8&amp;#39;,&amp;#39;9&amp;#39;,&amp;#39;10&amp;#39;,
			&amp;#39;11&amp;#39;,&amp;#39;12&amp;#39;,&amp;#39;13&amp;#39;,&amp;#39;14&amp;#39;,&amp;#39;15&amp;#39;,&amp;#39;16&amp;#39;,&amp;#39;17&amp;#39;,&amp;#39;18&amp;#39;,&amp;#39;19&amp;#39;,&amp;#39;20&amp;#39;,&amp;#39;21&amp;#39;,
			&amp;#39;22&amp;#39;,&amp;#39;23&amp;#39;,&amp;#39;24&amp;#39;,&amp;#39;25&amp;#39;,&amp;#39;26&amp;#39;,&amp;#39;27&amp;#39;,&amp;#39;28&amp;#39;,&amp;#39;29&amp;#39;,&amp;#39;30&amp;#39;,&amp;#39;31&amp;#39;,&amp;#39;33&amp;#39;,
			&amp;#39;34&amp;#39;,&amp;#39;35&amp;#39;,&amp;#39;36&amp;#39;,&amp;#39;37&amp;#39;,&amp;#39;38&amp;#39;,&amp;#39;39&amp;#39;,&amp;#39;40&amp;#39;,&amp;#39;41&amp;#39;,&amp;#39;42&amp;#39;,&amp;#39;43&amp;#39;,&amp;#39;44&amp;#39;,
			&amp;#39;45&amp;#39;,&amp;#39;46&amp;#39;,&amp;#39;47&amp;#39;,&amp;#39;48&amp;#39;,&amp;#39;49&amp;#39;,&amp;#39;50&amp;#39;,&amp;#39;51&amp;#39;,&amp;#39;52&amp;#39;,&amp;#39;53&amp;#39;,&amp;#39;54&amp;#39;,&amp;#39;55&amp;#39;,
			&amp;#39;56&amp;#39;,&amp;#39;57&amp;#39;,&amp;#39;58&amp;#39;,&amp;#39;59&amp;#39;,&amp;#39;60&amp;#39;), selected = &amp;#39;30&amp;#39; ),
br(), br(),

	h6(&amp;#39;Source code:&amp;#39;),
	h6(strong(&amp;#39;-  &amp;#39;), a(&amp;#39;server.R&amp;#39;, href=&amp;#39;https://osf.io/uj8z4/&amp;#39;, target=&amp;#39;_blank&amp;#39;, style = &amp;#39;text-decoration: underline;&amp;#39;)),
	h6(strong(&amp;#39;-  &amp;#39;), a(&amp;#39;ui.R&amp;#39;, href=&amp;#39;https://osf.io/8bwcx/&amp;#39;, target=&amp;#39;_blank&amp;#39;, style = &amp;#39;text-decoration: underline;&amp;#39;)),
br(),
	h6(a(&amp;#39;CC-By 4.0 License&amp;#39;, href=&amp;#39;https://osf.io/97unm/&amp;#39;, target=&amp;#39;_blank&amp;#39;), align = &amp;#39;center&amp;#39;, style = &amp;#39;text-decoration: underline;&amp;#39;),

br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(),
br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(), br(),

	h5(a(strong(&amp;#39;See paper, statistics, all data.&amp;#39;),
	href=&amp;#39;https://figshare.com/articles/EEG_study_on_conceptual_modality-switching_Bernabeu_et_al_in_prep_/4210863&amp;#39;,
	target=&amp;#39;_blank&amp;#39;), align = &amp;#39;center&amp;#39;),
br(), br(), br(), br(), br(), br(), br(), br()
),

# ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://osf.io/8bwcx&#34;&gt;— Whole script&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;deployment-and-logs&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;2. Deployment and logs&lt;/h5&gt;
&lt;p&gt;This script contains the commands for deploying the app on- or off-line, and for checking the session logs in case of any errors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;automatically-created-folder&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;3. Automatically created folder&lt;/h5&gt;
&lt;p&gt;When the app is first deployed on the internet, a subfolder is automatically created with the name ‘rsconnect’. This folder contains a text file which can be used to modify the URL and the title of the webpage.&lt;/p&gt;
&lt;p&gt;Steps to create a Shiny app from scratch:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://shiny.rstudio.com/articles/shinyapps.html&#34;&gt;&lt;strong&gt;1. Tutorials (link).&lt;/strong&gt; Being open-source software, excellent directions are available through a Google search.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/7.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://shiny.rstudio.com/articles/shinyapps.html&#34;&gt;The core ideas are:&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As mentioned above, create a &lt;code&gt;ui.R&lt;/code&gt; script for the code containing the user interface, and create a &lt;code&gt;server.R&lt;/code&gt; script for the code containing the main content (your plots / tables, etc).&lt;/p&gt;
&lt;p&gt;At the top of both ui.R and server.R scripts, enter the command &lt;code&gt;library(shiny)&lt;/code&gt; and also load any other libraries you’re using (e.g., &lt;code&gt;ggplot2&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Test your app by deploying it locally, before launching online. For this purpose, first save the &lt;code&gt;ui&lt;/code&gt; and &lt;code&gt;server&lt;/code&gt; parts independently, as in:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
ui =

 shinyUI(

   fluidPage(

# ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then deploy locally by running:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;shinyApp(ui, server)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Managing to run the app locally is a great first step before launching online (which may sometimes prove a bit trickier).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://shiny.rstudio.com/articles/shinyapps.html&#34;&gt;&lt;strong&gt;2. User token (link).&lt;/strong&gt; Sign up and read in your private key—just to be done once in a computer.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Go for it.&lt;/strong&gt; After locally testing and saving the two main scripts (&lt;code&gt;ui.R&lt;/code&gt; and &lt;code&gt;server.R&lt;/code&gt;), run &lt;code&gt;deployApp()&lt;/code&gt; to launch the app online.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Bugs and session logs.&lt;/strong&gt; Most often they won’t be bugs actually, but fancies, as it were. For instance, some special characters have to get even more special (technically, UTF-8 encoding). For a character such as ‘μ’, Shiny prefers ‘Âμ’, or better, the Unicode &lt;code&gt;expression(&#34;\u03bc&#34;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Cling to your logs by calling the line below, which you may keep at hand in your ‘Shiny deployer.R’ script.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;showLogs(appPath = getwd(), appFile = NULL, appName = NULL, account = NULL,
entries = 50, streaming = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At best, the log output will mention any typos and unaccepted characters, pointing to specific lines in your code.&lt;/p&gt;
&lt;p&gt;It may take a couple of intense days to get a first Shiny app running. Although the usual rabbit holes do exist, years of Shiny have already yielded a sizeable body of free resources online (tutorials, blogs, vlogs). Moreover, there’s also &lt;a href=&#34;https://community.rstudio.com/&#34;&gt;the RStudio Community&lt;/a&gt;, and then StackOverflow etc., where you can post any needs/despair. Post your code, log, and explanation, and you’ll be rescued out in a couple of days. Long live those contributors.&lt;/p&gt;
&lt;p&gt;It’s sometimes enough to upload a bare app, but you might then think it can look better.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5&lt;/strong&gt; (optional). &lt;strong&gt;Advance.&lt;/strong&gt; Use tabs to combine multiple apps on one webpage, use different widgets, include a download option, etc. Tutorials like &lt;a href=&#34;https://www.youtube.com/watch?v=Q9sRKkaNveI&#34;&gt;this one on Youtube&lt;/a&gt; can take you there, especially those that provide the code, as in the description of that video. Use those scripts as templates. For example, I made use of tabs on the top of the dashboard in order to keep the side bar from having too many widgets. The appearance of these tabs can be adjusted. More importantly, the inputs in the sidebar can be modified depending on the active tab, by means of ‘reactivity’ conditions.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
mainPanel(

	tags$style(HTML(&amp;#39;
	    .tabbable &amp;gt; .nav &amp;gt; li &amp;gt; a                  		{background-color:white; color:#3E454E}
	    .tabbable &amp;gt; .nav &amp;gt; li &amp;gt; a:hover            		{background-color:#002555; color:white}
	    .tabbable &amp;gt; .nav &amp;gt; li[class=active] &amp;gt; a 		{background-color:#ECF4FF; color:black}
	    .tabbable &amp;gt; .nav &amp;gt; li[class=active] &amp;gt; a:hover	{background-color:#E7F1FF; color:black}
	&amp;#39;)),

	tabsetPanel(id=&amp;#39;tabvals&amp;#39;,

            tabPanel(value=1, h4(strong(&amp;#39;Group &amp;amp; Electrode&amp;#39;)), br(), plotOutput(&amp;#39;plot_GroupAndElectrode&amp;#39;),
			h5(a(strong(&amp;#39;See plots with 95% Confidence Intervals&amp;#39;), href=&amp;#39;https://osf.io/2tpxn/&amp;#39;,
			target=&amp;#39;_blank&amp;#39;), style=&amp;#39;text-decoration: underline;&amp;#39;), 
			downloadButton(&amp;#39;downloadPlot.1&amp;#39;, &amp;#39;Download HD plot&amp;#39;), br(), br(),
			# EEG montage
			img(src=&amp;#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&amp;#39;, height=500, width=1000)),

            tabPanel(value=2, h4(strong(&amp;#39;Participant &amp;amp; Area&amp;#39;)), br(), plotOutput(&amp;#39;plot_ParticipantAndLocation&amp;#39;),
			h5(a(strong(&amp;#39;See plots with 95% Confidence Intervals&amp;#39;), href=&amp;#39;https://osf.io/86ch9/&amp;#39;,
			target=&amp;#39;_blank&amp;#39;), style=&amp;#39;text-decoration: underline;&amp;#39;), 
			downloadButton(&amp;#39;downloadPlot.2&amp;#39;, &amp;#39;Download HD plot&amp;#39;), br(), br(),
			# EEG montage
			img(src=&amp;#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&amp;#39;, height=500, width=1000)),

            tabPanel(value=3, h4(strong(&amp;#39;Participant &amp;amp; Electrode&amp;#39;)), br(), plotOutput(&amp;#39;plot_ParticipantAndElectrode&amp;#39;),
			br(), downloadButton(&amp;#39;downloadPlot.3&amp;#39;, &amp;#39;Download HD plot&amp;#39;), br(), br(),
			# EEG montage
			img(src=&amp;#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&amp;#39;, height=500, width=1000)),

            tabPanel(value=4, h4(strong(&amp;#39;OLD Group &amp;amp; Electrode&amp;#39;)), br(), plotOutput(&amp;#39;plot_OLDGroupAndElectrode&amp;#39;),
			h5(a(strong(&amp;#39;See plots with 95% Confidence Intervals&amp;#39;), href=&amp;#39;https://osf.io/dvs2z/&amp;#39;,
			target=&amp;#39;_blank&amp;#39;), style=&amp;#39;text-decoration: underline;&amp;#39;), 
			downloadButton(&amp;#39;downloadPlot.4&amp;#39;, &amp;#39;Download HD plot&amp;#39;), br(), br(),
			# EEG montage
			img(src=&amp;#39;https://preview.ibb.co/n7qiYR/EEG_montage.png&amp;#39;, height=500, width=1000))
	),
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;a href=&#34;https://shiny.rstudio.com/gallery/&#34;&gt;official Shiny gallery&lt;/a&gt; offers a great array of apps including their code (e.g., &lt;a href=&#34;https://shiny.rstudio.com/gallery/kmeans-example.html&#34;&gt;basic example&lt;/a&gt;). Another feature you may add is the option to download your plots, tables, data…&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
# In ui.R script

downloadButton(&amp;#39;downloadPlot.1&amp;#39;, &amp;#39;Download HD plot&amp;#39;)

#___________________________________________________


# In server.R script

spec_title = paste0(&amp;#39;ERP waveforms for &amp;#39;, input$var.Group, &amp;#39; Group, Electrode &amp;#39;, input$var.Electrodes.1, &amp;#39; (negative values upward; time windows displayed)&amp;#39;)

plot_GroupAndElectrode = ggplot(df2, aes(x=time, y=-microvolts, color=condition)) +
  geom_rect(xmin=160, xmax=216, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=270, xmax=370, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=350, xmax=550, ymin=8, ymax=-7.5, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_rect(xmin=500, xmax=750, ymin=7.5, ymax=-8, color = &amp;#39;grey75&amp;#39;, fill=&amp;#39;black&amp;#39;, alpha=0, linetype=&amp;#39;longdash&amp;#39;) +
  geom_line(size=1, alpha = 1) + scale_linetype_manual(values=colours) +
  scale_y_continuous(limits=c(-8.38, 8.3), breaks=seq(-8,8,by=1), expand = c(0,0.1)) +
  scale_x_continuous(limits=c(-208,808),breaks=seq(-200,800,by=100), expand = c(0.005,0), labels= c(&amp;#39;-200&amp;#39;,&amp;#39;-100 ms&amp;#39;,&amp;#39;0&amp;#39;,&amp;#39;100 ms&amp;#39;,&amp;#39;200&amp;#39;,&amp;#39;300 ms&amp;#39;,&amp;#39;400&amp;#39;,&amp;#39;500 ms&amp;#39;,&amp;#39;600&amp;#39;,&amp;#39;700 ms&amp;#39;,&amp;#39;800&amp;#39;)) +
  ggtitle(spec_title) + theme_bw() + geom_vline(xintercept=0) +
  annotate(geom=&amp;#39;segment&amp;#39;, y=seq(-8,8,1), yend=seq(-8,8,1), x=-4, xend=8, color=&amp;#39;black&amp;#39;) +
  annotate(geom=&amp;#39;segment&amp;#39;, y=-8.2, yend=-8.38, x=seq(-200,800,100), xend=seq(-200,800,100), color=&amp;#39;black&amp;#39;) +
  geom_segment(x = -200, y = 0, xend = 800, yend = 0, size=0.5, color=&amp;#39;black&amp;#39;) +
  theme(legend.position = c(0.100, 0.150), legend.background = element_rect(fill=&amp;#39;#EEEEEE&amp;#39;, size=0),
	axis.title=element_blank(), legend.key.width = unit(1.2,&amp;#39;cm&amp;#39;), legend.text=element_text(size=17),
	legend.title = element_text(size=17, face=&amp;#39;bold&amp;#39;), plot.title= element_text(size=20, hjust = 0.5, vjust=2),
	axis.text.y = element_blank(), axis.text.x = element_text(size = 14, vjust= 2.12, face=&amp;#39;bold&amp;#39;, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;),
	axis.ticks=element_blank(), panel.border = element_blank(), panel.grid.major = element_blank(), 
	panel.grid.minor = element_blank(), plot.margin = unit(c(0.1,0.1,0,0), &amp;#39;cm&amp;#39;)) +
  annotate(&amp;#39;segment&amp;#39;, x=160, xend=216, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=270, xend=370, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=350, xend=550, y=-7.5, yend=-7.5, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  annotate(&amp;#39;segment&amp;#39;, x=500, xend=750, y=-8, yend=-8, colour = &amp;#39;grey75&amp;#39;, size = 1.5) +
  scale_fill_manual(name = &amp;#39;Context / Target trial&amp;#39;, values=colours) +
  scale_color_manual(name = &amp;#39;Context / Target trial&amp;#39;, values=colours) +
  guides(linetype=guide_legend(override.aes = list(size=1.2))) +
   guides(color=guide_legend(override.aes = list(size=2.5))) +
# Print y axis labels within plot area:
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;\u2013&amp;#39; * &amp;#39;3 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = 3, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;) +
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;+3 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = -3, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;) +
  annotate(&amp;#39;text&amp;#39;, label = expression(bold(&amp;#39;\u2013&amp;#39; * &amp;#39;6 &amp;#39; * &amp;#39;\u03bc&amp;#39; * &amp;#39;V&amp;#39;)), x = -29, y = 6, size = 4.5, color = &amp;#39;grey32&amp;#39;, family=&amp;#39;sans&amp;#39;)

print(plot_GroupAndElectrode)

output$downloadPlot.1 &amp;lt;- downloadHandler(
	filename &amp;lt;- function(file){
	paste0(input$var.Group, &amp;#39; group, electrode &amp;#39;, input$var.Electrodes.1, &amp;#39;, &amp;#39;, Sys.Date(), &amp;#39;.png&amp;#39;)},
   	content &amp;lt;- function(file){
      		png(file, units=&amp;#39;in&amp;#39;, width=13, height=5, res=900)
      		print(plot_GroupAndElectrode)
      		dev.off()},
	contentType = &amp;#39;image/png&amp;#39;)
  } )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apps can include any text, such as explanations of any length and web links. For instance, we can link back to the data repository, where the code for the app can be found.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/8.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An example of a &lt;a href=&#34;https://pablobernabeu.shinyapps.io/ERP-waveform-visualization_CMS-experiment/&#34;&gt;Shiny app is available&lt;/a&gt;, which may also be &lt;a href=&#34;https://mybinder.org/v2/gh/pablobernabeu/Modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing/master?urlpath=rstudio&#34;&gt;edited and run in this RStudio environment&lt;/a&gt;, inside the ‘Shiny-app’ folder.&lt;/p&gt;
&lt;p&gt;The Shiny server (shinyapps.io) allows publishing dashboards built with various frameworks besides Shiny proper. &lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/&#34;&gt;Flexdashboard&lt;/a&gt; and &lt;a href=&#34;https://rstudio.github.io/shinydashboard/&#34;&gt;Shinydashboard&lt;/a&gt; are two of these frameworks, which have visible advantages over basic Shiny, in terms of layout. An &lt;a href=&#34;https://pablobernabeu.shinyapps.io/Dutch-modality-exclusivity-norms/&#34;&gt;example with Flexdashboard is available&lt;/a&gt;.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ &lt;b&gt; Flexdashboard &lt;/b&gt; ★
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ ★ &lt;b&gt; Shiny &lt;/b&gt; ★ ★
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
★ ★ ★ &lt;b&gt; Flexdashboard-Shiny &lt;/b&gt; ★ ★ ★
&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pablobernabeu/data-is-present/master/dashboard%20gif.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;logistics&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Logistics&lt;/h4&gt;
&lt;p&gt;Memory capacity can become an issue as you go on, which will be flagged in the error logs as: ‘Shiny cannot use on-disk bookmarking’. This doesn’t necessarily lead you to a paid subscription or to &lt;a href=&#34;https://www.r-bloggers.com/alternative-approaches-to-scaling-shiny-with-rstudio-shiny-server-shinyproxy-or-custom-architecture/&#34;&gt;host the website on a custom server&lt;/a&gt;. Try pruning the data file, outsourcing data sections across the five available apps.&lt;/p&gt;
&lt;p&gt;App providers have specific terms of use. To begin, Shiny has a free starter license with limited use, where free apps can handle a certain amount of data, and up to five apps may be created. Beyond that, RStudio offers a wide range of &lt;a href=&#34;http://www.shinyapps.io/#_pricing&#34;&gt;subscriptions&lt;/a&gt; starting at $9/month. For its part, Tableau in principle deals only with &lt;a href=&#34;https://www.tableau.com/pricing&#34;&gt;subscriptions&lt;/a&gt; from $35/month on. While they offer 1-year licenses to students and instructors for free, these don’t include web hosting, unlike Shiny’s free plan. &lt;a href=&#34;https://www.linkedin.com/pulse/r-shiny-v-tableau-dawn-graphics-anand-gupta?lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3BCDbB2MVuQA6l%2BRNxwqWzQg%3D%3D&#34;&gt;Further comparisons&lt;/a&gt; of these platforms are available online. Last, I’ll just mention a third language, &lt;a href=&#34;https://d3js.org/&#34;&gt;D3&lt;/a&gt;, which is powerful, and may also be used &lt;a href=&#34;https://rstudio.github.io/r2d3/&#34;&gt;through R&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the case of very heavy data or frequent public use, if you don’t want to host your Shiny app externally, you might consider rendering a PDF with your visualisations instead.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
pdf(&amp;quot;List of plots per page&amp;quot;, width=13, height=5)
print(plot1)
print(plot2)
# ...
print(plot150)
dev.off()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;High-resolution plots can be rendered into a PDF document in a snap. Conveniently, all text is indexed, so it can be searched (Ctrl+f / Cmd+f / 🔍) (&lt;a href=&#34;https://osf.io/2tpxn/&#34;&gt;see example&lt;/a&gt;). Furthermore, you may also &lt;a href=&#34;http://www.ilovepdf.com/&#34;&gt;merge the rendered PDF&lt;/a&gt; with any other documents.&lt;/p&gt;
&lt;div class=&#34;document-viewer-container&#34; style=&#34;height: 80vh; min-height: 400px;&#34;&gt;
&lt;iframe src=&#34;https://cdn.jsdelivr.net/gh/pablobernabeu/Modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing@master/ERPs/Plots/List%20plots%20per%20participant%20and%20brain%20area.pdf&#34; width=&#34;100%&#34; height=&#34;100%&#34; style=&#34;border: none&#34; title=&#34;Document Viewer&#34; loading=&#34;lazy&#34;&gt;
&lt;p&gt;
Your browser does not support embedded PDFs. &lt;a href=&#34;https://cdn.jsdelivr.net/gh/pablobernabeu/Modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing@master/ERPs/Plots/List%20plots%20per%20participant%20and%20brain%20area.pdf&#34; target=&#34;_blank&#34;&gt;Download the PDF&lt;/a&gt; instead.
&lt;/p&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-in-slides&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Summary in &lt;a href=&#34;https://www.slideshare.net/PabloBernabeu/presenting-data-interactively-online-using-r-shiny-126064157&#34;&gt;slides&lt;/a&gt;&lt;/h3&gt;
&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/mDJ6IF1RGTiAR8&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;div style=&#34;margin-bottom:5px&#34;&gt;
&lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/PabloBernabeu/presenting-data-interactively-online-using-r-shiny-126064157&#34; title=&#34;Presenting data interactively online using R Shiny&#34; target=&#34;_blank&#34;&gt;Presenting data interactively online using R Shiny&lt;/a&gt; &lt;/strong&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>EEG error: datasets missing channels</title>
      <link>https://pablobernabeu.github.io/2016/eeg-error-datasets-missing-channels/</link>
      <pubDate>Tue, 16 Aug 2016 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2016/eeg-error-datasets-missing-channels/</guid>
      <description>


</description>
    </item>
    
    <item>
      <title>Modality exclusivity norms for 747 properties and concepts in Dutch: A replication of English</title>
      <link>https://pablobernabeu.github.io/2016/modality-exclusivity-norms-for-747-properties-and-concepts-in-dutch-a-replication-of-english/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2016/modality-exclusivity-norms-for-747-properties-and-concepts-in-dutch-a-replication-of-english/</guid>
      <description>


</description>
    </item>
    
    <item>
      <title>Conceptual modality switch effect measured at first word?</title>
      <link>https://pablobernabeu.github.io/2015/conceptual-modality-switch-effect-measured-at-first-word/</link>
      <pubDate>Sat, 31 Oct 2015 00:00:00 +0000</pubDate>
      <guid>https://pablobernabeu.github.io/2015/conceptual-modality-switch-effect-measured-at-first-word/</guid>
      <description>


</description>
    </item>
    
  </channel>
</rss>
