
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Pablo Bernabeu</title>
    <link>https://pablobernabeu.github.io/post/</link>
    <description>Recent content in Posts on Pablo Bernabeu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <copyright>&amp;copy; Pablo Bernabeu, {year} — [CC BY Attribution licence](https://creativecommons.org/licenses/by/4.0/). Email: pcbernabeu@gmail.com. No visitor data collected by webmaster. Cookies only used by third-party systems such as [Disqus](https://help.disqus.com/en/articles/1717155-use-of-cookies).</copyright>
    <lastBuildDate>2021</lastBuildDate>
    
        <atom:link href="https://pablobernabeu.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Parallelizing simr::powercurve() in R</title>
      <link>https://pablobernabeu.github.io/2021/parallelizing-simr-powercurve/</link>
      <pubDate>2021</pubDate>
      
      <guid>https://pablobernabeu.github.io/2021/parallelizing-simr-powercurve/</guid>
      <description>The powercurve function from the simr package in R (Green &amp;amp; MacLeod, 2016) can incur very long running times when the method used for the calculation of p values is Kenward-Roger or Satterthwaite (see Luke, 2017). Here I suggest three ways for cutting down this time.
Where possible, use a high-performance (or high-end) computing cluster. This removes the need to use personal computers for these long jobs.
 In case you’re using the fixed() parameter of the powercurve function, and calculating the power for different effects, run these at the same time (‘in parallel’) on different machines, rather than one after another.</description>
      
            <category>power analysis</category>
      
    </item>
    
    <item>
      <title>Surviving discrimination and confronting it</title>
      <link>https://pablobernabeu.github.io/2021/surviving-discrimination-and-confronting-it/</link>
      <pubDate>2021</pubDate>
      
      <guid>https://pablobernabeu.github.io/2021/surviving-discrimination-and-confronting-it/</guid>
      <description>This blog post does not provide a complete overview of the atmosphere at university or in academia, where I think that fairness is far more common than discrimination. Let&#39;s keep up the good work and be mindful of the rest.  There is discrimination at European universities and in the corresponding academia. Minorities are too often oppressed and abused through the use of casual remarks, concerted attacks, unequal respect towards different groups, unfair hiring decisions and other negligent behaviours.</description>
      
    </item>
    
    <item>
      <title>Brief Clarifications, Open Questions: Commentary on Liu et al. (2018)</title>
      <link>https://pablobernabeu.github.io/2021/brief-clarifications-open-questions-commentary-on-liu-et-al-2018/</link>
      <pubDate>2021</pubDate>
      
      <guid>https://pablobernabeu.github.io/2021/brief-clarifications-open-questions-commentary-on-liu-et-al-2018/</guid>
      <description>Liu et al. (2018) present a study that implements the conceptual modality switch (CMS) paradigm, which has been used to investigate the modality-specific nature of conceptual representations (Pecher et al., 2003). Liu et al.&amp;lsquo;s experiment uses event-related potentials (ERPs; similarly, see Bernabeu et al., 2017; Collins et al., 2011; Hald et al., 2011, 2013). In the design of the switch conditions, the experiment implements a corpus analysis to distinguish between purely-embodied modality switches and switches that are more liable to linguistic bootstrapping (also see Bernabeu et al.</description>
      
            <category>conceptual modality switch</category>
      
            <category>conceptual processing</category>
      
            <category>cognition</category>
      
            <category>conceptual replication</category>
      
            <category>word recognition</category>
      
            <category>research methods</category>
      
            <category>event-related potentials</category>
      
            <category>experiment</category>
      
            <category>statistics</category>
      
            <category>bayesian</category>
      
            <category>frequentist</category>
      
            <category>bias</category>
      
            <category>methodology</category>
      
    </item>
    
    <item>
      <title>Collaboration while using R Markdown</title>
      <link>https://pablobernabeu.github.io/2020/collaboration-while-using-r-markdown/</link>
      <pubDate>2020</pubDate>
      
      <guid>https://pablobernabeu.github.io/2020/collaboration-while-using-r-markdown/</guid>
      <description>In a highly recommendable presentation available on Youtube, Michael Frank walks us through R Markdown. Below, I loosely summarise and partly elaborate on Frank&#39;s advice regarding collaboration among colleagues, some of whom may not be used to R Markdown (see relevant time point in Frank&#39;s presentation).
  The first way is using GitHub, which has a great version control system, and even allows the rendering of Markdown text, if the file is given the extension &amp;lsquo;.</description>
      
            <category>R</category>
      
            <category>R Markdown</category>
      
    </item>
    
    <item>
      <title>Notes about punctuation in formal writing</title>
      <link>https://pablobernabeu.github.io/2020/formal-punctuation/</link>
      <pubDate>2020</pubDate>
      
      <guid>https://pablobernabeu.github.io/2020/formal-punctuation/</guid>
      <description>When writing formal pieces, some pitfalls in the punctuation are easy to avoid once you know them. Punctuation marks such as the comma, the semi-colon, the colon and the period are useful for organising phrases and clauses, facilitating the reading, and disambiguating. However, these marks are also liable to underuse, as in the case of run-on sentences; misuse, as in the comma splice; and overuse, as it often happens with the Oxford comma.</description>
      
            <category>writing</category>
      
            <category>punctuation</category>
      
    </item>
    
    <item>
      <title>Stray meetings in Microsoft Teams</title>
      <link>https://pablobernabeu.github.io/2020/stray-meetings-in-microsoft-teams/</link>
      <pubDate>2020</pubDate>
      
      <guid>https://pablobernabeu.github.io/2020/stray-meetings-in-microsoft-teams/</guid>
      <description>Unwanted, stranded meetings, overlapping with a general one in a channel, can occur when people click on the &lt;kbd&gt;Meet (now)&lt;/kbd&gt;/:camera: button, instead of clicking on the same &lt;kbd&gt;Join&lt;/kbd&gt; button in the chat field. This may especially happen to those who reach the channel first, or who cannot see the &lt;kbd&gt;Join&lt;/kbd&gt; button in the chat field because this field has been taken up by messages.</description>
      
            <category>communications</category>
      
    </item>
    
    <item>
      <title>R Markdown amidst Madison parks</title>
      <link>https://pablobernabeu.github.io/2020/r-markdown-amidst-madison-parks/</link>
      <pubDate>2020</pubDate>
      
      <guid>https://pablobernabeu.github.io/2020/r-markdown-amidst-madison-parks/</guid>
      <description>This document is part of teaching materials created for the workshop &amp;lsquo;Open data and reproducibility v2.1: R Markdown, dashboards and Binder&amp;rsquo;, delivered at the CarpentryCon 2020 conference. The purpose of this specific document is to practise R Markdown, including basic features such as Markdown markup and code chunks, along with more special features such as cross-references for figures, tables, code chunks, etc. Since this conference was originally going to take place in Madison, let&#39;s look at some open data from the City of Madison.</description>
      
            <category>R</category>
      
            <category>R Markdown</category>
      
            <category>CarpentryCon</category>
      
            <category>workshop</category>
      
            <category>RStudio</category>
      
            <category>bookdown</category>
      
            <category>cross-references</category>
      
    </item>
    
    <item>
      <title>What&#39;s in a fluke? The problem of trust and distrust</title>
      <link>https://pablobernabeu.github.io/2020/whats-in-a-fluke/</link>
      <pubDate>2020</pubDate>
      
      <guid>https://pablobernabeu.github.io/2020/whats-in-a-fluke/</guid>
      <description>The label &amp;lsquo;fluke&amp;rsquo; may in principle be skewed by the eye of the beholder, the mind of the perceiver and the availability or lack of data.</description>
      
            <category>fluke</category>
      
            <category>false positive</category>
      
            <category>false negative</category>
      
            <category>perception</category>
      
            <category>bias</category>
      
            <category>discrimination</category>
      
            <category>trust</category>
      
            <category>distrust</category>
      
    </item>
    
    <item>
      <title>How to engage Research Group Leaders in sustainable software practices</title>
      <link>https://pablobernabeu.github.io/2020/how-to-engage-research-group-leaders-in-sustainable-software-practices/</link>
      <pubDate>2020</pubDate>
      
      <guid>https://pablobernabeu.github.io/2020/how-to-engage-research-group-leaders-in-sustainable-software-practices/</guid>
      <description>  </description>
      
            <category>research</category>
      
            <category>sustainable practices</category>
      
    </item>
    
    <item>
      <title>Incentives for good research software practices</title>
      <link>https://pablobernabeu.github.io/2020/incentives-for-good-research-software-practices/</link>
      <pubDate>2020</pubDate>
      
      <guid>https://pablobernabeu.github.io/2020/incentives-for-good-research-software-practices/</guid>
      <description>  </description>
      
            <category>research software</category>
      
            <category>best practices</category>
      
    </item>
    
    <item>
      <title>Data is present: Workshops and datathons</title>
      <link>https://pablobernabeu.github.io/2020/data-is-present-workshops-and-datathons/</link>
      <pubDate>2020</pubDate>
      
      <guid>https://pablobernabeu.github.io/2020/data-is-present-workshops-and-datathons/</guid>
      <description>This project offers free activities to learn and practise reproducible data presentation. Pablo Bernabeu organises these events in the context of a Software Sustainability Institute Fellowship. Programming languages such as R and Python offer free, powerful resources for data processing, visualisation and analysis. Experience in these programs is highly valued in data-intensive disciplines. Original data has become a public good in many research fields thanks to cultural and technological advances. On the internet, we can find innumerable data sets from sources such as scientific journals and repositories (e.g., OSF), local and national governments, non-governmental organisations (e.g., data.world), etc. Activities comprise free workshops and datathons.</description>
      
            <category>programming</category>
      
            <category>education</category>
      
            <category>workshop</category>
      
            <category>datathon</category>
      
            <category>data presentation</category>
      
            <category>dashboard</category>
      
            <category>reproducibility</category>
      
            <category>open science</category>
      
            <category>open data</category>
      
            <category>statistics</category>
      
            <category>R</category>
      
            <category>R Shiny</category>
      
            <category>Flexdashboard</category>
      
            <category>HTML</category>
      
            <category>CSS</category>
      
            <category>Software Sustainability Institute Fellowship</category>
      
            <category>N8 CIR</category>
      
    </item>
    
    <item>
      <title>Event-related potentials: Why and how I used them</title>
      <link>https://pablobernabeu.github.io/2020/event-related-potentials-why-and-how-i-used-them/</link>
      <pubDate>2020</pubDate>
      
      <guid>https://pablobernabeu.github.io/2020/event-related-potentials-why-and-how-i-used-them/</guid>
      <description>Event-related potentials (ERPs) offer a unique insight in the study of human cognition. Let&#39;s look at their reason-to-be for the purposes of research, and how they are defined and processed. Most of this content is based on my master&#39;s thesis (download), which I could fortunately conduct at the Max Planck Institute for Psycholinguistics (conference paper also available).
Electroencephalography The brain produces electrical activity all the time, which can be measured via electrodes on the scalp—a method known as electroencephalography (EEG).</description>
      
            <category>event-related potentials</category>
      
            <category>electroencephalography</category>
      
            <category>electrodes</category>
      
            <category>preprocessing</category>
      
            <category>methodology</category>
      
            <category>cognitive neuroscience</category>
      
            <category>Brain Vision</category>
      
            <category>R</category>
      
            <category>visualisation</category>
      
            <category>statistics</category>
      
            <category>linear mixed-effects models</category>
      
            <category>Max Planck Institute for Psycholinguistics</category>
      
    </item>
    
    <item>
      <title>Naive principal component analysis in R</title>
      <link>https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/</link>
      <pubDate>2018</pubDate>
      
      <guid>https://pablobernabeu.github.io/2018/naive-principal-component-analysis-in-r/</guid>
      <description>Principal Component Analysis (PCA) is a technique used to find the core components that underlie different variables. It comes in very useful whenever doubts arise about the true origin of three or more variables. There are two main methods for performing a PCA: naive or less naive. In the naive method, you first check some conditions in your data which will determine the essentials of the analysis. In the less-naive method, you set those yourself based on whatever prior information or purposes you had. The &amp;lsquo;naive&amp;rsquo; approach is characterized by a first stage that checks whether the PCA should actually be performed with your current variables, or if some should be removed. The variables that are accepted are taken to a second stage which identifies the number of principal components that seem to underlie your set of variables.</description>
      
            <category>principal component analysis</category>
      
            <category>statistics</category>
      
            <category>dimensionality reduction</category>
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Review of the Landscape Model of reading: composition, dynamics and application</title>
      <link>https://pablobernabeu.github.io/2018/review-of-the-landscape-model-of-reading-composition-dynamics-and-application/</link>
      <pubDate>2018</pubDate>
      
      <guid>https://pablobernabeu.github.io/2018/review-of-the-landscape-model-of-reading-composition-dynamics-and-application/</guid>
      <description>Throughout the 1990s, two opposing theories were used to explain how people understand texts, later bridged by the Landscape Model of reading (van den Broek, Young, Tzeng, &amp;amp; Linderholm, 1999). A review is offered below, including a schematic representation of the Landscape Model.
Memory-based view The memory-based view presented reading as an autonomous, unconscious, effortless process. Readers were purported to achieve an understanding of a text as a whole by combining the concepts, and implications readily afforded, in the text with their own background knowledge (Myers &amp;amp; O’Brien, 1998; O’Brien &amp;amp; Myers, 1999).</description>
      
            <category>reading</category>
      
            <category>psycholinguistics</category>
      
            <category>cognition</category>
      
    </item>
    
    <item>
      <title>At Greg, 8 am</title>
      <link>https://pablobernabeu.github.io/2017/at-greg-8-am/</link>
      <pubDate>2017</pubDate>
      
      <guid>https://pablobernabeu.github.io/2017/at-greg-8-am/</guid>
      <description>The single dependent variable, RT, was accompanied by other variables which could be analyzed as independent variables. These included Group, Trial Number, and a within-subjects Condition. What had to be done first off, in order to take the usual table? The trials!</description>
      
            <category>R</category>
      
            <category>statistics</category>
      
            <category>aggregate</category>
      
            <category>trials</category>
      
            <category>repeated measures</category>
      
            <category>assumption</category>
      
            <category>independence of observations</category>
      
            <category>variability</category>
      
            <category>standard error</category>
      
            <category>central assumption</category>
      
    </item>
    
    <item>
      <title>Modality switch effects emerge early and increase throughout conceptual processing: Evidence from ERPs</title>
      <link>https://pablobernabeu.github.io/2017/modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing-evidence-from-erps/</link>
      <pubDate>2017</pubDate>
      
      <guid>https://pablobernabeu.github.io/2017/modality-switch-effects-emerge-early-and-increase-throughout-conceptual-processing-evidence-from-erps/</guid>
      <description>Research has extensively investigated whether conceptual processing is modality-specific—that is, whether meaning is processed to a large extent on the basis of perceptual and motor affordances (Barsalou, 2016). This possibility challenges long-established theories. It suggests a strong link between physical experience and language which is not borne out of the paradigmatic arbitrariness of words (see Lockwood, Dingemanse, &amp;amp; Hagoort, 2016). Modality-specificity also clashes with models of language that have no link to sensory and motor systems (Barsalou, 2016).</description>
      
            <category>psycholinguistics</category>
      
            <category>conceptual processing</category>
      
            <category>experiment</category>
      
            <category>event-related potentials</category>
      
            <category>language comprehension</category>
      
            <category>open data</category>
      
            <category>cognition</category>
      
            <category>conceptual modality switch</category>
      
            <category>modality exclusivity norms</category>
      
            <category>reading</category>
      
            <category>statistics</category>
      
            <category>linear mixed-effects models</category>
      
            <category>ResearchGate</category>
      
    </item>
    
    <item>
      <title>The case for data dashboards: First steps in R Shiny</title>
      <link>https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/</link>
      <pubDate>2017</pubDate>
      
      <guid>https://pablobernabeu.github.io/2017/the-case-for-data-dashboards-first-steps-in-r-shiny/</guid>
      <description>Dashboards for data visualisation, such as R Shiny and Tableau, allow the interactive exploration of data by means of drop-down lists and checkboxes, with no coding required from the final users. These web applications run on internet browsers, allowing for three viewing modes, catered to both analysts and the public at large: (1) private viewing (useful during analysis), (2) selective sharing (used within work groups), and (3) internet publication. Among the available platforms, R Shiny and Tableau stand out due to being relatively accessible to new users. Apps serve a broad variety of purposes. In science and beyond, these apps allow us to go the extra mile in sharing data. Alongside files and code shared in repositories, we can present the data in a website, in the form of plots or tables. This facilitates the public exploration of each section of the data (groups, participants, trials&amp;hellip;) to anyone interested, and allows researchers to account for their proceeding in the analysis.</description>
      
            <category>data presentation</category>
      
            <category>dashboard</category>
      
            <category>reproducibility</category>
      
            <category>open science</category>
      
            <category>open data</category>
      
            <category>R</category>
      
            <category>R Shiny</category>
      
            <category>Flexdashboard</category>
      
    </item>
    
    <item>
      <title>Modality exclusivity norms for 747 properties and concepts in Dutch: a replication of English</title>
      <link>https://pablobernabeu.github.io/2016/modality-exclusivity-norms-for-747-properties-and-concepts-in-dutch-a-replication-of-english/</link>
      <pubDate>2016</pubDate>
      
      <guid>https://pablobernabeu.github.io/2016/modality-exclusivity-norms-for-747-properties-and-concepts-in-dutch-a-replication-of-english/</guid>
      <description>  </description>
      
            <category>research methods</category>
      
            <category>stimuli</category>
      
            <category>linguistic norms</category>
      
            <category>modality exclusivity norms</category>
      
            <category>conceptual modality switch</category>
      
            <category>dimensionality reduction</category>
      
            <category>conceptual replication</category>
      
            <category>Dutch</category>
      
    </item>
    
    <item>
      <title>The Louisiana-Minnesota-Texas crisis across media and time: A big data exercise</title>
      <link>https://pablobernabeu.github.io/2016/the-louisiana-minnesota-texas-crisis-across-media-and-time-a-big-data-exercise/</link>
      <pubDate>2016</pubDate>
      
      <guid>https://pablobernabeu.github.io/2016/the-louisiana-minnesota-texas-crisis-across-media-and-time-a-big-data-exercise/</guid>
      <description>Racism has long been ingrained in human societies. Ancient Greek Aristotle already claimed that non-Greeks were slaves by nature, as they easily submitted to despotic government (Reilly, Kaufman, &amp;amp; Bodino, 2002). This study focuses on racism in the United States, which extends from the foundation of the country, when black people were generally born into slavery, and were at any rate regarded as an inferior people. US racism stands out globally for two reasons. First, the country has played a hegemonic part in the World since soon after its foundation. Second, the US is regarded as the most advanced society technology-wise, as it sets the minutes for the technology sector worldwide. In spite of these advantages, the country has long suffered the plague of widespread racism. Indeed, the abolition of slavery in the mid-nineteenth century did not grant equal citizen rights to the black population. Over time, the black population started to confront this situation. Especially the mid-nineteenth century saw large uprisings and a patent division of different societal sectors, as reflected in literary works such as Ellison’s &amp;lsquo;Invisible Man&amp;rsquo; (1952). Inequality and confrontation about racism has extended to date, and the costs thereof have been large in terms of lives and otherwise (Feagin, 2004).</description>
      
            <category>big data</category>
      
            <category>data mining</category>
      
            <category>racism</category>
      
            <category>web scraping</category>
      
            <category>R</category>
      
            <category>API</category>
      
            <category>social media</category>
      
            <category>news media</category>
      
    </item>
    
  </channel>
</rss>
